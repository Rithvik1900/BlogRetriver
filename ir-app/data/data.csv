,url,title,content
0,https://medium.com/towards-data-science/becoming-a-real-data-analyst-dcaf5f48bc34,Becoming a ‚Äúreal‚Äù data analyst,"Previously, I introduced you to a few analytics tasks disguised as everyday activities to prove that you‚Äôre already a data analyst. For example, consider the image below. Digital photos are stored as a bunch of numbers (left) that make no sense to your brain until you open them with suitable tools (right).
Ta-da! You‚Äôve just done data visualization. The music swells as you discover that the power of data analysis was inside you all along.
But does this mean you‚Äôre ready to work as a professional analyst?
Not quite. There are some big differences between an amateur and a professional analyst.
Unlike most amateurs, the pro knows how to use software (e.g. Python and R) that allows them to interact with more data formats all in one place. While MS Paint only works for images, analytics software can handle images and tables and sounds and text and and and‚Ä¶ and the kitchen sink.
Here‚Äôs what it looks like when you open that same image with Python:
And here‚Äôs the same image viewed with R:
The second difference is that a pro can work with obscene amounts of data. Even though I‚Äôve been playing with data more than two decades, I still prefer to open a single photo in my browser or even MS Paint rather than in R or Python. So, besides the flexibility of being able to open lots of different data types, what‚Äôs the selling point for learning the analytics pro tools? Well, what if you want to make sense of a million photos?
You *could* try to use MS Paint to make sense of them all, but at the speed of 1 second per image, it‚Äôll take you more than a month of full time work. A pro can do it in minutes with the right tools by using code to process and summarize vast amounts of data.
How do you start learning these tools? You look up how to install them (R and Python are free) and start playing with them. Just like MS Paint, but better. Simply do a Google search for whatever task you‚Äôre trying to achieve with them and read the results.
Here‚Äôs the first result that comes up in response to the search query above:
Boom. That‚Äôs all you need.
Well, if you‚Äôve never used R before, your next search will need to be ‚ÄúHow do I install a package in R?‚Äù but after that, you‚Äôre golden. Just copy-paste the code in the answer, replacing ‚Äúmy image‚Äù with the filename and filepath for your photo. Not sure what those terms mean? Do a search to look them up. When you‚Äôve run out of things you have to look up, you will have mastered the task you set out to learn. Looking stuff up is how developers develop (pun intended).
Do a whole bunch of this and one day you‚Äôll wake up to the realization that you‚Äôve accidentally developed pro software skills.
One reason I love programming is that it‚Äôs a cross between magic spells and LEGO. To learn the abracadabra that gets your task done, you look it up on the internet‚Ä¶ which is itself data analytics!
One reason I love programming is that it‚Äôs a cross between magic spells and LEGO.
Seriously, you don‚Äôt need a course. Simply challenge yourself to look at as many new data formats as you can in R or Python (they‚Äôre both good), and, along the way, keep asking the internet how to overcome any hurdles that come up. After you open the data (here‚Äôs how to find data to look at), come up with a question that strikes your fancy and try to use the tool to get an answer. Start small and get more ambitious as you go along. There‚Äôs nothing stopping you! Have fun!
In my opinion, learning the tools is the easy part. The hard part is adopting the analytics mindset, which is what the next differences are all about. Starting with this one: the expert has developed an all-encompassing disrespect for data. Yes, you heard me.
Only a newbie pronounces ‚Äúdata‚Äù with a capital ‚ÄúD‚Äù and treats it as something magical. Professionals have been burned and had their hearts broken enough times to learn the hard way that data is just some stuff that humans decided to write down in electronic form. (More here.)
The advantage of data is memory, not quality.
Sprinkling some numbers into a story to make make it more ‚Äúsciency‚Äù might win the trust of amateurs, but seasoned analysts know better. They are immune to what I call data science bias ‚Äî trusting information more when it smells of the data sciences. Adding a pretty graph to a nonsense report doesn‚Äôt fool them.
Experts understand that the advantage of data is memory, not quality, so they‚Äôre as skeptical of formal datasets as they are of the sights and sounds they take in by strolling down the street.
‚ÄúWith data, you‚Äôre still just another person with an opinion.‚Äô‚Äô
One of my favorite pioneers of statistics, W. Edwards Deming, famously said that ‚Äúwithout data, you‚Äôre just another person with an opinion.‚Äô‚Äô That is true, but unfortunately so is this: ‚ÄúWith data, you‚Äôre still just another person with an opinion.‚Äô‚Äô Expert analysts understand this in their very bones.
To start building the same immunity, stop treating data as special. You‚Äôve already (hopefully*) learned how to be sensible and skeptical with photos. For example, you know better than to take anything you see on Instagram as a true unaltered, unbiased representation of reality. If you didn‚Äôt take the photo, you won‚Äôt trust the photo. Right? Right.
Stop treating data as special!
All the common sense rules you‚Äôve learned for navigating the sights and sounds you‚Äôre exposed to in the wild also apply to structured data (numbers in a table/matrix/spreadsheet).
Equating data with truth is the same thing as believing everything that‚Äôs written in a book without knowing anything about the author. If you keep your wits about you and maintain a healthy skepticism, you‚Äôre well on your way to good analytics.
*There are some darling people who seem to have reached adulthood without learning that not everything you find online is true. If that‚Äôs you, may I gently suggest that analytics might not be the best career choice for you?
In addition to more practice with professional tools, the professional analyst understands the, ahem, professional aspects of the profession, which we‚Äôll cover in the next article in this series. For a sneak preview, here are the upcoming section headings:
Data pro vs amateur difference #4 ‚Äî Understanding the careerData pro vs amateur difference #5‚ÄîRefusing to be a data charlatanData pro vs amateur difference #6 ‚Äî Resistance to confirmation biasData pro vs amateur difference #7‚ÄîRealistic expectations of dataData pro vs amateur difference #8‚ÄîKnowing how to add valueData pro vs amateur difference #9‚ÄîThinking differently about timeData pro vs amateur difference #10 ‚Äî Nuanced view of excellence
If you‚Äôve thought of any other differences that might not fall under these headings, let me know in the comments!
towardsdatascience.com
towardsdatascience.com
P.S. Have you ever tried hitting the clap button here on Medium more than once to see what happens? ‚ù§Ô∏è
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Carmem Stefanie
EternalCoders
Flynn Bailey
Anbbit
Christopher Tao
in
Towards Data Science
Ron Sielinski
in
Towards Data Science
Adam Sroka
in
Towards Data Science
Adam Whybrew ‚Äî Partner, BCG Gamma
in
GAMMA‚Ää‚Äî‚ÄäPart of BCG X
"
1,https://medium.com/towards-data-science/a-callable-float-fun-and-creativity-in-python-7a311ccd742d,A Callable Float? Fun and Creativity in Python,"Among the built-in data types in Python, we have a number of types representing numbers, the most important being int and float. As everything in Python, their instances are objects; and as objects, they have their own attributes and methods. For example, this is what instances of the float type offer:
As you see, float numbers offer many different methods to use. What they do not offer is a .__cal__() method, which means you cannot call them.
Have you ever pondered why we cannot call a floating-point number the way you can call a function? Look:
It does not work. And why does it not work?!
To be honest‚Ä¶ I‚Äôve never pondered why floats are not callable ‚Äî but it makes perfect sense to me. Why would they? What for? Do you see any particular reason for them to be? I don‚Äôt.
But this does not mean we will not implement such functionality ‚Äî we definitely will. And why would we want to do that? For three reasons: to learn Python, to be creative, and to learn being creative.
Learning to be creative is an important aspect of learning a programming language. One day, in a Python project, you will find yourself in a difficult situation, in which standard solutions do not work; you will have to think and be creative. Your creativity may help you figure out an innovative, atypical solution, one that will help you solve this strange or atypical problem.
Thus, in this article, we will implement a callable float type; this is definitely not a typical scenario. We will proceed in steps, and I will explain each step in detail. Try to be creative when reading on. Perhaps being so will help you come up with your own ideas as to how to improve the solution. If so, implement them, and please share them in the comments.
We will start by implementing the following simple approach: if a user calls a float, it returns the number rounded to three decimal digits. For the moment, let‚Äôs make it simple ‚Äî I will hardcode rounding to three decimal digits; We will change that later.
And that‚Äôs all we need! As you can see below, in normal use cases, an instance of Float behaves just like a regular float object:
Unlike floats, however, Floats are callable:
Voil√† ‚Äî a callable floating-point number.
No need to hardcode the number of decimal digits to be used in rounding, however. We can enable the user to decide how many decimal digits to use. In order to do so, enough to add a digits argument to the¬†.__call__() method:
Perfect! Note that this is exactly what we would get by rounding the corresponding float number:
We may want to return a Float object instead of a float:
Here, the only thing that Float numbers can do when being called is rounding. Boring! Let it be able to do anything.
To implement such a general concept, we need to make the .__call__() method a higher-order method, which here means that it takes a function (actually, a callable) as an argument. The below class implements this functionality:
Note that this time, we did not change the type of the returned value to Float, as the user may wish to use a func() function that returns an object of another type.
This is how this version of Float works:
It works as expected, but it definitely has a significant drawback: you cannot use additional arguments to the func() function. We can easily implement this functionality, thanks to the *args and **kwargs arguments, enabling the user to provide any arguments:
All works just fine and dandy. Consider the following notes:
We have implemented a class, Float, that enables a floating-point number to be called like a function. Calling the instance of a Float class with a function as an argument means calling this very function with the Float instance as an argument. You can use any additional arguments, both positional and keyword.
We did not do all that to actually use the callable Float class. I‚Äôve never implemented or used anything like that, and I do not expect to. But for me, this was fun, a lot of fun ‚Äî and quite a lesson of creativity in using Python. I hope you‚Äôve enjoyed this article, too. I think that doing such crazy stuff has a lot of value: it helps you learn Python, and it helps you learn the language‚Äôs intricacies that you wouldn‚Äôt learn any other way.
The Float class does not really matter. What does matter is that such brainstorming can help you stimulate creativity in your future projects, when you have to deal with atypical situations. Practicing such implementations as this one can help you both find and implement atypical solutions in your Python projects.
Thanks for reading. If you enjoyed this article, you may also enjoy other articles I wrote; you will see them here. And if you want to join Medium, please use my referral link below:
medium.com
Prajakt Karanjkar
in
Axiom
Rakesh Mali
Dhareshwarganesh
Cloud Evangelist
InterviewReady
Laurens De Lombaert
eXcelisys
Girija Viswanathan
"
2,https://medium.com/geekculture/struggling-to-land-a-data-science-job-try-these-virtual-internships-for-free-691933328dfe,Struggling to Land a Data Science Job? Try These Virtual Internships For Free,"‚ÄúStart the job before you have it.‚Äù
I wrote a blog post a year ago sharing my own lessons and experiences from applying‚Ä¶
Cambridge Spark
Drishti Jain
Acting Daily
Jurgis Samaitis
in
Towards Data Science
Sam Black
in
Towards Data Science
Rupali Sarwade
Navoneel Chakrabarty
in
Towards Data Science
Ajit Kumar Sahoo
in
Analytics Vidhya
"
3,https://medium.com/towards-data-science/how-id-learn-data-science-if-i-could-start-over-4-years-in-999f8144f897,How I‚Äôd Learn Data Science If I Could Start Over (4 Years In),"Two years ago, I wrote a similar article explaining how I‚Äôd learn data science if I could start over. Now that I‚Äôm four years into my career, which is double the amount of time, I‚Äôve realized that there is a much better approach to learning data science.
The problem with my previous guide is that it acts as a one-size-fits-all solution which simply isn‚Äôt the case. Because data science covers such a broad spectrum of skills and subjects, it‚Äôs only natural that particular skills matter a lot more for certain types of data scientists and a lot less for others.
And so, ‚ÄúHow I‚Äôd Learn Data Science if I could start over‚Äù really starts with the question, ‚Äúwhat aspects of data science am Interested in?‚Äù Is it statistical analysis? Is it deep learning? Is it building visualizations? Understanding this will help with prioritizing what skills to learn first. And if you‚Äôre unsure what aspects of data science you‚Äôre interested in, that‚Äôs completely okay because there are fundamental skills required by all types of data scientists that you can start with (as far I know).
Enjoying this article? Subscribe and become a member today to never miss another article on data science guides, tricks and tips, life lessons, and more!
Below is a simplified and generalized flowchart that I‚Äôd use to guide my learning if I had to learn data science all over again. I want to re-emphasize the simplicity of this flowchart in exchange for 100% completeness to make it as comprehensive as possible.
At a high level, the flowchart can be broken down into the following steps:
Let‚Äôs walk through each step in more detail‚Ä¶
Regardless of what area of expertise you want to specialize in, it‚Äôs inevitable that you‚Äôll have to know how to code in SQL and Python. And so, I recommend that you learn how to code as a starting point.
SQL is the universal language of data. Whether you‚Äôre a data scientist, a data analyst, a machine learning engineer, a data engineer, or a blend of any of these roles, you‚Äôre going to need to know SQL.
How I‚Äôd learn SQL is through a couple of resources in this order:
Python is important for data scientists, especially because there are so many packages and extensions of Python that are useful. R is equally as good of an alternative but doesn‚Äôt seem to be the main language that‚Äôs adopted in the data science world.Learning Python is a little less straightforward than SQL because I‚Äôve found that Python is better learned by ‚Äúdoing‚Äù, as in trying to build projects. That being said, here are a few resources that I found helpful in my career:
Once you learn the fundamentals, there are several subjects that you can specialize in. How I would determine what to focus on next first depends on whether I see myself as a Business-facing Data Scientist or a Research-facing Data Scientist.
A business-facing data scientist is focused on initiatives that directly impact the business and tends to work with business stakeholders directly, almost like a consultant. Projects and required skills revolve more around solving business problems directly, the lifecycle of projects is relatively shorter and the impact of one‚Äôs work is consistently seen.
A research-facing data scientist acts more like a researcher or a Ph.D. student. He or she will work on longer-term projects, like building intricate models or conducting complex research questions. The lifecycle of projects is relatively much longer and the work may or may not be used by the business depending on the cost-benefit tradeoff.
If you choose to pursue a role that has more of a direct impact on the business, then there are three sub-categories that I would dive deeper into experimentation & inference, analytics & insights, and visualizations.
Experimentation and Inference refer to a set of techniques that are used to determine the cause-and-effect relationship between two variables. This is extremely important for a business to understand the drivers of success and ultimately what allows businesses to learn, iterate, and improve.
Initial resources to learn the fundamentals are provided below:
Analytics refers to organizing and examining data, while insights refer to discovering information, like patterns and anomalies, in data. Data Scientists focused on analytics and insights are required to answer vague and generally tough questions using a set of analytical and statistical tools.
Initial resources to learn the fundamentals are provided below:
Data visualization is the graphical representation of information. Data scientists focused on visualizations are mainly focused on dashboarding, automated reporting, and developing visual insights.
Initial resources to learn the fundamentals are provided below:
On the other hand, if you‚Äôre more interested in diving into the intricacies of models, reading research papers to keep up with cutting-edge methods, and more interested in the deployment of models, then I recommend that you narrow in on a particular subject related to modeling. Some subjects include machine learning, deep learning, NLP, computer vision, network science, etc.
Saturn Cloud is a platform that allowed me to build computationally expensive models that I wouldn‚Äôt have been able to build locally. It‚Äôs a great solution if your specs are a bottleneck to your modeling.
Once you make it this far, it‚Äôs time to work on some data science projects and build your portfolio! Here‚Äôs a list of a couple of projects for inspiration if you don‚Äôt know where to start:
towardsdatascience.com
Some platforms that you can use to start building your own projects are below:
And with that, I wish you the best of luck in your endeavors!
If you enjoyed this, subscribe and become a member today to never miss another article on data science guides, tricks and tips, life lessons, and more!
Not sure what to read next? I‚Äôve picked another article for you:
towardsdatascience.com
or you can check out my Medium page:
terenceshin.medium.com
Andrew Smith
in
Analytics Vidhya
Rosaria Silipo
in
Low Code for Advanced Data Science
Iman Maulana
Oleg V.
Alexander Pavlov
Sara A. Metwalli
in
Towards Data Science
Wawira
Mostafizur Rahman
"
4,https://medium.com/towards-data-science/10-differences-between-amateurs-and-professional-analysts-3a1be1a06a4d,10 Differences Between Amateurs and Professional Analysts,"Analytics is like the writing profession: the basics are easy to get started with and it‚Äôs an art, so there are few barriers to entry and anyone can call themselves a ‚Äúwriter.‚Äù There‚Äôs no guarantee of quality in the title.
However, being literate enough to write some tweets hardly makes you Toni Morrison or Gabriel Garc√≠a M√°rquez‚Äî the best writers are light-years ahead of beginners. Their observations have power. Their insights change the world.
That‚Äôs how analytics is too. The variance in the profession is massive. On the other hand, barriers to entry for, say, statistics, are higher, so the minimum level statisticians are more impressive than minimum level analysts‚Ä¶ but there‚Äôs also a narrower range of virtuosity. I‚Äôm often far more impressed by expert analysts than by statisticians and ML engineers.
There are some big differences between an amateur and an expert professional analyst and in this article, I‚Äôll summarize 10 of them for you. For each one, I‚Äôve got a blog post that goes deeper, so feel free to follow the trail of links down whichever rabbit hole makes you curious.
Professional analysts push themselves to learn the most effective ‚Äî fastest and most flexible ‚Äî tools for the job, which means they rapidly transcend point-and-click analytics interfaces and learn to code. If they want to lay a claim to being a pro, they build scary-good skills in least two out of these three languages: R, Python, and SQL. Learn more.
How big is ‚Äúbig data‚Äù? That‚Äôs depends on the technology available to you ‚Äî what might have counted 10 years ago doesn‚Äôt count today. Data is ‚Äúbig‚Äù when you need extra engineering skills just to access it and move it around. It‚Äôs most certainly not ‚Äúbig‚Äù if you can load the whole dataset into your laptop‚Äôs memory.
Unlike a newbie, the professional analyst refuses to be daunted by dataset size. If extra engineering skills are what it takes to handle it, so be it. You‚Äôll learn whatever you must. As a result, expert analysts often pick up data engineering skills along the way. Learn more.
Another big difference between an amateur and an expert analyst is that the expert has developed an all-encompassing disrespect for data. They never pronounce data with a capital ‚ÄòD‚Äô.
‚ÄúWith data, you‚Äôre still just another person with an opinion.‚Äù
One of my favorite pioneers of statistics, W. Edwards Deming, famously said that ‚Äúwithout data, you‚Äôre just another person with an opinion.‚Äô‚Äô That is true, but unfortunately so is this: ‚ÄúWith data, you‚Äôre still just another person with an opinion.‚Äù Expert analysts understand this in their very bones.
Data can lull unsuspecting amateurs into a false sense of security, but experts know how to avoid quantitative gullibility. To start building the same immunity, stop treating data as special. Learn more.
Unlike amateurs, the professional analyst is an analyst by choice, not by misfortune. To them, analytics is a discipline of excellence in its own right, not a stepping stone to some other profession (like machine learning or statistics).
Professional analysts understand that it‚Äôs not your job title that makes you an analyst and it‚Äôs not the tools and technologies you use. What makes you an expert analyst is your expertise in exploration and inspiration‚Äîso that‚Äôs the best point of departure when pondering the nuances of the analytics career. Learn more.
Professional analysts refuse to be data charlatans: peddlers of toxic hindsight.
The first warning sign of a data charlatan is a failure to understand that analytics and statistics are very different disciplines. Whatever your official job title, there‚Äôs no rule that says you can‚Äôt learn both trades if you want to‚Ä¶ as long as you don‚Äôt get them confused. To avoid accidentally becoming a data charlatan, tackle these pursuits one at a time. Learn more.
Confirmation bias means we can all look at the same number and perceive it differently.
In other words, confirmation bias is the archnemesis of data-driven decision-making, since it means that a fact is no longer just a fact, no matter how much math and science you throw into getting it. It sucks all the value out of analyzing data.
If you‚Äôre keen to be a professional analyst, you need Jedi-level antidotes to confirmation bias. This means putting intentional practice into honing the best two skills for defending against confirmation bias: preregistration and open-mindedness in the face of apophenia. Learn how.
If you‚Äôre a professional analyst, you know that data doesn‚Äôt owe you anything. Not even decent quality for your troubles.
Bad data is a professional reality for analysts, in large part because their work involves spending more time with secondary data than with primary data. Analysts are painfully aware that you often need to start with bad data to figure out how to make better data. To understand why these distinctions matter and see a list of skills that help analysts cope with messy data, learn more here.
If you‚Äôre a decision-maker hybrid, your value is judged by the quality of your decisions and the actions you take. Your analytical skills are a means to that end ‚Äî analytics is just one of many weapons in your arsenal.
If you‚Äôre a pure analyst, your value is tied to your ability to serve your decision-maker. A decision-maker‚Äôs job is to turn information into better action, which is hopeless if they can‚Äôt access much information. That‚Äôs where the analyst comes in, supporting their decision-maker‚Äôs information needs both reactively (by looking up data that answers their specific questions) and proactively (by exploring data to inspire them to consider new directions, making them more innovative and threat-proof).
An analyst acts as a sort of sensory organ for their decision-maker.
Expert analysts know that starting anywhere except with your decision-maker‚Äôs needs and priorities is bound to lead you astray. Learn more.
There are few roles as laser-focused on the return-on-investment (ROI) of time.
A seasoned analyst understands that their work isn‚Äôt a sure thing with guaranteed results, but rather a time investment in inspiration, innovation, and early threat detection. That‚Äôs why they seek bosses who understand this too ‚Äî bosses who don‚Äôt punish them for coming back empty-handed.
If they‚Äôre working for a savvy organization, their team trusts them to make the best possible use of the time that‚Äôs allotted to them, so they‚Äôre constantly thinking in terms of the potential time ROI of their actions. To see how this guides the way expert analysts work, learn more here.
The analytics game is all about optimizing inspiration-per-minute.
Unlike amateurs, expert analysts don‚Äôt view speed as a dirty word but rather as a nuanced concept that guides how they think of their work, how they prioritize, how they assess performance, and how they develop their skills.
As a professional analyst, speed is also your north star for skills development. It‚Äôs true that the data landscape is changing rapidly, so you can‚Äôt afford to stagnate. The tools you use today won‚Äôt stick around for long. Keep sharpening your claws, but don‚Äôt chase the buzzwords.
Stop asking: ‚ÄúShould I learn this tool/method/technique that all the cool kids are talking about?‚ÄùStart asking: ‚ÄúWill learning this make me faster?‚Äù
Learn whatever makes you faster (in all the ways that matter). Since your work involves accelerating others, start by accelerating yourself. Learn more.
If all you‚Äôre after is a nudge to get started with analytics, go look at some numbers already. Don‚Äôt wait for special permission. You don‚Äôt need any fancy training. Skip the courses and the books and don‚Äôt worry if you‚Äôre not an expert yet‚Ä¶ just dive in! Feel free to call yourself an analyst as soon as looking at data starts feeling fun.
But if you‚Äôre keen to attain expert status, I have bad news for you. While I hope you‚Äôll explore my analytics minicourse, nothing I (or anyone else) could teach you will give you mastery. Reading about analytics can help you with mindset and attitude, but mastery takes talent and practice. Lots and lots and lots and LOTS of practice.
bit.ly
Head over to my mini course on analytics if you‚Äôre eager to supplement your practice sessions with some core concepts and musings on the analytics career.
Oh, and if you had fun here, see what happens if you click on the clap button more than once.
Adeline Ong
in
Towards Data Science
Tech in 3
Data Science Earth
in
Data Science Earth
Meeti Gandhi
Saikat Halder
Umesh Rindani
Kimoon Kim
in
Google Cloud - Community
Carly Chamberlain Sharma
"
5,https://medium.com/illumination/why-you-should-not-become-a-data-analyst-in-2023-5afe405bb824,Why You Should (not) Become a Data Analyst in 2023!,"RudderStack
in
Geek Culture
Natalia Kuraeva
Wallace Liu
Bill Franks
in
Analytics Matters
susan
Arif R
in
DataDrivenInvestor
RudderStack
in
CodeX
ReD Associates
"
6,https://medium.com/towards-data-science/20-resources-i-use-to-stay-up-to-date-with-data-science-dd665a16a473,20+ Resources I Use to Stay Up-to-Date with Data Science,"Things move VERY quickly in the world of data science. In my career alone (the past ~4 years), I‚Äôve had to learn several new, industry-leading tools like Prophet, dbt, and Dash Plotly.
Being a data scientist means that you have to continuously learn new skills and tools to broaden your arsenal of tools and also keep up with industry leading platforms.
In this article, I wanted to share with you all of the resources and hidden gems that I use to stay up-to-date with data science! With that said, let‚Äôs dive into it.
Be sure to SUBSCRIBE here to never miss another article on data science guides, tricks and tips, life lessons, and more!
There are six main ways that I keep up with data science, and all of them help me keep up with data science in different ways:
There are a few data science-specific platforms that I find really valuable, and were actually a huge contribution to my development.
Analytics Vidhya and Kaggle are particularly useful when I want a step-by-step guide on how to perform a data related task, whether it be building a time-series model or using DBSCAN for a clustering problem.
What‚Äôs nice about both is that they both have their own communities where you can ask questions and learn from other people‚Äôs code.
There‚Äôs a few ways that I use Data Science Platforms (Saturn Cloud, Analytics Vidhya, Kaggle) to learn:
saturncloud.io
www.analyticsvidhya.com
www.kaggle.com
Be sure to SUBSCRIBE here to never miss another article on data science guides, tricks and tips, life lessons, and more!
I‚Äôm not a huge fan of bootcamps, but there are a couple of websites that I find to be very strong, Vexpower and Coursera. These two are great if you want to step-by-step guides to learn technical topics. For example, Vexpower has a course that teaches you how to build a Marketing Mix Model step-by-step using Robyn.
I find these to be very helpful when I don‚Äôt need to learn an entire subject the way books do.
If you think of a spectrum, where textbooks cover broad subjects and Stack Overflow covers very specific questions, learning platforms lie somewhere in the middle.
When I have a foundation in a particular subject but I want to extend my knowledge or dive a little deeper on a topic, that‚Äôs when I like to use learning platforms. For example, when I was focused on Marketing Analytics, I used Vexpower to learn how to build a Marketing Mix Model using Robyn.
app.vexpower.com
www.coursera.org
While not always the easiest or the quickest to digest, I find that reading books is the best way to learn a new subject for a few reasons.
When I‚Äôm trying to learn a relatively new subject, like experimental design, and I have little foundation on the subject, I tend to look for textbooks. Similar to school courses, I feel that textbooks are a good way to learn the basics of a topic relatively quickly.
Whether it be experimental design, deep learning, network analysis, or machine learning, I personally recommend starting with textbooks if you‚Äôre net new to the subject.
www.amazon.com
www.amazon.com
www.amazon.com
One of the most underrated sources to learn new things is LinkedIn. This is where I learn a bulk of new information and resources. There are a ton of Data Science Influencers that take the time to share very insightful coding tips and curate extremely valuable resources.
In the article below, you can see my personal top data science influencers.
The beauty of this is that you can learn just from scrolling your LinkedIn home page after following several Data Science Influencers. You‚Äôll see a lot of curated content and knowledge sharing on a weekly, or even daily, basis.
terenceshin.medium.com
Be sure to SUBSCRIBE here to never miss another article on data science guides, tricks and tips, life lessons, and more!
YouTube is great if you‚Äôre an audio or visual learner. I like YouTube because I can learn about small concepts very quickly, and I can also learn entire subjects.
What I personally love about YouTube is that there‚Äôs a lot of free lectures from some of the best universities that you can learn from (MIT, Stanford, Harvard, etc‚Ä¶), which is a great way to learn an entire subject fairly quickly.
What I love about conferences is that it gives you the opportunity to listen to some of the most pronounced data leaders in the world. And typically, conferences are where companies and leaders share novel ideas and new practices related to data science and AI.
Below are some of the most popular annual data science conferences:
www.nvidia.com
datainnovationsummit.com
odsc.com
Be sure to SUBSCRIBE here to never miss another article on data science guides, tricks and tips, life lessons, and more!
Not sure what to read next? I‚Äôve picked another article for you:
towardsdatascience.com
and another one:
towardsdatascience.com
Terence Shin
in
Towards Data Science
Guillermo Barreiro
in
Towards Data Science
Ng Wai Foong
in
Towards Data Science
John Rosenberg
MARTIN YU
in
Analytics Vidhya
Terence Shin
in
Towards Data Science
Terence Shin
in
Towards Data Science
BAYU GALIH PRIANDA
"
7,https://medium.com/@ritabatalha/top-trends-for-web3-in-2023-e9fd6e0fbc30,Top Trends for Web3 in 2023,"Let‚Äôs talk about the new version of the internet
The term ‚ÄúWeb 3.0 or Web3‚Äù was coined in 2014 by Ethereum co-founder Gavin Wood, and the idea gained interest in 2021 from cryptocurrency enthusiasts and large tech companies.
Web3 is still evolving and being defined, as such, there is not an established and universally accepted definition. Yet, Packy McCormick, an investor who helped disseminate Web3, has defined it as ‚Äúthe internet owned by the builders and users, orchestrated with tokens.‚Äù
Alperen Ugurlu
International Society of Automation - ISA Official
Yegor Shytikov
Tanner Jones
in
Nerd For Tech
CoinTiger
in
CoinTiger
Akinbodunse Toluwalope
MRunal
J. Kelsey
in
FreeDAO
"
8,https://medium.com/towards-data-science/2022-world-cup-simulator-using-data-science-with-python-6bf56a543436,"2022 World Cup Simulator Using Data Science, with Python","Disclaimer 1: Please don‚Äôt use this for betting purposes. This is just a mathematical experiment made by a data scientist. I am not a tipper and I don‚Äôt want to give you any suggestions on your bets.
Disclaimer 2: This article is not related in any way to all the Qatar events that are happening these days and it is not a sponsor or anti-sponsor of the FIFA World Cup 2022 in Qatar
So, after we won the European Cup in 2020, Italy is out of the World Cup.This is a huge letdown for me as an Italian. Our nation is one that eats, sleeps, talks, and plays football every day.I remember being a young child the last time Italy won the World Cup. I wasn't even ten when I watched the game with my family in Rome on a small television at my uncle's house.
I am 25 years old now, and live in the US, but that memory is permanently inked in my soul. With all the controversy that this Qatar World Cup is bringing us, I still believe that the World Cup itself is a magical moment.
The thing is that today I am a researcher too, and I use data science every day of my life. I spent the last 4 years of my life doing data science and machine learning at the university, and I am completely in love with it.
So what I decided to do is build a data science algorithm (I wouldn‚Äôt properly call it machine learning, but you will be the judge) to predict the winner and the whole tournament of the next World Cup edition.
So let me tell you, in a few words, what you‚Äôll see in this article:
Let‚Äôs do it then!
Let‚Äôs import some libraries first:
The dataset that I used is a dataset of the matches and the number of time a national team has won or has passed the group stage, as we will see.
Now let me do some explorations. Let‚Äôs start with the obvious and look at how many times each team has won the World Cup before:
So we know that already, right? Brazil won 5, Italy won 4, and Argentina won 2 (Diego).
Another cool thing we can explore is related to the fact that, in the matches dataset, we have the stage of the tournament as well. It is interesting to explore it because the number of times that a country passed the group stage is obviously indicative of the quality of that team.
Let‚Äôs give it a look:
So we have a lot of group stage matches but only a few elimination stages. That makes complete sense because, of course, you have multiple groups but only one eliminatory phase.
Now that the boring part is done, let‚Äôs get into the details of how we are going to simulate this World Cup.
We have our teams and their matches. Let‚Äôs say that you have Team 1 (T_1) and Team 2 (T_2). What is the probability that Team 1 will win the match?
It is the same for T_2 and the draw.
Now, about the draw: of course, two teams can draw in the group stage, but they cannot draw in the elimination stage. I will tell you how to deal with it later. üòâ
Similarly to what I said before, we can find the probability of T_1 winning in general like this:
It is, of course, the same for drawing or losing. To do that, we need the following column for the matches, which we called ""Result"":
Now we have to select the team for the Qatar World Cup. It is not a great deal of work, and we can just copy and paste it from the website. I did it for you.
Now we have to match the data that we have with the team list. We have to make sure that we have all the teams that are going to be in Qatar in the dataset.
So Qatar is, ironically, the only team we don‚Äôt have in our dataset. We'll deal with this later. Let‚Äôs add a qatar_probabilities dictionary for now, which is a kind of made-up realistic probability of how many games Qatar can actually win in the World Cup.
Now if we want to do the probability conversion, let‚Äôs use the following function:
And let‚Äôs plot some random results:
Now, this is all cool, but we need to do the probability for the matches, not for the single teams only. We are applying the rule above in the following way:
Now, this probability only works if the two matches have a history of matches. This, for example, works in France and Italy:
But what are we going to do in this case?
Now. This is a real question. I don‚Äôt have the answer because it is hard to automatically predict the outcome of two teams that have never played each other. What we can do is build a score for each time and compare the two scores to convert them into a probability of Team 1 winning, Team 2 winning, or the two teams drawing.
I think that a good score for a team is the number of (normalized) times that the team has passed the group stage. This is, of course, a good point that indicates the quality of a team. We will call it a score of 1. Another important thing to consider is the number of times that a team has actually won the World Cup. We will call it a score of 2. As score_1 is between 0 and 1 and score_2 can be 2, 3, or 4, let‚Äôs multiply this by 0.5. So given a team T, we will have:
So if we have two teams, T_x1 and T_x2, and we define the probability of T_x1 winning as P(T_x1, T_x2), we can say that:
And:
But this definition of probability doesn‚Äôt allow us to draw. So we will rather say that the probability of drawing between two teams that have never played a match is fixed, let‚Äôs say at 0.10. Then we will say that:
And the probability of them drawing is:
Let‚Äôs implement this idea in the following way:
1. Function to get the score of a team
2. Function to convert the scores of two teams into a probability
*As we can see, the score of Qatar is 0.09. I got that from the first .25 quartile of all the score 1 distribution. This is a reasonably (low) value for a team that never played in the World Cup.
3. Function to extract the probabilities of the results between two teams in a match*
For example:
*I also made sure that it is not possible that a team has a 100% of probability of winning against another by adding some (small) probability in losing and drawing
Now it is time to make our model run. We have to build the exact groups for the Qatar World Cup. We will make this group run, and a team will win, draw, or lose based on the probability we have built.
We will also build the algorithmic elimination stage (first team of Group A vs. the second team of Group B, first team of Group B vs. the second team of Group A).
We will run this algorithm multiple times, and we will pick the winner.
Let me do this step by step. Let‚Äôs define the groups:
And group them together:
This, for example, is the probability for group B:
Based on these probabilities, we can run the simulation in the following way: This is for a given group:
This is for all the groups:
Let‚Äôs run the simulations for the groups:
Let‚Äôs show some results:
These are all:
I don‚Äôt know about you, but I think these results are pretty realistic.
Let‚Äôs move on!
From the groups, this is the eliminatory stage:
And this is the whole tournament simulation:
This is an example of the whole tournament simulation:
And we can run this for 10,000 times:
(it takes a while)
The following are the winner's statistics:
Some of them are surprising (Senegal, Costa Rica, Ecuador‚Ä¶, Brazil and Argentina not included), some of them are pretty reliable (Spain, Germany, England)
We can slightly alter the function to determine the winner and runner-up.
This is actually pretty exciting, and we can conclude a lot of things. The Netherlands, for example, plays a lot in the finals, as does England. Nonetheless, Spain has far more finals victories than either team. We can go on and keep analyzing. Iran, for example, reached the final (at least once), but never won.
In this article, we simulated the next World Cup winner. We actually simulated the whole tournament, and we extracted the winner and the second team.
We did that by doing the following:
There is a lot of room for improvement. For example, we can consider the actual quality of the teams, considering their players, their coach, and their form. In the meantime, let‚Äôs enjoy some good football. ü•∞
If you liked the article and you want to know more about machine learning, or you just want to ask me something, you can:
A. Follow me on Linkedin, where I publish all my storiesB. Subscribe to my newsletter. It will keep you updated about new stories and give you the chance to text me to receive all the corrections or doubts you may have.C. Become a referred member, so you won‚Äôt have any ‚Äúmaximum number of stories for the month‚Äù and you can read whatever I (and thousands of other Machine Learning and Data Science top writers) write about the newest technology available.
Piero Paialunga
in
Towards Data Science
Vardhini Manivannan
ENJOY AGUSTINTXO LIFE STYLE & PEOPLE POWER
Rutvik Deshpande
in
Dev Genius
Anastasia
Andrey Koleda
Shipra Saxena
in
Analytics Vidhya
Thomas Khau
"
9,https://medium.com/towards-data-science/5-python-libraries-to-learn-to-start-your-data-science-career-2cd24a223431,5 Python Libraries to Learn to Start Your Data Science Career,"If you want to study Python for Data Science to start a new career, I‚Äôm sure you are struggling with all these things to know and master. I know you are overwhelmed by all these new concepts, including all the mathematics you should know, and you may feel you‚Äôll never arrive at the goal of your new job.
I know: job descriptions do not help with that. It really seems like Data Scientists must be aliens; even juniors, sometimes.
In my opinion, an important skill to master is learning how to stop the fear of ‚ÄúI have to know everything‚Äù. Believe me: especially at the beginning, if you are pursuing a junior position, you absolutely do not have to know everything. Well, telling the truth: even seniors do not really know everything.
So, If you want to start a career in Data Science, in this article I show you five Python libraries you absolutely have to know.
As we can see on their website, Anaconda is:
The world‚Äôs most popular open-source Python distribution platform
Anaconda is a Python distribution specifically created for Data Science; so it is not properly a library, but we can intend it as a library because, in software development, a library is a collection of related modules; so, since Anaconda provides all the must-haves for Data Scientists ‚Äî included the most used packages ‚Äî we can intend it as a library and, also, is a must-have for you.
The first important thing provided by Anaconda is Jupyter Notebook which is:
the original web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience.
Jupyter Notebook is a web application that runs locally on your machine and it is created on purpose for Data Scientists. The main important characteristic that makes it attractive (and very useful) for Data Scientists is the fact that every cell runs independently giving us the possibility to:
To get started with Jupiter Notebooks, I advise you to read this guide here.
Then, when you gain experience, you may need some shortcuts to speed up your experience. You can use this guide here.
Also, as said before, Anaconda provides us with all the packages needed for Data Science. This way we don‚Äôt have to install them. For example, say you need ‚Äúpandas‚Äù; without Anaconda, you need to install it by typing $ pip install pandas in your terminal. With Anaconda you don‚Äôt have to do that because it installs pandas for us. A very good advantage!
Pandas is a library that makes you import, manipulate and analyze data. On their website, they say that
pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.
If you want to work with data you absolutely need to master Pandas because, nowadays, is widely used by Data Scientists and Analysts.
The power of Pandas relies on the fact that this library makes us work with tabular data. In statistics, tabular data refers to data that is organized in a table with rows and columns. We typically refer to tabular data as data frames.
This is important because we work with tabular data in a lot of situations; for example:
The reality of many firms is that, regardless of your role, you‚Äôll always have to deal, somehow, with data in excel/CSV and/or in databases; this is why Pandas is a fundamental resource for you to master.
Also, consider that you can even access data from databases and get them directly into your Jupyter Notebooks for further analysis in Pandas. We can do so using a library called PyOdbc. Take a look at that here.
After data manipulation and analysis with Pandas, you typically want to make some plots. This can be done with matplotlib which is:
a comprehensive library for creating static, animated, and interactive visualizations in Python
Matplotlib is the first library to plot graphs I advise you to use, because it is widely used and, in my opinion, it helps you gain experience coding.
Matplotlib helps us plot the most important plots we may need:
And many more. You can start with Matplotlib here, using their tutorials.
At a certain point, when you‚Äôve gained experience in analyzing data, you may not be completely satisfied with Matplotlib; mainly (in my experience) this may be due to the fact that to perform advanced plots we have to write a lot of code with matplotlib. This is why Seaborn may help you. Seaborn, in fact:
is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.
But what does it mean that Seaborn mainly helps us with advanced plots, letting us write less code than matplotlib? For example, say you have some data regarding people tipping waiters. We want to plot a graph of the total bill and the tip, but we want even to show if the people were smokers or not and if the people were at the restaurant at dinner or at launch. We can do so like that:
And we get:
So, as we can see, with very few lines of code we can achieve a great result thanks to Seaborn.
So, a question may arise: ‚Äúshould I use Matplotlib or Seaborn?‚Äù
My advice is to start with Matplotlib and then move to Seaborn when you‚Äôve gained some experience because the reality is that, most of the time, we use both Matplotlib and Seaborn (because remember: Seaborn is based on Matplotlib).
The main thing that distinguishes a Data Analyst from a Data Scientist is the ability to use Machine Learning (ML). Machine Learning is the branch of Artificial Intelligence that focuses on the use of data and algorithms to make classifications or predictions.
In Python, ML models can be invoked and trained using a library called scikit-learn (sometimes called sk-learn) which is a library of:
Simple and efficient tools for predictive data analysis.
As a Data Scientist, all the work related to Machine Learning is done in sk-learn and this is why is fundamental for you to master at least the basics of this library.
The libraries we introduced have been numbered in ascending order, and my advice for you is to follow this order. So, first of all, install Anaconda to set up the environment and gain experience with Python, using Jupiter Notebooks. Then, start analyzing data with Pandas. Then visualize data with Matplotlib first and then with Seaborn. Finally, use sk-learn for Machine Learning.
Consider becoming a member: you could support me with no additional fee. Click here to become a member for less than 5$/month so you can unlock all the stories, and support my writing.
Eirik Berge, PhD
in
Towards Data Science
Stefan Hrouda-Rasmussen
in
Towards Data Science
Grace Tian
Lorenzo M.
jakes Moskel
anu nura
puchalapalli gowthami
Jeff Plummer
in
ML Stories
"
10,https://medium.com/towards-data-science/the-5-most-critical-insights-youll-gain-in-your-first-5-years-as-a-data-scientist-341cc2d73232,The 5 Most Critical Insights You‚Äôll Gain in Your First 5 Years as a Data Scientist,"Your first five years as a data scientist are going to feel a lot like drinking from a firehose.
New information is going to be coming from left and right, you‚Äôre going to have to re-learn some of the things that you learned at the very beginning of your data science journey, and you‚Äôre going to have to figure out your place within the company.
By the end of your first five years, you may feel as though you‚Äôve been hit by a bus of information and learning experiences.
However, when you look back on those first five years, you‚Äôll probably notice something pretty cool: you‚Äôve gained several valuable insights that you can use to leverage your career in the next five years.
Be that as it may, some of us want to fast-track our learning and gather as many insights as we can before we hit that five-year mark. Who doesn‚Äôt want to begin leveraging their career right now?
The following five insights are those that you can expect to gain in your first five years as a data scientist.
One of the biggest complaints of data scientists is that the work can be monotonous and repetitive.
Therefore, why not skip to the good part and automate everything you could do in your sleep?
If you love 75% of your job, don‚Äôt quit because the other 25% is boring. Automate the boring stuff so you can use your time more effectively to tackle the exciting stuff.
According to one of the most quoted productivity conventions, roughly 20% of the work you do yields 80% of the results. This means that the other 80% of your time is spent doing tasks that yield little to no results.
Why waste your time (and your company‚Äôs time, for that matter) on tasks that yield little impact?
Automating tasks can be as simple as setting up automatic reminders for yourself, uploading data, converting file types, backing up files, and much more.
Your ability to automate is only as limited as your creativity. Just make sure that you‚Äôre able to admit when the automation goes too far.
Documentation can start or end your career as a data scientist.
Without good code documentation, code cannot be sent to engineers to be integrated into an existing process. Nor can it be reworked by an intern ten years down the road who‚Äôs told to modernize it.
Good code documentation not only tells those using it what it does and how it works, but it also provides a look into the original data scientist‚Äôs thought process about why the code had to be written a certain way.
Code documentation should go far beyond comments and should seek to include flow charts, full README files with step-by-step descriptions, and the provision of contact information if none of your documentation can help.
Many of the other data scientists you come across in your first five years won‚Äôt have entered the field with a software engineering or computer science degree. Therefore, you all must work together to produce code documentation to help others understand your work.
Writing solid documentation is often an afterthought once the major business problem has been solved. However, by looking to the future, data scientists can ensure that their company benefits from the impact first delivered from their analysis for years to come by simply writing good code documentation.
towardsdatascience.com
Most data scientists don‚Äôt enter the field with software engineering or computer science backgrounds. If anything, most data scientists you see today have majors in everything from physics to nursing to industrial engineering to political science.
Data scientists are infamous for writing code that has a severe absence of functions, unclear variable names, spaghetti code, not a single hint of a unit test, and a severe lack of style.
Therefore, the value of a data scientist is in their ability to write clean code that can be updated, debugged, and moved into a production environment with few swear words coming from the engineering department. It can‚Äôt be stressed enough how clean code can help a company run efficiently. The ability to assure stakeholders that you can get results rests on your ability to write clean code that runs correctly the first time. Not only that but writing clean code will make you a favorite with everyone you work with throughout the rest of your career.
Think I‚Äôm kidding? Talk to any software engineer about a time they had to work with a data scientist‚Äôs code and you‚Äôll quickly see that the horror stories outnumber the fairytales.
Good coding practices are what separate professionals from amateurs.
towardsdatascience.com
Everyone can write code and conduct data analyses.
Everyone can conclude on what the data is telling them.
Everyone can produce beautiful visualizations.
But can everyone tell a story that moves stakeholders to action? Can everyone develop stories that will tell everyone at the table how we‚Äôve gotten to where we are, what we‚Äôre currently doing, and how we will progress forward in the future?
Data scientists who can tell great stories are indispensable. Telling stories goes further than that though, and encompasses all forms of communication that are vital to data scientists.
The ability to communicate is not a skill held by all data scientists and is one that will help you advance rapidly in your career. Therefore, you must practice communicating at every chance you get. Develop stories for your stakeholder presentations that have a clear beginning, middle, and end. Shape your data analyses to provide a clear story of what that data is currently telling you. Practice producing stories of what future projections could hold for your company that gives stakeholders a clear glimpse into what the future could look like.
Becoming a great communicator isn‚Äôt something you can achieve overnight, but is something that you‚Äôll be able to do if you practice it every day.
Your fancy model doesn‚Äôt mean anything if it draws the wrong conclusion.
Your complex algorithm doesn‚Äôt prove that you‚Äôre better than everyone else if it makes mincemeat of already coherent data.
Your two lines of code that replace ten lines of code don‚Äôt make you a data science god if no one else can understand what it does.
For some reason, this adage seems to go out the window at one point or another in everyone‚Äôs career as a data scientist. Whether it‚Äôs because the work is boring or you‚Äôve learned something new, there will come a time when you no longer want to keep it simple. And that‚Äôs fine.
As long as you find your way back to keeping it simple.
Keeping it simple is what will make you a great mentor at your company, what will allow you to complete work quickly and efficiently, and what will eventually lead you to manage teams of your own.
Keeping it simple means looking for the shortest logical path between A and B. It also means being direct in your work ‚Äî if something makes more sense written as ten lines of code, then write it as ten lines of code. Furthermore, it also means you‚Äôre avoiding looking for overcomplicated ways of doing otherwise simple things.
This statement should be one you learn to live by, not only in the hopes of becoming a better data scientist, but as becoming a better teammate, manager, and boss. This statement will help you stand out after only five years as a data scientist.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
ODSC - Open Data Science
ODSC - Open Data Science
Jacob Hauter
ODSC - Open Data Science
Melon
Kevin Brol√∏s
in
Abzu
ODSC - Open Data Science
Team Robin
"
11,https://medium.com/towards-data-science/the-simple-things-you-need-to-know-as-a-data-science-beginner-c4b9ffd0b516,The Simple Things a Data Science Beginner Needs to Know,"This article is for anyone who wants a no-nonsense, easy explanation of what data science is, how it works, and what it is used for. Maybe you heard of data science and you wanted to learn more. Maybe you work with a data scientist and want to better understand their role, or you even have the goal of becoming one. This article, featuring uncomplicated term definitions and depicting examples, was made for you.
My name is Ken Jee, and I am a data scientist and content creator. In general, my mission is to help make this field more accessible for everyone (to be honest, I‚Äôm also kind of writing this article for my parents so they will stop asking me to explain what I do every week). With that being said, let‚Äôs jump right into it.
If you prefer a video format, watch here:
To understand what data science is now, it is important to understand where it started. Data science has been around for longer than most of us realize. In 1974, the famous computer scientist, Peter Naur, proposed data science as an alternative name for computer science. And funnily enough, in 1985, C.F. Jeff Wu used the term as an alternative name for a completely different field, statistics, in one of his lectures. If it isn‚Äôt obvious, there is some pretty killer foreshadowing here.
The official title of data scientist was first popularized by DJ Patil at LinkedIn who would go on to become the first U.S. Chief Data Scientist under Barack Obama.
While the origins of the field are actually quite old, the true evolution of the field is relatively new. There have been dramatic changes in the way data science has been done over the last 10 years due to the massive advancements in storage and computing capacity. These rapid changes over a short period of time are why people generally still consider data science a new or evolving discipline.
Although it may have been coincidence that both computer science and statistics were both called data science by a couple professors, it has become a reality. Data science is now a beautiful hybrid of these two domains. We also should throw in a little bit of business and subject area domain to even things out.
Many people still insist that data science is just calling statistics by a different name. In 1985, that could have been true during a C.F. Jeff Wu‚Äôs lecture. However, I don‚Äôt believe that to still be the case. Because of the huge volumes of data and the increased complexity of computing, many of the problems a data scientist faces today cannot be done without the help of computer science and some advanced understanding of the unique domain they are operating in.
So, in real terms, what is data science now? Enough with this abstract stuff.
Data science is an aptly named field. Data science is a domain where we work with data to generate some form of value. And we use scientific techniques to extract this value.
Let‚Äôs take that apart a bit farther; how do we work with data to generate some value? The ways that data scientists drive value from data is for the most part derived from the data science lifecycle. All organization‚Äôs data usually follows this path.
The first way that we generate value from data is through collecting it. While it isn‚Äôt necessarily a core role of all data scientists. Some data scientists use their skillset to collect data. This can be done through building systems for data intake like webpages or surveys or it can be done through writing code that collects data from different places online.
Another basic way a data scientist can create value from data is by organizing it. The vast majority of the data in the world is unstructured, meaning that it hasn‚Äôt been organized into a database. Some data scientists can transform this unorganized data into a structured format making it far easier to analyze. As part of this process, they can also ‚Äúclean‚Äù the data by correcting misspellings, fixing errors, identifying duplicate records, and identifying missing values. A lot of these tasks are handled by data engineers these days, but they still fall under the data science umbrella. If you want more details on the specific data science roles, here‚Äôs a video about that.
The next way that we create value from data is through analysis. Simple analysis starts with basic statistics. For example, we may want to look at the average spending of an online customer vs. an in-store customer. Insights like these can help us to make better informed decisions about how we merchandise or market. Often the best way to convey these insights is through beautiful data visualizations. We also may want to see if an ad campaign is effective or not. We could run an A/B test to see which of two ad placements drives the most sales.
This is where a lot of the science starts to pop up. For something like this we would want to use the scientific method and concepts like hypothesis testing to evaluate differences between groups and campaigns.
After data analysis, we start to get into what most people consider the sexy stuff: building predictive models. From past data, data scientists can often build models that predict future outcomes at better than random chance. This allows businesses to (hopefully) make better decisions about how they allocate their resources. For example, if we owned a farm we could build a model to predict how much fertilizer we need to purchase each month. Especially if fertilizer has a shelf life, it could save us money if we predict this very accurately. Another example would be if we were looking to franchise a new restaurant, we could in theory build a model that would predict the return on investment based on the geography, traffic patterns, and demographics of the new location.
The final main way that data scientists create value from data is through automation. If we put some of these models that we build into production, often they make recommendations at a pace that far exceeds humans. A great example of this is on Netflix. They have machine learning algorithms that recommend you videos in real time. For a real person to do that same service, it would take thousands and thousands of people and thousands and thousands of hours. In this case, it really only takes a few algorithms to do it almost instantaneously. And these things pay off. According to Comparitech, apparently Netflix‚Äôs recommendation algorithm is worth over 1 billion dollars per year to them.
Now you should hopefully understand where data scientists create value. But what tools do they use and what does their work look like?
In my mind, the most important tool in a data scientist‚Äôs toolkit is programming. Most data scientists use either Python or R with Python being the more popular of the two. Other languages are used, but it is usually for a specific domain or use case. Data scientists are able to access the data, manipulate it, create visualizations, build models, and productionize their models all through coding. Programming is a data scientist‚Äôs all purpose tool.
Data scientists also have specialist tools that they use. For getting and manipulating data, data scientists will often use SQL. This allows data scientists to communicate easily with databases where the data is stored. Another specialist tool would be something like Tableau or Power BI which provide a graphic interface for creating data visualizations and dashboards.
For some projects, there is so much volume that data scientists need to use more computing power. In these cases, they will access virtual computers owned by Amazon, Google or Microsoft, also known as cloud providers, to run their analysis.
The last tool that is becoming increasingly popular is Git. Git is a versioning tool for people who write code. Here‚Äôs a full video on this for those who want a deeper dive.
We get it, data scientists are using fancy tools. But what problems can they actually help solve? I think this is really important for non-data scientists to understand. There are 2 main types of problems that data science and machine learning are good at taking on: supervised learning and unsupervised learning.
The first is supervised learning. Supervised learning serves to predict specific outcomes. We want to predict things like if someone is wearing a mask or exactly how tall someone is. Supervised learning means that we have data where the outcomes we want to predict are labeled. Let‚Äôs say that we wanted to predict if a papaya was ripe or not. If our data had some papaya characteristics like length, softness, mass, and sugar content and if someone had labeled it ‚Äúripe‚Äù, then this would be a supervised learning problem. More specifically, this would be a classification problem. We are trying to classify if a papaya falls into a ripe or a not ripe category. If we were trying to predict how heavy a papaya is in grams, that would be a regression problem because we are trying to predict a continuous numeric value.
Unsupervised learning is another story. With unsupervised learning, we often don‚Äôt have pre-defined categories that things neatly fall into. Instead we see what data naturally groups together and we create new categories based on its similarities or differences. An example would be simple customer segmentation. Maybe you group customers together based on buying patterns and then name these groups based on similar characteristics. Another form of unsupervised learning is generative where we are creating text or images from a model trained on a massive corpus.
Again, I think it is really important to understand the limitations of machine learning and data science. Often I hear non-data scientists speaking about this field like a cure all. Yes, we can do some incredible stuff, but there are still limitations based on the specific business case. Here‚Äôs a great example of how data science can go wrong with poor assumptions.
I realize I‚Äôve mentioned machine learning quite a few times in this article. I‚Äôve probably said it interchangeably with data science in fact. Questions I get a lot are what is machine learning versus what is data science? And how do machines actually learn?
While I have a whole video that goes more into the specifics of this topic, machine learning algorithms are mainly what data scientists use to build their models. The supervised & unsupervised learning techniques that I listed above are all machine learning. On the other hand, most of the data analysis, data collection, and data cleaning techniques don‚Äôt fall into the machine learning bucket.
For the most part, whenever we are making a model that predicts future outcomes, groups data points algorithmically, or generates new material, what we are doing would be considered machine learning.
But where does the learning really take place?
Whenever a data scientist builds a model, they will split their data into a train set and a test set. They will use the train data to ‚Äúteach the model‚Äù and then they will see how their model does at predicting the outcomes in the test set. Let‚Äôs do a very simple example. Our data is predicting someone‚Äôs YouTube views based on how many videos they‚Äôve made. With a simple linear regression, training our model is fitting a line to this data. That is how our model would learn from this data. In order to make predictions on our test data, we just have to see the value of our line based on the data point we‚Äôre selecting in our training set.
If our model does a good job predicting these test values, then we may consider it ready to apply to new data. With model training, you will hear terms like overfitting and underfitting and bias and variance trade off. I think they‚Äôre probably outside the scope of this article, but let me know in the comments section if you want me to make separate articles on these.
We build these models but what does the end-product of a data scientist‚Äôs work look like? Honestly, this varies pretty greatly by the role. Data science deliverables generally come in three flavors: 1) a dashboard that guides business stakeholders to their own insights, 2) a deliverable that makes a recommendation or a prediction on a specific problem, and 3) a trained model that users can get real-time predictions from.
I think it is really important to understand that within this domain, there aren‚Äôt really ever clear right and wrong answers. There are just shades of certainty and uncertainty. What I mean by that is that a model that we build gives us an estimate about what will likely happen. The confidence of our model helps us to decide if we should take action or not. In theory, any model can be wrong even if it is predicting if the sun will rise the next day.
Most models that we build, especially if they pertain to real-time predictions, need to be constantly maintained, retrained, and updated with new data so they are as accurate as possible.
Hopefully this article helped you to better understand data science and some of the types of problems data scientists can help solve. If you think this article would be helpful to one of your friends, someone you work with, or someone who is looking to become a data scientist, I would appreciate it if you forwarded it along.
If you enjoyed this article, remember to follow me on Medium for more content like this and sign up for my newsletter to get weekly updates on my content creation and on additional learning resources in the data science industry! Also, consider supporting me and thousands of other writers by signing up for a membership.
Thank you so much for reading and good luck on your data science journey.
Rahul Araveti
in
Web Mining [IS688, Spring 2021]
Strata Scratch
Luke Thorp
in
Towards Data Science
Pallavi Krishna
Alec Morgan
in
Future Vision
Rahul Kumar
Abdulrahman
Koushik C S
in
The Startup
"
12,https://medium.com/codex/say-goodbye-to-loops-in-python-and-welcome-vectorization-e4df66615a52,"Say Goodbye to Loops in Python, and Welcome Vectorization!","Loops come to us naturally, we learn about Loops in almost all programming languages. So, by default, we start implementing loops whenever there is a repetitive operation. But when we work with a large number of‚Ä¶
John Fajardo
in
The Startup
AirGap
Keitaro
Crystal Villanueva
Yogesh Mali
in
Dev Genius
echo3D
in
echo3D
Steven Smith
Ross Ingram
"
13,https://medium.com/towards-data-science/the-mistake-every-data-scientist-has-made-at-least-once-3479002211b4,The Mistake Every Data Scientist Has Made at Least Once,"If you use a tool where it hasn‚Äôt been verified safe, any mess you make is your fault‚Ä¶ AI is a tool like any other, so the same rule applies. Never trust it blindly.
Instead, force machine learning and AI systems to earn your trust.
If you want to teach with examples, the examples have to be good. If you want to trust your student‚Äôs ability, the test has to be good.
Always keep in mind that you don‚Äôt know anything about the safety of your system outside the conditions you checked it in, so check it carefully! Here is a list of handy reminders that apply not only to ML/AI but also to every solution based on data:
Designing good tests is what keeps us all safe.
One difference between a data newbie and a data science expert is that the expert has some whopping trust issues‚Ä¶ and is happy to have them.
In our previous article, we looked at the testing mistake that beginners make and how to avoid it. Now it‚Äôs time to look at a more insidious mistake that even experts make.
For those who haven‚Äôt read the previous article, let‚Äôs catch you up on the setup (in a parallel universe where we didn‚Äôt make the newbie mistake, that is):
We competently trained a decent model using 70,000 input images (each one you see in the image above is a placeholder that stands for 10,000 similar photos with the label Banana, Huxley, or Tesla; one of these is my breakfast and the other two are my cats, you figure out which is which) and then tested it on 40,000 pristine images.
And we get‚Ä¶ perfect results! Wow.
When you observe perfect performance on a machine learning task, you should be very worried indeed. Just as the previous article warned you against having the wrong attitude towards 0%, it‚Äôs a good habit to smell a rat whenever you see perfect results. An expert‚Äôs first thought would be that either the test was much too easy or the labels leaked or the data weren‚Äôt as pristine as we hoped. Exactly the same thoughts a seasoned college professor would have upon discovering that every student answered every question flawlessly. They‚Äôd be furiously debugging instead of patting themselves on the back for good teaching.
Since 100% isn‚Äôt a realistic result for a lifelike scenario, let‚Äôs imagine we got some other high-but-not-too-high score, say 98%.
Great performance, right? Sounds like great performance in pristine data to boot. With all the statistical hypothesis testing bells and whistles taken care of! We‚Äôve built an awesome Tesla/Huxley detector, let‚Äôs launch it!!!
Watch out.
You‚Äôll be tempted to jump to conclusions. This might not be a Tes/Hux detector at all, even if it gives great results in testing.
The mistake that experts make has less to do with the technical and more to do with the human side of things: giving in to wishful thinking.
Experts aren‚Äôt immune to the mistake of wishful thinking.
Don‚Äôt assume these results mean anything at all about what the system is actually detecting. We have no evidence that it has learned ‚Äî or worse, that ‚Äúit understands‚Äù ‚Äî how to tell Hux and Tes apart. Look closer‚Ä¶
Did you notice that little detail that‚Äôs present in the background of all photos of Tes? Whoops, turns out we accidentally built a radiator / not-radiator classifier!
It‚Äôs a very human mistake to over-focus on the parts of the problem and data that are interesting to us ‚Äî like the identity of the two cats ‚Äî and snooze past the other bits, like the presence of the radiator in most of the photos of Tes since that‚Äôs where she likes to hang out and therefore it‚Äôs also where she‚Äôs most likely to be photographed. Unfortunately, she loves the radiator so much that the same issue turns up in both the training data and the test data, so you won‚Äôt catch the problem by testing on data from the same population you used for training.
All this is (somewhat) okay as long as you (and your stakeholders and users) have the steely discipline of:
This kind of discipline is rare, so until we have better data literacy, all I can do is dream big and do my bit for educating people.
This system will only work with photos taken in the same manner as the testing and training datasets ‚Äî photos from the same apartment in the winter, since that‚Äôs when Tesla tended to hang out by the radiator while Huxley didn‚Äôt. If you always take photos in the same way and you had a lot of them, who cares how your system does the trick of assigning labels as long as it gets you the right ones. If it uses the radiator to win at the labeling game, that‚Äôs fine‚Ä¶ as long as you don‚Äôt expect it to work in any other context (it won‚Äôt). For that, you need better, broader training and testing datasets. And if you move the system to a different apartment, even if you think you understand how your system works, test it afresh anyway to ensure it still works.
The world represented by your data is the only one you‚Äôre going to succeed in.
Remember, the world represented by your data is the only one you‚Äôre going to succeed in. So you need to think carefully about the data you‚Äôre using.
Never assume you understand what an AI system is doing. Believing that your simple understanding of a complex thing is bulletproof is the height of arrogance. You‚Äôll be punished for it.
Never assume you understand what an AI system is doing. Believing that your simple understanding of a complex thing is bulletproof is the height of arrogance. You‚Äôll be punished for it.
It‚Äôs easy to miss real-world subtleties in your data. No matter how talented you are at it, if you work long enough in data science, you‚Äôll almost surely make this wishful-thinking mistake at least once until you learn the hard way to severely curtail your conclusions ‚Äî the best habit you can build is to avoid reading extra meaning into things until you‚Äôve thoroughly checked the evidence. You‚Äôll know you‚Äôve arrived at the next level when your inner monologue about your results sounds less like a TED talk and more like the fine-print of the world‚Äôs most boring contract. This is a good thing. You can dazzle with your fast talk later if you insist, just don‚Äôt embarrass data literacy by believing your own hyperbolae.
The only thing you can learn from good testing results is that the system works well in environments, situations, datasets, and populations that are similar to the testing conditions. Any guesses you‚Äôd be tempted to make about its performance outside those conditions are fiction. Test carefully and don‚Äôt jump to conclusions!
If you had fun here and you‚Äôre looking for an applied AI course designed to be fun for beginners and experts alike, here‚Äôs one I made for your amusement:
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Get started with Google Cloud Vision API and AutoML Vision here:
bit.ly
Akhilvydyula
Ryan
Cassie Kozyrkov
Cassie Kozyrkov
in
Towards Data Science
Yariv Adan
in
Towards Data Science
Sarah Harrison
Saikat Halder
Christopher Tao
in
Towards Data Science
"
14,https://medium.com/towards-data-science/statistics-classes-dont-teach-you-about-money-8464ea9de330,Statistics classes don‚Äôt teach you about money,"Statistics and data science classes don‚Äôt do much to prepare students for hashing out project specifications on the job. Here are a few problems:
Imagine that you‚Äôre a data scientist who has been hired to estimate the average height of pine trees in the forest pictured below.
(Note: the links in this article take you to my lighthearted explanations of any jargon terms that crop up.)
If we were to perfectly measure every single tree, we would get something far better than an estimate; we would get a fact. The actual truth about the heights of the trees in this forest. When you have facts, you don‚Äôt need statistics.
When you have facts, you don‚Äôt need statistics.
Should you then go out and measure every tree‚Äôs Planck Length (the smallest unit of length in physics, with one unit equal to 0.00000000000000000000000000000000001616255 meters)? Which instrument would you use to get such a precise measurement? I bet you don‚Äôt have it lying around in your garage, especially since it hasn‚Äôt been invented yet.
Even if we settled for humanity‚Äôs most precise measuring device (orders of magnitude too imprecise if you have your heart set on Planck Length), one tree measured with it would likely be much too expensive for whatever purpose motivated your boss to hire you.
Furthermore, even if you settled for plank length instead of Planck Length and allowed yourself round off to the nearest meter, measuring every tree would be overkill‚Ä¶ your forest is much too big. Would your boss approve of your completionist desire to collect-‚Äôem-all?
Statistical sampling is all about getting a perspective on your problem that‚Äôs less perfect than a fact but good enough.
If you‚Äôre thinking like a good statistician, you‚Äôre immune to the perfectionist impulse ‚Äî why measure the whole population when you can get a good enough estimate by taking a sample? Sure, this introduces uncertainty (we‚Äôre no longer dealing with facts) but perhaps we can live with that. Let‚Äôs measure a good enough sample of trees so we don‚Äôt have to measure them all!
We haven‚Äôt even gone anywhere near a tree and we‚Äôre already stumbling into two hurdles with our seemingly-simple tree measuring task:
The way through both of these questions is to understand why your project exists in the first place: what is the purpose of the task and what does ‚Äúgood enough‚Äù actually mean? This is a cost-benefit kind of question which you can‚Äôt answer without understanding the real world aspects of the project.
Begin with why: why are you collecting data? What is the purpose of your project? What does ‚Äúgood enough‚Äù actually mean?
Unfortunately, if you‚Äôre the new hire on your team, setting the bar for ‚Äúgood enough‚Äù is, strictly speaking, someone else‚Äôs job. This someone is usually The Boss. Unless *you‚Äôre* the boss, it‚Äôs not your call to make. If you treat real-world data problems like homework questions, this will be a struggle for you.
The first problem is that classroom courses for data science professionals rarely rub your nose in data budgeting. Most homework problems ask you to take sample size for granted, wiring your brain to work with inherited data but doing nothing to help you handle data collection negotiations in the real world.
Stop treating data like it‚Äôs priceless. Data isn‚Äôt sacred; it‚Äôs a resource like any other.
Other homework problems teach you to calculate the sample size you need without ever preparing you for the next bit: how to scare up the money you‚Äôd need to actually get your hands on this ideal sample size. (Not to mention the etiquette of explaining a power analysis budget curve to a boss with a numbers allergy.) One of this educational oversight‚Äôs most pugnacious manifestations is a habit of treating data as priceless, resulting in odd behaviors that look damned-near infantile to every other adult on your team. In the real world, there‚Äôs scarcity and nice things cost money. This applies to data too. Data isn‚Äôt sacred; it‚Äôs a resource like any other.
The second problem is your boss‚Äôs skill level. If you take charge of the situation (you leader you!) and do the work without taking the time to fully understand your boss‚Äôs vision, you‚Äôre in danger of crafting a solution that doesn‚Äôt fit the problem.
On the other hand, if you approach your boss with a request for measurement and sample size specifications, well, here be dragons too.
Suppose your boss answers, ‚ÄúTwenty trees measured in feet, please.‚Äù
It takes skill to convert a vision for the project into sample size requirements and until you know your boss‚Äôs decision-making skill level, it‚Äôs hard to judge whether their response is well-considered or lazy. It could be exactly what you need in order to move forward, but unless your boss has experience with data and measurement, their off-the-cuff answer might shoot the project in the foot. There‚Äôs a solid chance they‚Äôre sending you on a wild goose chase.
Until you‚Äôve worked closely with your boss, you won‚Äôt know.
As soon as you‚Äôre dealing with uncertainty, you‚Äôre going to need a bridge from the facts you have (your sample of a few trees) and the facts you wish you had (your population of all the trees in the forest). That bridge is assumptions. Assumptions are what make a statistics project tick.
DATA + ASSUMPTIONS = INFERENCE
The tricky part is that your boss ‚Äî not you! ‚Äî is the one who‚Äôs responsible for setting the project‚Äôs assumptions. If you‚Äôre not the decision-maker, then your job is to serve as an interpreter between mathematics and whatever‚Äôs in your boss‚Äôs head. That‚Äôs another skill they rarely cover in class.
towardsdatascience.com
I‚Äôll cover this one in the next article, but the short version is that school leaves most of the real-world details out. Just like this paragraph.
Decision scientists and more seasoned data scientists start every project by interviewing the boss carefully to make sure that the specs of the data collection request are clear and that they match the boss‚Äôs vision for the project, while balancing the cost-benefit aspects of the data collection process. Alas, this is a skillset you‚Äôre unlikely to pick up in class. Without it, there‚Äôs a good chance that you‚Äôll either usurp the boss‚Äôs role or panic and do exactly what the boss says. Both are bad!
If you‚Äôre an inexperienced data worker, there‚Äôs a good chance that you‚Äôll either usurp the boss‚Äôs role or panic and do exactly what the boss says. Both are bad!
It‚Äôs only safe to move from the realm of facts to the realm of uncertainty when the person in charge has a clear vision of what ‚Äúgood enough‚Äù means for the project and has the ability (via their own skill or a colleague‚Äôs help) to convert this into language that data professionals can work with. Everything should start with purpose ‚Äî the why of the project ‚Äî and carefully consider the cost-benefit realities of information.
And that means that your first real task on any data project has relatively little to do with numbers and much more to do with psychology and communication.
Every data project begins with one essential step: understanding your boss and your business.
Every data project begins with one essential step: understanding your boss and your business. Skip this step at your peril!
If you enjoyed this article, continue the series to the next one: Is simple random sampling actually simple? coming soon! In the meantime, stop by and say hi on Twitter.
kozyrkov.medium.com
P.S. Have you ever tried hitting the clap button here on Medium more than once to see what happens? ‚ù§Ô∏è
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Abhishek Kandwal
Anatoly Zaytsev
in
Towards Data Science
Aakash Chugh
in
Towards Data Science
Tom Blanke
in
BroadHorizon Cmotions
Sarah Mason
in
Artificial Intelligence in Plain English
Priyam Chauhan
Nikola Ilic
in
Towards Data Science
Mike Lewis
in
Build Your Wealth
"
15,https://medium.com/@kozyrkov/babys-first-machine-learning-metric-1bbd7850cd08,Baby‚Äôs First Machine Learning Metric,"The mean squared error (MSE) is one of many metrics you could use to measure your model‚Äôs performance. If you take a machine learning class, chances are you‚Äôll come across it very early in the syllabus ‚Äî it‚Äôs usually baby‚Äôs first loss function for continuous data.
Rammya Dharshini K
Tufail Waris
in
Analytics Vidhya
Charles E. Gormley
Yuiti Ara
in
Liv Up‚Ää‚Äî‚ÄäInside the Kitchen
Ekin Ilseven
SaiGayatri Vadali
Dan Sutherland
Gal Yona
in
Towards Data Science
"
16,https://medium.com/@sslp23/predicting-fifa-2022-world-cup-with-machine-learning-640f1d2d7e98,Predicting FIFA 2022 World Cup with Machine Learning,"With the approach of FIFA 2022 World Cup, the interest and discussions about which team is going to win the championship increase. Thus, I decided to test my Data Science and Machine Learning skills to develop a model analyzing historical data to simulate all games from FIFA 2022 World Cup.
In order to build a Machine Learning model, we need the data from the teams. First, we need something that tells the performance of the teams, which can be extracted from previous games. Also, I decided to use the FIFA Rankings in the construction of the features. They will be helpful to quantify the quality of the opponent that the team faced in a game. Both data can be found on Kaggle: past games and FIFA rankings.
Which features can bring impact to decide the winner of a football game? This question has a very open answer. From the selected players to the temperature at the stadium during the day, everything can make an impact on the outcome. Then, I chose to build a dataset only with the past stats from each team involved in the game, prioritizing the quantifiable stats that can be gathered in a simple way, such as goals made, mean ranking faced, points won, and others that will be detailed. Those data can be found in the join of both datasets that I‚Äôve talked about in the last section.
Also, only the performance at the 2022 World Cup cycle will be analyzed. The idea is to take into account only the variation of performance in preparation for the World Cup.
Above is the code to create the wanted dataset.
The dataset is ready, as you can see above, and there is more information about the away team. It‚Äôs important to remember that the majority of games between national teams are played on neutral venue, but I‚Äôll use the names ‚Äúhome team‚Äù and ‚Äúaway team‚Äù to simplify the way in which I‚Äôll talk about the teams involved.
Now, I need a set of candidate features. With them I can make an analysis that will indicate if the feature has predictive power or not, to understand if I need to keep, remove, or modify the feature. The features I calculated as candidates were:
The first two features are used to quantify the offensive power and defensive power of a team. The difference between FIFA ranking position in the match is used to quantify the difference between both teams‚Äô strengths as calculated by FIFA. Ranking faced average is used to put in analysis the strengths of opponents that a team faced.
The increment in FIFA ranking points was calculated to analyze the team quality increasement at the World Cup cycle and in the last 5 games.
The game points average the team won is to purely quantify the team‚Äôs performance, while the weighted average of game points a team won is weighted by the opponent‚Äôs ranking position that the team faced, in order to analyze if the performance of the team is high due to low-level opponents.
To build features based on FIFA rankings, there are two choices: use FIFA ranking points, or use FIFA ranking position. I opted to use the ranking position in all features except the increment in ranking. I didn‚Äôt create the same feature changing only FIFA ranking position by FIFA ranking points because these columns are very negatively correlated, as you can see below.
Then, it doesn‚Äôt make sense to create two versions of features like the weighted average game points by the ranking using rankings‚Äô points and rankings‚Äô positions, because they would yield the same result.
Before the modeling, it‚Äôs needed to analyze what will be predicted. However the ideal is to predict between win, draw and lose, a 3-class classification problem is very hard to analyze and evaluate. Then, I decided to make the prediction between two classes: home team win and home team draw/lose.
To analyze the features in relation to the target, I‚Äôll use violin plots and boxplots. The idea is to analyze how their distributions behave in relation to the values of each class, and if they can separate well the data.
For the first set of data, the already created, only rank_dif, the difference between both teams‚Äô rankings, has an impact on the target classes. Thus, I will create more features based on differences, since it seems that they are good predictors:
And again I analyzed the violin plots.
The difference between goals and suffered goals features have a good impact. However, the features that map the differences between goals made by a team and suffered by the opponent have no impact. Therefore, we have, now:
Also, we could do the difference of points, difference of ranking position faced, and difference of points won by ranking faced. And, to weigh the level of the opponent, I thought of the following feature: difference between goals made by ranking faced, and the same to goals suffered.
The view by violin of this data is not very effective, due to the small scale. Then, I‚Äôll analyze the boxplot of these data too.
Points difference, goals difference by ranking, points difference by ranking, and difference of ranking faced are good features. But, their version for the last 5 games and their version of the full cycle seems to be correlated. I‚Äôll check this below.
Analyzing the correlation, it‚Äôs visible that in the difference of goals made by ranking the ideal is to choose only one of the versions, and I chose the one that considers the full cycle. For the others, it‚Äôs possible to use both versions.
Then, we have, as features:
With that, we have a database with the needed features to apply the Machine Learning model.
My idea here is to build two models, one of Random Forest and another of Gradient Boosting, and compare them to find which one is better, in order to be used in the simulation. I decided to use models based on decision trees because they did better in football problems when I studied the literature. Also, I don‚Äôt see the need to use more complex models, because of the size of the dataset.
I‚Äôll do a parameter variation using SkLearn‚Äôs GridSearchCV, and will use the best model at the simulation.
I avoided testing a lot of parameters due to the delay in execution, prioritizing tests with values that reduce overfit, such as a learning_rate not so low and a n_estimators not so high.
Did the same to the Random Forest:
I analyzed the models with confusion matrix and ROC curve, and the results were:
Random Forest model has a slightly better performance but it seems to overfit. Analyzing AUC-ROC of Gradient Boosting, we see a model with almost the same performance but lower risk to overfit, that‚Äôs why it was chosen.
Now, we arrived to the most interesting part: see which team the model will predict to win the World Cup!
The first thing to do is obtain the list of teams playing World Cup, and I used the method read_html, from Pandas. The method takes a dataframe from a web page, in which I put the Wikipedia World Cup page. With that, I‚Äôll recreate the World Cup table.
The table has the games, the points of each team in the group and a list storing the probabilities of the team winning each game. This will be used as tiebreaker in the case of two teams with the same points in the group.
As I already explained, the model makes the classification between win from home team and win/draw from away team. Then, how can we predict draws? I created an objetive rule for this: knowing that all games from the World Cup are played on neutral venue, the prediction will be made in two forms:
If Team A or Team B wins both predictions, then the win is assigned to that team. In case one team wins in the first prediction and the other team wins in the second prediction, then it will be assigned as a draw. In the playoffs phase, the probabilities will be calculated in both predictions, and the team with the highest average probability advances. Since the model assigns a ‚Äúwin‚Äù to the away team even if it draws, the draw probability is inside the away team‚Äôs probability. So, with this type of simulate, both teams get a simulation with the draw as an advantage.
The data used in the simulation is until the last game of the team. In other words, for Brazil, the features will be calculated until the game against Tunisia, which was the last one that Brazil played.
Now, we can run a code simulating game-by-game, calculating the points and seeing what‚Äôs happening in the first phase.
The outcome was:
It‚Äôs interesting to see some results, such as the draw between Brazil and Switzerland and Denmark and France. In general, the favorites passed the group phase.
In the playoffs, the idea is the same:
To see the results here, besides the text output, I decided to plot a graph with the playoff picture as done in this Kaggle notebook. It‚Äôs a very interesting way to see the outcomes of these problems.
The World Cup was simulated! My model is predicting a win from Brazil, with 56% of probability against England in the final! I think the biggest upsets were Belgium passing against Germany and England making it to the final, eliminating France in the quarter-final. It‚Äôs interesting to see some games in which the probability is very tight, as Netherlands vs. Argentina. From the quarter-final until the final, no team advanced with more than 60% of probability, which shows that the majority of the teams that advanced to playoffs are at a similar level.
The idea of this project was to practice my knowledge in Machine Learning using something that I like, football. I thought that simulating the World Cup was very interesting, for being a trending topic nowadays that attracts the attention of everyone that likes the sport. I believe that the goal was achieved, since the construction of all features and the data analysis brought me the opportunity to search and meet a lot of new techniques.
About the results, I‚Äôm hoping a lot that the model predicts right the champion!
In case you can see the code in detail, you can see my GitHub and my Kaggle. And if you want to reach me, that‚Äôs my LinkedIn. Thanks!
Victor Roman
in
Towards Data Science
Matthew Boutte
in
SolasAI
DrPadmavathi Roy
in
Experience Stack
Harshith
in
Towards Data Science
Data Labeler
Sravani Thota
Q Financial Advisors
Gpraneeth
"
17,https://medium.com/towards-data-science/how-to-create-a-professional-portfolio-on-github-that-will-help-land-your-first-job-in-data-science-e1fc8bd7a797,How to Create a Professional Portfolio on GitHub That Will Help Land Your First Job in Data Science,"Every piece of advice you reference for tips on landing a job in data science will talk about the importance of having a professional portfolio that showcases your work and gives employers the chance to take a deep dive into your personal projects.
Not only do portfolios allow employers to see the quality and thought process of your code, but they also allow them the chance to see if you have the initiative to embark on your own projects and the ability to create personal projects that make an impact or solve a real-world problem.
GitHub is the industry standard, which makes it an ideal place to host and maintain your professional portfolio. Furthermore, its additional features, such as its commit history or contributions calendar, provide employers with an idea of how regularly you add to and maintain your portfolio.
Especially for self-taught data scientists, portfolios can play a key part in determining whether an employer asks you for an interview and can provide context as to how you would fit into a company. While it may play a smaller part in the process of landing your first job as a data scientist, it cannot be underestimated the value of a solid, professional data science portfolio.
Personal projects are what you will most often find in a data science portfolio and can include everything from Titanic analyses (although you might not want to include this overdone theme) to climate modeling.
At a loss for personal project ideas? Check out the hundreds of data sets available on Kaggle for inspiration, see what others are building on Youtube, or talk to small businesses in your area to see if you can help them solve a problem using data.
While personal projects don‚Äôt necessarily have to be completely relevant to a sector you‚Äôre applying for, it can be beneficial to show employers that you‚Äôve worked with relevant data and have been able to draw practical insights. Not only that, but it helps you understand what kind of problems need to be solved in a given industry and the different ways in which you could solve them, thus giving you a leg up on the competition.
Assignments and class projects are a great idea as portfolio contributions, especially for up-and-coming undergraduate or master‚Äôs students or self-taught data scientists working through bootcamps or online learning programs.
They may not be the most mind-blowing projects, but they do add substance and can provide employers with an idea of your progression as a data scientist. It may also intrigue employers to see what they‚Äôre teaching in schools nowadays.
It can also be beneficial for contextual reasons to include a description of the assignment, the objective you were trying to achieve, and even your grade and instructor‚Äôs comments if available.
Nothing shows an employer that you‚Äôre able to meet deadlines and work under stressful conditions quite like hackathon or contest entries.
After you‚Äôve completed the competition, it‚Äôs a good idea to clean up your code before adding it to your portfolio. Include in the code documentation the name of the competition, the date, how long the competition ran for, a description of the competition and the problem you were trying to solve, information on the setting in which you were competing (e.g., as a team or individually, in a convention hall or at home, etc.), details on how you solved the problem, and perhaps even the results of the contest.
Hackathons or contest entries don‚Äôt have to be relevant to the job you‚Äôre applying for as employers will use them to determine how you work under pressure, the quality of work you can produce in a short period, and whether or not you can solve a problem shortly after it was given to you.
As a STEM communicator, I know the value of being able to teach something to others or the ability to guide others through important processes that you‚Äôve experienced.
For example, I use my education and background in tech to help others achieve their goals of learning new programming languages, self-learning difficult concepts, and getting that first job in the field. I do all this through blog posts, articles, and social media. These mediums are capable of showcasing my soft skills to employers, such as my ability to communicate, teach, write, present, and tell stories.
Including blog posts, articles, social media posts, and more into your portfolio are great ways to not only showcase your soft skills as I previously mentioned, but they also give employers a sense of how you would fit into a company and if you would be able to advance at a later date. Furthermore, these key pieces show employers that you have a range in your abilities and perhaps even interests outside of your regular work.
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
The value of a professional data science portfolio can‚Äôt be underestimated, especially when you‚Äôre looking for your first job in the field. Because you‚Äôll have little to no applicable work experience, employers will use your portfolio to gauge your fit for the company, as well as your technical skill and soft skills. An impressive portfolio is a great way to make a statement and also stand out from other candidates, therefore putting some time and effort into it using the tips provided here can produce worthwhile results further down the road as you achieve your first job in data science.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Support my writing by donating to fund the creation of more stories like this one: Donate
ODSC - Open Data Science
Artin Sinani
Vanshika Goel
ODSC - Open Data Science
Nicholas Leong
in
Towards Data Science
ODSC - Open Data Science
in
ODSCJournal
ODSC - Open Data Science
Yves Boutellier
in
Towards Data Science
"
18,https://medium.com/towards-data-science/top-5-python-programming-books-for-data-scientists-af6caf4ff7b,Top 5 Python Programming Books for Data Scientists,"From basic statistics to complex data sets only used by advanced computer scientists, data science has risen the charts in every industry. In previous years, everyone thought data was just an obscure topic requiring mastery from professionals in the tech industry. Now, data is everywhere. From that super bowl ticket purchase to a burger at MacDonald‚Äôs to an employee payroll ‚Äî we all enter data points into vast computer intelligence databases every day.
One of the benchmarks needed to succeed in data science is the knowledge of Python. The programming language has been the most sort after among data professionals due to its excellent libraries, interactive community, clean datasets, and high versatility. Python has become a mainstay in the data industry.
Data scientists can use python in almost any field such as AI, Machine learning, Big Data, Server Side development, Automation, and many more.
You might be asking, is it really necessary to master python as a data science beginner?
Yes.
The early stages of data science are the perfect time to acquire as many valuable resources that will help your analysis in the future. It‚Äôs not a good idea to leave python out of your frameworks.
Python started off in 1991 as a scripting solution for basic web development, but now python is the driving language behind data science as a whole.
Personally, books helped me a lot when I got started with data. I have selected five essential books to kickstart and navigate your career in data using Python. Some of these are my personal favorites and are applicable to both newcomers and professionals.
Data structures are critical components when analyzing data. They are versatile in solving a lot of problems, act like reusable codes, and allow you to use, store, and arrange data successfully. Data structures are sometimes tricky ‚Äî they perform virtually any role in data analysis ‚Äî most professionals often try to fit in one for a host of datasets.
That‚Äôs one of the major purposes of this book: to help you explore and then understand the analysis and design of python data structures.
After reading ‚ÄúHands-On Data Structures and Algorithms with Python,‚Äù you will be able to develop complex data structures, merge algorithms, sort insertion, practice concepts such as Big O notation, hash tables, stacks, and queues, algorithm design, modeling, and much more.
From a personal perspective, alongside model fitting ‚Äî data structure is one of those topics where you don‚Äôt really grasp the concepts at first. Don‚Äôt rush it. The book is 400 pages long, take it bit by bit, and make sure you are able to understand and perform operations with each chapter before moving to the next. It‚Äôs a total game changer when you get familiar with data structures and algorithms with Python.
When I started my data science career, I wasted a lot of time running away from programming. Worst decision ever. When I finally decided to mix programming with data a couple of years back, I started off with python. ‚ÄúIntroduction to Machine Learning with Python‚Äù was the first book I read. Loved it ever since.
Understanding Machine learning is paramount for a professional in the data science industry. Building datasets, developing and working with models, utilizing frameworks, and managing databases are important aspects of working with data. Most of these aspects are designed and built with the concepts of machine learning.
This book breaks down the fundamentals of machine learning using python. So even if you don‚Äôt have previous knowledge of python, you will benefit massively from the various programming techniques explained in the book.
The Art of Data Science dives completely into the fundamentals of data science. It navigates around the inner workings of data analysis, how to find data, tell stories with data, and filter it into efficient results.
Most times data doesn‚Äôt require a good mathematician ‚Äî it just requires a good storyteller. A data scientist with forehand communication knowledge and good python skills will be able to explore any pool of data.
The Authors, Peng and Matsui, both have impressive portfolios in managing data projects as well as being responsible for the growth of the most successful data professionals in the industry.
In this book, they use their experiences to guide both beginners and advanced learners through various concepts of analyzing basic and complex datasets.
For data scientists and programmers who are finding it difficult to transition into python, this is my favorite pick for you. Prior to its release, lots of professionals expressed their strong need for a book with pure practical solutions to working with real-life data science projects.
If you are someone who learns by doing things in real-time, this book is for you.
It will teach you all the basic practical things like scrapping data on the web, filtering datasets, appending into XLS, interpreting a database, module selection, and sorting algorithms all using python.
After reading the book, it is highly recommended that you follow up on an online course, ‚ÄúAutomate the Boring Stuff with Python Programming,‚Äù created by the author on Udemy. It‚Äôs a good idea to get updated solutions on some tips discussed in the book.
From beginners to professional developers, Mark Lutz carries everyone along in his complete, essential introduction to Python. A very powerful book for data scientists who want to understand the interworking of core Python and its diverse libraries.
It covers significant things you need to know about Python and its functions. You‚Äôll be able to complete quizzes, and exercises and practice self-understanding tutorials. It gets you started with version 2.7 and then transits into 3.3.
Programming in data science might seem like a daunting task, but with an easy-to-learn language such as Python, you are well on your way to mastery. Whether you are a beginner just starting out with python or a professional looking to expand your knowledge, these books will help you a lot. Some are only a few pages, others are long reads. It doesn‚Äôt matter, just ensure you get maximum value from each chapter.
Zdenko Hrƒçek
in
Towards Data Science
Antonio Soto
in
Artificial Intelligence in Plain English
Mlkristalyn
Tiberiu-ioan Szatmari
in
Analytics Vidhya
Ayoola Olafenwa (she/her)
in
Towards Data Science
Autorecentcar
Sandeep Giri
Rasheedat Atinuke Jamiu
"
19,https://medium.com/towards-data-science/how-are-data-analysts-hired-today-all-you-need-to-know-874f42cea97f,How Are Data Analysts Hired Today? All You Need To Know,"In the rapidly expanding data landscape of today, an increasing number of people are eager to enter the world of data science & analytics. New grads, professionals switching careers, and analysts wanting to become subject matter experts, all levels of expertise are coming together in this age of data.
Data and analytics jobs have grown exponentially and we can only forecast the trend to continue. Every passing day, the competition becomes fierce. There is a huge skill gap that companies are trying to close with the ever-evolving business needs. With exponentially increasing data, businesses need professionals to make sense of that data.
Every company portal now has data roles open to hiring around the year. When I was sitting at my very uncomfortable dorm desk, surviving a pandemic and applying for full-time jobs in 2021, I hoped for some tips on the hiring process. Now that I see things from the other side of recruiting, working as an analytics consultant with a healthcare company and involved in hiring decisions, here‚Äôs a know-all blog about how exactly managers hire data professionals!
Any hiring process starts with a candidate submitting an application and conveying an interest in the position. The applications may pass through the black hole called ATS (application tracking system) and/or are screened by recruiters based on the role description and job requirements.
Once your application is accepted and considered a fit for the open role, the hiring managers and/or recruiters will review your resume to determine who can be called in for an interview and reject the resumes that do not meet the requirements.
The world changed with the pandemic and so did the traditional face-to-face. Virtual interviews have an advantage eliminating the assessment of body language. However, everything now depends only on verbal cues.
This is it!
This is the round that usually makes or breaks the deal about a candidate getting offered the role.
The process usually differs by role, company, and level of expertise needed. Panel rounds are usually a backdrop to get a realistic preview of how candidates perform under stressful, tough conditions. You might also work on a case study at home and present it to a panel or write code during the interview.
Once you complete your interview, you may want to keep in touch with the interviewer.
I personally always did a quick analysis after an interview and noted down what went well and areas of improvement as a pivot for subsequent interviews.
With the above statements, that‚Äôs it from my end for this blog post. Thank you for reading! I hope you found the blog interesting. Let me know in the comments about your experience with the interview process, your journey in data, and what are looking for in 2022!
If you enjoy reading stories like these, consider signing up to become a Medium member from this link.
Happy Data Tenting!
Rashi is a data wiz from Chicago who loves to visualize data and create insightful stories to communicate business insights. She‚Äôs a full-time healthcare data analyst and blogs about data on weekends with a good cup of coffee‚Ä¶
TDS Editors
in
Towards Data Science
Mehmet Ali Korkmaz
in
BBM406 Spring 2021 Projects
Duygu ALTINOK
in
Towards Data Science
Cassie Kozyrkov
in
Towards Data Science
Federica Fragapane
Data Thistle
Christopher Tao
in
Towards Data Science
Plotly
in
Plotly
"
20,https://medium.com/towards-data-science/the-complete-roadmap-to-becoming-a-data-analyst-with-no-previous-experience-952c5b3a7cbc,The Complete Roadmap to Becoming a Data Analyst With No Previous Experience,"The Great Resignation is the perfect time to leave your unfulfilling career behind to become a data analyst.
Never before has there ever been such a mass exodus of people looking for better opportunities that encompass room for advancement, flexible working conditions, respect in the office, and fair wages.
With so many individuals looking for a fresh start, it‚Äôs no wonder that many of them are looking to the alluring world of tech to provide the change they need. Data science, analysis, and engineering are maintaining a foothold as some of the most sought-after jobs for those looking to make a career change.
While you may think that the three disciplines mentioned above are the same, they differ significantly in their qualifications and job duties. For those looking to make a quick switch to a different field without having to spend years studying, becoming a data analyst may be the right fit.
Data analysts are gifted with the ability to tell a story using data. Taking existing data, they use tools such as SQL, Excel, and data visualization tools (such as Tableau) to produce visualizations and reports that describe in laymen's terms the current landscape of the data. Job duties include determining business problems, pulling data from databases using SQL, conducting analyses and producing visualizations using Excel and visualization tools, and developing reports that present the findings of the analysis for stakeholders. Simple coding skills in R or Python may also be required depending on the job, and simple mathematical skills in arithmetic, algebra, statistics, and probability are standard.
With data analyst positions treading the line between a new challenge without having to spend years in school, it‚Äôs a no-brainer for those looking to make a career change.
Most roadmaps toward any data-focused position or career change are often so extensive that they would take a couple of years of full-time study to complete. The average worker takes 11 months to plan a career change, leaving most of these roadmaps out of touch with what people need. Furthermore, external factors, such as the amount of savings and life commitments, can make it impossible to complete a career change in an amount of time that is feasible for most, leaving many to stay at their previous jobs. Considering all of these factors, this roadmap has fine-tuned the process of becoming a data analyst into seven short phases that can be completed as quickly or as slowly as necessary.
Remember, to make a career change you only need to know enough to get a job. In-depth learning can take place on the job once you have satisfied the basic requirements and can carry out the daily tasks.
The following seven phases should be followed in approximately this order. Within these sections, you will find relevant online courses to get you started, as well as key things to focus on in each phase.
Programming requirements for data analysts can vary widely across fields and positions. However, most data analyst positions can be fulfilled with knowledge of Excel, SQL, and some simple Python or R.
While Excel may seem old school, it‚Äôs still a much-used and very valuable tool that is easy to learn and produces quick results that explain the current state of the data. Excel is mostly used for analyzing the data to determine summaries and trends. SQL is used to retrieve data for analysis from databases that can then be worked on using Excel or a programming language. Python and R are, essentially, the two go-to data analysis programming languages, with Python being more popular than R due to its ease of use.
Online courses you can take to learn programming skills:
Below is a selection of courses that can be used to quickly learn the fundamentals of the programming required for data analysis. By no means is this an extensive list or the only resources out there for learning how to code, but these resources give a good starting point. Remember, this is a roadmap to get you the basic skills necessary to be able to fulfill a data analyst position and carry out their daily tasks, therefore breadth of programming knowledge is the goal here, not depth. It‚Äôs also important to note that most data analysts pick between Python or R and make that their primary programming language. Make your decision based on that which is used most predominantly in your target industry.
End of phase objectives:
At the end of this phase, you should be able to write code that runs in your chosen target languages. Extensive knowledge of each language is unnecessary except for the parts that are necessary to conduct an analysis.
It‚Äôs important to note that programming is a task best learned through doing. You can sit and watch these videos and claim to know how to code by the end of them, but there is a distinct difference between watching and doing. Therefore, it‚Äôs advised to work along with the videos and then practice writing your own code afterward to see if you can still make it work. Further practice in writing code will occur in later phases.
If you completed high school/secondary school, or the first couple of years of college or university courses, you will have generally learned all of the mathematics you need to know to become a data analyst (field and position-dependent). Cool, right? Unlike data science where you need to have a deep understanding of linear algebra, calculus, and further mathematical disciplines which can take years to master, most data analyst positions will require just the basic mathematics that you learned early on in your education.
Therefore, this phase should just be a review of what you have already learned and possibly knocking the cobwebs off your statistics and probability knowledge. The courses below are from Khan Academy, a favorite for learning math through bite-sized, easily digestible lessons that explain concepts in simple terms for individuals at any level.
Resources you can use to refresh your math skills:
www.khanacademy.org
www.khanacademy.org
www.khanacademy.org
End of phase objectives:
At the end of this phase, you should be able to carry out simple calculations that could be used to find insights into data. For example, you should be able to calculate the mean value of salaries to determine what a company is paying its staff on average. Alternatively, you need to be able to determine who the outliers are in an age group of people who subscribe to a website newsletter. Or, you need to be able to calculate unknown values in a data set that is looking to determine the velocity of sparrows but only gives the distance and time of their flight.
The mathematical skills developed in this phase will be practiced and enhanced in later phases.
Data analysis is the meat and potatoes of what a data analyst does.
Data analysis can be broken down into five steps that are universally recognized.
Resources to help you learn the data analysis process:
End of phase objectives:
By the end of this phase, you should be able to conduct a full data analysis following the steps listed above using the programming and mathematical skills learned previously. Complete fluency in these stages is not required yet as there will be a phase later on where portfolio projects will be created that will enhance the skills learned here.
While data visualization is taught as part of the five steps of data analysis, it gets its own special phase here because it plays such a large part in the daily tasks of data analysts. Being able to create compelling, accurate visualizations is one of the fundamental cornerstones of this career, therefore special attention should be paid to these skills.
Tableau is one of the most common tools used by data analysts to create visualizations, and resources to learn it will be included below. Other data visualization tools include QlikView, Microsoft Power BI, Datawrapper, Plotly, and more that may be relevant depending on your target field or position. This roadmap will focus on one of the most prominent, Tableau.
Resources to help get you started with Tableau:
End of phase objectives:
By the end of this phase, you should be able to create data visualizations that accurately represent the findings of your data analysis. These visualizations must be eye-catching, easy to understand, and must provide a clear picture of the results without skewing the information visually.
Industry knowledge is the industry-specific tool you will use to solve problems daily.
For example, a background in forestry practices and forest management is important if you are to be conducting data analyses on how many trees should be cut each year to ensure a healthy forest grows. Alternatively, you need to have the business acumen to be able to determine if a company is in trouble financially based on the results of your analysis. Furthermore, if you were to work as an analyst for a hospital that is seeking to find the most efficient way to schedule staff during busy periods, it‚Äôs important to have an understanding of the important roles that each person plays in a hospital.
Essentially, you have the skills to conduct data analyses, but now you need to gain the skills and knowledge to be able to apply them to a specific industry. Whether you‚Äôre entering a completely different industry or are wanting to return to your current industry as an analyst, it never hurts to brush up on industry knowledge. Studying the latest can help keep you relevant as an analyst who can identify trends, see patterns, and improve the efficiency and effectiveness of those around them.
Gaining industry knowledge can be done by reading journal articles, attending lectures at universities, listening to podcasts, reading newsletters, talking to people in the industry, and more.
End of phase objectives:
By the end of this phase, you want to be able to conduct analyses that are relevant and provide insightful conclusions to problems in your target industry. For those looking to work in business, this means conducting analyses to determine the cost-benefit structure of a company decision. For those looking to work in the sciences, this means being able to disseminate conclusions from piles of evidence that may or may not support a hypothesis. For those looking to work in healthcare, this means being able to conduct analyses that provide insight into the working conditions required to maintain a happy staff. The point is that you need to be comfortable analyzing the data of a particular industry and be able to provide conclusions to the problems you are trying to solve.
Building your personal portfolio is the fun part of this roadmap.
Here, you get the chance to build personal data analysis projects that showcase your skills and prove to employers that they want you on their team. These projects should include all of the skills you‚Äôve learned thus far and should be relevant to your target industry. This means that they use the tools of the industry, draw meaningful conclusions, and perhaps even add to the knowledge base of the industry.
These projects can be unique, can be inspired by those seen on the internet, or can include pro bono work done for a small company in your area. A quick browse of the internet can give you a bunch of ideas and datasets you can use to create your own projects.
Resources to get you started in building your data analysis portfolio:
careerfoundry.com
www.kaggle.com
End of phase objectives:
By the end of this phase, you should have a portfolio that is full of data analysis projects that can be shared with potential employers. This portfolio should showcase not only your skills but also your best work. Portfolios are great tools for employers to see what you can do for their company, and can also provide beneficial talking points during interviews. Not only that, but portfolios give you the chance to put all of your skills together to produce projects that yield results.
The final phase of our roadmap brings us to networking, job applications, and technical interview prep.
You have the skills.
You‚Äôve built your portfolio.
Now you just need to tie it all together and get that job!
Networking will involve attending conferences and industry events, talking to recruiters, and rubbing elbows with people who are currently analysts who may be able to put in a good word for you. While no one likes networking, it‚Äôs one of the surest ways to get a job in the industry without prior experience. If a recruiter or hiring manager can meet you in person, the chances are high that they will hire you on attitude and teach you the rest later.
Job applications will come next, followed by preparing for technical interviews. While these two aspects are beyond the scope of this article, several resources will be listed below to give you a start in the right direction. In terms of both, it‚Äôs better to over-prepare so that on the day of, you can relax and enjoy the experience. Job applications and technical interviews are great ways to find out more information about the position, what skills you‚Äôre lacking, and the types of technical interview questions you should be prepared to answer. While tedious, this phase is a great learning experience that will further cement everything you have already learned.
Resources to get you prepared for networking, job applications, and technical interviews:
www.anaconda.com
www.indeed.com
www.simplilearn.com
www.hackerrank.com
End of phase objectives:
By the end of this phase, you should, ideally, have a new job as a data analyst!
All of your hard work should be paying off and the light at the end of the tunnel should be shining.
It‚Äôs important to remember that it‚Äôs normal to have to go through several interviews with many different companies when trying to break into a new role you‚Äôve never done before. The trick is to keep moving forward no matter the results and try to learn what you can from each experience. The difference between those who get data analyst jobs in less than a year and those that don‚Äôt is that successful individuals keep pushing forward no matter how many rejections they suffer. At the end of the day, you know that you‚Äôre capable of becoming a data analyst, you just need to find the right company that agrees with you.
Carrying out a career change into a new field that you have no previous experience in is an impressive endeavor.
While it no doubt seems intimidating, the roadmap toward becoming a data analyst is more manageable than most. Without having to worry about abstract mathematics, incomprehensible programming, and difficult concepts such as machine learning and artificial intelligence, you open yourself up to instantly becoming a storyteller and a key figure in the decision-making process of companies.
As mentioned previously, this is by no means a comprehensive list of resources that you can use in becoming a data analyst. The goal of this roadmap is to give the bare bones approach that can be completed through part-time study in a short amount of time to facilitate a quick career change. Some individuals may require more resources and time while others require less. However, by following this roadmap, you can begin to form your path toward becoming a data analyst.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Paolaliu
Neil Kakkar
Will Richardson
in
Modern Learning
OIF Ventures
Devansh Mittal
in
Devansh Mittal
CrackDAT Dental Admission Test
Maya Sundararajan
Christina Morgan
in
TruBeacon
"
21,https://medium.com/towards-data-science/how-can-an-aspiring-data-scientist-find-and-work-on-real-world-projects-48bed91af008,How Can an Aspiring Data Scientist Find and Work on Real-World Projects?,"As an aspiring data scientist, one of the best ways to gain experience and improve your skills is to work on real-world projects. This will not only allow you to apply what you have learned in a practical setting, but it will also allow you to learn new things and develop a portfolio that you can use to showcase your skills to potential employers.
The truth is almost all of us know the value of real-world projects. You‚Äôll always work on these when you get a job, but sadly most data science positions ask for experience. This puts us in a frustrating loop: you need real-world experience for the first job and your first job for real-world experience. So the question is, how do you break out of this loop?
Before I start giving suggestions, I want to be honest with you.
None of what I‚Äôm about to outline is easy. If it‚Äôs easy, we would have all 4.8M learners of the most popular machine learning course become data scientists. It need not be for everyone. But if you follow the suggestions and put in the work to go beyond the online courses, you can grow as a data scientist.
With that honest conversation, let‚Äôs approach this in a step-by-step fashion, assuming you have just finished a bunch of online courses and wondering what‚Äôs next for you.
You can start by searching for data science meetup groups in your area on meetup.com or eventbrite.com. Alternatively, you can follow the social media page of the nearest local auditorium or co-working space where generally these meetups are hosted. That‚Äôs what I did.
I remember a new co-working space called Hatch Works came up in my early days. I would attend every data science meetup hosted there since it was convenient. 99% of the time, the senior professionals in the AI industry would present their work, distil an evolving topic, and even discuss job opportunities in their organization.
I leave most meetup events with fresh project ideas that I want to add to my personal projects, try at work, or at least write about.
Being an active member of your local community/meetup groups:
It‚Äôs called step zero because you can start on it today‚Ä¶well..now!
If you have just completed a few online courses, you are not ready for real-world projects. Nobody is.
The data is insufficient to train a machine learning model. Even if you have the data, it is either messy, imbalanced, missing, or has privacy concerns. This has been the AI industry‚Äôs most pressing bottleneck for ages now.
Why not settle for something more beginner-friendly and transition to real-world projects slowly? Check out online data science competition websites like Kaggle and DrivenData.
Pick one of the past competitions based on your existing knowledge. The dataset is generally from actual companies but cleaned and masked to hide private information. They have solutions and guided tutorials, which can help you get used to data pre-processing, analysis, training and tuning machine learning models.
Most beginners directly attempt the existing competitions and are disheartened when they‚Äôre nowhere in the competition leaderboard. They start doubting their skills and go back to do a yet-another online course.
Common mistake.
Our purpose in using Kaggle or DrivenData is only to get used to solving machine learning problems. Once you‚Äôve had a go at one or two past competitions, only then attempt the live competitions. You never know; the results may surprise you.
I‚Äôm not a fan of unpaid internships, but contributing to open-source or volunteering for AI for Good projects is something I continue to do.
Many organizations, such as non-profits and government agencies, need data scientists to help them analyze their data. Volunteering your time to work on a project for one of these organizations can be a great way to gain experience and make a positive impact at the same time.
The choice of organization and the cause you want to support entirely depends on you. I have personally worked with Omdena, where beginner-to-expert AI practitioners work towards solving end-to-end real-world problems. Volunteering was straightforward, with an application and a short interview with the Founder.
Apart from AI for good, contributing to open source has several other benefits. They:
You asked for real-world projects ‚Äî you‚Äôve got it here in its best form.
Learning doesn‚Äôt have to stop while you seek real-world projects.
Udacity is my go-to platform when I want to acquire skills and build a portfolio simultaneously. I‚Äôve completed three nano degrees so far, and the absolute best for data scientists is, unsurprisingly, the data scientist nano degree (thanks for using my Udacity affiliate link, meaning I get a small commission if you choose to purchase at no extra cost to you, but I paid for mine in full.) The projects you‚Äôll work on are from partnered companies sharing their data which is why they require you to sign data usage agreements.
If you‚Äôre still a student or find their nano degrees expensive, you might want to check out their scholarships page. They regularly run scholarship programs, and chances are you‚Äôll be able to apply for one right now.
Most are surprised when I reveal this: all of their projects included in the nano degrees are open source on their GitHub, waiting for you to be completed and add to your data portfolio.
Don‚Äôt believe me? Here‚Äôs the one for the data scientist nano degree.
End of the day, nothing can replace the experience you get at a proper job. That‚Äôs a fact.
Since you‚Äôre an absolute beginner, consider starting with a data science internship or taking on freelance or contract work that involves data analysis and visualization. Almost all my freelancing work came from inbound leads who saw my work online and asked if I could help them. These experiences can demonstrate your skills without yet having a job.
Eventually, to convince an employer to hire you, you need to demonstrate your data science skills. It‚Äôs as simple as that. Steps 0‚Äì3 are mainly for this purpose. I have written an extensive guide to help you get your first internship. To keep it short for this article:
See, I understand it‚Äôs overwhelming at the start. The hardest part is getting stuck without a direction. By reading this, you‚Äôve overcome that. And if you genuinely enjoy data science, you‚Äôll be up and running in no time.
Ultimately, being proactive and persistent is the key to finding and working on real-world projects as an aspiring data scientist.
By joining data science meetup groups, participating in online competitions and forums, volunteering your time on impactful problems, and working on personal projects, you can gain valuable experience and build a strong portfolio of work that will help you stand out as a data scientist.
The bottom line of getting a job comes down to understanding that all recruiters are looking for candidates with proven experience, irrelevant of the level.
Even when I interview now, after 4+ years of experience, I have to showcase my ability to lead teams that solve business problems by delivering scalable machine learning systems. This is a journey in which you constantly improve and prove yourself.
Play the long game.
Offering you the job you want only becomes a no-brainer for the recruiter.
For more helpful insights on breaking into data science, honest experiences, and learnings, consider joining my private list of email friends.
If you value articles like these and want to support me as a writer, consider becoming a Medium member. It‚Äôs $5 a month, giving you unlimited access to stories on Medium.
arunnthevapalan.medium.com
Zohar Hod
Kritika Rashi
in
MiQ Tech and Analytics
James Ferguson
in
kWIQly
Levi9 Serbia
in
Levi Niners Crafts
Bhanu Yerra
in
Towards Data Science
Chloe Gillham
amr zaki
in
HackerNoon.com
Sahil Uppal
in
Towards Data Science
"
22,https://medium.com/@ritatynaokonkwo/life-as-an-entry-level-data-analyst-dda9ece75cbe,Life as an Entry-level Data Analyst,"Before I got a job, I always wondered what it would be like working as an entry-level data analyst with no prior experience except in projects. If you have ever had the same thought, then this article where I shared my three months of work experience as an entry-level data analyst would interest you.
I got a job as a data analyst four months into my learning journey. I had worked on a project that got a lot of people, recruiters inclusive interested in me, and a couple offered to interview me for a role.
The company I work for reached out as well, requesting to interview me for an open role in their company. The CTO of the company had seen my project and thought I would be a good fit for the role.
During the interview, I felt less pressured compared to those I have had in the past simply because they had reached out to me.
The interview process was a very interesting phase as I had up to three conversations leading to the time I received my offer letter.
In the first conversation, we talked about my skills as a data analyst and how I can add value to the company and contribute to a new project currently being worked on.
The second one was a take-home assignment to analyze a project and also give recommendations which I submitted the next day.
The last conversation was generally about what my duties would be in my new role and also my take-home expectation.
Something that stood out for me and made me happy was when, at the end of the call, my recruiter said;
‚ÄúMy takeaway from our conversations in the past couple of days is that you are passionate about storytelling with data and you understand the industry quite well‚Äù
After this, my offer letter was sent and I resumed almost immediately.
From the time of the first conversation to the receipt of my offer letter was approximately two weeks.
My first day was really exciting; it‚Äôs a remote role but there is a mandatory stand-up for every staff daily. I was introduced to the team. Everyone was nice and the atmosphere was quite friendly. This eased off the tension I felt before the meeting as I did not know what to expect. I didn‚Äôt do much on that day except try to set up my accounts on most of the platforms the company uses.
Some of these platforms were new to me so I had to ask for help from a colleague. I also spent the first week, maybe the whole of the first month studying the company‚Äôs process and policies.
By the end of the month, I was already getting busy with the company‚Äôs data. I did a lot of dashboard reporting every week and I was also in charge of gathering data every day.
Some of my day-to-day tasks involve; downloading CSV files, analyzing and updating google sheets, creating reports, and performing probability analysis.
I work closely with the IT team; Devs and Designers, collaborating on ideas and sharing insights on the next step for the company‚Äôs growth.
In the first month, I worked mostly with Google sheets and Power BI and whenever I needed to scrape data online, I used Python.
Recently, I started working with SQL and it‚Äôs been quite challenging for me, but someone on the team has been quite helpful in explaining things I should do and how to go about it.
I have also learnt how to use two new tools; Asana and Redash.
To be very honest, it‚Äôs been extremely exciting for me. I have learnt new kinds of stuff and the team I work with is accommodating and always ready to assist.
I would say the major difference between the project world and the real-life world is that you get to solve real-life problems and know that whatever the outcome of your analysis is, could make or mar the business.
Achievements in the past three months;
Tips that have helped me these past three months;
Mistakes and Challenges;
P.S: This is my personal experience and doesn‚Äôt mean it‚Äôs the same everywhere, it‚Äôs just to give you an insight into what my first three months as a data analyst have been like and the challenges I have faced.
I hope this was a great read. Are you a data professional? How were the first three months for you? leave a comment below.
If you are yet to land your first data analyst role; don‚Äôt fret, and don‚Äôt give up. Keep working on those projects, and keep applying, it won‚Äôt be long. You will be sharing your first month‚Äôs tips with me as well.
Click here to view the project that got me the interview.
If you enjoyed reading, follow me on Linkedin and Twitter for more stories.
Andre Vianna
in
My Data Science Journey
Jean Yves
in
Towards Data Science
Jun Gao
Jonny mcgurk
SangGyu An
in
CodeX
Takuma Fujiwara
Lychee Zhirun Li
Raven Welch
in
Analytics Vidhya
"
23,https://medium.com/@ritika_DataAnalyst/my-resume-which-landed-me-data-analyst-job-in-6-months-3056557bf145,My resume which landed me Data Analyst Job in 6 months,"Tips along with real snapshots of my resume!
If you‚Äôre a fresher and after seeing my resume you are thinking that how can I make my CV so attractive like her? or would you be able to make your CV like this ? Then I‚Äôm going to share with you every tip that is needed to reach this stage where you have so much to add on your resume.
I have seen many resumes which does‚Äônt have GitHub profile, LinkedIn profile etc. Just remember one thing, the hiring manager will not contact you to see your projects, he/she will just move on with the CVs which have better information.
Don‚Äôt put your entire address in your CV, they just want to know the city where you‚Äôre currently in. Thats‚Äôs it!
‚Äòritikajaiswal5768678@gmail.com‚Äô or ‚Äòritikajwork@gmail.com‚Äô
Which one looks better to you? Then just make your professional gmail id no? It just makes life easier, whenever you are applying to jobs, you have to check only this email_id that‚Äôs it.
The summary that I have written is a mixture of a lot of adjectives :) because I wanted to impress them. Add every adjectives possible even though it doesn‚Äôt define you. Remember that everyone does that to sell their skills.
I have done n no. of mistakes and one of the mistakes where ‚Äî I have‚Äônt put my educational details in my resume. Why? Because I thought that they will just come to my LinkedIn profile and see my qualification. BOOM!! BIGGEST MISTAKE.
Remember that they have to look to 1000 of CVs to fill a vacancy, if you‚Äôre making them work more harder they will ‚ÄòDelete‚Äô you from their option. See that I have also written subjects I have studied in my Undergrad, because not everyone is aware of the subjects people teach in Economics honours right?
Very important, Even though if you know SQL, just write ‚ÄúSQL, PostgreSQL, Oracle,etc.‚Äù In order to fill the space don‚Äôt write things like ‚Äî ‚ÄúCommunication skills, interpersonal skills, etc.‚Äù These are skills but not that skills on the basis of which you‚Äôre going to be selected. Be specific!
Try to add the correct year of graduation too! I have mistakenly written 2018-Present :) Reason being that companies want to hire people who will stay for a longer period of time.
I haven‚Äôt used STAR Method to describe my projects because I wasn‚Äôt aware of it. So, I made strategies like
Tip: Try to put projects which you‚Äôre super proud of at the top. Provide Project links so that they can visit your project once. And don‚Äôt forget the dates (I have made all the projects on one single month, but I have written it to show them that all these 6 months, I have been super productive)
In reality, it‚Äôs not my experience, it‚Äôs internship + freelancing experience, so one of the recruiters who called me for an interview told me that I shouldn‚Äôt write this as an work-experience because the company might ask for pay slip etc. which I don‚Äôt have because it‚Äôs doesn‚Äôt considered as a experience.
In this section, If you have done any internship or freelancing then you can be transparent about what tools you have used to gain that experience, In my case it was PowerBI. I have been very detailed about the work I have done and contributions I have made.
PS: Change your designation for the different roles, For example if you‚Äôre applying for jobs like Business analyst then replace Data Analyst, indeed you can make different CVs for different roles. Just save your profile like ‚ÄúRitika Jaiswal.pdf‚Äù not like ‚ÄúRitika Jaiswal CV.pdf‚Äù or ‚ÄúRitika CV.pdf‚Äù. HRs only have CVs in their pdf list so don‚Äôt do that.
End ‚Äî Don‚Äôt be harsh on yourself, things takes time. I have build this resume in one month with proper planning on what are the things i want to add. You have to plan beforehand and start looking for opportunities which are relevant to your job. It might be daunting initially because you might have to struggle to learn GitHub but be patient everything will fall in place.
I have not used ATS format for making my resume, It‚Äôs just my original experience, you can google it and follow ATS Format if you want to.
PS: I have used Novoresume.com for making this template, there are n no. of templates available, you can use any of them.
All the best üëçüèª You can rock too ‚úåüèªüòâ
www.linkedin.com
Shaon Majumder
Amol Pawar
John Rynecki
Pranshu Mahajan
in
Serious Scrum
Sahil Gupta
in
unpack
David Romanis
Catherine Cunningham
Gijs Nelissen
"
24,https://medium.com/towards-data-science/how-to-learn-data-science-if-you-want-to-accomplish-your-goals-619b16552d77,How To Learn Data Science If You Want To Accomplish Your Goals,"Periodically, I like to refresh my advice about how I would go about learning data science from ground zero. The data domain is changing rapidly, and, as my own knowledge grows, I think it is important to re-evaluate the approach that I would recommend.
My advice for learning data science has changed since my previous efforts at imparting this wisdom. In this article, I give you my updated approach about how I would take on learning this field and have some fun along the way. As a disclaimer, I don‚Äôt think there is one ‚Äúcorrect‚Äù way to learn data science. Different things work for different people, and your own experimentation is integral to your success in any career. Be sure to read until the end because I answer the most commonly asked question I get on this topic: how long does it take to learn data science?
If you prefer a video format, watch here:
Starting off, I want to debunk the idea that you can ‚Äúlearn data science.‚Äù This implies that data science is a static subject that can be learned in its entirety. For better or for worse, data science is constantly evolving and growing. I don‚Äôt know a single person, including myself, that could possibly know the whole field. Learning data science is a journey, not a destination, and coming in with this mindset can make this process far more enjoyable for you. I see so many students who get overwhelmed by how large the field is. It will 100% be completely overwhelming if your goal is to learn the entire domain. On the other hand, it becomes manageable if you focus on just learning a little at a time and growing your knowledge with specific smaller goals in mind.
With that being said, let‚Äôs jump straight into something that I would change about my prior approach. When looking back, my advice about the very beginning has been extremely vague. I usually say something like ‚Äúlearn enough Python and statistics to get started with projects.‚Äù While this isn‚Äôt bad advice, this time around I want to get more into the weeds about exactly how you should start this learning process.
The real first step is getting an understanding of the components of the field and creating a learning program for yourself to navigate the journey. If we just jump into the ocean without any clear direction and start swimming, we get tired out really easily and may just give up. On the other hand, if we have a map and a clear objective, we at least know what we are getting ourselves into. You need to create this map for your data science learning before you do anything else. The really cool part of this is that by creating this map, you will also learn a lot about the field of data science in general.
So, how would I create a map? There are a couple ways to do this that vary in the level of overhead work. The easiest way is to take an online course or certificate that lays it out for you. That is the huge benefit of online courses: they can lay out the entire learning path for you so you just have to follow along. On the other hand, they cost money. All the information is out there for you to learn for free if you are willing to put in the time to create this roadmap for yourself. To be clear, there is no right or wrong here. Whether or not you decide to pay an organization, do what‚Äôs worth it to you. If courses are your speed, I have a discount code for 365 DataScience in the appendix below; if not, I‚Äôve included some links to my favorite free resources.
The next way would be to look at multiple online courses, university courses, and other resources and to get a general feel for how they lay out the path. Most paid courses let you see how the offering is structured. You can then make your own roadmap based on the classes and concepts that you see there. By doing this, you also get a feeling for what skills and techniques are needed in the domain. The fun here is you get to see what is most interesting and appealing to you and adapt your roadmap to that. This is going to be really important later on.
Ok, let‚Äôs set up what my learning plan would look like for myself. I encourage you to do your own research here and adjust this based on your interests and aptitudes.
If I were to lay out a learning plan for myself, I would almost certainly start with learning Python. Coding languages allow you to build things. If you can build things, you can apply almost anything you‚Äôre working on to a real problem. I look at learning programming like building out my toolset. I could build a shed with just my bare hands, but it would be a heck of a lot easier with a hammer and a drill. Python is my power tool. I‚Äôve personally almost always felt that coding is what held me back from picking things up faster compared to math. To be clear, math is very important, I just wouldn‚Äôt personally focus on it first.
For the programming, I would make sure I had a solid understanding of the basics like variables, loops, and functions. I would also really focus on learning how to use imported libraries like pandas. In fact, I recommend looking through as much of the pandas documentation as possible. In my mind, coding for data science isn‚Äôt really coding. You are more leveraging tools that other people built that serve a specific purpose. For example, I think having a really great understanding of pandas would serve most data scientists better than having a phenomenal understanding of pure Python. I‚Äôve included some free and paid resources for learning Python in the appendix below. Additionally, if you are interested in my approach for learning coding, check out this article I‚Äôve written on the matter.
Previously, I would have told you to get started with projects right after this. In this article, I would recommend an extra step for most people. I‚Äôve gotten a ton of feedback that most people don‚Äôt know where to start with projects after learning some of these basic skills. There is a really good solution to this. Look at other people‚Äôs projects. You can go on Kaggle and see the other projects people have done. You get to see all their code and all the comments they leave about their thought process. To me, this is an absolute goldmine. You get to have a front row seat to how brilliant data scientists approach a problem. I‚Äôm definitely not brilliant, but here‚Äôs a video of me walking through the Titanic dataset:
When starting with projects, they don‚Äôt have to be original. You can go through the exact same analysis that someone else did and still learn something. A typical learning session could just be you having someone else‚Äôs project on half the screen, and typing it line by line and running it on the other half of the screen. As you do this, you can change the parameters, experiment with the different visuals, and see how it all works as you go. Obviously, you shouldn‚Äôt take credit for this work or publish it as your own, but you can absolutely learn from it in this way. Many people think that they won‚Äôt learn anything with this approach, but I personally do this and it is the single thing that has taken me the furthest recently.
While you‚Äôre going through these different workbooks. You‚Äôll inevitably start seeing different tools, algorithms, and techniques that you aren‚Äôt familiar with. You should be taking note of these and doing research about what they are.
Ok, now you‚Äôre getting familiar with the process. I recommend starting to get familiar with some of the statistics and algorithms you will be using. You will want to have a solid foundational understanding of statistics (e.g., central tendency, probability theory, etc.), linear algebra, and calculus (you could probably wait a little for this one). Start learning about what the difference is between classification, regression, and clustering algorithms, and start thinking about the types of problems that you can solve with these. Are there datasets that you are seeing that you could apply these algorithms to? Are there questions that you have that could be answered in one of these categories?
This is where projects would become the main focus of my learning. I would do as many projects that I could find. I would do them on Kaggle, with my own data, with any data I could find really. In the appendix below, I have added some data science project video playlists.
My friend the Data Professor says that the best way to learn data science is by doing data science, and I couldn‚Äôt agree more. Projects are the first place where you are doing real data science. Earlier, I mentioned that being introspective about what parts of data science are exciting to you was really important. This is where it plays a key role. In the early stages, you should focus your projects on things you‚Äôre interested in. The most important thing you can do with a project is actually make progress on it. If you‚Äôre excited enough about the topic or the techniques you‚Äôre using, you increase the odds that you learn as much as possible.
After learning the basics of Python and doing some projects, the world is really your oyster. I recommend doing more projects that are focused on skills that you have found to be relevant to your own journey. For example, in most companies, SQL is really important. If your goal is to get a job, it could be very worthwhile to pick that up. I don‚Äôt start with SQL because I think it is very easy to learn compared to Python and, if you can learn Python, you should be able to pick up SQL pretty quickly. If you‚Äôre fascinated with image analysis, you should probably direct your learning and projects towards deep learning or some of the other techniques there.
As you can tell, after a certain point in time, you really need to adapt your plan to fit your exact interests and aptitudes. You probably don‚Äôt want to hear this, but this is something you need to do for yourself.
And that is all there really is to it. If I want to learn a new skill or technique after this point, I read up on it and try to apply it as quickly as possible. Your projects and your work become a reference for how you have used many algorithms or techniques in the past.
As you grow, your iteration loops become tighter and you want to focus more on good learning habits than anything else. I created the #66DaysOfData to help perpetuate good habits on this process. You‚Äôre welcome to join in the initiative any time, I‚Äôve left some links below in the appendix about what it is.
Most of you are probably wondering: How long does this process take? And that is a very difficult question. To be honest, I think you can get a good understanding of the basics and do projects in as little as 3 months. Most people will probably take around 6 months, though. I really don‚Äôt recommend focusing too much on how long it takes. This is a lifelong learning process, so it is not important if you learn it in 3 months, 6 months, 1 year, or even 5 years as long as you acquire the knowledge.
One thing I want to end on is the concept of goals. When you create your roadmap, start thinking about your goals for learning. What concepts would you like to learn, what analysis would you like to do? Most people shouldn‚Äôt be learning data science just to know the material, it should be what you want to use these skills to achieve. Have these things in mind when you learn, but don‚Äôt be afraid to adjust accordingly. How could you possibly set accurate goals if you know so little about the field in the beginning? Your goal setting, your projects, and your learning have to evolve as you continue to grow in the domain. I see so many people getting disappointed that they didn‚Äôt accomplish what they set out to do when they really had no clue what they were actually setting out to do to start with.
It may be some extra work, but I recommend reading this article again and think about your learning plan. Share your plan and goals below so that we can all keep each other accountable!
If you enjoyed this article, remember to follow me on Medium for more content like this and sign up for my newsletter to get weekly updates on my content creation and on additional learning resources in the data science industry! Also, consider supporting me and thousands of other writers by signing up for a membership.
Thank you so much for reading and good luck on your data science journey!
Resources for Learning Data Science:
Resources for Learning Programming:
Pandas Documentation: https://pandas.pydata.org/docs/
Data Science Project Playlists:
#66DaysOfData: https://www.youtube.com/watch?v=qV_Al...
Fahim Shahriar
DataFromSky
in
DataFromSky AERIAL
Scottie Todd
Mae McKagan
Technical Source
Martin Barakov
in
Towards Data Science
Ajay Kotalwar
Piyush Jain
"
25,https://medium.com/towards-data-science/the-3-pillars-of-math-you-need-to-know-to-become-an-effective-data-analyst-9af50106ffa1,The 3 Pillars of Math You Need to Know to Become an Effective Data Analyst,"The amount of math you are told you should know and the amount of math you will use daily as a data analyst, are two very different things.
Field (and sometimes project) dependent, there are only a few small subsections of mathematics that most data analysts use daily. While most educational programs discuss the big three math topics all data analysts should know (linear algebra, statistics, and calculus), not all fields or positions require in-depth knowledge of calculus or advanced topics.
While learning the more advanced topics in math (such as calculus and beyond) adds valuable tools to your arsenal and teaches you how to problem solve, it can also be a sticking point that may stop you from advancing in your path towards becoming a data analyst.
Therefore, unless your target field or position requires advanced mathematical knowledge and training, it can be beneficial to focus on the essential parts that can fulfill 70% or more of the daily requirements of most data analyst positions. Luckily, you will likely have already studied the three pillars discussed in this article if you have completed a few years of university courses or took advanced math courses in high school or secondary school.
Linear algebra is the branch of mathematics dedicated to solving linear equations for unknown values and is also the foundation upon which knowledge of machine learning is built.
While machine learning may be outside the scope of what a general data analyst will do daily, the fundamental concepts of linear algebra will be used during data preprocessing and data transformation. Additionally, linear algebra teaches you to think logically through a progression of steps which is beneficial when working through an analysis that focuses on answering a particular question or solving a specific problem.
Concepts in linear algebra include vectors and spaces, matrix transformations, and alternate coordinate systems. Vectors can be used in data analysis to calculate how different a prediction from a data set is from an expected output after data transformation. Matrix transformations are used to transform one vector into another vector which allows for data to be represented geometrically in a two- or three-dimensional space during data transformation. Alternate coordinate systems can be used in data analysis to change how datasets are represented visually to ensure that data is appropriately represented.
www.khanacademy.org
towardsdatascience.com
Probability is the study of how likely something is to happen and is essential for drawing conclusions that can help decision-making during uncertain situations. While probability and statistics are linked and are often lumped together for study, the two are used to determine different conclusions.
Probability has many practical uses in all fields, such as determining the likelihood that a recession will occur, the likelihood that a disease is tied to the frequency of a gene, or even the likelihood that a visitor to a website will subscribe to its newsletter.
Two types of probability exist that can be used to look at data sets. Classical probability is the type of probability that involves a set of rules attached to it. For example, you could have a rule that the probability of a customer purchasing from a website must be greater than 0.33 for it to be valid. Relative frequency is the type of probability looking at the occurrence ratio of a single event in relation to the total number of possible outcomes. For example, this could be used to compare the result of a subset of data to the total amount of collected data.
Statistics is the branch of mathematics surrounding data collection, analysis, presentation, and interpretation. Statistics and probability are linked and often taught together, though they yield different answers to different questions. Statistics are the meat and potatoes of data analysis and provide the fundamentals for understanding what data sets tell us.
Statistics can be divided into two types: descriptive and inferential. Descriptive statistics describe the characteristics of a population and can be further subdivided into measures of central tendency and measures of variability which measure different aspects of a population. For example, descriptive statistics can be used to conclude a range of test scores collected or the mean age of people who sign up for a website newsletter. Inferential statistics make predictions about a population based on sample data. For example, hypothesis testing, a type of inferential statistics, can be used to determine whether or not a hypothesis about a population can be accepted or rejected based on a study sample.
towardsdatascience.com
towardsdatascience.com
While this article covers the fundamentals of mathematics that support most daily data analyst work, it‚Äôs not to say that your learning of math should stop there.
Data analysts remain relevant and advance in their careers by continuing to learn. This means expanding your knowledge beyond the fundamentals and taking deeper dives into more complex mathematics that, as mentioned above, will give you more tools in your arsenal and will fine-tune your problem-solving skills.
Mathematics is the way of understanding what data is telling us and helps us draw conclusions that allow us to make decisions moving forward. By furthering your knowledge you give yourself the ability to look at data sets and results from different standpoints and to resolve problems in different ways. Therefore, don‚Äôt stop at the fundamentals but keep building your mathematical toolbox as you continue throughout your career.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Aleksandra Osipova
in
Towards Data Science
Sharan Kumar Ravindran
in
Towards Data Science
Hashmap
in
HashmapInc
Alison Yuhan Yao
in
Towards Data Science
Mallory Hagen
ODSC - Open Data Science
Skyler Dale
in
Towards Data Science
ODSC - Open Data Science
"
26,https://medium.com/design-bootcamp/tools-i-use-as-a-product-manager-4b8022fd2adf,Tools I use as a Product Manager,"The responsibilities of a Product Manager often include, but are not limited to:
This exhaustive list of responsibilities makes it hard for any product manager to perform them manually. Oftentimes, product managers have to rely on various tools in the market to make their lives easier.
In this article, I wanted to share some of the tools that I use/heard of as a PM in Crypto.com and hopefully, it would help you in your current PM roles / PM interviews. If there are any useful tools I missed out on, I would also appreciate it if you could comment so that everyone can learn from them as well.
Objective: To understand the current market and your market competitors. Tools/Resources:
Objective: To understand your customer (their pain points) and their user behavior
Tools/Resources:
Objective: To be fully transparent about your upcoming projects and their respective timelines to your higher management
Tools/Resources:
Objective: As a PM, you will have to work on low-fidelity designs yourself before presenting it to your product designer.
Tools/Resources:
Objective: As a PM, while you are not involved in the actual development, you will need to be aware of the current status of development (e.g. any bugs/releases) and also update relevant stakeholders on it too.
Tools/Resources:
Objective: After you have released your product, you will need to track your key metrics and make further improvements based on the insights provided
Tools/Resources:
These are some of the tools I use on a daily basis as a Product Manager. If you want a career in Product Management, I would strongly advise you to be familiar with some of the tools I listed here.
If there are any other tools that you would strongly recommend, please do leave a note/let me know in the comments. Thanks!
If you like my content, please do give me a follow and subscribe. I would also really appreciate it if you could sign up for a membership using my referral. Thank you for your support!
anjana balakrishnan
in
Centre for Social and Environmental Innovation, ATREE
Porter360
Francis Gonzales
in
Bootcamp
Bithika Mehra
Elena Venieri
Studio Function
in
Studio Function
Aden Chan
Cappasity
in
Cappasity Blog
"
27,https://medium.com/better-programming/tiny-snippets-of-code-that-changed-the-world-fda104afc0d0,Tiny Snippets of Code That Changed The World,"In 1997, Ethan Zuckerman broke the Internet ‚Äî by inventing the pop-up ad.
He was working for Tripod.com, an online service that let people make little web pages for free. To make money, Tripod sold ads that ran alongside the pages. The problem was, ads would wind up running next to racy content ‚Äî a user‚Äôs page devoted to anal sex, say ‚Äî and advertisers did not like this.
Zuckerman‚Äôs boss asked him to figure out some solution. Wasn‚Äôt there some way to position the ads so they weren‚Äôt right next to sexytime user-generated stuff?
That‚Äôs when Zuckerman figured out a weird, hacky solution: When you visited a Tripod page, it would spawn an entirely new pop-up page with just the ad. That way the ad would not be, technically, associated with any particular user page. It‚Äôd just be floating there onscreen.
Here‚Äôs the thing, though: Zuckerman‚Äôs bit of Javascript, that created the popup ad? It was incredibly short ‚Äî a single line of code:
Basically, the Javascript tells the browser to open a new window that‚Äôs 200 pixels wide and 400 pixels tall, with no scrollbar or toolbar on the top, positioned on top of whatever other web pages are currently open.
Simple, but pernicious! Pretty soon, commercial websites had copied Zuckerman‚Äôs innovation, and the Internet was positively infested with pop-up ads. A coder I knew in the early 00s who worked for a download site told me that the absolute vast majority of their revenue came from porn pop-up ads.
You‚Äôve no doubt seen pop-up ads. You no doubt hate them. With luck you use a browser that now suppresses them.
As Zuckerman put it, he had written a single line of code ‚Äúthat made the world a measurably worse place.‚Äù
I ran across Zuckerman‚Äôs story in an essay he wrote for You Are Not Expected To Understand This: How 26 Lines of Code Changed the World. It‚Äôs an incredibly fun little collection of short essays, edited by Torie Bosch, that reflects on pieces of code that tilted the world on its axis.
What fascinated me is how brief most of these snippets are. This runs neatly counter to many pop-cultural assumptions about coding, which typically suggest that important code is massive and sprawling. Hollywood loves to depict ‚Äúprogrammers programming‚Äù with their fingers in an absolute blur, pouring out a Niagara of code. Tales of Google‚Äôs dominance used to refer to its ‚Äú2 billion lines of code‚Äù, as if that sheer heft were responsible for the firm‚Äôs success.
It‚Äôs often quite the opposite. Indeed, Google‚Äôs original innovation ‚Äî the piece of code that vaulted Google above its search-engine peers, back in the 90s ‚Äî was its ‚ÄúPageRank‚Äù algorithm, which calculated the value of web page based on how many other pages linked to it, and the quality of those linking pages. It‚Äôs not a terribly long piece of code; people have done their own versions in Python, and it‚Äôs only a few dozen lines.
Sure, Google‚Äôs overall operations ‚Äî like those of any big tech firm ‚Äî involve thousands of processes to keep things running. That‚Äôs why their total code base grows so huge. But some of the code with the biggest impact can be pretty pithy.
You should read the whole book (or buy it for nerds as a holiday gift), because the examples are fascinating and wide-ranging. There‚Äôs a chapter by Charlton McIlwain on the ‚Äúpolice beat algorithm‚Äù developed in the late 1960s, which tried to predict where crime was mostly likely to occur, so law-enforcement agencies could send more police that way. This, of course, wound up creating a racist feedback loop: Since poverty-stricken Black neighborhoods were already overpoliced compared to white ones, the algorithm directed more policing there, producing more arrests, which convinced the algorithm to send yet more police; rinse and repeat.
The police-beat algorithm isn‚Äôt terribly lengthy; in You Are Not Expected To Understand This, the artist Kelly Chudler draws it out ‚Ä¶
And here‚Äôs another, even shorter piece of code that changed the world: The ‚Äútracking pixel‚Äù.
You probably interact with this code every day, without realizing it, as Lily Hay Newman notes in her chapter on tracking pixels. Basically, it‚Äôs a little bit of HTML that embeds a single tiny pixel inside an email, so small you won‚Äôt see it. But if someone sends me an email with that tracking code, it spies on me. It works like this: The instant I open the message, the code requires my browser to request that single-pixel image. The person who sent the email to me watches to see, hey, has Clive‚Äôs browser asked for that pixel yet? So effectively, the person who sent me the email can tell the instant I‚Äôve opened the email.
Putting a tracking pixel into an email is incredibly easy ‚Äî it‚Äôs this single line:
And here‚Äôs one final, older example! The book has a chapter (written by Ellen R. Stofan and Nick Partridge) on the ‚Äúbailout‚Äù code for the lunar module of the Apollo 11 moon-landing mission. That ‚Äúbailout‚Äù code ran on the lunar module‚Äôs tiny little on-board computer, and was designed to help prioritize things: If the computer got overloaded, it would toss aside all but the most important work.
This turned out to be incredibly important when the lunar module was approaching the moon, because sure enough, the computer got way overloaded with work. The ‚Äúbailout‚Äù code jumped into action, shutting down everything that wasn‚Äôt utterly crucial to landing the module. It even shut down some of the display systems inside the lunar module, which scared the crap out of the astronauts. But it worked: The module landed safely.
Here‚Äôs that code ‚Äî only 22 lines ‚Ä¶
This book is really quite a blast ‚Äî very worth checking out!
I publish on Medium three times a week; follow me here to get each post in your email.
I‚Äôm a contributing writer for the New York Times Magazine, a columnist for Wired and Smithsonian magazines, and a regular contributor to Mother Jones. I‚Äôm also the author of Coders: The Making of a New Tribe and the Remaking of the World, and Smarter Than You Think: How Technology is Changing our Minds for the Better. I‚Äôm @pomeranian99 on Twitter and Instagram, and @clive@saturation.social on Mastodon.
aruva - empowering ideas
Shohei Kihara
Bharat Gameing
Eric Tsiliacos
akhil anand
in
DataDrivenInvestor
Sushmita Singh
in
Code  Writers
Karol Filipczuk
Alex Kuznetsov
in
Startup Everyday
"
28,https://medium.com/@kozyrkov/introducing-chatgpt-aa824ad89623,Introducing ChatGPT!,"Today, I want to demystify ChatGPT ‚Äî a fascinating new application of GANs* (Generative Adversarial Networks) that has been generating a lot of buzz in the AI community.
ANAI: An 'All-in-One' No Code AI platform
13D Research
in
13D Research
Fetch.ai Network
in
Fetch.ai
Team Snorkel
in
Snorkel AI
Stefan Kojouharov
in
Chatbots Life
Stefan Kojouharov
in
Becoming Human: Artificial Intelligence Magazine
Kabita Maharana
Praful Krishna
in
Artificial Intelligence in Plain English
"
29,https://medium.com/towards-data-science/10-pieces-of-single-sentence-job-advice-for-new-data-science-graduates-ffbca3454df2,10 Pieces of Single Sentence Job Advice for New Data Science Graduates,"You made it.
You graduated/completed a bootcamp/finished a certificate/taught yourself, and now, finally, you‚Äôve become a data scientist. After many exhausting months or years, you can officially call yourself a data scientist and begin applying for data science jobs.
Your hard work and effort have been worth it and you can now take those skills and begin making an impact in a company.
Now, though, it‚Äôs time to absorb just a little bit more information. Your brain is packed, but this community of data scientists with years of experience under their belt has something they want to share with you: single-sentence job advice for new data science graduates. Here, you will find 10 small pieces of advice to always keep in your back pocket that will help you succeed in your career as a data scientist.
When 500 new data scientists apply for the same entry-level position, what do you think sets one successful candidate apart from the rest?
Is it their ability to think through complex analyses? Perhaps.
Is it their ability to build a machine learning model faster than everyone else? Maybe.
Is it their ability to have the right combination of hard skills that a company is looking for as well as soft skills that will make them the right person for the team? In fact, yes. That‚Äôs exactly what sets a data scientist apart from the rest of the pack.
When competing against 500 other candidates for one data science position, it can be assumed that everyone will more or less have the right hard skills that a company is looking for, such as coding, mathematics, and the ability to construct machine learning and artificial intelligence models.
However, what everyone doesn‚Äôt have, is soft skills. The ability to work as part of a team, communicate, read, write, comprehend, problem solve, make critical decisions, and manage your time wisely, are all soft skills that will get you the job. These skills are what set you apart from all the other candidates and that will make you a valuable asset to a company.
In college, you will always have an appropriate data set with which to solve the problem you‚Äôve been tasked.
In a bootcamp, nice clean data sets will help you develop your skills.
Through your own personal projects, you‚Äôll find hundreds of data sets available online that contain thousands of complete entries which you can use to develop some pretty complex projects.
However, when you hit the real world, you‚Äôll realize that, more often than not, the data sucks. Whether the data set is incomplete, too small, or has too many outliers to make a concrete decision, the data of the real world is often the crux of an analysis you‚Äôve been tasked to complete.
And that‚Äôs okay. Bad data is a fact of life and you can only be expected to develop as accurate an analysis as you can with what you‚Äôve been given to work with.
Solving a problem using data science in school or in a bootcamp is one thing. There, the people that you‚Äôre ‚Äúworking for‚Äù have a great understanding of data science and the types of problems that you can and cannot solve given a particular data set. They‚Äôll be able to give you clear problems to work from and solve that actually make sense.
In the real world, it‚Äôs quite likely that some C-level executive will want your data science team to build a model or develop an analysis to solve a problem that no one really understands. There appears to be some kind of issue to those at the executive table, but what the actual problem is, no one can describe it very clearly.
This aspect of working in data science will become just another part of the job for you as time goes on. Your job is to help those executives gain a clearer understanding of what they think the problem is and to explain to them how you think it could be solved. Plain and simple. Don‚Äôt reinvent the wheel or break your back trying to conjure something from nothing. Your job is to just help them understand using data.
A ‚Äúbrag sheet‚Äù can be as comprehensive as additions to your resume or as simple as a sticky note stuck to the bottom of your monitor.
In essence, this should be the place where you list the accomplishments that you accumulate over your career.
Maybe you developed a model that found inaccuracies in your company‚Äôs reporting tactics which helped them discover what their numbers really were. Or perhaps you found ways to retain a website user‚Äôs attention for longer and reduced the website bounce rate. Or maybe you developed an artificial intelligence model that learned what a specific trend looked like in skin cancer that was used to help doctors improve the accuracy of their diagnoses by 17%.
Whatever your accomplishment or the impact delivered to your company or organization, it‚Äôs important to keep track of what you‚Äôve done over your career. When you‚Äôre first starting out, this will help keep you motivated when times get tough, and when you‚Äôve got several years of experience under your belt, it will help you progress throughout the rest of your career.
The unique thing about data scientists is that they need to be able to explain brutally technical concepts in simple terms that anyone in a company can understand.
Your best trick to achieve this is by developing the ability to create connections to concepts using an analogy. Analogies are great ways to provide a clear representation of a complex idea in a way that is easily digestible and understandable right from the get-go. These analogies should be relevant to the company or industry that you‚Äôre working in and should give enough information that a C-level executive can be confident in making a decision based on their understanding.
Analogies should be free from data science jargon and short enough to be memorable.
In my experience, creating analogies that people can relate to is vital to your success in being able to explain what‚Äôs going on within a company using data. I‚Äôve used sports analogies, cooking analogies, animal analogies, and more to help people understand the technical concept I‚Äôm trying to tell them. These analogies will not only help ideas stick in a stakeholder‚Äôs mind, but they‚Äôll also be tools the stakeholder can then use to spread your information.
This acronym will keep appearing, no matter how long ago you learned it when you first studied data science. Coming from the world of coding, this acronym will probably help advance your career more than any other piece of advice could.
Why make something complicated when you can keep it simple? Sure, it would be nice to impress your boss with some elaborate model that cleans your data using two lines of code. But wouldn‚Äôt it impress them more if you developed a model that could be easily understood, updated, integrated, and used after you left the company?
You may begin your career thinking that complexity in your work is the one way to remain relevant to a company. However, your ability to complete your work on time, in a simple manner, that can be used by anyone, and that produces the exact results sought after is what will keep you relevant. Furthermore, your ability to keep learning and increasing your skill set is what will also keep you a valuable member of the team.
If complexity is your thing, just make sure you check with your boss first to see if this is the right project for you to go all out.
Otherwise, keep it simple, keep it clean, and keep it easy to understand.
To be fair, it‚Äôs not exactly hard to impress a C-level executive with no wide-ranging technical experience. It‚Äôs important to remember that these people are often easily appreciative when something as simple as their modem gets reset.
However, there does come a time when you may want to be extra impressive. This need to impress must also be balanced with the boss‚Äô boss still being able to kind of understand what you did or what you solved.
Again, as mentioned above, this may involve developing a complex model to solve a difficult problem that can still be described using simple terms. You want to impress your boss‚Äôs boss but still have them be able to kind of understand the impact you brought to the company.
When in doubt, add color to a graph, try a histogram instead of a bar chart, and use words like ‚Äúmachine learning‚Äù and ‚Äúartificial intelligence‚Äù to impress without giving away too many details. Remember, arguably 90% of data science is a simple linear regression, you may just need to dress it up a little.
Data scientists are often given impossible tasks ‚Äî complete a data analysis with non-existent data, develop a machine learning model that solves the company‚Äôs problems, or integrate new models into a code base that hasn‚Äôt been touched since 2005.
Many see data scientists as jacks of all trades (which we are) who can solve any company problem using data (which we can‚Äôt). This means that the stakes are high when we‚Äôre given a business problem and told to solve it by the end of the week. Unfortunately, not all problems can be solved by slapping a linear regression bandage on the wound.
Therefore, it‚Äôs often best to err on the side of caution and be conservative with what you believe can be achieved with the tools you‚Äôre given and the conditions you‚Äôre under. Underpromising saves you when things go as planned and overdelivering endears you to your boss and your boss‚Äôs boss when things go unexpectedly well.
‚ÄúOnly you and God can read your code and you‚Äôre one flow session away from it being only God.‚Äù ‚Äî TheDragonSpark, r/datascience
Writing excellent code isn‚Äôt about being able to complete the most amount of work with the least amount of lines. It‚Äôs about being able to read and understand the code you wrote on a Friday after a long weekend away. It‚Äôs about being able to give your code to an intern five years down the road and have them be able to understand and optimize it. It‚Äôs about being able to leave a legacy at a company of easy-to-use code that doesn‚Äôt result in a disgruntled data scientist phoning you at odd hours for help.
Go further than naming your variables x , y , and z . Instead, use descriptive names that tell you exactly what a variable holds and what result it should produce when combined with another variable.
Name your functions to describe exactly what they do.
Use comments to clarify any confusing lines, logic, or places where you had to dive into the depths of StackOverflow.
Use the READ.me file to give a clear glimpse into your thought process, the goal of your code, and insight into how and why it should work.
Being diligent with accurately naming your variables and using comments will take you surprisingly far in your career as a data scientist.
Most data scientists suck at writing good code (compared to software engineers).
Most data scientists suck at math (compared to mathematicians).
Most data scientists suck at modeling (compared to machine learning developers).
This is what makes data scientists unique is that we hold the abilities for each of these specialties and can carry them out remarkably well even though they all come from vastly different areas.
Imposter syndrome is very common in data scientists, where everyone expects you to be a software engineer, mathematician, machine learning developer, business analyst, graphic designer, project manager, and more. Therefore, it‚Äôs important to remind yourself that you‚Äôre good at all of these skills but that it‚Äôs unnecessary to be an expert in each of them. If the company wanted a statistician, they would have hired a statistician. Instead, they hired you because you possess such a varied skillset that makes you the full package, capable of carrying out each of the tasks listed above, and then some.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Support my writing by donating to fund the creation of more stories like this one: Donate
ODSC - Open Data Science
in
The Startup
Tbemel
Suzanne Borders
in
badvr
Sharan Kumar Ravindran
in
Towards Data Science
Tara Lamorgese
S√©bastien V√©zina
Richard Hackathorn
in
Towards Data Science
Buse Yaren Tekin
"
30,https://medium.com/towards-data-science/building-a-simple-web-application-using-aws-605436d77407,Building a Simple Web Application Using AWS,"In this article, I will show you how to build a simple web application on AWS. First, we will create a static web application that displays ‚ÄúHello World.‚Äù Then, we will discover how to incorporate different AWS features into the web application and see how they work together.
In this project, as you can guess from the title, we will use AWS cloud, which stands for Amazon Web Services; an excellent cloud platform with endless services for so many various use cases from training machine learning models to hosting websites and applications (Around 200 AWS services are available as of writing this article).
This project is an excellent introduction to the cloud computing platforms. If you are new and just getting into cloud services, don‚Äôt worry. It‚Äôs never too late to start learning something new.
If you are ready, let‚Äôs get to work. Here is the structure that we will follow.
In this step, we will learn how to deploy static resources for our web application using the AWS Amplify console.
Basic web development knowledge will be helpful for this part. We will create our HTML file. As mentioned earlier, the website will be straightforward, a heading saying ‚ÄúHello World!‚Äù
As a code editor for this project, I used Atom. Feel free to use your favorite. Here is the code snippet of the page:
There are multiple ways to upload our code into Amplify console. For example, I like using Git. To keep this article simple, I will show you how to do it directly by drag and drop method into Amplify. To do this ‚Äî we have to compress our HTML file.
Now, let‚Äôs go to the AWS Amplify console. It will look something like this:
When we click ‚ÄúGet Started,‚Äù it will take us to the following screen (we will go with Amplify Hosting on this screen):
I‚Äôve created an Amplify application project and named it ‚ÄúStep1.‚Äù
Then, I dropped the compressed index file. Amplify deployed the code, and returned a domain URL where we can access the website.
For now, this is how our website looks like:
All done for this step! Our static HTML code is deployed and live, thanks to AWS Amplify.
We will create a serverless function using the AWS Lambda service in this step. AWS Lambda is a computing service that lets us do tasks without using full-time running computing engines. Instead, only works when something invokes it; a very efficient solution.
To give you some idea, a great example of Serverless computing in real life is vending machines. They send the request to cloud and process the job only somebody starts using the machine. You can learn more about AWS Lambda from here.
Let‚Äôs go to the Lambda service inside the AWS console. By the way, make sure you are creating the function in the same region in which you deployed the web application code in Amplify. You can see the region name at the top right of the page, right next to the account name.
Time to create a function. For runtime programming language parameters: I‚Äôve chosen Python 3.7, but feel free to choose a language and version that you are more comfortable and familiar with.
After our lambda function is created, we will see the following screen:
Now, let‚Äôs edit the lambda function. Here is a function that extracts first and last names from the event JSON input. And then returns a context dictionary. The body key stores the JSON, which is a greeting string.
After editing and saving lambda_function, let‚Äôs go ahead to the Test tab and create an event.
Here are the execution results after we deployed and ran the test:
The execution result has the following elements:
In this step, we will deploy our serverless lambda function to our web application. We will use API Gateway to create a REST API that will let us make requests from the web browser. API Gateway service, as we can understand from its name, is just like a bridge between the back-end and the application‚Äôs front-end.
REST: Representational State Transfer.
API: Application Programming Interface.
Let‚Äôs go ahead without losing time and open the API Gateway service from the AWS console, then create a new REST API.
API Gateway service main page:
API creation page, we are giving a name, choosing protocol type and endpoint type for our REST API.
And in the next page, we create a POST method from the Actions button. The integration type will be a lambda function, and ensure the Region is the same Region you‚Äôve used to create the lambda function.
Next, we will enable CORS, which stands for Cross-Origin Resource Sharing. It‚Äôs an HTTP header. Then click Enable CORS and replace existing CORS headers.
After enabling CORS headers, Deploy API from API actions.
Which will create a new stage; you will see it under the Stages tab on the left bar. When you view the stage, there will be a URL on top named Invoke URL. Make sure to copy that URL; we will use it to invoke our lambda function in the final step of this project.
Time to test our REST API.
Under the Resources tab, after clicking on POST, we will see the method execution screen. The Test page will come up when we click on the button that I circled below.
In this step, we will create a data table in Amazon DynamoDB, another AWS service. DynamoDB is a fully managed NoSQL database service that supports key-value data structures.
DynamoDB Dashboard:
Let‚Äôs click Create table and fill out some information about our data table:
Then, let‚Äôs view the details of the table and copy the ARN, which stands for Amazon Resource Name:
We will use the ARN in the next step when creating IAM access policies.
IAM: Identity and Access Management
I know what you are thinking. Policies are a boring topic, and I was feeling that way ‚Äî until I realized how policies play an essential role in keeping us safe. AWS recommends a least-privilege access model, which means not giving a user more than needed access. The same rule applies to AWS services.
For example, even for this simple web application project, we have already worked on multiple AWS services: Amplify, Lambda, DynamoDB, and API Gateway. It‚Äôs essential to understand how they communicate with each other and what kind of information they share. If you want to learn more about this, here is the official documentation section covering policies and permissions in IAM.
Back to our project.
The policy we will define here will give access to our lambda function to write/update the data table that we created using DynamoDB.
Let‚Äôs go to AWS Lambda console. And choose the lambda function. Then let‚Äôs go to the configuration tab, and we will see the execution role. Click the link, which will take us to the configuration settings of this function.
From permission policies, let‚Äôs create a new inline policy.
And then, let‚Äôs add the following code to the JSON section.
This policy will allow our Lambda function to read, edit, delete, and update items from the DynamoDB data table.
Lastly, we will update the lambda function python code. It‚Äôs on the main page where we‚Äôve found the execution role in the configuration tab.
Here is the Python code that goes inside the function:
The response is in REST API format. After making the changes, make sure to deploy the code. After the deployment is concluded, we can Test the program from the orange-colored test button.
We can check the results from the DynamoDB. When we run the function it has updated our data table. When we go to AWS DynamoDB, choose your table and click Explore items from left nav bar. Here is the object returned from the lambda function:
Congrats on making it this far!
In this final step, we will see everything we just built in action. We will update the front-end to be able to invoke the REST API with the help of our lambda function and receive data.
First, let‚Äôs open a new file inside our code editor and add the following lines of code (I saved it as index.html):
Before you save it, make sure to update the api-key text. You can find it on the API Gateway service under your REST API details, called invoke URL. The code is on line 34, where we fetch the URL link with the requestOptions variable.
When the code is all set, we will export it as a compressed file, just like in step 1. Then, upload the file to AWS Amplify using the console.
Our data tables receive the post request with the entered data. The lambda function invokes the API when the ‚ÄúCall API‚Äù button is clicked. Then using javascript, we send the data in JSON format to the API. You can find the steps under the callAPI function.
You can find the items returned to my data table below:
Congrats! We have created a simple web application using the AWS cloud platform. Cloud computing is snowballing and becoming more and more part of developing new software and technologies. Here is an article where I shared the top cloud computing platforms with certification programs. I would be glad if you learned something new today from this article.
Working on hands-on programming projects is the best way to sharpen your skills. Feel free to reach me if you have any questions or feedback about the project. I will do my best to get back to you.
I am Behic Guven, and I love sharing stories on programming, education, and life. Subscribe to my newsletter to stay inspired. Ty,
If you are wondering what kind of articles I write, here are some:
Trading Cyclist
Josh C.
Cleopatra Douglas
in
Towards Data Science
Pulse Lab Jakarta
Aiea
Adiamaan Keerthi
in
MLearning.ai
Usakahyel Rufus
Peilun (Ann) Liu
"
31,https://medium.com/towards-data-science/daily-weekly-monthly-and-yearly-goal-tips-to-guide-a-self-taught-data-scientist-in-2023-d405f8d0d87a,"Daily, Weekly, Monthly, and Yearly Goal Tips to Guide a Self-Taught Data Scientist in 2023","If you‚Äôre anything like me, December is the month that is dedicated to preparing for the upcoming year. Whether it‚Äôs cleaning up my computer files, donating old items I didn‚Äôt use in the past year, or purchasing the books I‚Äôll be reading for the upcoming year, I believe that December should be set aside for preparation so that you can enter January organized, focused, and without any baggage from the previous year.
Part of this preparation also involves setting my resolutions or goals for the upcoming year and preparing all of the things I‚Äôll need to achieve them. But why do I need an entire month to prepare my resolutions for the upcoming year?
The data suggests that even if you manage to complete your resolution or goal through the entire month of January, 80% of resolutions will still fail by the middle of February.
Why?
Because setting resolutions or goals in January is too late for them to be successful.
Have you ever considered why you see a sudden influx of gym-goers in the first week of January and then see them significantly taper off over the following weeks until by the end of the month it‚Äôs back to just the regulars? That‚Äôs because those people who resolved to ‚Äúlose weight‚Äù or ‚Äúhit the gym‚Äù in a giddy-induced stupor on New Year's Eve didn‚Äôt set themselves up for success to achieve that goal through adequate pre-planning.
So what does all this talk about resolutions have to do with data science?
Well, if I‚Äôve noticed anything about self-teaching yourself data science it‚Äôs that more often than not, you fail, at least once. The beginning of a new year is a great time to set the goal of teaching yourself data science, but only if you put in the appropriate amount of preparation beforehand. Therefore, December is the perfect time to begin preparing how you‚Äôll achieve your goal of becoming a data scientist in 2023. Not only does it give you enough time to get prepared, but it also gets you in the right frame of mind to enter January driven to succeed.
Daily goals for learning data science should be all about getting that 1% better each day. These goals shouldn‚Äôt be monsters that need an entire day to be tackled, but should instead be bite-sized endeavors that add up to a big win at the end of the year. Naturally, your daily goals will be guided by your overarching goal for the year, but they shouldn‚Äôt be anything too big or overwhelming.
For example, you could look for one menial task to automate, working for just 10 minutes on developing a new data science skill (think visualizations or mastering a new library), making progress on a passion or portfolio project (add a new feature every day, or take a few minutes to clean up your code and create documentation), listening to a data science podcast (or really just consuming any type of informal data science-related content), or sharing what you‚Äôve learned with a community such as Twitter or through a newsletter.
Daily goals are not where you conquer the world. Instead, they‚Äôre much smaller, similar to taking your vitamins or walking the dog every day.
These daily goals should also be something that stretches your abilities without frustrating you. Daily goals should be something that you look forward to each day without becoming discouraged. The minute you skip a day you become easily susceptible to the whole goal framework for the year crashing down. That‚Äôs why these goals need to be manageable and reasonable, something that will be different for each person‚Äôs circumstances.
For example, if you‚Äôre working a job while also trying to transition into data science, your daily goal may be as simple as listening to a data science podcast during your commute. This can then be scaled down for times such as vacations or days when you‚Äôre not feeling well to ensure that you don't break your daily goal streak without over-taxing yourself.
One of my daily goals for the last year has been to keep up with advances in data science by reading literature shared on sites like Towards Data Science. This takes less than 10 minutes each day as I skim what‚Äôs new and make a few notes of what I find interesting or relevant to my goals. This goal gets easily accomplished every day when I‚Äôm waiting for my tea to brew or when I‚Äôm needing a social media break from my regular work.
Remember, this goal has to be *easy* ‚Äî something you can accomplish every day for the entire year without getting burnt out. You can have as many daily goals as you want but they should all be small, and easily attainable with just a few minutes each day.
You should be reminded of your daily goals every day at the same time. My favorite tool for this is a simple to-do app that sends me a notification at the same time every day. It‚Äôs extremely satisfying getting to check the boxes of each of my daily goals and also helps me set aside time to do them. Depending on how you work best, you may find that doing your goals at varying times every day works or you may prefer having a scheduled allotment of time every day in which to do them. Regardless, you need to be reminded of your goals every day and have a physical way of ‚Äúchecking‚Äù them off the list. Bonus points if you write some code to have your daily goals pop up on your computer as your daily reminder.
Weekly goals should leave you feeling like you‚Äôve accomplished something at the end of the week.
When it comes to self-learning data science your weekly goal should have you moving one step forward in your learning curriculum having knocked some of the learning concepts off your list.
Depending on how much time you have to dedicate to your goal of becoming a data scientist in 2023, your weekly goals will range from learning single concepts to entire units of study. For example, if you have several other commitments on top of learning data science, your weekly goal could be ‚Äúmastering variables and functions in Python‚Äù. Alternatively, if most of your time is dedicated to learning data science, one of your weekly goals could be to ‚Äúdevelop AI code that implements predictive capabilities in determining which customers in this data set will return their order‚Äù.
Begin setting your weekly goals by taking out a calendar and writing on it the goals you want to accomplish each week. These goals should be closely aligned with your learning curriculum. For example, each concept or unit in your curriculum should be a weekly goal. This tactic helps you continuously move forward through your curriculum. I used this tactic during my bachelor‚Äôs degree which I completed online in an entirely self-taught method. By organizing my work for the week based on units, I was able to fly through courses by completing a unit or two each week as my weekly goal.
I prefer setting weekly goals up a little more liberally than daily goals. During your week you‚Äôll have more productive days and less productive days. Therefore, I find it unhelpful to schedule tasks for certain days (unless of course they have specific deadlines). Instead, I set up a master to-do list for the entire week where I list everything I have to accomplish. Then, as I go through the list throughout the week, I check things off. No more ‚ÄúI have to get this whole list of things done on Monday‚Äù. Instead, I just go with the flow. Sometimes I‚Äôll get tonnes of work done on Monday and not much work done on Tuesday because something came up. Regardless, I have an idea of all the work that needs to be accomplished by the end of the week instead of tying myself down to having to accomplish a certain number of tasks each day ‚Äî this also helps me achieve a better work-life balance.
Monthly goals should be substantial. They should be 12 milestones that give you big tangible benefits or advancements in your journey toward becoming a data scientist.
Monthly goals should be challenging without being unattainable. These goals could each be a portfolio project, a certification completed, or a tangible skill that was mastered. Regardless of what they are, you should be able to look back at the end of the year and see 12 big accomplishments that all helped add up to you achieving your yearly goal.
Monthly goals should first and foremost align themselves with your weekly goals. For example, if your weekly goals for January primarily reside around learning Python, your monthly goal should be something along the lines of creating a project in Python where you can create, update, delete, and search contacts in a digital address book (for example). Or, if your weekly goals for April are about putting together your programming, math, and visualization skills to build a full data analysis, your monthly goal could be to create a simple analysis of World Bank data that determines the amount of political participation in a given region. Alternatively, if your weekly goals for October were centered around career prep, your monthly goal could be applying to 10 viable data science jobs.
Monthly goals should culminate all of the things you worked on with your weekly goals and push them to the next level.
I like to work on monthly goals throughout the month instead of leaving them to the last minute. Organization is essential to ensure that you‚Äôre balancing your daily and weekly goals on top of completing your monthly goal. Monthly goals should be chunked into manageable portions so you‚Äôre not left rushing to a deadline (unless that‚Äôs your thing of course). For example, if your monthly goal is to apply for 10 viable data science jobs (which involves developing unique resumes and cover letters for each if you want a good shot at it), you could pick 2‚Äì3 jobs to work on applications for each weekend. Alternatively, if your monthly goal is to put together an entire data analysis, you could work on the data acquisition and cleaning the first weekend of the month, do the analysis the second and third weekend, and complete your visualization and project conclusion on the fourth weekend.
I think the success of monthly data science goals is to make them enjoyable or exciting to work on. Daily and weekly goals are easy to get done because you get in a rhythm and it just becomes part of your daily grind. However, monthly goals are always looming over your head and can feel like an unnatural burden to have to accomplish on top of all the other work you have to do. You can mitigate this feeling by making your monthly goals involve something you enjoy. For example, your monthly goal could be to predict the upcoming World Cup winner or help a small business in your community with pro bono data science work. Whatever it is, your monthly goal should incorporate data science with something you love to make it something you look forward to completing. Not only that, but monthly goals that become portfolio projects will leave you with a bunch of projects that you‚Äôll be able to describe to future employers passionately.
Your goal for the year is quite simple: to become (or at the very least get closer to becoming) a data scientist.
However, this yearly goal isn‚Äôt specific enough to be attained.
When you set your goal for the upcoming year this December, you need to set some parameters for it first. While I‚Äôve always hated the SMART acronym, I think it gives us a good foundation on which to lay this specific goal.
Your goal of becoming a data scientist (or getting closer to becoming one) by the end of 2023 has to be specific, measurable, attainable, relevant, and timely (yes, I also cringe every time I use this acronym, but bear with me). The point of SMART goals is to set better goals that have a higher chance of being achieved. When you set specific goals, you ensure that you have a clear picture of what needs to be achieved. The goal must be measurable so you can measure how well you‚Äôre doing in accomplishing it. Your goal also needs to be realistically attainable in the time you‚Äôve allotted for yourself (which is why your goal may be to become a data scientist by the end of 2023, or to just get closer to becoming one). Relevancy will help you stay committed to your goal because it aligns itself with your broader aspirations. Finally, your goal must have time constraints placed upon it otherwise (and trust me here) you‚Äôll never accomplish it.
Therefore, your overarching goal for 2023 could be to:
Become a data scientist by December 31st, 2023, by acquiring a data science job at a company after preparing for the interview throughout the year by honing my data science skills using my self-learning data science curriculum. This will lead me closer to my overall aspiration of starting my own data science consulting company.
Why does this goal work? Because it‚Äôs specific (become a data scientist), measurable (by acquiring a data science job at a company), attainable (after preparing for the interview throughout the year by honing my data science skills using my self-learning data science curriculum), relevant (this will lead me closer to my overall aspiration of starting my own data science consulting company), and timely (by December 31st, 2023).
Another example of a great yearly goal for 2023 could be:
I will become proficient in customer marketing data analysis using Python, Excel, SQL, and Tableau by December 31st, 2023, by completing my project on which apps are more likely to attract users, which will be added to my data science portfolio that will be used to help me attain a job in data science.
This goal is also specific (become proficient in customer marketing data analysis using Python, Excel, SQL, and Tableau), measurable (the project will be added to my data science portfolio that will help me attain a job in data science), attainable (by completing my project on which apps are more likely to attract users), relevant (to help me attain a hob in data science), and timely (by December 31st, 2023).
While these goal attributes are verifiably cringe-worthy, they can help give you a direction that will guide you, as well as your daily, weekly, and monthly goals, throughout the coming year.
Your yearly goal needs to be posted prominently in your workspace where you will see it every day. I‚Äôm a big fan of the humble sticky note for this. Look at this goal every day, even when you‚Äôre not lacking ambition, to remind yourself why you‚Äôre working so hard on your daily, weekly, and monthly goals.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Support my writing by donating to fund the creation of more stories like this one: Donate
Patricia
Anonymous Kawaii Animal
in
Kawaii Computer Science
Insyiraah Oxaichiko Arissinta
Yves Boutellier
in
Towards Data Science
S Abhishek
Ankur Tomar
Etienne Ramstein
in
Towards Data Science
Siavash Nejadi
"
32,https://medium.com/towards-data-science/simple-random-sampling-is-it-actually-simple-71014e58e0d1,Simple random sampling: is it actually simple?,"No matter how hard you may try to forget your STAT101 course, you‚Äôll likely tend to default to simple random sampling (SRS) as your knee jerk approach. It was, after all, an assumption you were told to make for every single homework problem. I don‚Äôt blame you ‚Äî SRS is a great option when it‚Äôs feasible.
But what I find tragicomic is that every single time I ask a fresh batch of students how they suggest approaching a data collection challenge, I hear the word ‚Äújust‚Äù as part of the answer. As in, ‚ÄúJust select them entirely at random.‚Äù
Let‚Äôs spend a moment in the real world, shall we?
Imagine that you‚Äôre a data scientist who has been hired to estimate the average height of pine trees in the forest pictured below and describe the distribution.
Given the all-you-can-eat buffet of tree height info you can find on the internet, it‚Äôs clear that you wouldn‚Äôt be the first intrepid tree-measurer to tackle this kind of job. Plenty of people have measured tall trees‚Ä¶ How hard could it possibly be?
(Note: the links in this article take you to my lighthearted explanations of any jargon terms that crop up.)
If you measured every single tree perfectly, you wouldn‚Äôt need statistics; you‚Äôd have the facts. But do you need the facts? Or are you willing to settle for statistics?
Statistics gives you a way to proceed even if you don‚Äôt have all the data you wish you had. Measuring a few trees (sample) rather than the whole blessed forest (population) results in a less perfect but hopefully less expensive perspective on the information you‚Äôre interested in. Which is a relief since you don‚Äôt even know how many trees there are in this massive forest.
Let‚Äôs measure a good enough sample of trees so we don‚Äôt have to measure them all!
Thinking statistically, your boss asked you to carry out measurements to the nearest foot on a random sample of 20 trees, so you followed the advice in our previous article and confirmed that these specs make sense for your project. They do; the stage is set!
What does STAT101 tell you to do next?
I‚Äôve taught this example to over 100 classes of students and when I‚Äôve asked them how we should pick the trees, I‚Äôd hear one or both of these (equivalent) responses from someone in the crowd every time:
‚ÄúJust select them [entirely/completely] at random.‚Äù
AND/OR
‚ÄúJust take a simple random sample.‚Äù
I don‚Äôt blame you for defaulting to simple random sampling (SRS) as your knee jerk answer. It‚Äôs a fabulous option when it‚Äôs feasible. That‚Äôs not the bit I take issue with.
What I find tragicomic is that every single time, I hear the word ‚Äújust‚Äù as part of the answer.
Whoever tells you that the way to take a simple random sample of these trees is to ‚Äújust select them entirely at random‚Äù ‚Ä¶does not know how to use the English word ‚Äújust‚Äù correctly. There‚Äôs no ‚Äújust‚Äù about this! Data design can be surprisingly tricky when the real world rears its ugly head.
Imagine that you passionately hate the great outdoors, so you sneakily outsource the actual tree measuring to someone who can stand fresh air. You‚Äôve hired an avid hiker with no technical background who‚Äôs eager to follow any instructions you give, so you tell this person to, er, ‚Äújust‚Äù select 20 trees entirely at random?!
If I were the hiker, I‚Äôd ‚Äújust‚Äù grab the first 20 trees which look convenient ‚Äújust‚Äù to teach you a lesson about being careful with your instructions.
Simple random sampling and simple random sample and entirely at random and SRS are all technical terms. They refer to a sampling procedure where each sampling unit (tree) has the same probability as any other tree of being selected.
It‚Äôs only a true simple random sample (SRS) if comes from equal selection probability. Otherwise it‚Äôs just sparkling nonsense.
There‚Äôs a reason SRS is the first (and sometimes only) sampling procedure we teach newcomers to statistics and that reason is that it‚Äôs‚Ä¶ easy. Easy in terms of the calculations, that is. There are other sampling procedures, but they require adjusted calculations which are usually outside the scope of your first year stats course.
There are other sampling procedures, but they require more advanced calculations.
It‚Äôs only a true simple random sample (SRS) of trees if it comes from a forest with equal selection probability for every tree, otherwise it‚Äôs just sparkling nonsense when you use SRS calculations on it.
Unfortunately, if you analyzed your data the way STAT101 teaches you to do it, but you didn‚Äôt actually use a true simple random sampling procedure to get hold of the data, then your results will technically be wrong.
Always strive to give foolproof instructions, because you never know when a wild fool will appear.
If your hiker picks the more convenient trees closer to the edge of the forest, that‚Äôs most definitely not a simple random sample. It‚Äôs something called a convenience sample ‚Äî which is a procedure you should avoid like the plague ‚Äî more on that in a future article. Analyzing such data with SRS math is statistically inappropriate‚Ä¶ what if those trees get a different amount of sunlight are are thus unrepresentative of the entire forest? Basing your inferences on them will lead you to the wrong conclusions.
So, what would a professional statistician‚Äôs answer look like? To find out, head over to Part 2!
kozyrkov.medium.com
P.S. Have you ever tried hitting the clap button here on Medium more than once to see what happens? ‚ù§Ô∏è
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Bhargavi Reddy
Matthias Funke
Tilak Zade
Salma El Shahawy
in
Towards Data Science
Satyam Kumar
in
Towards Data Science
Thomas Chaigneau
in
Towards Data Science
Sadequl Hussain
in
The Startup
Laurence Hubbard
in
toyotaconnected
"
33,https://medium.com/towards-data-science/how-to-study-data-science-even-if-you-work-or-study-full-time-b52ace31edac,How to Study Data Science even if you Work (or Study) Full Time,"There are few certainties in this world, but one is becoming very clear: the career path is anything but a straight line. Forget the ‚Äústudy, get a job, get married, and retire‚Äù approach: it won‚Äôt be like that for our generation (and, maybe, for the future ones). This may be due to the fast-growing digital professions: many people were stuck in their jobs while they have seen a world growing around them; and, maybe because of the pandemic, they even started thinking about making a career change.
Some days ago, I wrote an article on how to study Python for Data Science. Now, I want to give you my advice on how to study Data Science even if you are working or studying full time.
If you are thinking about Data Science, but you feel you don‚Äôt have time to study and practice because you work or study full time, then this is the right article for you: I‚Äôve started studying and practicing Data Science while working full-time (and with two little daughters), and I want to share with you my recommendations on how you can do it too.
This is the hardest part, and I want to treat it in the beginning.
If you are working or studying full time you don't have much time to spend on other stuff, but, saying the truth, you‚Äôll be surprised to see how much time we waste, during the day, if we just use a time-tracker as an app on our smartphones.
Anyway, I believe you only have two chances to find some time for yourself:
Here there is one piece of advice I want to give you: you have to try, for some time (days/weeks) to understand when you are most productive. For example, I found myself most productive in the morning, so I get up at 5.30 a.m., have some breakfast, and have a 5-minutes meditation session and then I start studying and practicing.
Also, I even practice in the evening, after dinner, but since in this time window I‚Äôm typically very tired because of the whole working day, I do some tasks that ‚Äúare not hard‚Äù; like, for example, beginning to write an article such as this one.
Instead, in the most productive time window, you have to perform ‚Äúcomplicated‚Äù tasks, like studying (understanding concepts) or developing projects.
An agenda must become your best friend, in my opinion.
The most important thing you have to learn is to subdivide every task into smaller tasks and to plan them. For example, I know I have about 40‚Äì50 minutes every morning before preparing to go to work, and I have learned to schedule my tasks so that I know I‚Äôll be busy for about 20‚Äì30 minutes; you‚Äôll often have troubles with your code to solve, as well as some concepts may be more difficult than other to understand, so, in my opinion, it‚Äôs a good habit to schedule your time less than the full capacity you have.
The idea is to schedule your time, writing the stuff to do on an agenda. I usually take 5 minutes in the evening to schedule my time for the next day.
An important thing to understand from now is that you have to be very specific. You can't write:
I have to study
This is for a couple of reasons:
Instead, you have to write something like that:
Whit the time goes by, you will understand how much time you need to get things done and you will be even more specific in scheduling yourself. For example, if tomorrow you want to do an exercise on dictionaries, and you know you have 20 minutes but you believe you‚Äôll need more than that, then schedule it like that:
This way, at the end of the day, you‚Äôll have a sense of completion that will make you progress in your learning path and you will feel very fulfilled.
As your day is filled with work/study and other stuff (family, friends, some shopping, etc‚Ä¶), time management is your best friend. You can not waste your time anymore.
Please, stop reading for a second. I said ‚Äúyou can not waste your time anymore‚Äù; I haven‚Äôt said: do not take your rest (we‚Äôll talk about it in the next section).
For example, if you‚Äôll study in the morning when you get up you can not open your Facebook app ‚Äújust for a second‚Äù, because you‚Äôll get into a rabbit hole and‚Ä¶goodbye to your 40‚Äì50 study minutes!
Also, I wank to give you a couple of more pieces of advice:
We are not all the same; there are people who feel trapped having a day scheduled in advantage; also, sometimes if you miss some of your goals (for every reason: maybe you get the flu and you simply can not study!) you may end procrastinating for the next few days and, eventually, you will end your journey into Data Science.
So, one of the most important things I‚Äôve learned is to be flexible with my agenda. Some nights my daughters simply do not make me sleep, and I can‚Äôt wake up a 5.30 a.m to study; so, I simply reschedule the tasks I had to do in the morning, to the evening, or to the day after.
As simple as that.
Flexibility is your ace in the hole when you learn to be flexible. You won‚Äôt get hungry with yourself; you won‚Äôt blame yourself for not getting up (or for watching a tv show for an evening!).
Also, the most important thing to learn is to listen to yourself. If you‚Äôre feeling tired because you spend days working, studying, playing with your children, and so on‚Ä¶your body will ask you to get some rest, and you have to listen to it. Otherwise, you‚Äôll pay a hefty bill; believe me.
If you don‚Äôt take some rest when your body asks for it, it will eventually crash and will oblige you to a longer rest; for example, I‚Äôve learned on my skin ‚Äî when I was at the University ‚Äî that if I do not take some rest when my body asks for it, I‚Äôll spark a fever; this way, my body needs 2‚Äì3 days to rest: a much longer time than a couple of mornings when I stay in bed because I‚Äôm (really) tired!
I perfectly know the mental dialog:‚Äù if you do not study today, it will take longer to get to your result of a career change‚Äù; but, believe me: your brain is tricking you. This is a marathon, and you need to preserve your energies for the long run. When you are tired just relax and get some rest: you‚Äôll thank me!
These are my recommendations on how to develop your career in Data Science while working or studying full-time.
Flexibility is the most important one to me because you have to learn how to manage your brain; so, this is a work ‚Äúinside you‚Äù; meditation may help you (as it helps me) on this.
SPOILER ALERT: if you are a newbie or you want to learn Data Science and you liked this article, then consider that in the next few months I‚Äôll start tutoring aspiring Data Scientists like you. I‚Äôll tell you in the next weeks when I‚Äôll start tutoring, and if you want to reserve your seat‚Ä¶subscribe to my mailing list: I‚Äôll be communicating the beginning of my tutoring journey through it, and in the next articles.
Let‚Äôs connect together!
MEDIUM
LINKEDIN (send me a connection request)
TWITTER
Consider becoming a member: you could support me and other writers like me with no additional fee. Click here to become a member.
Hamza B., PhD
in
Towards Data Science
James Hickey
in
Data@NewDay
Anggito Sulistyo
an amygdala
Rodrigo Ledesma
M. N. Muhaimin
Bisman Preet Singh
in
Analytics Vidhya
IKAROS STUDIO
"
34,https://medium.com/towards-data-science/why-arguing-about-metrics-is-a-waste-of-time-b1c6f9026724,Why Arguing about Metrics Is a Waste of Time,"When the science of psychology was still in its infancy, I imagine there might have been quite a few academic correspondences of this flavor:
Psychologist 1: ‚ÄúDear sir! That which you are calling ‚Äòmemory‚Äô is not, in fact, memory! Instead the following is memory‚Ä¶‚Äù
Psychologist 2: ‚ÄúDear sir! I quite assure you that my definition is memory and yours is not memory‚Ä¶‚Äù
Picture them bickering back-and-forth and getting nowhere, like squabbling children. I wasn‚Äôt around in those days (and neither were you), but I get the distinct impression that the discipline‚Äôs history fueled enough professional embarrassment to ensure that all psychology grad students ‚Äî myself included ‚Äî had the practice of operationalization beaten into us from day one until we can‚Äôt help but scream ‚ÄúOPERATIONALIZATION‚Äù in our sleep.
If you haven‚Äôt heard the term operationalization before, I‚Äôve written a primer for you here. Long story short, it‚Äôs the creation of measurable proxies for rigorously investigating fuzzy concepts.
Operationalization is the creation of measurable proxies for rigorously investigating fuzzy concepts.
To exit the dark ages and lay claim to calling itself a proper science, psychology has had to come a long way in developing solid best practices in this area, starting with a hugely important one: a shared understanding that metrics are not universal and that they‚Äôre simply useful proxies.
You and I are allowed to use the same word to mean different things, as long as we define our terms at the beginning, just like I‚Äôm allowed to let X=5 and you‚Äôre allowed to let X=10 when we‚Äôre working different problems. As a result, today‚Äôs academic discourse takes a substantially more civilized tone:
Modern Psychologist: ‚ÄúDear Colleague! Your operationalization of memory is not the same as my operationalization of memory. Therefore, I cannot use your findings in my own research, but I still think you‚Äôre wonderful and I wish you the best of luck with whatever you‚Äôre doing over there. Have a great life!‚Äù
So much better, wouldn‚Äôt you agree? Who doesn‚Äôt love the civilizing influence of operationalization?
Unfortunately, the young discipline of data science, as well as much of the tech industry, seems to have skipped the entire chapter on operationalization. I don‚Äôt know whether to laugh or to cry every time I hear some aggressive greenhorn warbling a version of ‚ÄúDear sir! That which you are calling user happiness is NOT user happiness!!!‚Äù in a style that‚Äôs best relegated to a previous millennium (or at least to the Bad Part of the internet‚Äôs discussion forums).
Getting angry about how other people define their terms (I see you, my small-but-loyal contingent of readers who love to hate me for using AI and ML interchangeably) is not only unproductive, it reduces the range of human creativity by trying to shove all potential meanings into the same box. Either the box gets too big to be useful or it stays small but is useless to everyone except the few winners of the original argument. To keep it small, the language police must busy themselves with casting nasty all caps at anyone who dares repurpose one of their sacred terms.
What do you want people to do, name their metric UserHappiness1092412? Or perhaps use-an-entire-hyphenated-paragraph-every-time-they-want-to-express-something?
I‚Äôd prefer to live and let live. After all, shorthand and casual language is useful, as long as it isn‚Äôt misleading. Authors can do their part by defining what we mean when it‚Äôs important to be understood and chilling out (with softening language) when it‚Äôs unimportant.
Frankly, if I write at the top of my next blog post, ‚Äúfor the purposes of this discussion, define AI to mean airplane‚Äù and then I write about flying around in an AI, I‚Äôd argue that I‚Äôm in the clear. Put your fangs away, purists, you‚Äôll feel plenty silly when you realize that you do the same thing every time you write code or solve equations.
The first place you learned operationalization was in your childhood math class.
The first place you encountered operationalization was in your childhood math class, where you learned that you may use X as a placeholder for anything you like. You simply write, ‚ÄúLet X be‚Ä¶‚Äù at the top of the page.
Psychologists and other social scientists are trained to understand that the same holds for abstract concepts, but I wish this training were the norm. If only everyone‚Äôs teachers emphasized that it‚Äôs dangerous to read too much into abstract words without doing the legwork to look up the exact definition first‚Ä¶
Build the habit of not taking scientific findings too seriously before you‚Äôve looked up how the metrics are defined.
If you want to use the term ‚Äúuser happiness‚Äù as a shorthand of ‚Äúthe propensity of a user to click on at least 4 stars out of 5 on this survey question‚Äù then you‚Äôre free to do so. And it‚Äôs my responsibility to ensure that I look up your definition before I transplant any of your findings into my own applications. If I choose to offer you advice, it‚Äôll be oriented on your stated objective, not on my taste in words.
But even if I dislike your metric, there‚Äôs no need for us to fight over whether something is or is not user happiness, just as no reasonable person squabbles over whether the quantity that I‚Äôve chosen to label X is the same as the X in your notebook from 8th grade.
We‚Äôd have so much less to argue about if we learned to keep that in mind X is just a placeholder, and so is the word ‚Äúhappiness‚Äù and all its fuzzy friends.
If you had fun here and you‚Äôre looking for an applied AI course designed to be fun for beginners and experts alike, here‚Äôs one I made for your amusement:
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Saikat Halder
Data Science Earth
in
Data Science Earth
Ivo Patty
in
theFactor.e
Schaun Wheeler
in
Towards Data Science
Dave Levy
Plotly
in
Plotly
Sreemanto Kesh
The Modern Data Company
in
The Modern Data Company
"
35,https://medium.com/towards-data-science/dont-start-your-sql-queries-with-select-clause-d30fa1b701f6,Don‚Äôt Start Your SQL Queries with the ‚ÄòSelect‚Äô Statement,"The majority of developers start writing their SQL queries with the ‚ÄòSELECT‚Äô clause, then write ‚ÄòFROM‚Äô, ‚ÄòWHERE‚Äô, ‚ÄòHAVING‚Äô‚Ä¶.and so on. But this is not the ‚Äòright‚Äô way of writing your SQL queries as this is very prone to syntactic errors, especially if you are a beginner in SQL.
The ‚Äòideal‚Äô query writing sequence should be in line with how the SQL executor executes the queries. This will ensure that you don‚Äôt commit any syntactic errors and write efficient SQL queries. You will know how to filter data before performing join, when to use ‚ÄòHAVING‚Äô or ‚ÄòWHERE‚Äô clause and more.
In this blog post, we will look at the ‚Äòideal‚Äô way of writing a SQL query that will help you become an efficient SQL developer.
We will be using the Customer and Order tables (below) to find the top 2 customers from the USA/UK who have a total spend of more than $300.
Let‚Äôs dive into the right way of writing SQL queries.
Intuitively, the first step is to read the tables using the FROM clause and perform JOIN(if required). So, you should always start your query with the ‚ÄòFROM‚Äô/‚ÄòJOIN‚Äô statement.
We can also filter rows from the input tables even before the join is executed. We can do this by adding the ‚ÄòAND‚Äô clause after the ‚ÄòON‚Äô clause of the join.
The second clause in the order of execution is the WHERE clause. It is used to filter the data tables after the join has been applied.
The WHERE clause is very helpful to reduce the number of rows especially when we are working with big datasets having millions of rows.
Group By clause should be written after the Where clause. It is used to group the rows based on the selected column/columns.
In the following query, we are grouping the rows based on the customer id. After grouping, each customer id will have one row in the output. We generally use aggregations(sum, min, max, etc.) when we group the data. In this example, we will find the sum of the amount column in the Orders table.
The HAVING clause gets executed after GROUP BY, it is used to filter the aggregated rows that were generated in the group by operation.
In our example, we will filter the sum of the amount spent by each customer to be greater than 300.
The WHERE clause gets executed before GROUP BY while HAVING gets executed after it. So, the WHERE clause cannot filter aggregated data.
Columns that we want to show in the output are selected using the SELECT clause.
If we group our data using the GROUP BY clause, we need to select the grouped column using the SELECT statement.
In our example, we will select the customer id and sum(amount) to show the spending corresponding to each customer.
After selecting the columns, the next step is to provide the order in which we want to output the rows.
In our example, we can use the ORDER BY clause to order the rows in descending order of total spend.
The last step in the writing sequence is to limit the number of rows that we want to see in the output.
In our example, we can limit the total number of output rows to 2.
We looked at the ideal way of writing the SQL queries which is in line with how the SQL queries are executed.
I hope you will be writing your SQL queries in the below sequence if you are not doing that already.
You can get all my posts in your inbox. Do that here!
If you like to experience Medium yourself, consider supporting me and thousands of other writers by signing up for a membership. It only costs $5 per month, it supports us, writers, greatly, and you get to access all the amazing stories on Medium.
Follow me to see my data science posts in your feed.
Reyvaldo Wildan A
Kanellis Georgios
in
Towards Data Science
Gaurav Tank
Tierion
in
Tierion
Krushna Dash
Zachary Lee
in
Level Up Coding
Boleigha Mark
in
Sprinthub Engineering
Miguel Pereira
"
36,https://medium.com/@britney.lovett/1-secret-to-breaking-into-analytics-no-one-is-talking-about-5cd28f481291,1 Secret to Breaking into Analytics No One is Talking About,"Out of all the advice you will hear about breaking into data, whether it be in Tech or Healthcare or Education (pick your flavor), I think there is one more secret that we are overlooking and can be summed up in 6 words‚Ä¶
That‚Äôs it. That‚Äôs the thing.
StanMan
Phil La Duke
in
Authority Magazine
Candice Georgiadis
in
Authority Magazine
Zach Hughes
Tochukwu Evans Okoro
in
Change Your Mind Change Your Life
Tamara Fraser
Garrett Alley
Arthur Poropat
"
37,https://medium.com/geekculture/6-ways-you-can-make-money-while-learning-data-science-9933f9f5bcf1,6 Ways You Can Make Money While Learning Data Science,"Data Science is considered the sexiest field out there right and it is also one of the highest-paying jobs that companies are constantly looking to hire. Opportunities for data scientists will continue the grow as innovations and technologies emerge. However, it‚Äôs not an easy field to get into and it requires a lot of theoretical knowledge and hands-on skills to be to become a successful data scientist and able to land a‚Ä¶
Rehana Riaz Noorani
Le Wagon Tokyo
in
Le Wagon Tokyo
Bruno Gon√ßalves
in
Data For Science
Jared
Luciano Galv√£o
Bilgin Ko√ßak
Eve Law
in
Towards Data Science
Minhaaj Rehman
"
38,https://medium.com/interviewnoodle/youre-doing-leetcode-wrong-here-s-how-to-get-the-maximum-benefit-5517af5ce884,You‚Äôre doing Leetcode wrong ‚Äî Here‚Äôs how to get the maximum benefit in 2023,"Now you might be wondering, ‚Äú A generic blog post about leetcode? Are people still doing that? Haven‚Äôt checked it ages. Do I even need it at this point in my career and where is all this advice coming from? Some random guy?‚Äù. Well I‚Äôm not special but if you need some sort of social proof, you can scroll to the end of the blog and see that this is the advice that I have formulated and followed, and it‚Äôs worked out pretty well for me. So have a read and see if you have in fact, been using leetcode the wrong way.
If you‚Äôre in the tech industry, you are quite familiar with the LC grind. Solving numerous problems everyday in hopes that your dream company‚Äôs interview process will be composed of the exact same coding question whose index you‚Äôve memorized. You then brush off your acting skills trying to pretend this is the first time you ever saw that problem and suggest an initial brute force approach‚Ä¶‚Ä¶because figuring out the most optimal solution right off the bat within 15 minutes, is kind of susüòú. Additionally, it would invite harder problems in the remaining time.
If you‚Äôve held a tech programming job for at least 2‚Äì3 years and worked on anything impactful, you definitely had moments where you realized that the basics mattered and knowing them would have sped up your code by 10-100x. Perhaps you could have achieved your desired result with a simple one-liner instead of multiple hacky codes and function calls. While some may complain that the interview process itself is inherently flawed and you‚Äôd never use those data structures or algos in your daily job anyway ‚Äî
‚ÄúWe should just do away with these coding questions because they don‚Äôt determine who is a good programmer or data scientist‚Äù
is what they would say. These are the same people who complained when they were being taught algebra in high school. They just don‚Äôt get it. Distance yourself from such talk because that is the wrong algorithm to live your life. They live life on a rocking chair, which gives them the illusion of motion, but they aren‚Äôt going anywhere in life. Complaining all the time instead of realizing where the root cause of their misery lies is a recipe for ineffectual grandstanding. The system is fine because it is not a system at all. It‚Äôs a filter.
If you qualify that interview, get the job, and then complain that the process was crappy, you have a leg to stand on, so I‚Äôll hear you out. However, if the simple reality is that you didn‚Äôt qualify to meet the standards, you don‚Äôt have the justification to complain or ask the other side to lower or change their standards for you. If you got by the filter through sheer luck, well , good for you. If not, sucks to be you. That‚Äôs just how life works. Let‚Äôs change that, shall we?
Now, if you can muster enough of a growth mindset and get past the denial and the other 4 stages(what does it matter what they are called, you‚Äôve been stuck at stage 1 for ages) you‚Äôll see that you might not be using leetcode the right way.
I‚Äôll break this down into the following points and even though they seem intuitive and common-sensical, you might realize that you pretty much, straight up ignored it and brute-forced your way even through this simple procedure.
For the lack of a better analogy, Leetcoding isn‚Äôt like heading to a beauty parlor to put fake stuff on your face before the main event. It‚Äôs a daily skin care routine, so that you don‚Äôt need to fake anything. Think about this, if you‚Äôre on a time crunch and need to speed run through a game, you‚Äôd only be able to accomplish that task if you played it consistently for multiple days and enjoyed it.
Repetition and Practice are the key to remembering anything. This works well for programming. When you have slacked off for years and need to find a new job due to circumstances, this will save you. This is your insurance against the job market where you‚Äôre available to crush interviews anytime while the competition asks for a 2 week preparation buffer.
You might have been the king of Data structures and algorithms during your senior year, but after 4‚Äì5 years in the industry , working on a lot of cool stuff, you clearly won‚Äôt have retained the same edge you previously had. Coding isn‚Äôt a skill you just learn once. It‚Äôs a iterative process of refinement and inclusion. You haven‚Äôt learnt everything there is to know about coding because it is constantly evolving. Undergrad courses and bootcamps will only give you the basics. Coding isn‚Äôt about knowing the theoretical solution. It it about logically breaking down the problem and arriving at the implementation yourself. Given the turbulent times we currently live in , with the pandemic, recession, lay-offs etc., it pays to be prepared. Literally.
There are 2500 problems on leetcode. You don‚Äôt have to solve them all, but it‚Äôs great that it‚Äôs available for practice . You could spend an hour a day to do 3‚Äì5 problems. Once you formed this habit, within a month you would have solved around 120 , and within a year almost 1500 ! That sounds badass üòé. Imagine if you did this last year. Heck, with some new found confidence you could even participate in Weekly contests or branch out to Code Chef and Project Euler for more challenging exercises for your üß†.
Do you really need a Netflix special to get on this train, like how people suddenly are into Chess after Queen‚Äôs Gambit? Silicon Valley won‚Äôt get you there. You need to put in conscious effort on a daily or weekly basis depending on your life schedule. Don‚Äôt stop doing it after you found a job, and if you are currently looking for a job ‚Äî
This is a no-brainer. The central-limit-theorem does not miss. The 150 most frequently asked questions, cover almost every topic and algorithm you‚Äôd need to know to solve the rest 2500 problems. If you use this as your training set, you will be able to generalize and solve for any other variant of the problem by combining concepts . This is your skill tree. Is it really that much to overwhelm you, or does it seem doable within a year?
You will obviously have strengths and weaknesses. Try to improve your strengths and compensate for weaknesses. You‚Äôll probably look at company specific questions if needed. Facebook might focus on Graphs, Amazon might focus on Tries and Arrays. When the day comes, depending on the team, the kind of questions they ask can be completely different. If you go by the frequency filter, you will cover everything and be ready for harder variants of the different categories and new problems. If you do come across a completely new problem during your learning phase ‚Äî
Some of you might have scoffed at the previous point when I mentioned solving 3‚Äì5 problems within an hour. I wasn‚Äôt kidding. If a problem is taking you 50 minutes, you clearly have no clue what you‚Äôre doing. This is a learning phase where you don‚Äôt need to prove yourself to anyone. If you don‚Äôt have a concrete idea in your head 5 minutes after reading the problem, you don‚Äôt know how to solve it. If you don‚Äôt have a map, you‚Äôll keep wandering aimlessly. Stop doing that ! What do you expect will happen if you throw more time at it ? You‚Äôll magically devise and invent a new algorithm that uses 15 if conditions, for loops and some arrays and strings logic?
If you can‚Äôt think of a solution, just look at it. There is probably a simpler method or algorithm to solve that and you aren‚Äôt aware of it. Don‚Äôt spend more than 20 minutes on a problem that you can‚Äôt solve. Learn from it instead. Understand how to solve it . Better men and women than you have spent their entire careers devising algorithms that solve a particular use-case or problem variant. You CANNOT do that in 15 minutes!
You‚Äôre not that guy. So don‚Äôt beat yourself up over it. If you don‚Äôt know how to solve it with everything you‚Äôve learnt so far, this is a new learning opportunity. Add a new tool to your bag of tricks. What you learn now from this obstacle, will clear multiple roadblocks, up ahead in your path. Don‚Äôt be stuck at the first attempt
PRO-TIP : Don‚Äôt just look at the solution. Internalize it. Run it on a local IDE like PyCharm and debug each line of the code and look at what it does for various test case inputs. Write your thoughts as comments.
If you specifically want to learn every algorithm first and then give practice a shot, I wouldn‚Äôt recommend it because you learn more through hands on experience on the job or as mentioned above. If you still insist, I‚Äôd recommend the following YouTube channels for their visualization and example based explanations
And for interview prep and some support for getting into coding, you can try
Sometimes the same problem can have multiple solutions or the same solution can be implemented in a more efficient and optimal manner. that is why you need to go to the ‚Äî
This is a haven of experienced individuals where you will find tried-and-tested methods that noble souls in the leetcode platform have shared. Thanks to millions of people actively going through them and suggesting improvements in the comments, the best and clever solution posts are usually ranked at the top with titles like ‚ÄúPython O(n) and beats 99.98%‚Äù or ‚ÄúSimple one-liner in Python‚Äù. Click on those golden nuggets of coding wisdom!
In addition to that you will find more innovative ways to mix interesting ideas not just from algorithms but also other fields. What you need to soak up is not just the solution but also the thought process of the programmers and how they either frame or divide the problem into easier tasks.
Sometimes I‚Äôve even found clever ways of using additional packages like functools, numpy, scipy etc., within these posts that I was able to adopt into my work. You can port solutions across languages as well if you look at the comments in the posts.
Apart from problem specific discussions, you should also check out the general discussion forums and posts that have every information you need about a certain sub topic. Bookmark such links. Here are some of mine
Even if you amass all this knowledge, when the time comes and you‚Äôre facing a problem, either on leetcode or at work, you need to ‚Äî
As you saw above in the frequency filter part, you need to have multiple tools in your utility belt. The same problem can be solved using multiple approaches. You need to understand what the question really means. Is this a recursion or a dynamic programming problem? Is this trees or Graph traversal? If you only have a hammer, every problem will look like a nail. You need to identify what the problem actually calls for, which tool to pick and what order to use them in.
Here are some tips to get you started . You should be reactive to problem statements and come up with some initial ideas and test your hypothesis.
Keep improving your intuition. Once you do, you‚Äôll find that you can categorize most problems based on Algorithms ‚Äî The Major Five topics are:-
Every problem you‚Äôll see will be a version of these. So make sure you know enough of each to attempt a new problem of that type, otherwise you‚Äôll be going in blind.
Similar to every app or platform you use, following the right people will be of paramount importance to give you the best content. Same with leetcode , except it‚Äôs not built that way. You can‚Äôt seem to navigate the platform to get user specific posts beyond some recency. If you‚Äôve been going through discussions for a while, you‚Äôll definitely admire some coders who post clever solutions to different problems on the discussions tab. You might even want to emulate or adopt their style and learn from it. Here‚Äôs a simple google search trick to get all the posts made by a specific leetcode user.
Search for posts by the specific person. Here are some of my favorites and go-to‚Äôs when I was learning
I‚Äôve admired some of them for a while now, especially the first two in this list. I‚Äôve definitely learnt a lot of cool tricks and different ways of thinking and I‚Äôm positive you will too, when you see what I‚Äôm talking about.
This is an interesting concept in martial arts called SHUHARI. After following multiple solutions and discussions from multiple programmers, when attempting new problems , you are bound to develop your own unique style of problem solving.
You might be using more of functional programming, inbuilt python tricks, or some algorithms you learnt in a different context once you get through the first 150. Vectorizing loops, using binary for strings, traversing graphs using complex numbers, using math and number theory to get to O(1) mare just some of the various examples of code improvement I have noticed.
If you enter into an interview and solve problems in your unique way, you‚Äôll instantly stand out from the rest of the people who only look at it as a DSA problem and not as a work problem. Most people subconsciously do not identify such issues in their work code. They keep writing lines of bogus that doesn‚Äôt scale well because they never learnt how to think of a problem from different perspectives or even considered alternate approaches. A recursion problem is a recursion problem and a DP problem is a DP problem to them. They fail to see the overlaps and crossovers.
I have certainly learnt a lot from StefanPochmann and lee215 and several other programmers mentioned above. One of the most interesting pieces of code I adopted was using ‚ÄòComplex Numbers for Graph Traversals‚Äô.
If there is interest in expanding on this topic in the comments, I‚Äôll write about it and some other tricks in a new blog post.
Write down stuff. You might put your faith in your memory but that will fail you. How much of your undergrad coursework do you remember right now? Making notes will allow you to update information and be more disciplined. Keeping them safe digitally somewhere online like a Github repository would be best!
You might come up with some ingenious solutions at times when you are driven by a spark of creativity or feeling the zone. Document it. All of this will also reflect in your work life and you‚Äôll get better and write better code.
SOCIAL PROOF :
On the off chance that you need to interview you have a great system of notes and codes to refer to ! If you think I‚Äôm just preaching, here is some social proof . I had interview calls through the year while I was preparing (even from Deepmind in March). I continued the grind despite failed interviews, taking up a start up job for 6 months and doing more interviews towards the end of the year.
I got job offers from Amazon, Google and some other start ups towards the end of 2018 and picked Walmart because it was the best fit for my career and personal goals at that time. You can look at my career trajectory on LinkedIn to see how that turned out !
Your efforts and the skills you build up, will never betray you. So assimilate everything you read in this blog and act on it this year, and you can be self assured that you won‚Äôt have any issues or worry about the ‚Äúinterview process‚Äù ever again.
Once there is some feedback, and I gain more insights to add, I‚Äôll be sure to update this blog post. I hope after reading this, you‚Äôll start looking at Leetcoding as a daily continuous habit to improve your skills and not simply as a tool for interview cramming. Till next time !
If you liked this blog and found it helpful, please share this with your network and hit that claps (üëè) button below to update the recommendations. Feel free to add any insights you‚Äôd like to share, in the comments below! If you ever lose your way, come back and read this
Jessica Lim
in
Better Programming
Stefan Wolpers
in
Food for Agile Thought
Aleksandra Sikora
Fernando Eugenio Correa
in
Mercado Libre Tech
Katelyn Ashok
Alibaba Tech
Aanchal Patial
Ran Isenberg
"
39,https://medium.com/pipeline-a-data-engineering-resource/creating-the-dashboard-that-got-me-a-data-analyst-job-offer-de3f0d6b1771,Creating The Dashboard That Got Me A Data Analyst Job Offer,"On this and any other platform I purposefully avoid meta content, that is, content about content for the purpose of being, well, content.
Darpan Lekharu
Mackenzie Tittle
in
Getting Into Chess
HR Remedy India
GEOLYTIX
Gary A. Stafford
in
Towards Data Science
Elijah Meeks
in
nteract
Stitaprajna Panda
Stefano Bosisio
in
Towards Data Science
"
40,https://medium.com/towards-data-science/5-python-projects-to-automate-your-life-from-beginner-to-advanced-90fe29a7d664,5 Python Projects to Automate Your Life: From Beginner to Advanced,"If you‚Äôre learning Python, you should try to automate your everyday tasks.
You not only learn more Python by implementing what you already know but, in the end, you can see how all your hard work pays off.
Recently, I‚Äôve been automating tasks more than often due to my lack of time. Thanks to that I have 5 new projects that I classified as beginner, intermediate, and advanced.
You‚Äôll find links to the full scripts and tutorials to solve each project. Also, I‚Äôm leaving a challenge to each of them to test your Python skills.
How many times did you have to use Word to create a cover letter, contract agreement, resume, or report?
If you can‚Äôt count them, this project is for you.
In this project, we‚Äôll automate the generation of Word documents using Python and a library called docxtpl. To do so, we need a template of the document we want to automate. Inside the template, we have to introduce placeholders on each word we want to replace later. We use the {{}} symbol for the placeholders.
To make things easier, I already created a template for a cover letter. Here‚Äôs how the template we‚Äôll use looks.
To make this project beginner friendly, we only have to replace the elements {{my_name}}, {{my_phone}}, {{my_email}}, {{my_address}}, and {{today_date}} using Python anddocxtpl.
In the video below, you can download this template, get the script, and follow the steps to solve this project.
Challenge: If you want to take this project to the next level, try extracting data from an Excel file and introducing it in the Word template.
The second part of the tutorial above has an Excel file that looks like the following table.
You should generate a new document, for each row of the table.
One of the most boring tasks I used to do at work was sending emails.
Important emails should be read and answered carefully, but emails that are part of an everyday routine should be automated! This is what this beginner project is all about. We‚Äôll automate sending emails with Python.
How to automate this task? We only need to use the built-in modulessmtplib and email . We need smtplib to send emails using the Simple Mail Transfer Protocol (SMTP), and the email package to read, write, and send more complex MIME messages.
Below there‚Äôs a step-by-step tutorial to automate sending emails with Gmail. Here‚Äôs what it covers:
Challenge: If you want to make this project more challenging, try to attach images to your emails. To do so, you need to use the imghdr library.
One of the most tedious tasks on the internet is filling in web forms.
I mean that! Filling in your name, phone number, address, and more information over and over again is boring. Things get worse when you have to come up with fake data to fill in a survey.
In this intermediate project, we‚Äôre going to fill in fake data to web forms using Python. How to automate this task? We‚Äôll use a web automation library that allows us to perform web actions like going to websites, typing text, and clicking on buttons.
Below there‚Äôs a step-by-step tutorial on how to automate this tedious task with Python. Here‚Äôs what it covers:
Challenge: The form we fill in this tutorial has simple elements. If you want to make this project more challenging, add advanced elements to the form such as dropdowns and multiple-choice questions. Then try to automate this advanced form.
Swiping right on dating apps is so time-consuming that once you get matches, you become lazy in the most important part of online dating ‚Äî converting matches into actual dates!
This advanced project consists in creating a bot that swipes right for you, so you can later work on your pickup lines and increase your conversion rate.
What makes this project advanced? Many things. To name a few:
Below there‚Äôs a tutorial that will help you deal with all of these issues.
Challenge: Make your bot send random pickup lines to your matches.
But only do that for the sake of this project! If you want to get more replies, don‚Äôt automate sending messages. I tried it and it‚Äôs not as effective as sending high-quality messages yourself.
Without a doubt, automating a tedious task you have at work is the most challenging thing you can ever do.
Why? Most likely there isn‚Äôt a step-by-step tutorial to guide you, you have to think of what library you‚Äôd need to use, and the best approach to automate it.
To find out what to automate ask yourself these questions:
Even if you don‚Äôt find a tutorial on how to automate that specific task if you know the answer to question #3, you can check the documentation or read a guide to get started with that library.
Let me know in the comments what task you‚Äôre automating right now!
Automate your life with Python! Get my FREE Automation Cheat Sheet by joining my email list with 20k+ people.
If you enjoy reading stories like these and want to support me as a writer, consider signing up to become a Medium member. It‚Äôs $5 a month, giving you unlimited access to thousands of Python guides and Data science articles. If you sign up using my link, I‚Äôll earn a small commission with no extra cost to you.
frank-andrade.medium.com
Tjiputra Yapeter
Nina Gurkova
Saber Coder
Florent Biville
in
Neo4j Developer Blog
Play Store
Claire Vo
in
Product Experimenters
James Bowen
Mohammad-Ali A'R√ÇBI
in
ITNEXT
"
41,https://medium.com/towards-data-science/5-quick-and-easy-tips-to-become-an-efficient-data-scientist-in-both-python-and-r-5cf51f6396a3,5 Quick and Easy Tips to Become an Efficient Data Scientist in Both Python and R,"A recent discovery of mine is that many data scientists out there use both Python and R.
The story typically goes that they use R for graduate studies and research while using Python for work. Since many individuals work while going to school, these data scientists have to switch back and forth between Python and R, sometimes several times within a single day depending on what they‚Äôre working on for school or work.
Because many (read: most) data science courses or learning paths focus on one language over another, there‚Äôs a gap in the learning that neglects to help data scientists switch between both languages. Perhaps surprisingly, switching between two programming languages on the same day can be challenging, especially when you‚Äôre seeking to complete similar work. For example, it‚Äôs easy for web developers to switch between HTML, CSS, and JavaScript in the same day because each language handles a very specific piece of a website. However, data scientists can be completing one type of data analysis in R for a graduate studies class while later in the day they need to use Python to complete a different analysis for their job.
The high number of data scientists who are switching between both languages in a given day means that a gap in data science education needs to be filled by providing tips on how to better manage using both languages regularly. Here, we‚Äôll look at a few simple tips that can be implemented as part of your routine in less than 10 minutes that will help you become a more efficient data scientist at switching between Python and R.
One of the best things you can do to begin working efficiently in both Python and R is to complete your Python and R work on separate days. By batching all of your Python tasks on one day and R tasks on another, you guarantee that you won‚Äôt need to switch languages in the same day.
This takes less than 10 minutes to implement as all you need to do is plan your week around what language you will be working with on a given day. For example, Monday, Wednesday, and Friday could be days you complete work done in Python, whereas Tuesday and Thursday are reserved for tasks requiring R.
Day-batching your tasks based on programming language allows you to remain in a state of flow where you don‚Äôt have to worry about using different conventions, syntax, or documentation. Instead, you can focus on writing clean code given the language‚Äôs conventions without falling into the trap of coding with an accent (see tip 5).
I remember in my first year of university studying software development that most of my coding courses used C#. This made it easy to move from class to class without having to get in the groove of using a new language. Things got substantially harder in my second year when we began taking courses in a variety of languages, including C#, Java, PHP, and more. This made it more difficult to get into a groove due to constantly having to change programming languages every two hours.
This challenge is comparable to speaking several different verbal languages in a day. Before you become fluent, it‚Äôs mentally taxing to make the switch between different languages. The shapes your mouth and tongue need to make are different, the sounds are different, the grammar is different, and the conventions and customs of the language are different. However, if one day you speak English, the next French, the next Italian, and so on, you‚Äôll find that you‚Äôre much more capable of speaking efficiently and effectively because you‚Äôre working for an extended period of time in one language.
However, it‚Äôs not always possible to dedicate entire days to either Python or R. Sometimes, work, school, and life all happen at once which means that you need to juggle several different deadlines, projects, and requirements. An alternative, though similar method, of batching your tasks is to make one-half of the day dedicated to Python tasks and the other half dedicated to R tasks. Then, by having a substantial break in the middle where you do nothing related to either batch of tasks, you can move into the latter half of the day ready to work in a different language. The break in the middle of the day is essential to reset your brain and give it a chance to expunge your usage of the other language. By blocking your tasks into two separate halves of the day, you ensure that you‚Äôre not working on one task in Python for 20 minutes, followed by a task in R for one hour, and a continuation of this cycle that leaves your brain exhausted and not working at its maximum efficiency.
When I was in university, I had the easiest time working in both C# and Java because I was able to find the similarities between the two languages and use them to work efficiently in both languages. Similarly, when I was learning Russian, I found several similarities between it and French and English, both of which I already knew. These similarities allowed me to understand the connotation of a lot of words even if I didn‚Äôt understand the words immediately at first.
The trick to being able to switch between Python and R is to find the similarities between the two languages, however finite they might be. For example, both Python and R can be used to write object-oriented code and they both can be used to conduct statistical analyses. Sure, working from two differently-indexed languages can be a pain, but at least they have more in common than is different. However, you can at least use the same IDE for both languages, as RStudio and Atom are capable of supporting both R and Python.
It‚Äôs important to remember that the similarities you find between the two languages may be different from those found by someone else. Similar to how analogies can be created in different ways to describe the relationship between two things, so too can the similarities you find between R and Python be used to ease the transition between both languages.
This tip takes less than 10 minutes to complete as you can begin creating a list of Python and R similarities on a sticky note. This note can be left on your desk as a reminder of the key points of each language that can be used to easily transition from one to the other.
The most valuable skill I learned while studying software development at university is how to ask good questions. I‚Äôm sure you can relate that asking ill-defined questions will lead you to about 200 different StackOverflow hits that may or may not sort of relate to the answer you‚Äôre looking for but are not exact enough that you get a quick solution to your issue.
Working in two languages like Python and R requires that you can ask the right question for the language you‚Äôre currently working in. For example, asking a question about R in the same way you would ask a question about Python may return results that, as mentioned previously, aren‚Äôt quite what you were looking for.
While copying error messages into search bars is a great way to find StackOverflow responses to similar problems. However, do you really understand what went wrong if all you‚Äôre doing is copying an error message? A better way to become familiar with the quirks of Python and R is to work towards understanding what went wrong in your code. Then, you can ask better questions that apply more directly to the problem you‚Äôre having than a generic error message ever could.
Practicing how you ask your questions can be completed in less than 10 minutes each day and is best completed while you‚Äôre working. You‚Äôll quickly begin to notice which questions are yielding the right answers versus those that are leading you to general answers that could solve any myriad of problems.
While I mentioned earlier that Python and R have more in common than not, one of the key places where they differ significantly is their documentation.
Python is known for having some of the best code documentation out there, whereas R is seriously lacking. Keeping whatever documentation you can get your hands on close by can help you write code efficiently when transitioning between two languages. For example, during university when I was working on a project in Typescript (Angular, to be exact) I kept its documentation open in a browser, ready for whenever I hit a gap in my knowledge. While Typescript languages are similar to JavaScript, I found that there was enough of a learning curve between the two languages that having the documentation handy at all times made the transition that much smoother.
However, I‚Äôve always found that creating my own documentation or cheat sheets is a million times more valuable than looking up technical documentation, no matter how well-written. You know how you end up subconsciously studying while creating cheat sheets or study guides, even if it doesn't feel like it? The same thing applies when creating reference guides, documentation, sheet sheets, or study guides for Python and R. I‚Äôve seen some beautiful reference sheets shared on LinkedIn that condense a vast amount of knowledge down into a single 8.5 x 11 sheet of paper that you can print out and have beside you at all times. These reference sheets can be used as inspiration and a starting point to begin creating your own cheat sheets and documentation that can help you make the transition between Python and R. I like including code snippets, formulas, function information, and package descriptions in my reference sheets for different programming languages.
Creating your own reference sheet or nabbing someone else‚Äôs can take less than 10 minutes and can save you the need of having to memorize the intricacies of both Python and R.
Speaking with an accent while working on a new natural language is cute. Coding with an accent is not.
I‚Äôve said this before and I‚Äôll say it again: it doesn‚Äôt do you any good to code with a language as if it were another language.
When you code in a language not using it the way it was intended, it will be unreadable by anyone else using it, it won‚Äôt run the way it was intended to run, or worse, it won‚Äôt even know what it is that you‚Äôre asking it to do.
Becoming deeply familiar with how Python and R are uniquely designed, their conventions, and their strengths and weaknesses will help you write code in each language to the best of its potential. This is where having quick reference sheets (see tip 4 above) can be handy as they provide you with the correct syntax, variable conventions, and format that's standard for the language you‚Äôre working in.
Python and R are pretty dissimilar when it comes to their syntax, format, and conventions, which is why having a reference guide can be handy to keep you from slipping into coding with an accent.
My favorite way to make sure that I‚Äôm not coding with an accent is to set an alarm for every half hour or hour of work that I complete that reminds me to check my code for fluency. This code check takes less than 10 minutes and is just a reminder for you to look over syntax, conventions, and format to ensure that you‚Äôre writing Python while working in Python or that you‚Äôre writing R while working in R.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Support my writing by donating to fund the creation of more stories like this one: Donate
Doug Steen
in
Towards Data Science
Matt
in
Towards Data Science
Golff Protocol
Madison Hunter
in
Towards Data Science
Simon Carter
in
Towards Data Science
Paul Sina√Ø
Machine Learning Express
Prasanthi Nayunipati
in
Jovian‚Ää‚Äî‚ÄäData Science and Machine Learning
"
42,https://medium.com/gitconnected/how-to-learn-data-science-in-6-months-while-working-a-9-5-job-f2f240d470aa,How to Learn Data Science in 6 Months While Working a 9‚Äì5 Job,"Are you interested in learning data science but don‚Äôt know how to fit it into your busy schedule?
Don‚Äôt worry, you‚Äôre not alone!
In this post, we will discuss some tips for how to start learning data science while working a full-time job.
We will also provide a list of resources that you can use to get started. So whether you are just starting out or you are already familiar with the basics of data science, there is something for everyone in this post!
When you start learning data science, you will likely be introduced to a variety of different concepts and terms.
It can be difficult to keep track of everything at first, but don‚Äôt worry, this is normal!
Here are some of the most important concepts that you will learn when studying data science:
If this sounds overwhelming, don‚Äôt worry! You‚Äôll learn more about each of these topics as you study data science further.
If you‚Äôre trying to break into the industry it‚Äôs important that you make use of your free time.
A great way to do this is by studying data science from Monday-Friday, from around 6 to 9 PM.
Giving yourself 3 hours every weekday to study will help you keep pace with your goals and get in front of the curve.
When I first started learning data science my schedule was work from 9‚Äì5, go home and eat, study from 6‚Äì9 then go to the gym from 9‚Äì10. This was a great way for me to study without taking away from my free time on the weekends.
Of course, this may not work for everyone so feel free to tailor it to fit your own needs!
If you‚Äôre looking for a more structured learning experience, consider buying an online course.
These courses usually last around 12 weeks and will teach you everything from the basics of data science to more advanced topics like machine learning and deep learning.
Some of our favorite courses are:
These courses will give you the skills necessary to enter into a career as data scientist or data analysis.
You could also try learning from books if that‚Äôs what works best for your schedule!
In addition to the previously mentioned resources, there are also many free sources that you can use. Here is a list of some of our favorite places:
Once you have a good grasp on all the data science concepts and know your way around Python or any other coding language, it‚Äôs time to put them into practice.
The best way to learn is by doing so I recommend creating your own projects that relate directly with what interests you most.
For example, if you love sports try building an NBA player predictor model or if you enjoy music maybe create a model that can identify different genres based on song lyrics.
The possibilities are endless when it comes to data science projects so don‚Äôt be afraid of getting creative!
If you‚Äôre looking for a more structured way to do projects, consider doing them through Kaggle.
Kaggle is a website where people can compete in data science competitions. There are usually different types of competitions available (such as regression, classification, and clustering) and they vary in difficulty level.
This is a great way to not only practice your data science skills but also to learn new ones.
Plus, you can compete against others from around the world and see how well you stack up.
One of the best things about data science is that there‚Äôs a huge online community of people who are willing to help.
If you ever get stuck or need some guidance, don‚Äôt be afraid to reach out and ask for help!
A great way to stay motivated and connected with other people learning data science is by joining a Slack group.
There are plenty available online for free so do some research and see what works best for you!
Here‚Äôs one I found after a quick Google search if anyone is interested in joining:
datascience.slack.com
Additionally, you can try to find a mentor or someone who has more experience than you to help guide and support your learning.
Finding a mentor can be tough but it‚Äôs definitely worth the effort if you want to accelerate your progress.
I recommend trying online communities or forums first, and if that doesn‚Äôt work then reach start outreaching on LinkedIn to see if someone is willing to help or know anyone who is.
We all want to be successful in our careers, but it‚Äôs not always easy. Data science is a tough field and you need to put in a lot of hard work if you want to succeed.
If that‚Äôs something that interests you then don‚Äôt hesitate ‚Äî start learning today! The sooner you start the sooner will become an expert!
There are plenty of resources available online so don‚Äôt let lack of resources stop you from achieving your goals.
If this post has helped motivate anyone at all then I consider it a success :) Good luck on your journey and happy learning!
Join my email list with 5k+ people to get ‚ÄúThe Complete Python for Data Science Cheat Sheet Booklet‚Äù for FREE
Pƒ±nar Ersoy
in
Towards Data Science
Bupe Kolosa
Archit Yadav
in
Towards Data Science
Evgen Bodunov
IPL- School of Data Science
Virginia Backaitis
in
Digitizing Polaris
Rutuja Pawar
in
WiCDS
Vaibhavkrishna Bhosle
"
43,https://medium.com/towards-data-science/5-things-in-your-resume-that-are-keeping-you-from-getting-your-first-job-in-data-science-1a4d474ff3e4,5 Things in Your Resume That Are Keeping You from Getting Your First Job in Data Science,"Learning the required skills can sometimes be considered the easy part of the process of becoming a data scientist. The hard part often comes later when it comes time to apply for jobs.
Nowadays, many people can call themselves data scientists despite varying competencies and abilities to provide impact, which means that you need to stand out from the crowd.
With modern job application processes, however, it can be hard to even get a foot in the door when you can‚Äôt talk to a recruiter in person. With many of us sending our resumes off into the void, it‚Äôs difficult for us to know whether or not we‚Äôre making the right impression.
Luckily, there are some easy ways to spruce up your resume to ensure that recruiters are giving it a longer look than the average 7.4 seconds.
Here are five different things in your resume that are keeping you from getting a job in data science, and five ways you can fix them:
One of the latest trends in resume-building is a graphical representation of skills using skill bars, percentages, or other graphics to demonstrate your level of skill mastery.
While this option is available on many resume templates intended to be sent to hip, startup-style companies, it‚Äôs doing you a disservice and potentially discounting you from the job application process before they‚Äôve even met you.
First, skill-level bars are utterly meaningless to a recruiter. Unless you develop a point scale system, how does a recruiter know what ‚Äú4/5 stars in Python proficiency‚Äù means? Additionally, to help the recruiter better understand your rating system, you would need to develop criteria and objective tests for each skill so you can explain why you would give yourself the score you did.
Second, skill bars are subjective. In short, people are bad at rating themselves on anything, with more intelligent people tending to rate themselves lower on skills they are proficient in and less intelligent people rating themselves higher on skills in which they are not proficient.
Third, your ability to rate your skills is completely unreliable. Without a reliable, widely-known testing scale you can use to determine your skill proficiency, such as the Common European Framework of Reference for Languages which is recognized the world over, your own ability to ascertain your skill level is entirely unreliable. For example, if you tell someone that you are at the B2 level in Spanish, they will likely understand what your proficiency in the language is. However, if you tell someone that you know 80% of Python, they‚Äôll have no idea what your abilities are ‚Äî because, let‚Äôs face it, what does an 80% knowledge of Python look like?
In the end, using skill-level bars on your resume discounts you from many job opportunities before you even get a foot in the door. A recruiter may see that you are an ideal candidate, but they may prefer to interview someone who has an 80% proficiency in Python compared to your 70%.
Instead of skill-level bars, discuss the skills you used in your description of previous employment and projects to demonstrate how long you‚Äôve been using the skill, what you‚Äôve used the skill for, and what impact you‚Äôve made by using the skill.
One of the most common complaints of tech recruiters is that candidates lack real-world problem-solving skills.
It‚Äôs easy to solve a problem in university when it‚Äôs laid out nicely in front of you with a beginning, middle, and end, and a professor who is ready to answer your questions about how to get the desired result.
However, the real world will throw problems at you that have no concrete beginning, middle, or end, and may only have a vague business question that is now your job to answer using data. The working environment that you‚Äôre applying to be a part of isn‚Äôt as squeaky clean as what you experience in university or coding bootcamps, which means that you need to develop your real-world problem-solving skills.
The key here is to remember the old saying, ‚Äúshow, don‚Äôt tell.‚Äù
While it‚Äôs easy to tell an employer that you‚Äôve solved many real-world problems through your independent research projects, it‚Äôs much more impactful if you show them the problem you solved and exactly how you solved it.
A personal portfolio linked to your resume will help you out here. In that portfolio, you want to develop several projects that figure out specific problems within your target industry. For example, if you want to work in climate science, fill your portfolio with projects that analyze air pollution levels and describe how pollution levels correlate to hospitalizations in contrasting major cities.
First-time data science job seekers will set out with a resume listing 20 programming languages they are proficient in without focusing on the languages that an employer is specifically looking for in their job ad.
Listing 20 programming languages or skills is useless to an employer who only wants to know if you can work with Python, Excel, and SQL. Additionally, this shows a lack of preparation on the job seeker‚Äôs part because it presents a resume designed to be a ‚Äúone-size-fits-all‚Äù solution to the job application process.
Remember, more isn‚Äôt always better. A recruiter would rather see concrete evidence of experience with the programming languages and skills their company is looking for than random experience in 20 different languages.
Recruiters tend to only look at resumes for 7.4 seconds, which means that if they have to sift through your list of skills for the ones that apply to their company, then you‚Äôve already made yourself an easy candidate to weed out.
Prepare a tailored resume for each job application by carefully scanning the job ad and highlighting the exact languages and skills they are looking for. Then, make it easy for the recruiter by giving concrete examples of your ability with that language or skill, and link to projects that demonstrate your abilities (again, ‚Äúshow, don‚Äôt tell‚Äù).
It‚Äôs important to focus on the languages and skills a specific company is looking for because their business processes revolve around those languages. Unless a company is hiring you to set up their data analytics processes, they're looking for a candidate who can meld seamlessly into their existing teams and processes using their required languages and skills.
In short, the focus should be on making it easy for a recruiter to visualize you successfully becoming a valuable member of the team.
As so succinctly stated in Ken Jee‚Äôs article Data Science Resume Mistakes to Avoid, the value of data scientists is their impact on a company and the outcomes their work produces.
If your resume focuses more on the fancy algorithm you used to solve a problem instead of the outcome of solving that problem, a recruiter may not be convinced of the impact your analysis had.
Instead of focusing the entirety of your project description on the algorithms you used or the code you wrote, add sentences where you describe the impact your analysis had or the outcome of your project.
This description should include quantifiable information, such as percentages, growth indexes, and monetary values. For example, you could say that the pro-bono project you did for a small business in your town helped them increase the efficiency of their processes such that they increased their revenue immediately by 30%. Alternatively, you could say that your analysis of atmospheric pollution levels has indicated that global temperatures could rise 2 degrees Celsius by 2100.
Giving a recruiter concrete details about the impact your project had or the outcome it produced will give them a sense of your ability to impact their company positively.
As mentioned previously, a data scientist is only as valuable as the impact they provide to a company. This means that data scientists must remain relevant throughout their careers by constantly learning new skills, educating themselves about new trends in the sector they work in, and exposing themselves to the latest technology that could prove more efficient for the company‚Äôs processes.
No indication of continued education, whether through courses, bootcamps, or personal projects, or a large gap between the current time and the last course you took, could indicate to a recruiter that you‚Äôre not serious about the lifelong learning that is so critical to data scientists.
Make a point of working on personal research projects every so often to show recruiters that you are always looking to improve your skills and learn something new.
Each new project doesn‚Äôt have to be huge, but it should integrate some new technology, algorithm, or thought process to tackle a real-world problem. Each project should be described in your resume using a short sentence that captures the advancement of your skillset and the problem you set out to solve.
Adding links to your project repository or portfolio is a great way to give recruiters a hands-on look at your work.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
ODSC - Open Data Science
Yuri Veremeyenko
in
Homeday
Ming Zhong
ODSC - Open Data Science
ODSC - Open Data Science
Priyanka Dobhal
Adrian Macal
Hitesh Gautam
"
44,https://medium.com/geekculture/is-a-mac-or-windows-pc-better-for-programming-d5556bf06f1,Is a Mac or Windows PC Better for Programming?,"If you‚Äôre learning to code or have some experience programming, you‚Äôve probably wondered whether a Mac or a PC is better for programming.
Gabriel Toney
Len Chen
Sam Ngu
Tremaine Eto
in
Towards Dev
Jreyes
Soumendra Mishra
in
Google Cloud - Community
Karishma Agrawal
James Crosby
"
45,https://medium.com/towards-data-science/the-5-fundamentals-you-need-to-efficiently-self-teach-data-science-81b5d103cb0e,The 5 Fundamentals You Need to Efficiently Self-Teach Data Science,"Self-teaching yourself anything is an admirable undertaking.
Whether it‚Äôs a new language, baking, weightlifting, structural geology, basket weaving, driving manual transmission vehicles, or anything else you can think of, I have insane amounts of respect for anyone who decides they‚Äôre going to teach themselves something.
When someone tells me they‚Äôre going to self-teach themselves data science, my awe is tenfold. Not only are you announcing that you‚Äôre going to teach yourself one of the most demanding fields in tech, but you‚Äôre also choosing to study a myriad of not-so-easy skills to get there, including programming, mathematics, data visualization, and more. Anyone who decides that they‚Äôre going to study multivariable calculus in their downtime as a means of becoming a data scientist is not only crazy but also someone to be admired.
Along the way, I‚Äôve discovered a few fundamentals that will help you master the most difficult topics in data science. Not only do they make the self-teaching process easier once established, but they also set you up for success in achieving your goals.
Have you ever been on a consistent schedule that stopped all of a sudden and then you weren‚Äôt able to get back on track? For example, you could be going to the gym consistently every morning at the same time before work until all of a sudden you begin working a different shift which throws off your entire schedule. Now, you no longer have time to work out at your regular time in the morning. This may result in you not working out anymore until you find a new time that works with your new schedule.
The same thing applies to learning data science.
Successfully self-studying data science requires that you put aside a dedicated chunk of time every day in which you carry out your studies. This study period needs to be at the same time for the same length every day (or at a rate each week that is maintainable and effective in allowing you to learn concepts). Turning your studying into a habit that you complete every day is a fool-proof strategy to ensure that you consistently complete your studies.
To optimize your study habit, you need to plan what you‚Äôre going to be working on each day. This allows you to jump right into your study period without having to take time to determine what you‚Äôre going to be working on. Not only does this waste time, but it also opens the door for you to become distracted by emails, social media, or other life things that come up. Therefore, on a day when you‚Äôre not studying, take some time to block out what topics you‚Äôre going to be studying each day. Then, when it‚Äôs time to study, you can sit down, put on your noise-canceling headphones, turn on your site blocker, and get started.
This fundamental may not be the same for everyone. However, in my experience, I‚Äôve found that a solid foundation in whatever programming language you‚Äôre going to be using for most of your data science learning is extremely beneficial.
I‚Äôve found that when you have a good grasp of the programming language you‚Äôll be using, the rest of the learning process is extremely easy because programming is one less thing you need to think about. This is why many data science learning plans encourage students to learn the programming part first ‚Äî including the ones I‚Äôve created.
towardsdatascience.com
towardsdatascience.com
By learning how to code properly first, you can begin to solve your data science problems by thinking in code. While your brain is working in the background on writing your code, the majority of your brain power can go towards solving data analysis questions or putting together the logic behind machine learning models.
A lot of people think they‚Äôre good at data science until they no longer have the answer in front of them to work off of.
What often happens when individuals are teaching themselves data science is that they gain a false sense of confidence in their skills because all they‚Äôre doing is following along exactly with video tutorials and getting the same answer. Unfortunately, if you were to throw the same person in front of an unsolved data science problem, you‚Äôd probably be stuck there for hours while they try to work it out without having any actual skills to use.
To properly teach yourself data science, you need to work through a slew of practice problems and projects to really put your skills to the test. The reason why in-person learning can be so effective is that you‚Äôre constantly being given practice problems to solve, homework questions, quizzes, and tests that force you to learn how to do the thing and then apply it.
What can sometimes go wrong with self-teaching is that people will follow along with YouTube videos thinking that they‚Äôre working their way through a problem when, in reality, they‚Äôre just copying code, math, and logic from someone else.
The trick is to find as many practice problems, loose data sets, and projects as you can and then work your way through them without knowing the answers. This not only works out your brain and forces you to actually learn the skills required, but it also trains your brain on how to solve problems ‚Äî a skill that will become extremely valuable when it comes time to go through data science interviews.
You can‚Äôt learn anything unless you have a specific, time-sensitive goal breathing down your neck.
For in-person learners, this goal is usually to pass an exam that has a set date. However, for self-learners, this goal will look a little different.
Self-teaching requires an insane amount of discipline, something that can be best achieved when you set specific, time-sensitive goals. I hate the SMART goal acronym, but here, it makes perfect sense, especially for those who are looking to make a quick career change into data science.
SMART goals are specific, measurable, achievable, relevant, and time-sensitive. When it comes to self-teaching data science, your SMART goals could be related to learning a specific concept, passing a certain online certificate, or getting prepared for data science interviews that you plan on having in one year. SMART goals are nice to use because they can be as big or as small as you want, which can help you plan for your short-term learning as well as your long-term future.
In my experience, no learning gets done if you don‚Äôt have some specific and time-sensitive milestones to work towards. These goals or milestones are most effective when you have them displayed in a prominent place, such as written into your calendar or post-its that are attached to the bottom of your monitor. A constant visual reminder, as well as even a reminder from those around you, is a great way to remain motivated with your self-learning goals.
Self-learning is an arduous, draining experience that, let‚Äôs face it, is very rarely fun.
I‚Äôve found that the best thing you can do for yourself when self-learning is to have an outlet where you can let off steam and give your brain a break. For me, that‚Äôs taking the dog for long walks in nature, writing on Medium or for my newsletter, volunteering, and reading fiction books. None of these things have anything to do with data science (even my articles on Medium aren‚Äôt on the nitty-gritty technical aspects of data science) and that‚Äôs exactly what my brain needs regularly.
towardsdatascience.com
As much as you may feel pressured to study every single day, especially if you‚Äôre trying to make a quick career change, it‚Äôs vital that you remember to take breaks. As someone who once worked 180 days straight with no breaks, I understand that constant work or studying is unsustainable. Not only that, but at that point, you‚Äôre not actually learning that efficiently. Your brain will be taking a lot longer to process information than if you were to attack it after a day spent doing something you love. Therefore, it‚Äôs vital for your learning success that you give your brain the break it needs before diving back in head first into self-teaching data science.
There are a lot of ways to correctly self-study data science. Everyone you talk to will have a different formula for success and it‚Äôs important to remember that what works for one person may not work for you.
However, if there are five lessons or fundamentals that I could leave you with, it‚Äôs those listed above that I‚Äôve found to be the most effective in ensuring an efficient self-learning process.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Support my writing by donating to fund the creation of more stories like this one: Donate
ODSC - Open Data Science
Nic Wayand
in
pangeo
Agile Alliance
Team
in
Studio X
Ethnographic Machines
Madison Hunter
in
Towards Data Science
Madison Hunter
in
Towards Data Science
LightOn
"
46,https://medium.com/towards-data-science/why-is-mean-squared-error-mse-so-popular-4320d5f003e5,Why is Mean Squared Error (MSE) So Popular?,"As a recovering statistician, I‚Äôll be the first to tell you that my people are a painfully literal-minded bunch. Where AI folk favor more sci-fi-flavored names, stats folk love things to do exactly what it says on the tin‚Ä¶ and nowhere is this attitude more in-your-face than popular metrics MSE and RMSE.
These names are ‚Äî wait for it ‚Äî literally the recipe for calculating them, backwards. Like summoning a very tame tiny demon.
How to calculate mean squared error (MSE):
Ta-da, that‚Äôs the MSE.
And if you‚Äôre looking for the root mean squared error (RMSE), you simply take a square root at the end.
4. You take the root. (R)
Yup, these metric names are so creative it hurts. They really are just recipes in reverse.
If you grok what the MSE is from this quick explanation, keep reading! If you‚Äôre not sure what an ‚Äúerror‚Äù is and/or you‚Äôre feeling a bit confused, take a quick detour to my intro MSE walkthrough here.
The mean squared error (MSE) is one of many metrics you could use to measure your model‚Äôs performance. If you take a machine learning class, chances are you‚Äôll come across it very early in the syllabus ‚Äî it‚Äôs usually baby‚Äôs first loss function* for continuous data.
(If you‚Äôre fuzzy on what any of the bolded words mean, you might like to follow the links for a gentle intro to each concept.)
You‚Äôve seen what the MSE *is* ‚Ä¶but why is it so popular? Why does it seem to be everyone‚Äôs favorite scoring function?
There are a few reasons, and some of them are even good reasons.
Why might we wish to calculate the MSE?
Performance evaluation and optimization are two different goals‚Ä¶ and there‚Äôs no law of the universe that says you *must* use the same function for both. Understanding this subtlety will mitigate a lot of future confusion if you stick around in applied ML/AI.
For this discussion, I‚Äôll assume you understand how and why a function is used for evaluation versus optimization, so if you‚Äôre fuzzy on that, now might be a good time to take a small detour.
When it comes to model evaluation, the MSE is rubbish. Seriously. There‚Äôs everything wrong with it as a metric, starting with the fact that it‚Äôs on the wrong scale (a problem often solved by taking the square root to work with RMSE instead) but not ending there. It also overweights outliers, making both the MSE and RMSE confusing to interpret. Neither one accurately reflects the meaning that would be most interesting to a person who wants to know how wrong their model is on average. For that, the ideal metric is something called the MAD. And there‚Äôs no reason not to use the MAD for evaluation ‚Äî it‚Äôs easy to calculate.
So why is everyone so obsessed with the MSE? Why is it the first model scoring function you learn? Because it‚Äôs really great for a different purpose: optimization, not evaluation.
If you want to use an optimization algorithm (or calculus) to quickly find the ideal parameter settings that give you the best ‚Äî most optimal! ‚Äî performance, it‚Äôs nice to have a convenient function to work with. And it‚Äôs hard to beat the MSE for that. There‚Äôs a good reason that the first derivative you‚Äôre ever taught is x¬≤ ‚Äî in calculus, squares make things super easy. The next things you‚Äôre taught in calculus 101 is what to do with constants and sums, since those are super easy too. Guess what? Squares, sums, and constants (1/n) is the whole formula for MSE!
And that, my friends, is the real reason the MSE is so popular. Pragmatic laziness. It‚Äôs literally the easiest vaguely sensible function of the errors to optimize. And that‚Äôs why it was the one Legendre and Gauss used at the turn of the 19th century for the first ever regression models‚Ä¶ and why we still love it today.
But is it perfect for all your needs? And does it outperform other loss functions in all conditions? Certainly not, especially when you‚Äôve got an infestation of outliers in your data.
In practice, you‚Äôll often have two functions you‚Äôre working with: a loss function and a separate performance evaluation metric. Learn more about that here.
Now that you know the reason for liking the MSE, you‚Äôre also free to choose other loss functions if they‚Äôre available to you, especially if you have a lot of computing resources and/or smaller datasets.
If you had fun here and you‚Äôre looking for an entire applied AI course designed to be fun for beginners and experts alike, here‚Äôs the one I made for your amusement:
Here are some of my favorite 10 minute walkthroughs:
* ‚ÄúLoss function‚Äù is the machine learning word for ‚Äúobjective function‚Äù ‚Äî they‚Äôre the same thing.
T Z J Y
Christopher Tao
in
Towards Data Science
Bvk Bhattarai
Corey Walker
in
DevOps Authority
Abdelrahman Ghallab
in
Beirut AI
Alicia Phan
Priyanshu Bhatt
Shaip
in
Becoming Human: Artificial Intelligence Magazine
"
47,https://medium.com/datadriveninvestor/5-imaginative-data-science-projects-that-can-make-your-portfolio-stand-out-6371802a686d,5 Imaginative Data Science Projects That Can Make Your Portfolio Stand Out,"In my past content, I‚Äôve highlighted this important fact: Your portfolio should differentiate you from other candidates. It shouldn‚Äôt make you look the same as them. If you all have similar projects with similar methods, how are you going to stand out enough to get hired?
What does it mean to stand out though? Standing out can be through novelty, impact, skill, or creativity. In this article, I will highlight 5 of the most interesting data science project ideas, coupled with concrete examples, that will be sure to turn heads.
If you prefer a video format, consider the following to learn more about this topic:
Are you looking for a project that is out of this world‚Ä¶ literally? Well, outer space is a good place to start. NASA collects and shares a huge amount of data about its missions, research, and activities for free online.
With this incredible dataset, you can look up data by planet and by mission. You even look into the life-science data that NASA has been collecting from anything ranging from humans to beetles. They even have human-resources data from NASA available to you.
A simple project with this data could be a planet-comparison dashboard. Someone with prior knowledge or interest in space could do something even more impressive, like creating an algorithm to classify space debris or even identify objects in images that astronauts took.
The next topic is a little closer to home. I have a question for you: Do you consume content? Maybe a podcast, YouTube videos, or tweets? What if I told you that the data from these channels that you love could be the topic of your next project?
One of my own first projects was analyzing the text data from one of my favorite podcasts, Bigger Pockets. Many podcasts publish transcripts of their data for free online because it helps with searchability. So, you can go in and scrape this data and use it in a project.
These are perfect types of projects to explore natural language processing techniques, topic modeling, and some unique styles of visualization.
If an interviewer also likes the podcast you did a project on, you will have plenty to talk about!
It isn‚Äôt just podcasts either, in particular, YouTube has a good API for scraping video descriptions, and there are some fairly reliable libraries for getting Twitter data as well.
There aren‚Äôt many things that are more valuable than a project that provides real-world impact. Fortunately, there are plenty of free datasets that allow you to explore and potentially influence phenomena in the real world.
The first of these is the data from the Food and Agriculture Organization (FAO) of the United Nations. This is data that I personally explored for the first challenge in Z by HP‚Äôs Unlocked movie. That exercise is a great place to get started, but it is only the beginning of the data that is accessible there.
Through the FAO you have global data on food production, food security, trade, forestry, sustainability, employment, and at least a dozen additional fascinating areas.
This data is great for creating informative dashboards or for doing various forms of time series analysis.
If you live in the US, another dataset that may be directly relevant to you and your community is the FBI Crime Data Explorer.
This set has historic and, in some cases, real-time data on different types of crime by state. Again, this would be great for a descriptive analysis or maybe an inferential analysis to determine which crimes have increased rates based on state-specific policies.
Data science is moving faster than it ever has before. It seems like GPT-3, a revolutionary algorithm is now old news. From what I heard, there are actually rumblings of GPT-4 coming out in the near future. The awesome thing about this domain is that you can have access to some of the most cutting-edge technology at a fairly low cost.
A project that could open eyes would be leveraging some of the new language models or generative models into a simple website. A huge portion of data science is knowing how to use models after you‚Äôve built them, and this is a great way to showcase those skills. One that is of personal interest to me is DALL-E 2, which I am still on the wait list for. If anyone can get me access I would be eternally grateful!
I personally enjoy seeing how people find unique applications for GANs and reinforcement learning as well. My friend Nick Renotte has an awesome series where he uses reinforcement algorithms to play some of his favorite games from his youth. If you‚Äôre working in tech, I expect at least one of your interviewers would be pretty interested in that project.
The last type of project I recommend is probably the most respected but arguably least attractive to most people. That is contributing to open-source libraries. If you are using a library and can think of ways to make it a little better, why not actually do that? My friend Stefanie Molin was recently a bit frustrated with how some of the graphing methods operated in pandas. It was a bit clunky and she thought it could be improved upon. It took her an afternoon, but she made some adjustments and submitted them. Guess what? They were approved!
I know this seems very advanced, but it doesn‚Äôt have to be. In a lot of libraries, they are in dire need of code examples of implementation. Sometimes just updating the readme with examples can make you a contributor to various projects. Here are some ideas for open-source data science projects:
www.analyticsvidhya.com
This is something that can very much differentiate you from the crowd!
I did my best to try to give you some flexibility within the domains of these projects. If I gave very specific projects, there would likely be a huge influx of them in the market and then no one would stand out. With that being said, after this article, it is likely that the use of these datasets will still increase and you will need to find additional ways to differentiate yourself again. Fortunately, you can get creative and find your own projects. Check out this video on how to come up with data science projects on your own next.
I hope this article gives you some fun ideas about how to separate yourself from the pack and do something useful with your project work. If you enjoyed this article, remember to follow me on Medium for more content like this and sign up for my newsletter to get weekly updates on my content creation and on additional learning resources in the data science industry! Also, consider supporting me and thousands of other writers by signing up for a membership.
Until next time, good luck on your data science journey!
Subscribe to DDIntel Here.
Join our network here: https://datadriveninvestor.com/collaborate
Atsushi Takayama
TIP Co. (The Immigrant Project)
Matt Graham
Rabiacim
Jessica Jiaxi Qu
in
Tell Data Stories with Us
Nik Piepenbreier
in
Towards Data Science
kyle powers
Technical Source
"
48,https://medium.com/@Harrygurjar_7/online-courses-wont-get-you-a-job-7fbba20c8779,Online Courses Won't Get You a job!,"Why No more signing up, for more data science or analysis courses?
We all did our best to get into top tech giant companies, then we look out and search for advice, yeah google is the only Mother Teresa here to help us with every problem most of the time
Online courses are hilarious, most of them have very lovely, exaggerated___ versions of descriptions they gave almost guarantee by completing the Course we will land our dream jobs sounds crazy, right? I also got into this trap, I am not saying all courses are bad or best or anything like that judgment I leave it to you my point here is ___,,,__we all pick the courses not because only we will learn the new skills, concepts, or amazing techniques to solve a problem in data sciences there is in last or most important reason for signing a course is In final after completing the course we will get Certificate The reality here is what if when one day in front of your employer you are sitting and what if they say okay that is great you completed these certificates but you not having any project so we can judge your skills based on your certificates
This is the point, All certificates are useless if there is no evidence of your skill like projects individually or in a team. Yes It was our investment of time and money still it doesn't matter Projects showcase is a big advantage in front of an employer instead of just having a number of certificates, its very great to see by completing these courses but they worth nothing, that's hurt really things happened in actual in your dream job interview
SO HERE WE LEARN PROJECTS ARE KEY TO GETTING A JOB NOT ONLY CERTIFICATES
Let's move to the next phase of this writing blog that is!
WHY PROJECTS????
In an interview look at how we apply and how much we applied our learning from the courses we have done this is a very fair deal for us because what is the edge or X factor here hundreds of students opt for the same courses to get the skill, right
Implementing knowledge is a very significant phase of exporting any skill, also it recalls all concepts and cross-checks like is this the concept I understand fully and there is no scope of learning more on that particular topic, etc
Practice builds confidence and clears all doubts and there in classes, there is a time constraint that's why any tutor can not tell more in detail about particular topic or chapters They need to finish the course on time
We make a lot of mistakes far more worthful than we face in any online course
===============Follow for more ===============
keep supporting, keep learning, and keep helping each otherüôè‚ù§
here we are for each otherüôå
You can also connect with me on LinkedIn: click here
Keep Learning, Keep supporting üëçüëç
If you want to know about any specific topic article please comment below
Some useful links I found for a data analyst are
For data Visualization: click here and here
For Practice SQL LEETCODE PROBLEMS: Click here and SQL
For EDA Projects PRACTICE: Click here
For making Any resume for yourself faster: click here Resume for DATA ANALYST
Cyril Le Mat
Linda Ngo
in
Towards Data Science
Andrea Meyer Munoz
Gabe S
Susmitha Gudapati
Will Bates
Puttanun
All About Sensors
in
All about sensors
"
49,https://medium.com/towards-data-science/metric-design-for-data-scientists-and-business-leaders-b8adaf46c00,Metric Design for Data Scientists and Business Leaders,"In order to make good data-driven decisions, you need 3 things:
Requirements #2 and #3 have been written about plenty (including by me), but what about requirement #1?
Now that data collection is easier than ever, many leaders feel pressure to drag numbers to every meeting. Unfortunately, in the midst of the feeding frenzy, many of them fail to give metric design the amount of thought it deserves. Among those who are willing to put the effort in, most are making it up as they go along, as if it‚Äôs brand new.
It isn‚Äôt.
Psychology ‚Äî the scientific study of mind and behavior ‚Äî has had over a century to stub its toe on the dangers of attempting to measure vague quantities that haven‚Äôt been properly defined, so the field has learned some solid gold nuggets that business leaders and data scientists would be wise to borrow when designing metrics.
towardsdatascience.com
If you‚Äôre not convinced that metric design is hard, grab a pen and paper. I challenge you to write down a definition of happiness that‚Äôs so ironclad that no one could take issue with your way of measuring it‚Ä¶
Tricky, right? Now try it with some other abstract nouns people throw around daily, like ‚Äúmemory‚Äù and ‚Äúintelligence‚Äù and ‚Äúlove‚Äù and ‚Äúattention‚Äù and so on. It‚Äôs damned near miraculous that any of us understand ourselves, let alone one another.
And yet, this is exactly the first hurdle that psychology researchers must clear in order to make scientific progress. In order to study mental processes, they must create precise and measurable proxies ‚Äî metrics ‚Äî to work with. So, how do psychologists and other social scientists think about metric design?
How do you rigorously, scientifically study concepts that you can‚Äôt easily define? Concepts like attention, satisfaction, and creativity? The answer is‚Ä¶ you don‚Äôt! Instead, you operationalize. For the purposes of this example, let‚Äôs suppose you‚Äôre interested in measuring user happiness.
What is operationalization? I‚Äôve written an intro article to it here for you, but the upshot is that when you operationalize, you first say to yourself, ‚ÄúI am never going to measure happiness and I‚Äôve made my peace with that.‚Äù Philosophers have been at this for thousands of years, so it‚Äôs not like you‚Äôre suddenly going to come up with a single definition that satisfies everyone.
towardsdatascience.com
Next, you distill the measurable essence of your concept into a proxy.
Always remember that you are not actually measuring happiness. Or memory. Or attention. Or intelligence. Or any other poetic fuzzword, no matter how grand it sounds to you.
Now that we‚Äôre okay with the fact that we‚Äôll never measure happiness and its friends, it‚Äôs time to ask ourselves why we even considered that word in the first place. What is it about this concept ‚Äî in its fuzzy form ‚Äî that seems relevant and pertinent to the decision we want to make? What concrete (and obtainable!) information would lead us to prefer one course of action over another? (Metric design is much easier when you have actions in mind before you begin. If possible, think about potential decisions before attempting to design a metric.)
Then we distill the core idea that we are after to create a measurable proxy ‚Äî a metric that captures this core essence we care about.
Define your metric before you name it.
And now comes the fun part! We‚Äôre allowed to name our metric anything we like: ‚Äúblorktibork‚Äù or ‚Äúuser happiness‚Äù or ‚ÄúX‚Äù or whatever.
The reason it doesn‚Äôt make sense for us to be arrested by the language police is that no matter how hard we work at designing it, our proxy will *not* be the Platonic form of user happiness.
While it may suit our needs, it‚Äôs important to remember that our metric is unlikely to fit everyone else‚Äôs needs too. That‚Äôs why it would be silly to lock horns in a useless debates about whether our metric does or does not capture True Happiness. It doesn‚Äôt. If you‚Äôre desperate for some kind of One Metric To Rule Them All, there‚Äôs a Disney song for you.
Any metric we create is simply a proxy that suits our own needs (and possibly no one else‚Äôs). It‚Äôs our personal means to a personal end: making an informed decision or summarizing a concept so we don‚Äôt have to write a whole paragraph every time we mention it. We can get along just fine without involving the language police in either one.
So far, so good. You simply determine what information you‚Äôd need for your decision, then you figure out a way of summarizing that information in a way that makes sense for your needs (ta-da, that‚Äôs your metric), and then name it whatever you like. Right? Right, but‚Ä¶
There is a hardest part to all this. Continue to the next installment to find out what it is‚Ä¶
If you‚Äôre keen to learn more, watch lessons 039‚Äì047 from my Making Friends with Machine Learning course. They‚Äôre all short videos of a couple of minutes long. Start here and continue in the attached playlist:
If you had fun here and you‚Äôre looking for an applied AI course designed to be fun for beginners and experts alike, here‚Äôs one I made for your amusement:
kozyrkov.medium.com
P.S. Have you ever tried hitting the clap button here on Medium more than once to see what happens? ‚ù§Ô∏è
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Here are some of my favorite 10 minute walkthroughs:
Sanghamesh Vastrad
in
Towards Data Science
Data Science Earth
in
Data Science Earth
NinjaTrader
Andrew J. Zahuranec
in
Data Stewards Network
Gusde Melis
Thuwarakesh Murallie
in
Towards Data Science
Elizau
Tech in 3
"
50,https://medium.com/towards-data-science/how-to-easily-get-football-data-with-a-python-package-without-web-scraping-c922e7ebfb41,How to Easily Get Football Data with a Python Package (Without Web Scraping),"Every minute of a football match generates data that can be used to get high-quality insights that could power player recruitment, and match analysis and help coaches make better decisions.
Unfortunately, most free datasets available online only contain basic stats such as goals, team names, and the day of the match.
We can‚Äôt get many valuable insights with only basic stats.
To get advanced stats you could try web scraping, but before spending time on that, you could explore the data some sports data providers share for free.
In this guide, we‚Äôll explore all the free football data that Statsbomb shares on its Python package statsbombpy.
To get access to all the football data Statsbomb shares, we need to install statsbombpy.
Once we have the library installed, we have to import it.
To see all the competitions that Statsbomb shares for free, we only need to run sb.competitions().
I‚Äôm going to drop the duplicates in the country_name and competition_name columns to show the unique competitions that Statsbomb has.
As we can see, the free version of Statsbomb has competitions such as the FIFA World Cup, Champions League, La Liga, and Premiere League.
That said, Statsbomb has more competitions that are only available through API access for paying customers only.
Let‚Äôs explore one of the competitions available in the dataset ‚Äî FIFA World Cup 2018.
To explore the data for the FIFA World Cup 2018, we need the competition_id and season_id. From the image above, we can see that the values are 43 and 3, respectively.
Although it‚Äôs not visible in the image above, the dataframe df_2018 has 22 columns. That‚Äôs a good start for a football analytics project.
To get more information about a specific match we need the match_id. For example, the 2018 World Cup final, France vs Croatia, has the id 8658.
Now that we verify that 8658 is the correct id, let‚Äôs explore the lineups and all the events that happened in the 90 minutes.
Let‚Äôs see the lineups of the match France vs Croatia.
If we print lineups, we‚Äôd see that it‚Äôs a dictionary. Let‚Äôs its keys.
We can get access to the lineups of France and Croatia through the keys.
Here are the lineups of Croatia.
Here are the lineups for France.
To get the events in the match France vs Croatia, we need to use the id of the match again.
Let‚Äôs see all the columns inside df_events
We can see that there‚Äôs a lot of information in this dataset. Let‚Äôs select only a few columns and sort the dataframe by the minute and timestamp columns.
Let‚Äôs see the events at the last minute of the match France vs Croatia.
That‚Äôs it! Now you can explore this package on your own to start your football analytics project.
Learning Data Science with Python? Get my FREE Python for Data Science Cheat Sheet by joining my email list with 20k+ people.
If you enjoy reading stories like these and want to support me as a writer, consider signing up to become a Medium member. It‚Äôs $5 a month, giving you unlimited access to thousands of Python guides and Data science articles. If you sign up using my link, I‚Äôll earn a small commission with no extra cost to you.
frank-andrade.medium.com
Poornachandra Sarang
in
Geek Culture
Yusuf Ali
Monica Ramirez
GumGum
in
GumGum Advertising Blog
We Predicted That
Ananya Bhattacharyya
in
Towards Data Science
Adam Symington
in
Towards Data Science
Mirea Cartabbia
in
WAI GERMANY
"
51,https://medium.com/dev-genius/leetcode-21-merge-two-sorted-lists-f290ef5d21e9,LeetCode ‚Äî 21. Merge Two Sorted Lists (get solution with images),"Link : ‚Üí https://leetcode.com/problems/merge-two-sorted-lists/
You are given the heads of two sorted linked lists list1 and list2.
Merge the two lists in a one sorted list. The list should be made by splicing together the nodes of the first two lists.
Return the head of the merged linked list.
Example 1:
Example 2:
Example 3:
Constraints:
Solution :‚Üí
Here, we need to follow below steps, Let me show you by images.
2. Now over here we will take two another linked list, first one is head and another one which is temp, here temp will be used to fill resultant linked list, head will be first node (head) of temp linked list.
3. We have existing two linked list (list1, list2), suppose these are filled with below values.
4. Because of we need to get linked list in ascending order, we will fill linked list with lowest value, so we will check, out of two linked list which linked list have lowest value on first node.
5. Here list1.val and list2.val both have 1.
6. So we will take list1.val and append it to head and temp linked list.
7. After filling data from list1 to temp as well as head linked list, we are moving to next node of list1,
8. Now we are moving to another condition which is while loop, over here we will transverse till both linked list will reach to tail (end).
9. Now as above step: 5, we are checking which linked list have smaller value, we will fill that first. Here list2 has value 1 which is smaller then list1‚Äôs value which is 2.
We will append that value to temp linked list‚Äôs next node.
10. As we got the value from list2, now we are moving ahead in linked list2 or you can say, we are pointing to next node of list2.
11. After that we are pointing to next node of temp.
12. Now, again we are repeating those steps, which node has smaller value, we are going to append that to temp linked list‚Äôs next node, like below image.
13. As per above image, list1 has smaller value which is 2, so we are appending to temp linked list.
14. Now we are moving ahead in linked list1‚Äôs next node which is 4.
15. Now, we are moving ahead in temp link list.
16. As per above image, list2 has smaller value which is 3, so we are appending to temp linked list‚Äôs next node.
17. Now we are moving ahead in linked list2‚Äôs next node which is 5.
18. Now, we are moving ahead in temp link list.
19. As per above image, list1 has smaller value which is 4, so we are appending to temp linked list‚Äôs next node.
20. Now we are moving ahead in linked list1‚Äôs next node which is NULL.
21. Now, we are moving ahead in temp link list.
22. Now, list1 points to NULL and list2 points to some value. So as per else if condition we are appending list2‚Äôs value which is 5 to temp linked list‚Äôs next node.
23. Now we are moving ahead in linked list2‚Äôs next node which is NULL.
24. Now, we are moving ahead in temp link list.
25. Now list1 points NULL and list2 points to NULL, both linked list reached at end, so here while loop will become false and it will be terminated.
26. This is singly linked list, we can not go back, so we need to return HEAD (which is pointing to first node of temp linked list), as an answer.
So, this is the solution. Now we are moving forward to whole code part.
Code (Java): ‚Üí
Code (Python): ‚Üí
If the number of nodes are m and n in both lists, then the overall time complexity will be O(m + n) because we are traversing all the nodes of both the lists.
We are creating a linked list to store our result, hence the space complexity according to me will be O(m + n).
Thanks for reading this article ‚ù§
If I got something wrong? Let me in the comments. I would love to improve.
Clap üëè If this article helps you.
Digital Education Portal
Chandrashekar
Amine Dirhoussi
Macrium Software
in
Macrium Software
Chad Gutterman
Adam Aposhian
Techigai
Jagdish Paryani
in
Mindful Engineering
"
52,https://medium.com/gitconnected/use-git-like-a-senior-engineer-ef6d741c898e,Use Git like a senior engineer,"I‚Äôve used these features of Git for years across teams and projects. I‚Äôm still developing opinions around some workflows (like to squash or not) but the core tooling is powerful and flexible (and scriptable!).
Git logs are gross to go through out of the box.
Using git log gives you some information. But it‚Äôs extremely high-resolution and not usually what you‚Äôre looking for.
Let‚Äôs be real. These logs aren‚Äôt impressing anyone. They are boring. And they‚Äôre full of information that you don‚Äôt really need right now. You‚Äôre trying to get a high-level understanding of what has been going on in your project.
There‚Äôs a better way.
Using --graph and --format we can quickly get a summary view of git commits in our project.
Wow! These are some good-looking logs! There‚Äôs even a semblance of a branched tree beside it.
These logs show you who has been working on what, when changes were made, and where your changes fit into the bigger picture.
--graph adds the tree graph to the left. It‚Äôs not the most stylish graph, but it helps to visualize changes in the project‚Äôs branches. (Read the docs here.)
--format lets you customize the format of your logs. There are preset formats to choose from, or you can write your own format like this example. (Read the docs here.)
--all includes all of the refs, tags, and branches in the logs (including remote branches). You might not want everything so adjust this as you see fit. (Read the docs here.)
See the git-log docs for more information on how you can level up your git logs. ‚Üí
You‚Äôll often want to understand what‚Äôs happening with a specific commit. git show can show you a high-level view of changes in a commit, but it also lets you see changes to specific files.
Using the --stat flag you‚Äôll see the commit summary along with the files that changed and details on how they changed.
When you want to dive into the specific line changes in a particular file, use git show with the file path.
This gives you specific line changes for your file. By default, it will show you line changes along with 3 additional lines on either end to give you the context of where the changed lines are in the file.
See the git-show docs for more information on how you can level up your git commit understanding. ‚Üí
You‚Äôve made a branch on the project, committed some changes to your branch, and you‚Äôre ready to merge those changes back into main. Since you branched, another engineer has made changes to the same files. üò±
If you‚Äôre using a service like GitHub, your PR will tell you if you have merge conflicts.
Git will prompt you to resolve those merge conflicts before you shove your changes back into main. This is good since you don‚Äôt want to stomp out all the hard work others are doing.
To get started resolving this locally you will usually take one of two paths: merge or rebase.
When there are changes on the main branch that you want to incorporate into your branch, you can either merge the changes in or rebase your branch from a different point.
merge takes the changes from one branch and merges them into another branch in one merge commit.
rebase adjusts the point at which a branch actually branched off (i.e. moves the branch to a new starting point from the base branch).
Generally, you‚Äôll use rebase when there are changes in an upstream branch (like main) that you want to include in your branch. You‚Äôll use merge when there are changes in a branch that you want to put back into main.
I used to be pro-squash. But an article by Dr. Derek Austin ü•≥ changed my opinion on this. I recommend the article and don‚Äôt think I have anything useful to add beyond what he said.
betterprogramming.pub
Yancy Dennis
in
Python in Plain English
Alain Saamego
Julian Friedman
Sridhar Rao
LangLMS
Siddhesh Rane
in
We‚Äôve moved to freeCodeCamp.org/news
Emin Aliyev
Eren Baydemir
in
TeamSQL
"
53,https://medium.com/towards-data-science/overusing-the-term-statistically-significant-makes-you-look-clueless-f96e1ad1a78e,Overusing the Term ‚ÄúStatistically Significant‚Äù Makes You Look Clueless,"If you‚Äôre in the market for a new tongue-twister, try this paraphrase of a classic:
‚ÄúThe difference between statistically significant and statistically non-significant is not necessarily significant.‚Äù
As a recovering statistician, I have the pleasure of knowing many data experts and the displeasure of meeting a lot of posers. Though the people I bump into are hardly a random sample, I have noticed a correlation between throwing around the term ‚Äústatistically significant‚Äù and having no idea what it means. Data experts hardly ever use it in casual conversation. Why is that?
Genuine data experts hardly ever shove the term ‚Äústatistically significant‚Äù into casual conversation.
In short, because the term ‚Äústatistically significant‚Äù isn‚Äôt especially significant from the perspective of other people. It matters who set up the calculation‚Ä¶ and if it wasn‚Äôt you, the results likely aren‚Äôt for you either. Experts understand this, so they don‚Äôt foist their statistical significance on innocent bystanders.
There‚Äôs a gap between what the term really means and how charlatans (mis)use it to trick you. I tackled this in detail in an earlier article and I‚Äôve also added a quick summary to the bottom of this page.* In this article, I‚Äôll illustrate the process of statistical decision-making with an example and help immunize you against charlatans by giving you a primer on interpreting other people‚Äôs hypothesis test results.
Imagine you have a friend who makes blue ribbons. Your friend also happens to be a skilled decision-maker who understands how to use the classical hypothesis testing framework, which I‚Äôve explained here for those who need a refresher. I‚Äôll assume that you know what a default action is and how the process works. If you don‚Äôt, I recommend a small detour before continuing here:
towardsdatascience.com
Now let‚Äôs apply this framework, giving you a taste of the reasoning.
By default, your friend will switch to a cheaper supplier of blue dye as long as there‚Äôs no human-detectable visual difference (operationalized to include a tolerance for the fallibility of human sense) from the blue ribbons they‚Äôve already made.
Before ‚Äî before, not after (!!) ‚Äî doing a hypothesis test, your friend must pick the quality at which the decision will be made by adjusting some settings, including significance level, power/sample size, and assumptions. I wish more people appreciated just how many knobs and dials statistics has.
Then your friend collects data, magic happens**, and they end up with a p-value summarizing whether they should cheapen up (default action) or stick with the expensive blue dye (alternative action).
At the end of the statistical odyssey, your friend rejects the null hypothesis, and stays loyal to the more expensive blue dye. It was statistically significant! That‚Äôs what your friend concluded‚Ä¶ but what should you conclude?
Should you conclude that there was a big difference between the dyes?
Not necessarily. Without knowing how your friend operationalized ‚Äúhuman-detectable visual difference‚Äù you won‚Äôt know if the difference that they now believe in is something you‚Äôd call big.
At this point in the angry statistician rant, we often get the misplaced fuss about ‚Äúeffect sizes‚Äù (which I‚Äôll cover in another post). For now, all I‚Äôll say is that those discussions often miss the point, since effect sizes should already be built into the hypotheses whenever we‚Äôre dealing with proper statistical decision-making.
Was it an important finding? Not necessarily. It‚Äôs only important to folks who care about blue dyes and what your friend chooses to do (a list of dramatis personae that surely includes your friend, who took a statistical approach because the blue dye question was personally or professionally important to them).
Why would it be important to anyone else? Why indeed. It might not be.
Statistical summaries essentially boil down to a statement about surprise. As in, ‚Äúthis surprises me.‚Äù
Sometimes, though, the very fact that particular individuals were surprised enough to change their minds about something can be important news for the rest of us. For example, you might consider a result published in a medical journal important because the scientists who wrote it changed their minds about how a medication works. Sure, you have no idea what they actually did in that lab of theirs (even if they summarized it for you in their paper, you still lack a bunch of context... and if you‚Äôre not an expert, you won‚Äôt grokk the nuances of the assumptions they made), but if you choose to trust them, you might take an interest in whatever they find important and surprising. Is the finding true, though? Unfortunately, if all you have is a summary, the only conclusion you‚Äôre allowed to make is this: ‚ÄúThe people I trust are convinced, therefore I‚Äôm convinced.‚Äù If you‚Äôre not going to do the research yourself, all you can do is trust in your chosen experts and parrot their conclusions, or waft through life doing your best to avoid forming opinions about anything at all. But I digress ‚Äî if you want more on this line of reasoning, you‚Äôll find it here:
kozyrkov.medium.com
Let‚Äôs get back to those blue ribbons, shall we?
Everyone except your friend may consider the whole question of getting the perfect blue dye for ribbons to be trivial. So, is your friend‚Äôs finding meaningful?
Unless you know (and buy into) the entire set of assumptions, risk settings, and decision framing, the only meaning here is that you know which dye your friend will be using next week.
Does it mean the other dye from the cheaper supplier is of lower quality? Certainly not.
Your friend‚Äôs decision setup had nothing to do with the question of dye quality, so their conclusions don‚Äôt cover that topic. And even if quality were part of the question, it‚Äôs important to check the operationalization. Your notion of dye quality might be different from your friend‚Äôs.
Unfortunately, the leap from your friend‚Äôs question (‚ÄúShould I abort my planned move to Supplier B‚Äôs cheaper blue dye?‚Äù) to a gossip blogger‚Äôs hot take (‚ÄúSupplier B makes bad quality dye, according to science!!!‚Äù) is exactly the kind of feebleminded data illiteracy that makes us statisticians want to punch something.
It's exactly the kind of feebleminded data illiteracy that makes us statisticians want to punch something.
How about if you‚Äôre in the blue ribbon business too ‚Äî should you be convinced that the cheaper dye is no good? Not necessarily.
You might not buy into the assumptions your friend made about the population of dye (maybe they assumed zero variance for all bottles of dye and only tested one, which might have been a bad batch) and/or you might have different tolerance for statistical risks and/or your wouldn‚Äôt frame your own decision about dye suppliers the same way, so their findings might have nothing to do with you.
Good questions come from possibilities that make you curious about probabilities.
However, you are welcome to consume your friend‚Äôs findings from an analytics perspective‚Ä¶ as long as you don‚Äôt take them too seriously. There‚Äôs plenty of inspiration you could take from other people's work to help you ask better questions and structure your own approach to decision-making. After all, good questions don‚Äôt arrive out of nowhere. They come from possibilities that make you curious about probabilities. You need to have some exposure to what might be possible in order to start asking those good questions. If your friend‚Äôs findings make you curious and blue dye is a big deal in your life, you might perhaps be inspired to do your own testing. If not, well, isn't that what noise cancelling headphones are for?
If you had fun here and you‚Äôre looking for an entire applied AI course designed to be fun for beginners and experts alike, here‚Äôs the one I made for your amusement:
Here are some of my favorite 10 minute walkthroughs:
While ‚Äústatistically significant‚Äù sounds like a cousin of ‚Äúimportant‚Äù or ‚Äúmeaningful‚Äù ‚Ä¶it isn‚Äôt. Unfortunately, the term is often abused in precisely this way. This is a trap.
Please don‚Äôt confuse a piece of dry statistical jargon with the poetry of a word that means something entirely different.
Welcome to statistics, where The Answer is p = 0.042 but you don‚Äôt know what the question was.
More on this here:
towardsdatascience.com
ilker cam
in
rakam
Cassie Kozyrkov
in
Towards Data Science
Jesse
in
Giant Scam
DataGeeks
Brendon Ho
Jason Wu
Ankit Gupta
in
Towards Data Science
Roshini Johri
in
Towards Data Science
"
54,https://medium.com/@cndro/top-10-data-science-certifications-in-2022-81ddc08a835b,Top 10 Data Science Certifications That Will Advance Your Career in 2023,"The Data Science field is booming, and certifications in this field will make it easier for you to land a better job. According to Glassdoor, there are over 14,000 Data science jobs in the US in November, and a number of them pay more than $100k a year. As more companies realize the importance of data analytics, machine learning, and visualization, there is a demand‚Ä¶
Eduardo Blancas
in
Towards Data Science
Paul de Lange
in
Expedia Group Technology
Ashin Shakya
Jesus Larrubia
Caitlin Kindig
Divya Choudhary
in
Towards Data Science
Board Infinity
in
Board Infinity
Alexander Kahanek
in
rubbish stories
"
55,https://medium.com/codex/why-you-should-learn-programming-through-books-672f1148abbb,Why you Should Learn Programming through Books,"As you know there are plenty and I mean PLENTY of online courses like in Udemy, Coursera, YouTube and the list goes on‚Ä¶
The amount of learning materials about development online is fast and ever expanding and people read less and less and want to become a Full-Stack developer just by taking an online course that often over promises and undelivered.
Nico Grashoff
Dave de Kort
in
incentro
Jake Hyde
Shanu Kumar
in
Typeset Blog
Jitesh Sinha
Nikhil S Wani
Deborah Bittencourt
in
Traders Eco
Bradley Rose
in
Milling for 53
"
56,https://medium.com/towards-data-science/k-means-clustering-and-principal-component-analysis-in-10-minutes-2c5b69c36b6b,K-means Clustering and Principal Component Analysis in 10 Minutes,"There are 2 major kinds of machine learning models: supervised and unsupervised. In supervised learning, you have input data X and output data y, then the model finds a map from X to y. In unsupervised learning, you only have input data X. The goal of unsupervised learning varies: clustering observations in X, reducing the dimensionality of X, anomaly detection in X, etc.
As supervised learning has been discussed extensively in Part 1 and Part 2 of the series, this story is focused on unsupervised learning. You will meet k-means clustering and Principal Component Analysis (PCA) for dimensionality reduction among many other unsupervised learning tasks.
The idea of clustering is to group the observations such that observations within the same group will be similar (or related) to one another and different from (or unrelated to) observations in other groups. There are many algorithms to achieve this, one of which is k-means.
K-means is a centroid-based clustering algorithm that works as follows.
Two hyperparameters must be determined by the user in implementing k-means clustering: the number of centroids k, and the distance metric used (usually Euclidean distance or Manhattan distance). In this story, we will use Euclidean distance which is defined by the formula
where x = (x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô) and y = (y‚ÇÅ, y‚ÇÇ, ‚Ä¶, y‚Çô) are observations with dimension n. Hence, now we only need to determine k.
Since we want groups of observations such that observations in a group will be similar to one another, a good clustering is the one with the lowest Within Sum of Squares (WSS), that is, the lowest sum of the quadratic distance from each observation to the centroid of the cluster it belongs to.
Also, since observations in a group need to be different from observations in other groups, a good clustering has a bigger Between Sum of Squares (BSS). Here, BSS is just the sum of the quadratic distance from each centroid to the global mean, weighted by the number of observations in each cluster.
Mathematically, WSS and BSS are formulated as
where
It‚Äôs easy to see that higher k corresponds to lower WSS. However, k has to be determined from the business point of view, or more objectively using what‚Äôs called the elbow method.
The elbow method says we should pick k where increasing it will result in no more significant decrease of WSS. Hence from the plot above, for example, we choose k = 2 or k = 3.
Below is an example of the step-by-step k-means clustering process, with k = 3 and ‚Äú√ó‚Äù denoting the centroids.
Given enough time, k-means will always converge. However, the convergence may be to a local minimum. This is highly dependent on the initialization of centroids. As a result, the computation is often done several times, with different initializations of centroids.
One method to help address this issue is the k-means++ initialization scheme. This initializes the centroids to be (generally) distant from each other, leading to probably better results than random initialization.
K-means is not always a good choice for clustering. Some data may exploit k-means weakness due to the distance choice and some implicit assumptions that k-means clustering makes. The plot below shows some of these scenarios.
So, it‚Äôs always a good practice to transform the data first. There is also a plethora of other clustering algorithms to tackle this very issue, such as spectral clustering, agglomerative clustering, ward hierarchical clustering, DBSCAN, BIRCH, and many more.
The goal of PCA is to identify the most meaningful basis to re-express data. Hopefully, this new basis will filter out the noise and reveal hidden structures. PCA asks: is there another basis, which is a linear combination of the original basis, that best re-expresses your data?
Let X be a dataset with m observations and n features, so X is an m √ó n matrix. To ease calculations, we standardize X so that it has zero mean.
Let Y be a new representation of X, another m √ó n matrix related to X by a linear transformation P (which itself is an n √ó n matrix). PCA will find P that transforms X to Y linearly, that is,
We can see from this equation that the columns of P are a new set of basis vectors for representing observations (rows) in X, and are called principal components of X. Now, what is a good choice of basis P? What features we would like Y to exhibit? To answer these questions, consider two perspectives: noise and redundancy.
Remember, in Y, we hope to filter out the noise. A standard measure for noise is the signal-to-noise ratio (SNR), or a ratio of variances
A high SNR (‚â´1) indicates high precision data, while a low SNR indicates very noisy data. We assume the dimensions of interest exist in directions with the largest variance and presumably the highest SNR.
Let‚Äôs say your data is two-dimensional with features x‚ÇÅ and x‚ÇÇ. Consider the three plots below.
We say the data has high redundancy if x‚ÇÇ can be calculated from x‚ÇÅ (and vice versa). If this is the case (such as in the rightmost plot above), one of the features is redundant and the data dimension can be reduced without sacrificing much information. Indeed, this is the central idea behind the dimensional reduction.
From both perspectives, PCA needs variance of features (for maximizing the signal) and covariance of pairs of features (for minimizing redundancy) of X. This is exactly what the covariance matrix of X is used for, which by definition can be written as
C‚Çì is an n √ó n matrix with diagonal terms representing the variances of features and off-diagonal terms representing the covariances of pairs of features, which reflect the noise and redundancy in our data:
Let C·µß be the covariance matrix of Y. So when X is transformed to Y using P, what would the optimized C·µß look like?
There are many methods for diagonalizing C·µß. For PCA, it assumes that P is an orthonormal matrix. Now, the task of PCA has been reduced to:
Find some orthonormal matrix P in XP = Y such that C·µß = Y·µÄY / (m‚àí1) is a diagonal matrix. The columns of P are the principal components of X.
Since C‚Çì is symmetric, there exist matrices D and E such that C‚Çì = EDE·µÄ, where D is a diagonal matrix and E is a matrix of eigenvectors of C‚Çì arranged as columns. Moreover, C‚Çì and D have identical eigenvalues. Since D is a diagonal matrix, its eigenvalues are the elements of its main diagonal.
Choose P ‚â° E, then
To sum up, until this point, we‚Äôve successfully found a matrix P where its columns are the eigenvectors of the covariance matrix C‚Çì of X. Matrix P satisfies XP = Y such that the covariance matrix C·µß of Y is a diagonal matrix where the diagonal consists of the eigenvalues of C‚Çì. So, there‚Äôs no redundancy in Y.
To ensure each successive dimension in Y is rank-ordered according to variance so that interesting structures get highlighted, you can reorder the eigenvalues of C‚Çì in D such that the bigger eigenvalues are positioned more to the top left corner (of course by also rearranging columns of P and rows of P·µÄ to satisfy C‚Çì = PDP·µÄ still).
PCA can be used for dimensionality reduction by choosing only the first several columns of rearranged P. The number of columns to select is a hyperparameter for PCA. More columns mean more variance explained in the original data X. This is useful for visualization, summarizing large data, or finding discriminative features.
After this long derivation, we‚Äôre finally able to compose the PCA algorithm:
You will use the iris dataset for this case study.
The dataset contains 3 classes of 50 observations each, where each class refers to a type of iris plant (Iris Setosa, Iris Versicolour, or Iris Virginica). There are four features in the dataset from the results of measurement of the plants‚Äô sepal and petal.
You want to visualize the dataset in a scatterplot. However, this is impossible since there is no such thing as a four-dimensional scatterplot. So, you extract as much information as possible from the dataset to obtain a two-dimensional representation and you use PCA for this.
X is a 150 √ó 4 matrix with feature-wise means as follows.
For each feature, subtract the corresponding mean from every observation. You should now have this zero-centered dataset. Let‚Äôs call it X_meaned.
The covariance matrix of X_meaned is a 4 √ó 4 matrix
Thus, there exist four eigenvalues and the corresponding eigenvectors of C‚Çì. They are
Sort eigenvectors based on descending eigenvalues. Since you want a two-dimensional representation of X, pick two eigenvectors with the largest eigenvalues, indicating the maximum information that can be extracted. These are v‚ÇÑ and v‚ÇÉ. While considering the order, arrange them as columns. You‚Äôll get a 4 √ó 2 linear transformation matrix
The reduced data Y can be calculated by multiplying X_meaned and P. You obtain Y as follows.
With this two-feature dataset, you can now visualize the essence of the Iris dataset as follows.
You‚Äôve learned in great detail K-means Clustering and applied Principal Component Analysis for visualization purposes. Now, not only you can build them using established libraries, but you also confidently know how they work from the inside out, the best practices to use them, and how to improve their performance.
Congrats!
üî• Thanks! If you enjoy this story and want to support me as a writer, consider becoming a member. For only $5 a month, you‚Äôll get unlimited access to all stories on Medium. If you sign up using my link, I‚Äôll earn a small commission.
üîñ Want to know more about how gradient descent and many other optimizers work? Continue reading:
Albers Uzila
Albers Uzila
[1] A Tutorial on Principal Component Analysis (arxiv.org/abs/1404.1100)
[2] Scikit-learn user guide
Monodeep Mukherjee
Zahra Elhamraoui
in
Python in Plain English
Monodeep Mukherjee
Jorge Gonzalez S
in
Saturdays.AI
Manan Parekh
in
Nybles
Monodeep Mukherjee
Lakshmi Narayana Santha
in
Towards AI
Zachariah Zhang
"
57,https://medium.com/@hipiyushjain/10-new-websites-all-free-so-good-for-your-data-analytics-journey-youll-think-you-found-a-e595d32fac77,"10 new websites (All Free), so good for your data analytics journey, you‚Äôll think you found a hidden pile of cash","I have created a list of websites where beginner-level data analyst can start their journey and directly learn whats required.
Go to: https://sqlzoo.net/wiki/SQL_Tutorial‚Ä¶
2. Python: W3schools ‚Äî Want to learn python along with the certificate?
Visit: https://w3schools.com/python/default.asp‚Ä¶
3. Datasets: Kaggle ‚Äî want free datasets and participate in data analytics competitions.
Go to: https://kaggle.com/datasets
4. Power BI: Want to learn the most used and trending data visualization tool?
Go to: https://learn.microsoft.com/en-us/power-bi/
Host your Power BI portfolio on @NovyPro
5. Cheat Sheets: want cheat sheets for data analytics tools.
Go To : https://datacamp.com/cheat-sheet
6. Icons: want free icons for your dashboard.
Go to: https://flaticon.com
7. Themes: confused about which colors to use in your dashboard.
Go to: https://coolors.co
8. Statistics: Learn Statistics before moving to machine learning.
Go to: https://w3schools.com/statistics/index.php‚Ä¶
9. Excel: Want to learn excel? Learn Excel from Microsoft itself.
Go to: https://support.microsoft.com/en-us/office/excel-video-training-9bc05390-e94c-46af-a5b3-d7c22f6990bb‚Ä¶
10. Our world in data: My personal favorite. want to see what you can do with data.
Visit: https://ourworldindata.org
That‚Äôs It. I hope it helps you in your journey of becoming a #data analyst. I have used and used most of these websites not just for practice but for revision of multiple aspects of the data analytics journey.
I am thinking of explaining these websites in an elaborated manner through a notion template. How to go about learning from these websites.
Let me know if you are interested. For now, I am creating a data analytics cheat sheet template. That will surely help you in the long run.
If you have any questions or any inputs you want to share, just comment or hit me up on LinkedIn or Twitter. I am pretty much active there.
All the best.
Please Start. You will not regret it.
Thanks,Piyush Jain
Monica P.
Dr. Walid Soula
Jack T.
Shashank Shekhar
Setumo Raphela
Thomas Zo√´ga Rams√∏y
in
BrainEthics
Zelros AI
Mohit Gaikwad
"
58,https://medium.com/@dima806/data-engineer-data-analyst-data-scientist-which-role-to-choose-6bcb192904f2,"Data Engineer, Data Analyst, Data Scientist‚Äî which role to choose?","Currently, there are several distinct jobs related to data, the most common of them are Data Engineer, Data Analyst, and Data Scientist:
Below, I briefly summarise their main activities, typical yearly gross salaries (interquartile ranges, calculated for the United States, European Union, and India, respectively, using public survey data available on‚Ä¶
Apsara G
Greg Rafferty
in
Towards Data Science
Brad Henderson
Vandhana Visakamurthy
in
Towards Data Science
Nicole Janeway Bills
in
Towards Data Science
Scott Vogel
in
Towards Data Science
Bennett Meares
in
Towards Data Science
Arthur Mello
in
DataSeries
"
59,https://medium.com/techtofreedom/chatgpt-this-incredible-ai-chatbot-may-replace-junior-software-engineers-9bdfaee7ff7e,ChatGPT: This Incredible AI Chatbot May Replace Junior Software Engineers,"Imagine that you tell an AI assistant to write a program, and then it delivers the proper program to you within a few seconds.
Jordane Gengo
Gordian Etim
Matt Eland
in
The Startup
Sylvain Saurel
Tyler Brewer
Katarzyna GƒÖdek
in
ergonode
Calculus_io
in
calcu_io
Michal Mazurek
"
60,https://medium.com/codex/3-most-effective-yet-underutilized-functions-in-python-d865ffaca0bb,"Don‚Äôt Run Loops in Python, Instead, Use These!","Loops are expensive(slow) in Python and if you are working with a large number of rows then it‚Äôs a crime. In this blog, I will take you through the theory and practical examples of the functions that you can use as an alternative to loops in python.
Jasmine Dogu
Kathleen Febiola
Nicholas Zeisler
Benjamin Adjei
vipfixphone
Leo Lau
eBay TechBlog
in
eBayTech
Rog√©rio Chaves
"
61,https://medium.com/towards-data-science/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48,How Does Back-Propagation Work in Neural Networks?,"Neural Networks learn through iterative tuning of parameters (weights and biases) during the training stage. At the start, parameters are initialized by randomly generated weights, and the biases are set to zero. This is followed by a forward pass of the data through the network to get model output. Lastly, back-propagation is conducted. The model training process typically entails several iterations of a forward pass, back-propagation, and parameters update.
This article will focus on how back-propagation updates the parameters after a forward pass (we already covered forward propagation in the previous article). We will work on a simple yet detailed example of back-propagation. Before we proceed, let‚Äôs see the data and the architecture we will use in this post.
The dataset used in this article contains three features, and the target class has only two values ‚Äî 1 for pass and 0 for fail. The objective is to classify a data point into either of the two categories ‚Äî a case of binary classification. To make the example easily understandable, we will use only one training example in this post.
Understand: A forward pass allows the information to flow in one direction ‚Äî from input to the output layer, whereas the back-propagation does the reverse ‚Äî allowing data to flow from output backward while updating the parameters (weights and biases).
Definition: Back-propagation is a method for supervised learning used by NN to update parameters to make the network‚Äôs predictions more accurate. The parameter optimization process is achieved using an optimization algorithm called gradient descent (this concept will be very clear as you read along).
A forward pass yields a prediction (yhat) of the target (y) at a loss which is captured by a cost function (E) defined as:
where m is the number of training examples, and L is the error/loss incurred when the model predicts yhat instead of actual value y. The objective is to minimize the cost E. This is achieved by differentiating E with respect to (wrt) parameters (weights and parameters) and adjusting the parameters in the opposite direction of the gradient (that is why the optimization algorithm is referred to as gradient descent).
In this post, we consider back-propagation on 1 training example (m=1). With this consideration, E, reduces to
The loss function, L, is defined based on the task at hand. For classification problems, Cross-entropy (also known as log loss) and hinge loss are suitable loss functions, whereas, Mean Squared Error (MSE) and Mean Absolute Error (MAE) are appropriate loss functions for regression tasks.
Binary cross-entropy loss is a function suitable for our binary classification task ‚Äî the data has two classes, 0 or 1. A binary cross-entropy loss function can be applied to our forward-pass example in Figure 1, as shown below
t=1 is the truth label, yhat=0.521 is the output of the model and ln is the natural log‚Äî log to base 2.
You can read more about the cross entropy loss function on the link below.
towardsdatascience.com
Since we now understand the NN architecture and the cost function we will use, we can proceed directly to cover the steps for backward propagation.
The table below shows the data on all the layers of the 3‚Äì4‚Äì1 NN. At the 3-neuron input, the values shown are from the data we provide to the model for training. The second/hidden layer contains the weights (w) and biases (b) we wish to update and the output (f) at each of the 4 neurons during the forward pass. The output contains the parameters (w and b) and the output of the model (yhat) ‚Äî this value is actually the model prediction at each iteration of model training. After a single forward-pass, yhat=0.521.
Important: Recall from the previous section: E(Œ∏)=L(y, yhat) where Œ∏ is our parameters ‚Äî weights and biases. That is to say, E is a function of y and yhat and yhat=g(wx+b), => yhat is a function of w and b. x is a variable of data and g is the activation function. Effectively, E is a function w and b and, therefore can be differentiated with respect to these parameters.
The parameters at each layer are updated with the following equations
where t is the learning step, œµ is the learning rate ‚Äî a hyper-parameter set by the user. It determines the rate at which the weights and biases are updated. We will use œµ=0.5 (arbitrary choice).
From Equations 4, the update amounts becomes
As said earlier, since we are dealing with binary classification, we will use the binary cross-entropy loss function defined as:
We will use Sigmoid activation across all layers
where z=wx+b is the weighted input into the neuron plus bias.
Unlike the forward pass, back prop works backward from the output layer to layer 1. We need to compute derivatives/gradients with respect to parameters for all layers. To do that, we have to understand the chain rule of differentiation.
Let‚Äôs work on updating w¬≤‚ÇÅ‚ÇÅ and b¬≤‚ÇÅ as examples. We will follow the routes shown below.
By chain rule of differentiation, we have
Remember: when evaluating the above derivatives with respect to w¬≤‚ÇÅ‚ÇÅ, all the other parameters are treated as constants, that is, w¬≤‚ÇÅ‚ÇÇ, w¬≤‚ÇÅ‚ÇÉ, w¬≤‚ÇÅ‚ÇÑ, and b¬≤‚ÇÅ. The derivative of a constant is 0 that is why some values were eliminated in the above derivative.
Next is the derivative of Sigmoid function (refer to this article)
Next, the derivative of cross-entropy loss function (reference material)
The derivatives with the respect to the other three weights on the output layer are as follows (you can confirm this)
We need to compute
From the previous sections, we have already computed ‚àÇE and ‚àÇyhat, what remains is
We used the same arguments as before, that all other variables except b¬≤‚ÇÅ are treated as constants therefore on when differentiated they reduce 0.
So far, we have computed the gradients with respect to all the parameters at the output-input layers.
At this point we are ready to update all the weights and biases at the output-input layers.
Please compute the rest in the same way and confirm them in the table below
As before, we need derivatives of E with respect to all the weights and biases at these layers. We have a total of 4x3=12 weights to update and 4 biases. As example, lets work on w¬π‚ÇÑ‚ÇÉ and b¬π‚ÇÇ. See the routes in the Figure below.
For weights, we need to compute the derivative (follow the route in Figure 6 if the following equation is intimidating)
As we go through each of the above derivatives, note the following important points:
Doing the same process as in Section B, we get:
Then, we can put together all those as
Using the same concepts as before, check that, for b¬π‚ÇÇ, we have
All gradient values for the hidden-input are tabulated below
At this point, we are ready to compute the updated parameters at the hidden-input.
Lets go back to the update equations and work on updating w¬π‚ÇÅ‚ÇÉ and b¬π‚ÇÉ
So, how many parameters do we have to update?
We have 4x3=12 weights and 4x1=4 biases at the hidden-input layers, 4x1=4 weights, and 1 bias at the output-hidden layers. That is a total of 21 parameters. They are called trainable parameters.
All the updated parameters for hidden-input layers are shown below
We now have the updated parameters for all the layers in Figure 8 and Figure 5 using back-propagation of error. Running a forward pass with these updated parameters yields a model prediction, yhat of 0.648, up from 0.521. This means that a model is learning ‚Äî moving close to the true value of 1 after two iterations of training. Other iterations yield 0.758, 0.836, 0.881, 0.908, 0.925, ‚Ä¶ (In the next article, we will implement back-propagation and forward pass for many training examples and iterations, and you will get to see this).
Example: If we have 2000 training examples and set batch size of 20, then it takes 100 iterations to complete 1 epoch.
In this article, we have discussed back-propagation by working on an example. We have seen how chain rule of differentiation is used to get the gradients of different equations ‚Äî the loss function, activation function, weighting equations and layer output equations. We have also discussed on how derivative with respect to the loss function can be used to update parameters at each layer. In the next article (attached below), we implement the concepts learnt here in Python.
towardsdatascience.com
Please sign up for medium membership at 5$ only per month to be able to read all my articles on Medium and those of other writers.
You can also subscribe to get my article into your email inbox when I post.
Thank you for reading, see you in the next!!!
Robert Dargavel Smith
in
Towards Data Science
Omar Barakat
Jaime Dur√°n
in
yottabytes
SkillRary
Ayse Yaman
Aparna Singh
in
DeveLearn
Marco Pasini
in
Towards Data Science
Processing Foundation
in
Processing Foundation
"
62,https://medium.com/towards-data-science/6-learnings-from-working-as-a-data-analyst-for-ayear-8f5531e99f8a,6 Learnings from Working as a Data Analyst for a Year,"Hi! I am Rashi and I work with a health insurance company based out of Chicago. Before starting my full-time job last year, I interned with PepsiCo for almost a year, and the learnings from my internship and a year as a data analyst taught me experiences, best practices, and more importantly the mistakes that data analysts can be vulnerable to.
In the past two years, I have worked with a large number of datasets, and hundreds of data points, and every dataset answered as many questions as you wish. You only learn each day from your job, your mistakes, from collaboration, and from talking to peers about their challenges. As a data analyst in 2022, to be the most relevant that I can be, I have grasped six valuable and quintessential lessons that I will keep with me to the time I work with data.
As a societal measure, human lives seek validation in every aspect of life. Then why not your data analyses too?
Data Validation is the process of ensuring consistency and accuracy within a dataset. Validation could range across data types, code, format, consistency, and output of a model, and could be before the analysis or after. Data validation is a very critical step in any data science lifecycle to create the best possible results and is often looked over.
For example, the business asks you to predict sales for the next holiday season. You code your predictive model and send an Excel sheet for a product-wise sales prediction with charts. Now, a senior manager emails you (and your boss and twenty other people) why a high-performing product would have below-average sales in the holiday season. You start to validate your analysis only to realize you used the wrong VLOOKUP to populate the product name from the product ID.
As data analysts, our job is not only to predict the absolute best of a scenario but also to ‚Äî
As data analysts, I cannot stress enough how important it is to understand the business at the deepest depth you can.
Every time I talk with a data professional working for at least 10 years (before this all was called data science & analytics), I always connect on the conversation on understanding the business to be a better storyteller with data.
Unless you have the relevant business understanding, I‚Äôve found it difficult to navigate through the steps in a project. As you understand the business problem, more important than anything, you know what questions to ask the stakeholders, the data and what to expect in your analyses. Times when I was raw as an analyst and spent no time understanding the business, my data and analysis both had tend to lose the essence and impact it is supposed to create.
For any analysis at hand, unless you understand the business, you can never know what an error in the model means for the business, why are the numbers inflated for a county with a low population post product launch, etc.
Be it any industry, hone on your business acumen everyday
Every time I have a new problem statement in hand to work on and if I am not aware of the steps from start to end, I have learned to establish a practice for myself where I set up interviews with the stakeholders, understand the metrics that are important to the ask and note down the three most important things the business would like to know.
Data points are basically the building blocks of data analysis.
They serve the purpose to accept or reject a hypothesis, spotting trends or patterns, or making predictions. Over these years, I've realized, as you have a stronger understanding of your data points and key performance indicators, which has enabled me to suggest rather effective recommendations that best fit the business goals.
Data cannot lie and so if you can align your key measurements to the business, data will provide objective feedback on business performance.
As a data analyst, I am (and surely you are too) faced with new challenges every day ‚Äî with data, business problem statements, organizational alignment, data management, quality, etc. When I started, I had an attitude that leaned toward identifying those challenges and then just talking about them to my manager.
Now, your manager may not always understand your challenge or be able to suggest a solution for ‚Äúyour‚Äù problems as an analyst. As we stepped into the new year, in retrospect I understood that this attitude can only take me so far.
I changed my perspective to look at the bigger picture if there are challenges, what is the next best thing I or someone else can do, and recommend innovations in the team (and if it gets approved, you have your achievement right there). I now constantly think of ways I can add value as an analyst to the business.
Innovation is taking two things that exist and putting them together in a new way.
The role of data is to empower the business to make decisions based on stats, trends, and insights.
In the ever-expanding data world of today, there is a need and demand for people to simply make sense of that million gigabytes of data. The business is seeking more and more people to translate data to drive decision-making. Data is never pretty and as data analysts, it is our job to make that data palatable and for the business to consume the insights discovered from that data.
Over the past years, working on multiple projects, I‚Äôve now carved a niche for myself where I love working at the intersection of data and business. This is one of the biggest learnings I had as an analyst. Data can‚Äôt talk or influence change by itself, and I, as a data analyst must be able to organize and interpret it.
While I‚Äôm working on creating a strategic roadmap to support platform modernization impacting 17 million people, I am also working on creating a predictive model to forecast the number of customer service issues in the next quarter given x number of data points. As consumers of data, create your analyses and visualizations keeping the business in mind.
As an analyst, you cannot code for almost the same task again and again. You have to reuse your code. I was never big on version control when starting nor would I save structures to only change data points. I have discovered that saving an SQL query or a sample data model for later use saves me a lot of time, increases my productivity, and invites best practices in version control.
Second, a thing I have now made a point of is to maintain documentation.I know it sounds very boring to write down steps in a Word document as a data professional but trust me, documentation has helped me and my team a lot to understand the project context & set expectations better. It is also very easy for other members to onboard the process quickly.
Third, (in whatever capacity you can) promote centralizing data processes in your team. While 100% data centralizing can seem a dream, it indeed supports continued improvements in decision-making processes ‚Äî
You deserve the data consistency as an analyst.
That‚Äôs it from my end for this blog. Thank you for reading! Let me know in the comments about your interesting projects, your journey in data, and what are looking for in 2022!
If you enjoy reading stories like these, consider signing up to become a Medium member from this link.
Happy Data Tenting!
Rashi is a data wiz from Chicago who loves to visualize data and create insightful stories to communicate business insights. She‚Äôs a full-time healthcare data analyst and blogs about data on weekends with a good cup of coffee‚Ä¶
Harshana Weragama
Tanner Lund
sarang ravate
Herv√© Choi
Aryan
Playcodformoney
D√©borah Mesquita
in
Towards Data Science
Francesco Cursi
"
63,https://medium.com/towards-data-science/its-time-to-say-goodbye-to-pd-read-csv-and-pd-to-csv-27fbc74e84c5,It‚Äôs Time to Say GoodBye to pd.read_csv() and pd.to_csv(),"Input-output operations with Pandas to a CSV are serialized, making them incredibly inefficient and time-consuming. It's frustrating when I see ample scope for parallelization here, but unfortunately, Pandas does not provide this functionality (yet). Although I am never in favor of creating CSVs in the first place with Pandas (read my post below to know why), I understand that there might be situations where one has no other choice but to work with CSVs.
towardsdatascience.com
Therefore, in this post, we will explore Dask and DataTable, two of the most trending Pandas-like libraries for Data Scientists. We‚Äôll rank Pandas, Dask and Datatable based on their performance on the following parameters:
If we read a CSV through Dask and DataTable, they will generate a Dask DataFrame and DataTable DataFrame respectively, not the Pandas DataFrame. Assuming that we want to stick to the traditional Pandas syntax and functions (due to familiarity), we would have to convert these to a Pandas DataFrame first, as shown below.
2. Time taken to store a PANDAS DATAFRAME to a CSV
The objective is to generate a CSV file from a given Pandas DataFrame. For Pandas, we are already aware of the df.to_csv() method. However, to create a CSV from Dask and DataTable, we first need to convert the given Pandas DataFrame to their respective DataFrames and then store them in a CSV. Thus, we‚Äôll also consider the time taken for this DataFrame conversion in this analysis.
The plot below depicts the time taken (in seconds) by Pandas, Dask, and DataTable to read a CSV file and generate a Pandas DataFrame. The number of rows of the CSV ranges from 100k to 5 million.
The plot below depicts the time taken (in seconds) by Pandas, Dask, and DataTable to generate a CSV file from a given Pandas DataFrame. The number of rows ranges from 100k to 5 million.
I am not a big fan of CSVs, to be honest. If you thoroughly read my earlier post (which I linked above or you can read here) and the post you are reading right now, you might also resonate with me. Since I discovered the numerous issues associated with CSVs, I have stopped using them as much as possible.
To conclude, I would say that unless you need to view your DataFrame outside a non-pythonic environment such as Excel, YOU DON‚ÄôT NEED A CSV AT ALL. Prefer formats like Parquet, Feather, or Pickle to store your DataFrames to. Still, if you see no other options, at least optimize your input and output operations by leveraging DataTable instead of Pandas.
Shruthi Jain
Max Mazel
Ankit Madhukar
Ali Cenk Baytop
Wannes Rosiers
in
Experience Stack
The Data Observer
David Hundley
in
DataDrivenInvestor
Hadir Khan
"
64,https://medium.com/towards-data-science/6-habits-to-include-in-your-daily-routine-for-a-long-happy-career-as-a-data-scientist-de733eae6250,"6 Habits to Include in Your Daily Routine for a Long, Happy Career as a Data Scientist","Data science Reddit is an interesting place right now where many data scientists are describing feelings of burnout and lack of fulfillment in their careers.
Many are describing how they don‚Äôt feel the inspiration to advance in their careers or are feeling like they‚Äôve plateaued and need a new challenge.
I think everyone feels this way at least once in their careers, which perhaps is why statistics suggest that people are averaging over 10 different jobs in their lifetime.
While it‚Äôs easy to look at disenchantment with work as an excuse to change careers, it‚Äôs vital to remember that you‚Äôve endured a long tough slog to become a data scientist in the first place. Therefore, it‚Äôs also important to make sure you‚Äôre doing everything possible within your daily routine to ensure that you‚Äôre feeling fulfilled, energized, and inspired to keep working as a data scientist.
Daily routines can play a large part in the happiness you feel while working, which is why it‚Äôs important to establish some constructive habits that take little time but leave you feeling refreshed and ready to take on whatever is thrown at you on a given day.
Some famous saying suggests that we should aim to improve ourselves by 1% each day. I like to think of it as we can become 1% more efficient each day if we simply look at the processes around us and try to think of how they can be done better.
One of the biggest complaints or realizations of data scientists, when they enter their careers, is that the majority of the work is menial. Data scraping, cleaning, and preparation probably take 80% of the time required for a project ‚Äî 10% is required for project planning and the other 10% is needed for the actual analysis and visualization work. This means that 80% of the work you do, while incredibly vital for the success of the project, is repetitive, mind-numbing work that is relatively akin to scraping a cheese grater against your forehead for 8 hours a day.
towardsdatascience.com
Many data scientists have found respite by automating some of these kinds of tasks so that more time can be spent doing the ‚Äúfun‚Äù stuff. One of the greatest realizations that occur when learning how to code is the sheer number of things you can do just by writing a few commands. This realization can lead you to get creative with all sorts of menial tasks that no one has time for.
By looking for one menial task a day to automate, you‚Äôll find a slew of free time on your hands that can be used for other aspects of projects. They say that 20% of your work produces 80% of your results, so by getting to dedicate more time to that vital 20%, you may just produce some amazing results. From personal experience, the tasks you automate can be as simple as alarms that remind you to blink every once in a while or that automatically sort your emails, not to mention data cleaning and processing. The important thing is that it takes care of work you would otherwise have to do manually.
Besides, automating tasks is a great mental break and gymnastic rolled into one fun activity that not only gives you a chance to stretch your legs as a programmer but will also pay dividends very soon.
towardsdatascience.com
Microlearning is a verifiable way to pick up new data science skills in less than 10 minutes per day.
Developing this habit is a great way to keep you interested in advancing your skills as a data scientist by picking up new technologies or ways of doing things. Medium, Reddit, Substack, and various podcasts (see below) are great sources of information about new advances in data science that may inspire you to try learning something new.
The key for adult learners is to keep the learning short and pointed toward a specific, tangible goal. This means keeping the learning to short 10-minute blocks with objectives that are easily achievable within that time. Not only does this keep you motivated to keep moving forward in your studies because of the short time they take to complete but they also ensure that you‚Äôre advancing your skills after a study session. Furthermore, it doesn‚Äôt seem like a hardship to complete a habit that takes less time than you need for a coffee break.
In my experience, taking 10 minutes a day to work on a skill doesn‚Äôt provide huge gains immediately, but compounds slowly over time to produce something you can be proud of at the end of a year. Some of the uses I‚Äôve seen for 10-minute data science studies include learning R, knocking the rust off of multi-variable calculus skills, gaining a deeper understanding of Excel‚Äôs various functions, and increasing business acumen by reading industry newsletters.
The most gung-ho data scientists I‚Äôve ever met are often those who enjoy working on their own data science projects outside of work. This true passion of theirs for data science never seems to wane on account of them working on personal projects that are completely unrelated to their day-to-day work.
towardsdatascience.com
I‚Äôm not going to waste your time listing all of the possible projects you could work on as many before me (myself included) have done this already. However, what I do want to do, is provide you with a bunch of fun places where you can retrieve free data to work on:
data.nasa.gov
data.worldbank.org
www.google.com
www.ncdc.noaa.gov
These passion projects should be something you can pick up every day for ten minutes to add a feature or attempt a new kind of analysis or visualization. Nothing too strenuous or taxing, but just enough to encourage you to try new skills. While your daily work as a data scientist may be monotonous, this project doesn‚Äôt have to be and can be where you truly stretch your legs as a data scientist.
Many data scientists lately have been saying that they feel as though they‚Äôve maximized their impact on the company they‚Äôre currently working for. I remember reading one post from a data scientist describing how they had optimized every process they could, revealed every possible insight into how the company could work more effectively, and implemented as many new processes as they could think of.
I‚Äôm always optimistic that there are always more things that a company can do better. Even if you started small, with say, a more efficient way of retrieving customer data, you will have improved the effectiveness of your company by a small percentage that will add up to a much larger percentage the more customer data you work with. Alternatively, you could get more creative and design a plugin that formats everyone's code to company standards.
This daily habit not only proves your continued value with the company but also gives you some extra mental exercise during the day that reinforces your inspiration to advance and improve your skills and the processes of the company you work for.
It‚Äôs fair that you may not want to surround yourself 24/7 with the stuff that you do for work. However, there are some great data science podcasts and audiobooks you can listen to that aren‚Äôt as mentally taxing as trying to force a weak data set to show you answers.
The podcasts suggested below tackle everything from ethical AI to breaking glass ceilings to professional development as a data scientist. I prefer listening to stories about non-technical aspects of data science that dig into the nitty gritty of ethical issues, interviews with data scientists I look up to, and how to become a better data scientist because they offer a slight break from the rigors of programming and mathematics.
banana-data.buzzsprout.com
www.widsconference.org
theartistsofdatascience.fireside.fm
Data science audiobooks are a great way to get in some extra learning when few of us have time to read anymore. I like to listen to audiobooks that tackle more academic or rigorous topics because listening to something explained to you often yields a clearer understanding of the topic than if you were to read it for yourself. However, on more than one occasion, I‚Äôve found myself scrambling for a pen to write down a pertinent idea, so make sure you‚Äôre prepared with pen and paper to ensure you don‚Äôt miss any ideas that just click with you.
www.audible.com
www.audible.com
www.audible.com
www.audible.com
I got my first job in tech thanks to a relationship I maintained with a former school colleague. I paid this kindness forward by contacting another school colleague about a position that was perfect for them at the company I was now working for.
Helping talented friends and colleagues around you get jobs is just one of the ways you can share your experience and knowledge and pay your success forward. This can be as elaborate as helping someone else get a job to as simple as sharing a tip with a co-worker who asked you for help at the water cooler.
For example, I share my experience and knowledge with the Towards Data Science community by writing articles on how to get jobs in data science, how to become better data scientists, and how to learn data science more effectively. I‚Äôm also considering turning these teachings into a book that can be shared and accessed by a wider audience.
Sharing what you know with others is a surprisingly inspirational and fulfilling daily habit because it not only shows you how far you‚Äôve come as a data scientist but also helps others around you to attain the same success. Data science is one of the hardest fields to break into, which is why helping others with learning Python, sharing tricks on how to automate data cleaning, or helping others prepare for presentations is such a vital thing to get in the habit of doing.
Some other ways to help others and share your knowledge include writing blog posts, starting a data science podcast or Twitter account, responding to questions on r/datascience or Quora, or starting a Youtube channel as many data scientists have already done.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Support my writing by donating to fund the creation of more stories like this one: Donate
ODSC - Open Data Science
The Data Observer
Lawrence Alaso Krukrubo
in
Towards AI
Mert Alperen Be≈üer
Sharan Kumar Ravindran
in
Towards Data Science
Osgood Gunawan
Abhishek Singh
Yash Gupta
in
Data Science Simplified
"
65,https://medium.com/@thestartupjournal/14-best-software-business-ideas-that-will-boom-in-2023-3a116329cf2c,14 Best Software Business Ideas That Will Boom In 2023,"Software is becoming an essential technology on the planet. That‚Äôs why everyone needs to pay attention to how Software can shape our lives and the lives of others.
There are many new ways to do things through Software ‚Äî from your phone, tablet, or computer‚Ä¶
DLT Labs
in
DLT Labs
Marcel Blijleven
Seth Massarsky
DeSo News - en
Nova Lailatul Rizkiyah
in
IT Newbie‚Äôs Note
Gajendra Gulgulia
Murat Uenlue
Brian Yu
"
66,https://medium.com/towards-data-science/machine-learning-algorithms-cheat-sheet-2f01d1d3aa37,Machine Learning Algorithms Cheat Sheet,"The ‚Äúno free lunch‚Äù theorem for machine learning states that there is no single machine learning algorithm that can solve all types of machine learning problems.
Machine learning tasks can vary widely and the choice of algorithm will depend on things such as the size, dimensionality and sparsity of the data. The target variable, the quality of the data and the interactions and statistical relationships that exist both within features and between the features and target variable.
As a result, it is not possible to simply select one algorithm for a one-size-fits-all approach. Depending on exactly how they work different algorithms will be better suited to certain tasks. Data scientists will typically select the final algorithm to use by first determining a sub-set of algorithms appropriate for the specific problem and then experimenting with these algorithms to find the optimal choice.
In this article, I will provide a quick reference guide to five of the most common algorithms used for machine learning. This will provide an introduction to the inner workings of the algorithms and the considerations that make each algorithm better suited to certain tasks.
This will include a brief introduction to Linear Regression, Logistic Regression, Random Forest, XGBoost and K-means. For each algorithm I will cover the following:
Linear regression is a supervised machine learning algorithm that is used to predict a continuous target variable. For simple linear regression, where there is one independent variable (feature) and one dependent variable (target) the algorithm can be represented by the following equation.
y = a + bX
Where y is the dependent variable, X is the explanatory variable, b is the slope of the line and a is the intercept.
Simple linear regression can be visualised as a scatter plot where the x-axis contains the dependent variable and the y-axis contains the independent variable. The linear regression algorithm draws a line of best fit through the data points minimising the variation between the predicted and actual outputs.
Logistic regression is essentially linear regression moulded to fit a classification problem. Instead of fitting a straight line, logistic regression applies the logistic function to squeeze the output of a linear equation between 0 and 1. The result is an S-shaped curve rather than a straight line through the data points, illustrated in the diagram below.
A threshold between 0 and 1 is chosen to separate the classes, typically this is 0.5. In essence, we draw a horizontal line across the S curve at 0.5. Any data points above this line belong to class 1 and any below to class 0.
The random forest algorithm builds a ‚Äòforest‚Äô of decision trees. Each tree in the forest generates a prediction from a given set of features. Once all predictions have been generated a majority vote is taken and the most commonly predicted class forms the final prediction.
A decision tree is a very intuitive algorithm. It has a flowchart-like structure containing a series of nodes representing a test. The outcome of each test results in a split and one or more leaf nodes are created until the final prediction is achieved. Hyperparameters determine how deep the tree grows and the node-splitting function used.
The Random Forest algorithm follows the below sequence of steps:
XGBoost is an algorithm based on gradient-boosted decision trees. It is similar to Random Forest in that it builds an ensemble of decision trees but rather than training the models in parallel, XGBoost trains the models sequentially. Each decision tree learns from the errors produced by the previous model. This technique of training models sequentially is known as boosting.
The gradient in XGBoost refers to a specific type of boosting where weak learners are used. Weak learners are very simple models that only just perform better than random chance. The algorithm starts with an initial weak learner. Each subsequent model targets the errors produced by the previous decision tree. This continues until no further improvement can be made and results in a final strong learner model.
K-means is one of the most popular algorithms for clustering, a form of unsupervised machine learning which aims to find groups of similar examples within a training dataset.
The algorithm works by first initialising random cluster centroids. Then for each datapoint a distance measure commonly the Euclidean distance or Cosine similarity is used to assign it to the nearest centroid. Once all data points are assigned the centroid is moved to the mean of the assigned data points. These steps are repeated until the centroid assignment ceases to change.
The algorithms covered in this article are some of the most widely used for machine learning. There are many more algorithms available each of these having its own suitability for specific problems. As mentioned at the start of this article there is no one algorithm that can solve all problems currently although XGBoost is the closest thing to it for solving problems with structured data.
This introduction will provide some insight into how and why a data scientist may select one algorithm over another. Here is a brief summary of the suitability of each algorithm to specific types of problems:
Thanks for reading!
The iris dataset was used to generate the decision tree diagram featured in this article: This dataset was used in R.A. Fisher‚Äôs classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems. It is used under the CCO 1.0 Universal (CCO 1) license.
De Jun Huang
in
dejunhuang
Devansh- Machine Learning Made Simple
in
Geek Culture
Î∞ïÍ±¥ÎèÑ
Boris Smus
in
Towards Data Science
Matteo Tortoli
Prateek Joshi
in
Analytics Vidhya
Susumu Yamazaki
Fabio Gomez Silva
"
67,https://medium.com/towards-data-science/python-3-14-will-be-faster-than-c-a97edd01d65d,Python 3.14 Will be Faster than C++,"Python is one of the most used scripting languages in data science (DS) and machine learning (ML). According to ‚ÄòPopularitY of Programming Languages‚Äô Python is the most searched language on Google. Next to being a great glue language to connect various DS/ML solutions together, it has many libraries to virtually do anything with data.
In about a month we get a fresh new yearly release of Python: version 3.11. I am quite excited about this new version as the main feature for this version is a significant increase in speed.
On LinkedIn I have already seen a couple of post of people testing the new version and their results were stunning. But the best method to get a feeling on how fast Python 3.11 truly is, is to run the tests yourself.
In this post I will share my step-by-step analysis of Python 3.11. All code is available on my github page.
Benchmarking a programming language is not trivial at all. When you read x is faster than y, you should always take the result with a grain of salt. One implementation of an algorithm can be better of x while another is better on y. For our benchmark it is a bit simpler as we are testing Python against Python, but we might have selected elements from the language that are only marginally effected. With this in mind, I want to present the algorithm I used to benchmark: the estimation of Pi using a Monte Carlo method.
The idea of this algorithm is simple but the first time a saw it during some mathematics course at the University it blew my mind. We have a square of size 2r and in this square we fit a circle of radius r. Now we take a random number generator that generates numbers on a plane: <-r, r>, <-r, r>. The ratio between the points that are on the circle and the points on the square (read: all points) is an approximation of the ratio of area, which we can use to approximate Pi. This is a bit more clear in the equation:
In Python, I have split up the actual estimation from a testing script such that I can repeat the test and take the average. Not shown here, but I have also parametrized the script using Argparse, a standard library to parse arguments from the command line interface (CLI). The Python code looks like this:
This script is ready to run, however we want to use it to test various versions of Python, not only the currently installed (or activated) version. The easiest way to test multiple versions of Python is to use Docker. Python maintains many docker images. Naturally all supported version, but also some versions that are end-of-life (EOL) such as 2.7 or 3.2. It also has images for release candidates such as version 3.11. To use Docker, you need to have it installed. In Linux and Mac it is relatively easy, in Windows I am not so sure but probably not difficult as well. I would advice to install only the docker CLI, the desktop is too much bloat for me. To run a local script in a containerized Python environment run:
To automate the tests for the various version, we will of course also use Python. This script will simply start a subprocess to start a container with the particular Python version and collects the results afterwards. Nothing special:
When running these tests, the absolute number differs from machine to machine, depending on the processor (it is CPU heavy). Here are the results for the last 7 major Python versions:
The benchmark took on average 6.46 seconds for Python 3.11. Comparing this to the previous version (3.10), this is almost 37% faster. Pretty impressive! Approximately the same difference is between version 3.9 and 3.10, making 3.11 almost 70% faster! I have plotted all times in figure 2.
When talking about speed, we always have that one guy saying: if you want speed why not use C.
C is much faster that Python! ‚Äî that one guy
While my C is a but rusty, I thought I give it a try anyway. I used GNU C++ as it comes with a nice time measurement library (chrono). Find the code below:
As we all know, C++ is a compiled language and therefore, we need to compile the source before we can use it. When you have the typical build-essentials installed, you can type:
After the compilation, simply run the build executable. The output should be like this:
And we have to agree with that one guy as it is really (read: REALLY) fast. It took only 0.257 seconds to do the same loop we programmed in Python before. Lets add this as a line in our previous plot, shown in figure 3.
Now, after appreciating the previous figure for a bit longer, we clearly see the momentum Python has gained. Since version 3.9, Python has increased in speed about 35%. The Python developers mentioned that the next couple of versions will have a significant speed increase, therefore, we could assume that this pace will be kept (yup, super safe assumption).
Now the question is, with this momentum fixed, when would Python surpass the time of C++. For this we can of course use extrapolation to predict the loop times of the next Python versions to come. These can be seen in figure 4.
The result is really stunning! Keeping at this pace, Python 3.14 will be faster than C++. To be exact, the loop time will be -0.232 seconds, so it will be done just before you want to do the calculation. There appears to be a hole in time-space continuum but these calculations are rock solid. Therefore, I think we might have to question the work of Einstein and friends.
While these benchmarks for Python 3.5 .. Python 3.11 are valid, the extrapolation is of course meant as a joke. The XKCD style figures are meant as an additional reminder to that ;-).
If you want to run these or your own tests on the various Python version, download the code on my Github page.
Please let me know if you have any comments! Feel free to connect on LinkedIn.
TensorFlow
in
TensorFlow
IHME
in
Acting on Data
TensorFlow
in
TensorFlow
Harish Kandan
Kan Nishida
in
learn data science
Nosratullah Mohammadi
in
Analytics Vidhya
Chanak Karki
Rashida Nasrin Sucky
in
Towards Data Science
"
68,https://medium.com/towards-data-science/how-i-became-a-data-scientist-7f5b10606612,How I became a data scientist,"As a data scientist with a PhD in economics, I always get a lot of questions about why I chose to be a data scientist, and how was my journey like. I think it is a great opportunity to share my experience here, especially the things I did right and wrong, to help anyone who is interested in data science but comes from a different background.
Back in 2015, I was thrilled to receive a PhD offer in economics and started my graduate study in the US. I was once an exchange student in the US during undergrad and was really impressed by the education quality here thus decided to continue my graduate study here. I didn‚Äôt think too much about the choices of majors. I majored in economics in undergrad, and I was fascinated by the ways economists interpret the rules guarding the world. Continuing in Econ was a natural choice at the time. When applying to the programs, I expressed my passion for teaching, and I was determined to be in academia as my dream was to be called a professor. I knew it would be a hard journey but was ready for the challenges.
As I came to the PhD program directly as an undergrad, I had vague and probably wrong impressions about what is research. I was inspired by a number of devoted researchers in different fields who are determined to extend the knowledge frontiers in certain areas, but soon discovered the difficulties of dealing with the ambiguities needed when constantly working on things without the right answers. I really had a hard time coming up with research ideas and implementing them with trustworthy methodologies. Gradually, I realized I want to work on projects that are fast-paced, where I can receive feedback and see my impacts at the early stage before I go on the rabbit hole of ‚Äúrevising and improving‚Äù. Thus I set my sight outside of academia and started to look for potential opportunities in the industry.
There are many potential positions for economic PhDs in the industry, as the questions we solve and the skills we develop in school align with what companies need in a lot of fields. My research background is in empirical studies, which is problem-solving using data-driven approaches. Facing a problem, how to get relevant data, how to clean data, and how to drive insights from the data to answer the question, were what I practice every day while completing my thesis. I came across Python when I was working on the data analysis process for one chapter and took online classes to get to know more about Python and get introduced to data science. That‚Äôs when my interest in data science emerged and kept growing after I found my skill sets already partially matched what needed to be a data scientist.
Taking online classes in Python, machine learning basics, and other data science techniques were the first steps. Utilizing Python to complete the data analysis processes in my thesis really gave me a head start in conquering this language. Then I got an idea of using machine learning algorithms in one of my thesis chapter, and implemented it with two other colleagues sharing the same interests. With this background, I applied and got accepted into an eight-week data science Bootcamp with a scholarship, and that‚Äôs when I really got my hands dirty in various projects and took a racket in developing relative skills. The Bootcamp also offered career coaching, job-searching aids, and alumni panels, which helped me understand more about what data scientists do daily, and enhance my determination of applying for data scientist positions. For more details about the Bootcamp experience, you can read the article here:
pub.towardsai.net
As someone who benefited from working on hands-on projects while ramping up data science skills, I want to give back and I am developing a data science course with hands on projects in time series forecasting, for those who are interested in learning time series analysis and getting hands dirty with the best practices in the industry. Unfortunately, the course is only in Chinese for now, but I will have a series of blog posts coming up for tutorials and interesting practices related to this course. DM me here or on LinkedIn if you are interested in attending the course.
For the ones who have followed me for a while, you might know my job hunting journey was not an easy one. It‚Äôs a seven-month-long process with a lot of rejections along the way. It‚Äôs a constant challenge to self-confidence and made me doubt myself multiple times on whether I have made the right choice. As I look back, I am so grateful to have not given up easily and kept putting myself together for the next interview. Finally, it paid off. I have also written an article to share the whole experience, you can check it out here:
towardsdatascience.com
What I really benefited from the long job searching process, is not only the valuable interview experiences, but also all these articles I have written summarizing my preparation for each interview, and many found also useful in preparing for their own interviews:
Zijing Zhu
Time flies, now I have worked as a data scientist for over one year and a half. I have switched companies in the middle, and maybe in the future I will share more details about how to apply and prepare for interviews while employed. Here I want to focus on what I do as a data scientist.
Basically what I do, in either company, aligns with my expectations. As a PhD, I might take on more responsibilities in the researching and modeling part, I have the trust of my coworkers and managers in exploring and applying new methodologies. I have also always worked on end-to-end projects, meaning that I take charge of the whole process of a project from collecting data to delivering the final product. My first job is project-based, where my coworkers and I got to work on different projects solving different problems. My current position has a fixed goal, which is making time-series predictions. While the goal is the same, as to improve forecasting accuracy, we need to keep adjusting our models and methodologies to cope with the ever-changing macroeconomic situations. Both positions are challenging but rewarding, where I get to see how my work is put into production, or how my forecasts are adopted to the upstream. The feedback, no matter negative or positive, shows the impact of my work, and that‚Äôs what motivates me to continuous improvements.
Data science is a challenging position that requires continuous learning since the world is changing rapidly, and the techniques are changing even faster. It has so many potential use cases and it has been shaping the industry for all the companies to make data-driven decisions. The more time I have in this role, the more I have realized data science is not only about fancy data analysis, advanced modeling, or complicated coding, as I assumed, but more about collaborations with different teams, revealing and communicating insights, and data storytelling. In the next article, I am discussing what makes a good data scientist and seven principles I follow based on my experience:
towardsdatascience.com
To summarize, my journey from Econ PhD to data science contains a lot of happiness, challenges, rejections, fulfillments, and so on. I am glad to work in a position I have both passion and skills. Hopefully, this article gives a reference point to those who are in a career or any life intersection, and I will be thrilled if it somehow inspires anyone. Thank you for reading! Check these other articles of mine if interested:
zzhu17.medium.com
Subscribe to my email list:
zzhu17.medium.com
Sign up for medium membership:
zzhu17.medium.com
Or follow me on YouTube:
youtube.com
Zijing Zhu
in
Towards Data Science
Florence Egwu
Kaushik Choudhury
in
Towards Data Science
Swetha Suresh
in
UX Planet
Annapurani Vaidyanathan
EOS
Gursev Pirge
Angelica Lo Duca
in
Analytics Vidhya
"
69,https://medium.com/towards-data-science/meet-julia-the-future-of-data-science-52414b29ebb,Meet Julia: The Future of Data Science,"As a data enthusiast, you have probably heard about Julia, the ‚Äúfuture programming language of data science.‚Äù There are claims that Julia will replace Python and R in the data science field, as it offers significant benefits in terms of performance, efficiency, and ease of use.
In this article, we will look into what Julia is, its applications, and whether it is worth learning the language for data science.
Julia was created to offer the simplicity of Python, the statistical capabilities of R, and the speed of C and C++.
Python is a high-level language with a simple syntax that most programmers find easy to use. It allows data scientists to spend less time writing code and focus on model building. Many libraries within Python allow users to build statistical algorithms and perform scientific programming.
However, a major drawback of Python is that it is slow. While it is easy to use and offers a high level of abstraction, the language comes with high latency.
When building large-scale machine learning applications, Python tends to slow down considerably, and it can take days to run predictive models using the language.
On the other hand, languages like C++ offer fast execution, but the low level of abstraction makes it impractical, complex, and time-consuming to write code.
Julia solves this problem by offering both ease of use and efficiency.
It was designed to give users low latency like C++, which means that it is 1.5 to 5 times faster than Python‚Äôs Pandas library, even without multithreading enabled.
Also, similar to Python, Julia is a general-purpose programming language that also offers statistical capabilities. Its syntax is easy to pick up.
Due to this, Julia is being used increasingly by data scientists, economists, statisticians, scientists, and mathematicians.
As mentioned above, Julia is used by many professionals due to its simplicity and fast performance. Here are some examples of where the language is used in the real world:
Julia is often used by data scientists to build predictive models. The language offers machine learning packages such as ScikitLearn.jl and Tensorflow.jl, which are wrappers for the ScikitLearn and Tensorflow libraries in Python.
Many data scientists prefer Julia over Python since it is faster, easier to debug, simple, and has better package management.
Julia‚Äôs Genie.jl framework is similar to the Django library in Python and can be used for full-stack web development.
If you are looking to create an end-to-end dynamic web application but find Python too slow and don‚Äôt want to spend time learning a more complex language, Julia is a great option.
If you are looking to build a dashboard application to showcase visualizations, Dash.jl can be used, which is equivalent to the Plotly Dash library in Python.
All data professionals will find the need to analyze and extract meaningful insight from data. Julia provides users with a collection of libraries designed specifically for data analysis and visualization.
Julia has a package called DataFrames.jl that allows you to load and manipulate tabular data. You can think of this as an equivalent to Pandas in Python. With multithreading, Julia can be up to 20 times faster than Pandas.
Julia also allows users to visualize data using Plots.jl, which is similar to Matplotlib in Python.
For instance, you can create a simple scatter plot using Plots.jl in a single line of code:
You can even create dynamic animated plots like this in Julia:
Some other charts you can build in Julia include heatmaps, bar graphs, bubble plots, violin charts, and dot plots.
Julia is used extensively in fields like chemistry, physics, astronomy, and bioinformatics.
Professionals in these industries often need to perform statistical analysis on large amounts of data, and Julia has a wide array of packages that allow them to do so.
BioSequences.jl, Molly.jl, and QuantumLab.jl are some examples of scientific packages that can be used to solve domain-specific problems in Julia.
According to reviews, Julia‚Äôs ecosystem of scientific libraries is much more extensive than Python‚Äôs and allows users to tackle problems without having to code a solution from scratch.
The first programming language I learnt was Python, and this is almost always my language of choice when working on data science tasks.
I first came across Julia when working on a linear programming problem. I had to build an optimizer to minimize a function based on a set of constraints.
This solver then had to be automated so that the rest of my team could use it, which meant that it had to ingest constraints and functions as user input.
I found a few libraries in Python to do the job, but wasn‚Äôt happy with the way the output was presented. The packages I used were also inefficient, so I turned to Julia for an alternative solution.
The Optim.jl library was a lot faster than anything I had tried in Python (Scipy, Gekko), and saved at least 4‚Äì5 hours of the team‚Äôs time.
I still perform a lot of data analysis and model building workflows in Python since I‚Äôm familiar with the language, but will switch to Julia if I ever find Python too slow or come across a problem like the above again.
Professionals like data scientists, analysts, developers, and economists can use Julia for their everyday work. Since the language is easy to use, fast, and has an extensive array of scientific libraries, it is a great alternative to Python and R.
Julia has increased in popularity in the past three years and is currently being used by large organizations like Apple, Amazon, Google, IBM, and Microsoft.
While Julia is currently not as well documented or widely used as Python and R, some experts claim that the language is the future of data science.
According to them, it is a good idea to learn Julia now ‚Äî so you can stay on top of the field when the language becomes the ‚Äúnext big thing‚Äù in the next 10 years.
Other data scientists have a more conservative opinion and believe that it is better to learn Python or R first since they are currently the most widely used programming languages in data science.
At the end of the day, only time will tell what the future holds for Julia.
Personally, I think it‚Äôs always a good idea to learn something new if you have spare time on your hands. Learning Julia can help you speed up machine learning workflows and showcase portfolio projects that set you apart from the crowd.
Here are 3 steps you can take to learn Julia for data science:
4. Once the installation is complete, download the latest version of Anaconda and follow the instructions in the executable.
5. Navigate to Start -> Jupyter Notebook. This should open a tab that looks like the image below:
6. Click on New and select Julia from the drop-down list. This will open a notebook in which you can write Julia code:
If you already use Jupyter to run Python programs, then you should be familiar with this interface.
Now that you have Julia up and running, take an online course to learn the basics of the programming language. This includes concepts such as variables, data structures, operators, methods, and functions.
Julia is well-known for a feature called multiple dispatch, which uses all of a function‚Äôs arguments to choose which method should be invoked.
Once you have a grasp of the basics of Julia, learn about the different packages offered by the language for statistics, data analysis, machine learning, and visualization.
Finally, use these packages to analyze data and perform predictive modeling.
I suggest taking an Introduction to Julia course to learn the language, as it will cover all the concepts mentioned above in more detail.
Once you understand the fundamentals of Julia, you can start creating personal projects using the language. Not only will this improve your programming skills, but it is also a great value add to your data science portfolio.
Here are some project ideas with source code that you can implement using Julia:
Seth K
Ashish Kumar Singh
YS Chng
in
Towards Data Science
Nitin
in
Analytics Vidhya
Frederik Bussler
in
DataSeries
Davis David
Jay Feng
in
Towards Data Science
Danielle Sweeney
in
Central Maryland Transportation Alliance
"
70,https://medium.com/better-programming/build-a-react-app-using-dall-e-api-bd15d5d67b31,Build a React App Using DALL-E API,"This is the first image I generated, and I love it.
On the 4th of November, I received an email from OpenAI.
DALL¬∑E is now available as an API
You can now integrate state of the art image generation capabilities directly into your apps and products through our new DALL¬∑E API.
I couldn‚Äôt resist, and I jumped into trying it out!
Below is a quick guide to building a simple React app that takes a description as input and generates a unique image using the new DALLE API.
The app is a simple React frontend, but I am working on a simple full-stack React and Node app to use OpenAI and DALLE more appropriately. Follow, subscribe, save, bookmark, and set an alarm on your phone!
The app will look like this:
I used the prompt: ‚ÄúA cangaroo jumping on the Sydney Opera House during a beautiful sunset.‚Äù
I didn‚Äôt know kangaroos have wings, but I have never seen one. Also, my typo doesn‚Äôt help DALLE, I guess.
Anyway, create your own and show me! Let‚Äôs start!
DALLE 2 is a generative language model that produces original images based on text input, among other things.
DALLE 2 produces photos with a 4x higher resolution than DALLE, and human judges favor it +70% of the time for caption matching and photorealism.
As reported in the documentation, ‚ÄúThe OpenAI API is powered by a family of models with different capabilities and price points. You can also customize our base models for your specific use case with fine-tuning.‚Äù
And the models are:
This is the first step to building our image generation app in React.
Save it somewhere because you won‚Äôt see it again.
As reported on their website:
‚ÄúYour secret API keys are listed below. Please note that we do not display your secret API keys again after you generate them.
Do not share your API key with others or expose it in the browser or other client-side code. To protect the security of your account, OpenAI may also automatically rotate any API key that we‚Äôve found has leaked publicly.‚Äù
This is just common sense!
So, it could be smart not to deploy a frontend app using this API. If you do that:
We can create a React app with the following command:
where demodalle is the name of the app.
We then move to the folder using cd ./demodalle.
Now we need to install the OpenAI library.
As reported, ‚ÄúYou can interact with the API through HTTP requests from any language, via our official Python bindings, our official Node.js library, or a community-maintained library.‚Äù
Run the following command from the root folder of your app:
After the installation, you should see the OpenAI package among your dependencies in package.json.
We will work in App.js and broadly follow the documentation to create images using OpenAI.
The Images API offers three methods for interacting with images, and in the following app, we will use the one that allows for creating images from scratch based on a text prompt.
Let‚Äôs begin by adding some configuration info at the top of App.js:
First, we import Configuration and OpenAIApi.
Second, we initialize a configuration variable that uses the new Configuration provided by the OpenAI library to prepare configuration data. Here we need to pass the value of the secret API key to apiKeyas a string.
Third, we initialize another variable called openai. This is the last step of the configuration to use the API.
Be aware that your API key will be exposed if you upload your code to any public repositories. We know that is bad.
One way to hide the secret API key is by using an .env file.
If you created your React app by using Create React App, environment variables are supported out of the box.
You need to follow these steps:
If you go this way, remember to update your configuration variable as follows:
Now that the configuration is done, let‚Äôs write the code to get the user input.
The following code starts from the boilerplate you get by creating a React app with CRA (e.g., what we did above using npx create-react-app).
On line 2, we declare a state variable, userPrompt, as an empty string. Remember to import the useState hook at the top of your App.js file:
The state variable userPrompt gets updated every time the user types something in the input element on line 14.
On line 16, the onChange event handler takes the value of the input (e.g., the text written by the user) and assigns it to userPrompt by using the setter function setUserPrompt provided by the useState hook.
On line 3, we declared another state hook. We will use the state variable imageUrl to store the URL of the image created by DALLE.
The app will show the React logo until we don‚Äôt have an image. The code from lines 7 to 11 does exactly this by displaying the React logo as long as imageUrlis falsy.
The user should be able to click on the button on line 18 to create an image.
Technically, we want to trigger an HTTP POST request toward the OpenAI API when the user clicks the button.
First of all, we add an event handler to the button:
When the button is clicked, it triggers the generateImage function that will call the API. Let‚Äôs create the function.
As shown, we create an async function called generateImage.
On line 2, we create an object called imageParameters that takes three keys: prompt, n, size.
They are, respectively, the userPrompt (e.g., the text from the input element), the number of images you want to create and the size of those images.
I left the number of images and their size hardcoded because creating images is expensive. You might get 18 dollars to start. Otherwise, you need to pay for each creation.
Line 7 is where the magic begins. We call the createMethod of the openai API, and we pass the imageParameters object created above.
Note that we use the await keyword to wait until we get data before assigning it to response.
On line 8, we extract the URL information from the response and save it in urlData. This is a simple URL string pointing to your unique AI-generated image.
Finally, we update the state of imageUrl by using the setImageUrl function on line 10.
As soon as your image is ready and you save the URL in imageUrl, this will evaluate to a truthy value, and your image will get displayed in your app.
It might take 5‚Äì10 seconds, and it could be a good idea to add a loading spinner to inform the user that something is happening.
So, here I leave my last creation: ‚ÄúA cangaroo is surfing on a surf board in Bondi beach.‚Äù As you can see, not all kangaroos have wings!
However, all kangaroos without typos are very similar to dogs! Here is ‚ÄúA kangaroo is surfing on a surfboard in Bondi beach.‚Äù
If you are laughing at this kangaroo-dog, you are on a good path to enjoying your app. However, these oddities highlight a bigger aspect of AI.
The technical part of the article ends here. Below I present some aspects of AI that you should be aware of. Some images might hurt your sensibility, so feel free to stop here.
I am looking forward to seeing your creations!
AI fails in very different ways compared to regular software
You won‚Äôt get any big red errors saying that the generated image isn‚Äôt a kangaroo. Nor will you get told the image might convey an unintended meaning.
At best, you get a joke.
At worst, you get something harder to digest.
‚ÄúA photo of a horse sleeping in a pool of red liquid.‚Äù
In the same way, it might be hard to grasp subtle implications like biases and stereotypes. Furthermore, DALLE is also pretty bad at spelling words in images.
All this shows that we made incredible progress, but the journey is still long!
Matt
Puksakm
Jeongiitae
M grin
Robert Han
Lena Tyson
Business Access Report
Matt Dionis
in
Vuefinder
"
71,https://medium.com/gitconnected/dont-just-leetcode-follow-the-coding-patterns-instead-4beb6a197fdb,Don‚Äôt Just LeetCode; Follow the Coding Patterns Instead,"What if you don‚Äôt like to practice 100s of coding questions before the interview?
Coding Interviews are getting harder to pass. To prepare for coding interviews, you will need weeks, if not months of preparation.
No one really likes spending that much time preparing for the coding interviews. So is there a smarter solution?
First, let‚Äôs look at the problem.
Anyone preparing for coding interviews definitely knows LeetCode. It is probably the biggest online repository for coding interview questions. Let‚Äôs take a look at what problems people face when using LeetCode.
There are more than 2k problems in LeetCode. The biggest challenge with LeetCode is its lack of organization; it has a huge set of coding problems, and one isn‚Äôt sure where to start or what to focus on.
One wonders, is there an adequate number of questions one should go through to consider themselves prepared for the coding interview?
I would love to see a streamlined process that guides me and teaches me enough algorithmic techniques to feel confident for the interview. As a lazy person myself, I wouldn‚Äôt like to go through 500+ questions.
One technique that people often follow is to solve questions related to the same data structure; for example, focusing on questions related to Arrays, then LinkedList, HashMap, Heap, Tree, or Trie, etc. Although this does provide some organization, it still lacks coherence. For example, many questions can be solved using HashMaps but still require different algorithmic techniques.
I would love to see question sets that follow not only the same data structure but also similar algorithmic techniques.
The best thing I came across was the problem-solving patterns like Sliding Window, Fast and Slow Pointers, Topological Sort, etc. Following these patterns helped me nurture my ability to ‚Äòmap a new problem to an already known problem‚Äô. This not only made this whole coding-interview-preparation process fun but also a lot more organized.
Coding patterns enhance our ‚Äúability to map a new problem to an already known problem.‚Äù
I have gathered around 20 of these coding problem patterns that I believe can help anyone learn these beautiful algorithmic techniques and make a real difference in the coding interviews.
The idea behind these patterns is that once you‚Äôre familiar with a pattern, you‚Äôll be able to solve dozens of problems with it. For a detailed discussion of these patterns and related problems with solutions, take a look at Grokking the Coding Interview.
So, without further ado, let me list all these patterns:
Following is a small intro of each of these patterns with sample problems:
Like it or not, LeetCode-type questions are part of almost every programming interview, so every software developer should practice them before an interview. Their only option is to prepare smartly and learn problem-solving by focusing on the underlying problem patterns. Learn more about these patterns and sample problems in Grokking the Coding Interview and Grokking Dynamic Programming for Coding Interviews .
Check Design Gurus for some interesting courses on Coding and System Design interviews.
medium.com
medium.com
medium.com
Steve Konves
ALEXON HUNG
Yi don't know Kung Fu
Nathan Hughes
in
Silk Studio
Chandrashekar Munukutla
GeekyAnts
in
The GeekyAnts Blog
EKbana
in
EKbana
Shinichi Okada
"
72,https://medium.com/@kingsleyofori/my-first-day-as-a-data-analyst-daa15eaf9c7a,My first day as a data analyst,"I have always been a big data fan. Maybe I am obsessed with it because of my love for mathematics and statistics, or maybe, just maybe, the computer is just a way for me to escape the realities of the real world and enter the limitless virtual universe, where nearly anything is possible.
Truth be told, this adventure of mine began as a check on my previously unchecked tendency to procrastinate. I can still picture myself picking up the phone, downloading the Minimalist goal app, and typing, ‚ÄúI have to learn MySQL, Python, and Excel before the end of the month.‚Äù From there, the rest is history!
Now, I am a proficient user of these tools for analysis.
As a self-acclaimed guru with one day of experience in data analysis, I believe I qualify to share one or two silver nuggets for the youngsters who will like to venture into data analysis.
For starters, I want to emphasize that the path to becoming good or great in a field begins with ignorance and the conviction that there is much to learn. A cup that is already full can not hold more liquid. As technology and computer programming continue to advance daily, I believe there is no room for folks who believe they know it all.
I also came to the realization that understanding concepts quickly is not always important; rather, it is important to give yourself plenty of room to make mistakes and learn from them. Even though I‚Äôm still rather excellent, when I run my SQL codes, they keep duplicating themselves.
Do I complain? Yes, I do.
Do I get frustrated? Yes, absolutely!
But, do I seek help with it? Yes, and I will continue to do it each and every day till I get the concepts right.
I think that learning the fundamentals thoroughly is sometimes the fastest method to pick up concepts, and I believe that this is the key to all brilliance.
I participated in the Microsoft Excel for Data Analysis Boot Camp by FestMan in the pursuit of my ‚Äúguru-shipness‚Äù in data analysis and learned how simple and straightforward it is to use Excel to build one‚Äôs confidence in thinking they have what it takes to be a data analyst. The dashboard below is an example of how I made the audacious decision to use the world as my blank canvas for greatness.
Given that this dashboard is presumed to have been created by a Human Resource Data Analyst, its main purpose is to describe a company‚Äôs workforce and pay scale. Here‚Äôs the detailed approach in solving the FestMen Coursework presented below ‚Äî Click Here
My own critics: Personally, I think I did well with the color patterns and the use of the charts. However, I could have done well with the clip art image that lies beneath ‚ÄòTOP 10 Populated Staff by Departments‚Äô and explained most of the subject topics with a single-lined description of what I wanted to communicate.
This ends my first day of being a data analyst!
Join me as I strive to master data analysis at LinkedIn.
Jake from Mito
Pradeep VSR Pydah
in
Frontier Technologies Hub
Matias Calderini
in
Towards Data Science
Robert Smith
Hetal Kapadia
in
atoti
DataBridge Market Research
Hetal Kapadia
in
atoti
Skillslash
"
73,https://medium.com/towards-data-science/fooled-by-statistical-significance-7fed1bc2caf9,Fooled by statistical significance,"Behold the world‚Äôs shortest lecture on #statistics and everything that is wrong with how people approach it:
42.
Or, rather: p=0.042
Contrary to popular belief, the term ‚Äústatistically significant‚Äù does not mean that something important, momentous, or convincing took place. If you think that we‚Äôre using the word significant here in a way that would make your thesaurus proud, you‚Äôre falling victim to a cunning bit of sleight of hand. Don‚Äôt let poets lie to you.
‚ÄúYou shouldn‚Äôt let poets lie to you.‚Äù ‚Äî Bj√∂rk
For those who prefer to keep their exposure to statistical nitty gritty to a minimum, here‚Äôs all you need to know about the term statistically significant:
To everyone other than the decision-maker in question, statistically significant results are rarely significant in the sense of ‚Äúimportant‚Äù ‚Äî they‚Äôre occasionally great for raising interesting questions, but often they‚Äôre irrelevant.
Be on extra high alert when non-experts use this term, especially when it is accompanied by breathless exuberance. Sometimes especially cheeky charlatans go one step further and drop the ‚Äústatistically‚Äù bit, tapping into the full power of the poetry. ‚ÄúHey look,‚Äù they tell you, ‚Äúwhat we‚Äôre talking about is SIGNIFICANT in the eyes of the universe.‚Äù
No, it isn‚Äôt.
The worst possible offenders are those who pronounce ‚Äústatistically significant‚Äù like it‚Äôs a synonym for ‚Äúdefinite‚Äù or ‚Äúcertain‚Äù or ‚Äúflawless knowledge‚Äù ‚Äî there‚Äôs some irony getting lost here. The term comes from a field that deals with uncertainty and thus (by definition!) only belongs in settings where our knowledge is not flawless.
For those who prefer to fight jargon with jargon, I‚Äôll help myself to more formal language in the next section. Feel free to nope out of that bit, but if you‚Äôre simultaneously curious and new around here, take a little detour to zoom through all the biggest ideas in statistics in just 8 minutes:
towardsdatascience.com
Most of the links in my articles take you to blog posts where I‚Äôve given you a deeper overview of highlighted topics, so you can also use this article as a launchpad for a Choose Your Own Adventure minicourse on data science.
‚ÄúStatistical significance‚Äù merely means that a p-value* was low enough to change a decision-maker‚Äôs mind. In other words, it‚Äôs a term we use to indicate that a null hypothesis was rejected.** What was the null hypothesis, though? And how strict was the test? ¬Ø\_(„ÉÑ)_/¬Ø
Welcome to statistics, where The Answer is p = 0.042 but you don‚Äôt know what the question was.
Technically, the decision-maker who set up the conditions of the hypothesis test is the only person for whom that test‚Äôs results can be statistically significant.
Statistics gives you a set of tools for decision-making, but how you use them is up to you ‚Äî it‚Äôll be as individual as any other decision.
The process involves phrasing your decision question very carefully, picking the assumptions you‚Äôre willing to live with, making some risk tradeoffs about the different ways your answer might be wrong*** (because randomness is a jerk), and then using mathematics to get a risk-controlled answer to your particular question.
There‚Äôs something perverse and comical in its popularity as a prop for rhetorical bullying.
That‚Äôs why real experts would never use statistics like a hammer for beating Truth into one‚Äôs enemies. Two decision-makers can use the same tools on the same data and come to two different ‚Äî and completely valid ‚Äî conclusions‚Ä¶ which means that there‚Äôs something both perverse and comical in its popularity as a prop for rhetorical bullying.
Statistical significance is personal. Just because I am surprised enough by the data to change my mind doesn‚Äôt mean you should be.
As soon as I understood how statistics works, I couldn‚Äôt help but marvel at how remarkably arrogant ‚Äî almost rude ‚Äî it is to declare something to be statistically significant in the presence of people who aren‚Äôt fluent in the limitations of statistical decision-making. The term sounds much too universal for anyone‚Äôs good; it plays like a ‚Äúshut up and trust me because my methods are fancy‚Äù rhetorical device. I hope you‚Äôll join me in giving that brand of rhetoric the ‚Äúpffft‚Äù it deserves.
Hang on, is there nothing at all we can learn from someone else‚Äôs statistically significant result?
Here‚Äôs where it gets somewhat philosophical, so I‚Äôll need a separate article for my take on that question:
kozyrkov.medium.com
In a nutshell, my advice is that it‚Äôs fine to delegate some of your decision-making to other people as long as you trust them to be competent and have your best interests at heart. When they‚Äôre convinced, you‚Äôll borrow their opinion so you don‚Äôt have to redo all their work yourself.
By using someone else‚Äôs statistical conclusions, you‚Äôre not basing your decision on data but rather on your trust in an individual human being.
Just be aware that by using someone else‚Äôs results, you‚Äôre not basing your decision on data but rather on your trust in an individual human being. There‚Äôs no problem with choosing to trust others so you don‚Äôt need to build your whole worldview empirically from scratch ‚Äî knowledge sharing is part of what makes the human species so successful ‚Äî but it‚Äôs worth being aware that you might be a few rounds of broken telephone downstream of whatever ‚Äúknowledge‚Äù you think you‚Äôre tuning into.
If you let someone step up to make decisions on your behalf ‚Äî that‚Äôs what it means to consume someone else‚Äôs p-value and conclusions for decision-making ‚Äî then be sure it‚Äôs someone you consider sufficiently competent and trustworthy.
What if the person shoveling statistical jargon at you is someone you don‚Äôt trust? Run for the hills!
Whenever there‚Äôs a whiff of persuasion clinging to declarations of statistical significance, be extra cautious of whatever wares the utterer is peddling. If you trust the person you‚Äôre talking to, you don‚Äôt need their appeals to statistical significance. All you need to know is that they‚Äôre convinced. If you don‚Äôt trust them, you can‚Äôt trust their stats jargon any more than you‚Äôd trust their jazz hands.
What good is an answer if you haven‚Äôt bothered to understand what the question was?
If there‚Äôs one thing I‚Äôd like you to take away from this blog post, it‚Äôs this: If you don‚Äôt know much about the decision-maker and how they set about figuring out whether they should change their minds (and precisely about what), then their claims related to statistical significance are utterly meaningless to you. What good is an answer if you haven‚Äôt bothered to understand what the question was?
If you had fun here and you‚Äôre looking for an applied AI course designed to be fun for beginners and experts alike, here‚Äôs one I made for your amusement:
kozyrkov.medium.com
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Here are some of my favorite 10 minute walkthroughs:
*If you‚Äôre keen to learn what a p-value is, here‚Äôs a video I made to help you out:
**For an explanation of hypothesis testing, head over to my blog post on the topic or check out this pair of videos:
you.com
Mike Chung
Hien Vo, M.S.
in
Towards Data Science
Aman Sharma
in
Towards Data Science
Andrew J. Zahuranec
in
Open Data Policy Lab
Binh Nguyen
Domingorizzi
Christopher Tao
in
Towards Data Science
"
74,https://medium.com/towards-data-science/pandas-isnt-enough-learn-these-25-pandas-to-sql-translations-to-upgrade-your-data-analysis-game-af8d0c26948d,Pandas Isn‚Äôt Enough. Learn These 25 Pandas to SQL Translations To Upgrade Your Data Analysis Game,"This is my 50th article on Medium. Thank you so much for reading and appreciating my work üòä! It‚Äôs been an absolutely rewarding journey.
If you like reading my articles here on Medium, I am sure you will love this as well: The Daily Dose of Data Science.
What is this? It‚Äôs a data-science oriented publication that I run on substack.
What will you get from this? Here I present elegant and handy tips and tricks around Data-science/Python/Machine Learning, etc., one tip a day (See publication archive here). If you are interested, you can subscribe to receive the daily doses right in your inbox. And it‚Äôs completely free. Would love to see on the other side!
SQL and Pandas are both powerful tools for data scientists to work with data.
SQL, as we all know, is a language used to manage and manipulate data in databases. On the other hand, Pandas is a data manipulation and analysis library in Python.
Moreover, SQL is often used to extract data from databases and prepare it for analysis in Python, mostly using Pandas, which provides a wide range of tools and functions for working with tabular data, including data manipulation, analysis, and visualization.
Together, SQL and Pandas can be used to clean, transform, and analyze large datasets, and to create complex data pipelines and models. Therefore, proficiency in both frameworks can be extremely valuable to data scientists.
Therefore, in this blog, I will provide a quick guide to translating the most common Pandas operations to their equivalent SQL queries.
Let‚Äôs begin üöÄ!
For demonstration purposes, I created a dummy dataset using Faker:
CSVs are typically the most prevalent file format to read Pandas DataFrames from. This is done using the pd.read_csv() method in Pandas.
To create a table in your database, the first step is to create an empty table and define its schema.
The next step is to dump the contents of the CSV file (starting from the second row if the first row is the header) into the table created above.
We get the following output after creating a DataFrame/Table:
We can use the df.head() method in Pandas.
In MySQL Syntax, we can use limit after select and specify the number of records we want to display.
The shape attribute of a DataFrame object prints the number of rows and columns.
We can use the count keyword to print the number of rows.
You can print the datatype of all columns using the dtypes argument:
Here, you can print the datatypes as follows:
Here, we can use the astype() method as follows:
Use ALTER COLUMN to change the datatype of the column.
The above will permanently modify the datatype of the column in the table. However, if you just wish to do that while filtering, use cast.
There are various ways to filter dataframe in Pandas.
#6: You can filter on one column as follows:
The above can be translated to SQL as follows:
#7: Furthermore, you can filter on multiple columns as well:
The SQL equivalent of the above filtering is:
#8: You can also filter from a list of values using isin():
To mimic the above, we have in keyword in SQL:
#9: In Pandas, you can also select a particular column using the dot operator.
In SQL, we can specify the required column after select.
#10: If you want to select multiple columns in Pandas, you can do the following:
The same can be done by specifying multiple columns after select in SQL.
#11 You can also filter based on NaN values in Pandas.
The same can be extended to SQL as follows:
#12 We can also perform some complex pattern-based string filtering.
In SQL, we can use the LIKE clause.
#13 You can also search for a substring within a string. For instance, say we want to find all the records in which last_name contains the substring ‚Äúan‚Äù.
In Pandas, we can do the following:
In SQL, we can again use the LIKE clause.
Sorting is another typical operation that Data Scientists use to order their data.
Use the df.sort_values() method to sort a DataFrame.
You can also sort on multiple columns:
Lastly, we can specify different criteria (ascending/descending) for different columns too using the ascending parameter.
Here, the list corresponding to ascending indicates that last_name is sorted in descending order and level in ascending order.
In SQL, we can use order by clause to do so.
Furthermore, by specifying more columns in the order by clause, we can include more columns for sorting criteria:
We can specify different sorting orders for different columns as follows:
For this one, I have intentionally removed a couple of values in the salary column. This is the updated DataFrame:
In Pandas, we can use the fillna() method to fill NaN values:
In SQL, however, we can do so using the case statement.
If you want to merge two DataFrames with a joining key, use the pd.merge() method:
Another way to join datasets is by concatenating them.
Consider the DataFrame below:
In Pandas, you can use the concat() method and pass the DataFrame objects to concatenate as a list/tuple.
The same can be achieved with UNION (to keep only unique rows) and UNION ALL (to keep all rows) in SQL.
To group a DataFrame and perform aggregations, use the groupby() method in Pandas, as shown below
In SQL, you can use the group by clause and specify aggregations in the select clause.
And we do see the same outputs!
To print the distinct values in a column, we can use the unique() method.
To print the number of distinct values, use the nunique() method.
In SQL, we can use the DISTINCT keyword in select as follows:
To count the number of distinct values in SQL, we can wrap the COUNT aggregator around distinct.
Here, use the df.rename() method, as demonstrated below:
We can use ALTER TABLE to rename a column:
Use the df.drop() method:
Similar to renaming, we can use ALTER TABLE and change RENAME to DROP.
Say we want to create a new column full_name, which is the concatenation of columns first_name and last_name, with a space in between.
We can use a simple assignment operator in Pandas.
In SQL, the first step is to add a new column:
Next, we set the value using SET in SQL.
Congratulations! You now know the SQL translation of the most common methods in Pandas.
I have tried to cover translations for most of the data scientists use on a regular basis in Pandas. However, I understand I might have missed a few.
Do let me know in the responses.
As always, thanks for reading!
üöÄ Subscribe to the Daily Dose of Data Science. Here, I share elegant tips and tricks on Data Science, one tip a day. Receive these tips right in your inbox daily.
üßë‚Äçüíª Become a Data Science PRO! Get the FREE Data Science Mastery Toolkit with 450+ Pandas, NumPy, and SQL questions.
Get a weekly summary of the top 1% Research papers, News, Open Source Repos, and Tweets in Machine Learning.
Syed Aslam
Christophorusdavin
Raji Gali
Kevin Martins
Uve Poom
in
Budget Matador
Ari Love
in
Bold Brave Daily
Pramono Winata
Susana G
"
75,https://medium.com/the-research-nest/solving-the-leetcode-weekly-contest-with-chatgpt-eb697c2fb2,Solving The Leetcode Weekly Contest With ChatGPT,"For this experiment, I will be giving prompts to ChatGPT based on the Leetcode questions asked in Weekly Contest 321 (which was chosen randomly).
I will read the problem statements on Leetcode and present them to ChatGPT. A new conversation thread will be started for each problem to avoid interference from previous problems.
If the code generated doesn‚Äôt work, I will retry and modify the prompt to get better results. The objectives of this experiment are to understand how accurate the code is and how helpful it can be to a developer.
Let us get started.
(Note that I took the Leetcode snapshots in the end after the experiment by rerunning the code)
Problem 1: Find the pivot integer
XQ: Given a positive integer n, find the pivot integer x such that:
The sum of all elements between 1 and x inclusively equals the sum of all elements between x and n inclusively.Return the pivot integer x. If no such integer exists, return -1. It is guaranteed that there will be at most one pivot index for the given input.
Example 1:Input: n = 8Output: 6Explanation: 6 is the pivot integer since: 1 + 2 + 3 + 4 + 5 + 6 = 6 + 7 + 8 = 21.Example 2:Input: n = 1Output: 1Explanation: 1 is the pivot integer since: 1 = 1.Example 3:Input: n = 4Output: -1Explanation: It can be proved that no such integer exist. Constraints:1 <= n <= 1000
Write code to solve this in python
ChatGPT: To solve this problem, we can iterate over the integers from 1 to n and check if the sum of the elements from 1 to x equals the sum of the elements from x to n for each integer x. If this is the case, we can return x. Otherwise, if we reach the end of the iteration without finding any integer x that satisfies the given condition, we can return -1 to indicate that no such integer exists.
Here is one possible implementation of this solution in Python:
It got it ALMOST correct. This result was like the first try. The only thing it didn‚Äôt understand was the word ‚Äúinclusive.‚Äù The only mistake in the code was that you had to add x while calculating the right sum. (because X is inclusive to both sides).
Problem 2: Append characters to string to make subsequence
XQ: You are given two strings, s and t, consisting of only lowercase English letters. A subsequence is a string that can be derived from another string by deleting some or no characters without changing the order of the remaining characters. Return the minimum number of characters that need to be appended to the end of s so that t becomes a subsequence of s.
ChatGPT: To find the minimum number of characters that need to be appended to the end of s so that t becomes a subsequence of s, we can follow these steps:
Here is some sample code that implements the steps described above:
This code will return the minimum number of characters that need to be appended to the end of s so that t becomes a subsequence of s.
Second try, it worked. The was no code change required either. I copy pasted and submitted it, and it passed all test cases. It was super fast for me.
Problem 3: Remove nodes from linked list
XQ: You are given the head of a linked list. Remove every node which has a node with a strictly greater value anywhere to the right side of it. Return the head of the modified linked list.
Example 1:Input: head = [5,2,13,3,8]Output: [13,8]Explanation: The nodes that should be removed are 5, 2 and 3. ‚Äî Node 13 is to the right of node 5. ‚Äî Node 13 is to the right of node 2. ‚Äî Node 8 is to the right of node 3.
Example 2: Input: head = [1,1,1,1] Output: [1,1,1,1] Explanation: Every node has value 1, so no nodes are removed. Constraints: The number of the nodes in the given list is in the range [1, 105]. 1 <= Node.val <= 105
ChatGPT: Here is a simple approach that solves this problem in linear time and space complexity:
Here is the implementation of the above approach in Python:
This approach has a time complexity of O(n) and a space complexity of O(n), where n is the number of nodes in the linked list.
Once again, it gets almost correct. There is a tiny error in the code. While iterating the array from right to left, the if condition should be if arr[i] >= mx and not just ‚Äúgreater than.‚Äù
I made that change and submitted the code, and it passed all the test cases. This was a second try. The first time, I didn‚Äôt include the examples in the prompt, which did not give the desired result. Then I started a new thread with the entire question it (almost) solved it seamlessly.
Problem 4: Count subarrays with median K
XQ: You are given an array nums of size n consisting of distinct integers from 1 to n and a positive integer k. Return the number of non-empty subarrays in nums that have a median equal to k.
ChatGPT: To find the number of non-empty subarrays of nums that have a median equal to k, we can iterate over all possible subarrays of nums and check if the median of each subarray is equal to k.
To find the median of a subarray, we first need to sort the subarray in ascending order. If the length of the subarray is odd, the median is simply the middle element of the sorted subarray. If the length of the subarray is even, the median is the average of the two middle elements of the sorted subarray.
Once we have the median of a subarray, we can check if it is equal to k and increment a counter if it is. After iterating over all possible subarrays, we can return the final value of the counter as the result.
Here is some example code for finding the number of non-empty subarrays in nums that have a median equal to k:
Note that this solution has a time complexity of O(n¬≥) because there are n possible starting indices for the subarrays, n possible ending indices for the subarrays, and we need to sort each subarray, which takes O(n log n) time. This may not be efficient enough for large inputs, so we may need to find a more efficient solution.
The brute force approach given by ChatGPT is almost correct. There is only 1 line of code change required.
Instead of using the actual formula, this is given as a condition in the question. However, this code is not optimized and gave a time limit exceeding the error in Leetcode.
This is a Leetcode hard problem, and I did have a tough time getting to an answer using ChatGPT, unlike the other three questions. The primary solution it shared was straightforward. However, making it write the correct optimized solution was somewhat hard. I tried a few things and then gave up as it was getting convoluted.
Here‚Äôs my current take.
From what I have seen, ChatGPT raises the bar for the lower end of the skillset. Assume everyone has a skill level from 0 to 100. With ChatGPT, the lower level probably rises to 30. Now, everyone has a skill level from 30 to 100. In the end, it is still a level playing field when everyone has access to it.
Perhaps, it will help someone who is a beginner to become 10X more efficient. However, it might be a 100X boost for a subject matter expert as they understand things to nudge ChatGPT better and combine ideas to reach where they want to. They will also be quick in identifying the errors and fixing them.
Let‚Äôs make things a bit meta :P
XQ: Write a blog article on ‚ÄúSolving The Leetcode Weekly Contest With ChatGPT‚Äù. Give some examples of ChatGPT successfully solving Leetcode questions. Show where ChatGPT fails to solve as well.
ChatGPT: If you‚Äôre a software engineer or data scientist, chances are you‚Äôre familiar with Leetcode, the popular platform for practicing coding and algorithmic problem-solving. One of the highlights of the Leetcode experience is the weekly contest, where users can test their skills against other users and compete for prizes.
Solving Leetcode problems can be a challenging and time-consuming task, especially when it comes to the weekly contest. That‚Äôs where ChatGPT comes in. ChatGPT is an AI-powered chatbot that can help you solve Leetcode problems in real-time, providing you with step-by-step solutions and helpful hints along the way.
Here are some examples of ChatGPT successfully solving Leetcode questions:
While ChatGPT is able to successfully solve many Leetcode problems, there are also instances where it fails to solve a problem. For example, ChatGPT was unable to solve the Leetcode problem ‚ÄúPalindrome Number,‚Äù which asks you to determine whether an integer is a palindrome. This is because the problem involves complex logic and requires a deep understanding of the problem, which is beyond the capabilities of ChatGPT at this time.
Overall, ChatGPT is a valuable tool for solving Leetcode problems, particularly when it comes to the weekly contest. Its step-by-step solutions and helpful hints can save you time and effort, allowing you to focus on the more challenging problems and improve your skills as a software engineer or data scientist.
This is what I mean when I say it sort of looks like a simulator. Notice how it simulates ChatGPT failing to solve the ‚ÄúPalindrome number‚Äù simply because I made a prompt to tell where it failed. I am pretty sure it would otherwise quickly solve that question as well.
SUMAN KUMAR PAUL
Stan Garfield
Morganne Gagne
Ion Dodon
Brent Gruber
Aishi Jenes
Roargh
in
The Startup
Ronan O Ciosoig
in
eDreams ODIGEO Tech
"
76,https://medium.com/towards-data-science/7-best-research-papers-to-read-to-get-started-with-deep-learning-projects-59e11f7b9c32,7 Best Research Papers To Read To Get Started With Deep Learning Projects,"Research papers are an integral part of learning more about the several new methodologies that are introduced and discovered in the world of Artificial Intelligence (AI). All the data scientists and researchers come together on a common platform to share their meticulous work and knowledge to help grow the flourishing AI community even more to reach greater heights.
With tons of research and innovation brought forward by skilled individuals each day, the overall experience of staying updated with the latest technologies can be overwhelming. This is especially the case for a beginner who is just trying to get engrossed in the world of deep learning. It might be hard to figure out which research papers are the best starting point for developing new projects and gaining an intuitive understanding of the subject.
In this article, we will look at seven of the best research papers that developers and data science research papers that developers and data science enthusiasts must read. These research papers have stood the test of time and provide a baseline for many of the implementations that are already implemented or yet to be implemented in the future.
For deep learning, it is always best to have your own device or system for computing complex problems. Before proceeding further into this article, I would suggest looking into some of the best PC builds for deep learning in the numerous price ranges from the following article link provided below.
towardsdatascience.com
The field of deep learning is enormous. There are several options of research papers to choose from, as each one of the introduced works introduces a new concept or methodology that is useful for the data science and artificial intelligence community. In this section of the article, we will explore seven of the most beneficial and intriguing research papers that have stood the test of time.
Research Paper: Deep Residual Learning for Image Recognition
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
Summary:
There are several transfer learning models that are used by data scientists to achieve optimal results on a particular task. The AlexNet model was the first to be introduced to win an image processing challenge in 2012, and since then, transfer learning models like VGG-16 have been the most influential pieces of deep learning.
We will focus on the ResNet architecture for this article because the ResNet network manages to achieve slighter improvements than its counterparts. Another significant reason for considering the ResNet network is the fact that it has many variations depending on the type and number of residual blocks that you plan to include. Some of the ResNet structures are ResNet-18, ResNet-34, ResNet-50, ResNet-101, etc.
The ResNet architecture makes use of residual blocks. This concept is quite significant as it solves some of the issues of other shallower networks, which suffer from the problems of vanishing or exploding gradients. The residual blocks pass to compute the sum of the output of a previous block with a current and deeper layer in the model.
Significance (Why read this paper?):
Transfer learning is a major part of deep learning. We can make use of the learned information from one model and utilize the data to build another custom model on top of it for performing numerous tasks. Even without building a custom architecture, we can use the original transfer learning model to perform a particular task. Using transfer learning models avoids the need to create and build your own model from scratch each time.
Research Paper: You Only Look Once: Unified, Real-Time Object Detection
Authors: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi
Summary:
Object detection (alongside face recognition) has always been a catching point of deep learning models. Ever since the introduction of the YOLO model, we have been able to solve the complex problem of object detection by creating a bounding box around specific objects of significance that the model is trying to determine. The YOLO network makes use of a series of convolutional neural networks for learning how to detect objects in real-time during training.
The YOLO model has been improved and developed continuously since its original release in 2015. We have had substantial improvements to each of these methods with each version, such as YOLO-v2 and YOLO-v3. The most recent YOLO version, as of writing this article, is the YOLO-v6 mechanism. Each of these architectures has constantly progressed to make additional refinements to improve efficiency for the object detection tasks.
Significance (Why read this paper?):
Computer vision is one of the most popular branches of artificial intelligence. A deep learning model that can solve these complex computer vision problems, such as real-time object detection and face recognition, is highly valuable. YOLO is one of the best methods for solving the object detection problem with high precision. If the viewers are interested in mastering the basics of computer vision, I would recommend the viewers check out the following guide provided below.
towardsdatascience.com
Research Paper: U-Net: Convolutional Networks for Biomedical Image Segmentation
Authors: Olaf Ronneberger, Philipp Fischer, and Thomas Brox
Summary:
The task of segmenting involves categorizing similar parts of an image into a cluster. All the identical classes are classified and segmented into a particular entity. With the segmentation of images, most of the complexities of an image can be removed, allowing the user to make further computations for image processing and analysis.
Once the segmentation is performed on an image, it opens up numerous possibilities to interpret the data more effectively. One such model that performs this task effectively is the U-Net network. The U-Net model architecture, which comprises an encoder and decoder type network, accepts an input image that needs to be segmented.
Depending on the number of classes and the particular type of task, the image that is passed through the network goes through several stages of convolution, downsampling, and finally upsampling to meet the specific task. The network also makes use of skip connections to avoid any degradation problems and carry out the useful information in each downsampling to the upsampling stage.
Significance (Why read this paper?):
The U-Net is a revolutionary paper for the segmentation of different types of computer vision problems. A large number of tasks, especially in the field of medical image processing, make use of the U-Net architecture. There have been several variations that have been derived from the U-Net network that are also useful for segmentation projects. Once we have a U-Net model to segment a type of specific image, we can make use of that image for further analysis and computing.
Research Paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
Authors: Sergey Ioffe, Christian Szegedy
Summary:
In deep learning, it is often a good idea to normalize the data. Let us consider the MNIST dataset. In the MNIST dataset, once we get the values of the grayscale images of the digits 0‚Äì9 in the form of numpy arrays, we have the range of values from 0‚Äì255. It is often a good idea to standardize and normalize these data elements to a range of values between 0. and 1. floating variables.
Batch Normalization layers do a somewhat similar action where a mini-batch mean, and a mini-batch variance is computed to normalize the data accordingly. The Batch Normalization layer helps to speed up the training process and also reduces the significance of weight initializations. These layers also help to regularize the model training and slightly overcome issues of over-fitting.
Significance (Why read this paper?):
Batch Normalization layers are an integral part of most modern deep learning architectures. While constructing any type of complex neural network, a batch normalization layer can be considered a high utility entity. These layers take the input from one of the layers and map it to the other layer by normalizing the data to accelerate the computation by reducing the internal covariate shift. These Batch Normalization layers are especially useful for convolutional neural networks, where they allow each of the layers to function more independently.
Research Paper: Attention Is All You Need
Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin
Summary:
Making use of simple recurrent neural networks to solve complex tasks often results in a few major issues for highly complex tasks. Two of the primary drawbacks were exploding and vanishing gradients, where essential information was lost during longer sequences of transmission of data. Long-Short Term Memory (LSTM) models were able to fix most of the elemental issues with RNNs. By utilizing these LSTM networks in sequence-to-sequence models, we were able to achieve highly successful results on a wide variety of natural language processing tasks.
The transformers network utilizes a connection of an encoder and decoder type architecture with an attention mechanism. The attention layer provides interconnectivity between the decoder and the encoder allowing it to access the hidden states. This process allows the model to have a higher weightage towards the specific entities (such as keywords across sentences). There are different types of attention mechanisms, such as dot attention, self-attention, and multi-head attention, among others.
Significance (Why read this paper?):
Transformers are some of the best deep learning tools that are extremely useful for solving a wide array of natural language processing tasks. These transformers have the capability to perform complex language tasks ranging from machine translation from one language to another, question-answering systems, chatbots, text classification problems, and so much more. The possibilities of transformers are limitless, and this research paper serves as a great conceptual base platform for every other research paper inspired by it, such as the Generative Pre-trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT) models.
Research Paper: Generative Adversarial Nets
Authors: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio
Summary:
One of the more popular research papers which were initially introduced in 2014 by Ian Goodfellow and his team was Generative Adversarial Networks. These architectural deep learning frameworks are extremely impactful in generating completely new data. The functionality of these adversarial networks makes use of a generator and discriminator network, where the two architectures compete with each other to improve the overall results.
The generator tries to generate unique data that looks like real sample images. On the other hand, the discriminator tries to detect the generated samples and classify them as real or fake. Both these two networks are trained simultaneously in a constant continuous loop. Once the generator is able to bypass the discriminators checking system and generate realistic images, we have a completely trained Generative Adversarial Network. This model can generate unique data from scratch for a specific type of data.
Significance (Why read this paper?):
There are multiple iterations and variations of Generative Adversarial Networks such as DCGANs, Cycle GANs, SRGANs, W-GAN, and so much more. These architectures are some of the most used elements in the generation of new data in the field of deep learning today. With generative networks gaining more popularity now than previously before, the advancements that await this branch are enormous. It is highly recommended to start with the following research paper to keep up with the continuous advancements of these generative archetypes.
Research Paper: Autoencoders
Authors: Dor Bank, Noam Koenigstein, Raja Giryes
Summary:
The autoencoders are another type of generative network that is useful for numerous applications. The autoencoders make use of an encoder and decoder type network along with a latent dimensional space. The encoder stage of the autoencoders takes in an input that can be interpreted by the network into the latent dimensional space containing vectors. These vectors contained in the latent space are compressed in nature.
Hence, the autoencoders are useful for tasks of dimensionality reduction where the original image of a particular size is compressed into a latent dimensional space. With this compressed latent dimensional space, the decoder can reconstruct the same image with a lesser dimensional space. The reconstructed image is similar to the original image but has lesser vectors in comparison to the original image.
Significance (Why read this paper?):
Autoencoders have numerous applications that are used by data scientists and deep learning researchers. Apart from dimensionality reduction applications, as we previously discussed, these autoencoders are also useful for tasks like image denoising, feature extraction, and anomaly detection. Apart from the mentioned applications, a variation of the autoencoders called variational autoencoders is useful for image generation, similar to GANs. Hence, it is safe to say that these autoencoders have enormous potential in the field of deep learning.
Research has formalized curiosity. It is poking and prying with a purpose. ‚Äî Zora Neale Hurston
Research and innovation are integral pillars of development and learning. The modern quality of research has risen to reach greater heights. Each of them contains large amounts of knowledge for an individual to enlighten themselves with. The quality of the high-level research papers is especially true for deep learning, which involves tons of research and time investment.
In this article, we understood the basic aspects of the seven best research papers that have stood the test of time. Hence, they are a resourceful asset for all beginner data scientists to learn more about and explore further. We also understood the significance of these research papers and the specific concepts they cover. Before diving into the millions of options for research papers for specific topics, I recommend checking these out to gain further exposure to the subject of deep learning.
If you want to get notified about my articles as soon as they go up, check out the following link to subscribe for email recommendations. If you wish to support other authors and me, then subscribe to the below link.
bharath-k1297.medium.com
If you have any queries related to the various points stated in this article, then feel free to let me know in the comments below. I will try to get back to you with a response as soon as possible.
Check out some of my other articles in relation to the topic covered in this piece that you might also enjoy reading!
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
Thank you all for sticking on till the end. I hope all of you enjoyed reading the article. Wish you all a wonderful day!
Jeff Epstein
in
Chatbots Life
Alexander Fleiss
WorkKey - Get Set, Grow‚Ä¶
Jeremie Harris
in
Towards Data Science
Oluseyi Akindeinde
Bharath K
in
Towards Data Science
Roman Kucev
in
Neatsy AI
Jeremie Harris
in
Towards Data Science
"
77,https://medium.com/better-humans/10-weird-little-behaviours-that-make-people-attracted-to-you-9e9c718f25e6,10 Little Behaviours that Attract People to You,"I‚Äôve spent most of my life awkward and in my head.
I always wanted to make a good impression, so I learned a ton from my mistakes and did everything possible to improve my relationships.
Sujit Mourya
HOPE - FM
Michael Zick
in
Hello, Love
Relish
Bright Johns
Katia Beeden
James Bond
"
78,https://medium.com/javascript-in-plain-english/14-essential-developer-tools-to-10x-your-productivity-7ff779ec55de,14 Essential Developer Tools to 10X Your Productivity üöÄ‚ú®,"Developing a good workflow is essential in such a fast-paced environment with strict deadlines. Therefore, productivity is one of the most important metrics to consider.
In this post, I will share some of the best developer tools to help you work more efficiently.
Each tool will include a direct link, a description, and an image preview.
It makes the color of your computer‚Äôs display adapt to the time of day.
Every tool you need to work with PDFs in one place.
A convenient world clock, a time zone converter, and an online meeting scheduler.
A regex cheat sheet that also explains the commonly used expressions.
A temporary e-mail address that expires after 10 minutes.
Make images smaller using best-in-class codecs right in the browser.
Turn any screenshots into a browser mockup.
A minimalistic typing test featuring many test modes.
An HTTP client in your web browser.
Create a logo with the perfect font, icon, and color scheme.
Drag and drop some images and you get a zip file containing the thumbnails.
Enhance image resolution with AI without losing quality.
Open-source and self-hosted kanboard style project manager.
Open-source plugins for metrics about your programming.
I love writing, and it brings me so much joy to be able to help people find inspiration in my content. If you have any questions, feel free to reach out!
Connect me on Twitter and check my blog here for more content!
More content at PlainEnglish.io.
Sign up for our free weekly newsletter. Follow us on Twitter, LinkedIn, YouTube, and Discord.
Interested in scaling your software startup? Check out Circuit.
Jaiinfoway
The Pragmatic Programmers
in
The Pragmatic Programmers
Steve Hankin
Anthony Perez
The Pragmatic Programmers
in
The Pragmatic Programmers
The Pragmatic Programmers
in
The Pragmatic Programmers
Parth2412
in
JavaScript in Plain English
The Pragmatic Programmers
in
The Pragmatic Programmers
"
79,https://medium.com/@universidadedosdados/get-recruiters-to-call-you-with-these-3-simple-tips-94d7516163f4,Get recruiters to call you with these 3 simple tips!,"As a Data Science mentor, I‚Äôve always dealt with beginners having trouble in finding their first job. Even though we have a booming job market with so many positions for data professionals, these people keep complaining that they can‚Äôt even get to an interview. For those more experienced, this is very atypical, we get several messages, I would say on a weekly basis at least. So, what‚Äôs wrong?
I would summarize the problem into two perspectives:
Now, take a look at the 3 tips below, I guarantee you that recruiters will start calling you about their positions:
Last year, I recruited two entry-level data scientists and I have to tell you that the market is booming on the demand side, but also on the supply. We received more than 50 resumes in less than a month, being a startup. Can you imagine what‚Äôs happening on the big companies? Can you imagine how many resumes they receive on a daily basis?
There‚Äôs too much competition, you need to stand out among other candidates. Besides, we have no idea how much you know regarding data science, machine learning, statistics, Python and so on. Having a portfolio is the perfect strategy to stand out between so many resumes. It‚Äôs like you are right in front of us, programming and saying ""Hey, this is everything I know, look at this dataviz, look at this model etc etc"".
Please, start your portfolio right now! It‚Äôs a must! If you have no idea on what to do, here a few links which might help you:
As I mentioned in the intro, the search for job candidates is made by machines and you need to appear in their search. The simplest way of doing that is by using the correct keywords. If you have a strong knowledge in Python and in SQL, mention both tools in your Linkedin (and in your resume).
Take a look at your Linkedin right now, check if you mentioned all the programming languages you know, all the dataviz tools, the statistics techniques, the Machine Learning models and so on. Basically, I‚Äôd recommend you to have a headline containing two or three data terms you‚Äôre willing to work with and one or two languages (or tools) you have some proficiency. If you are only a beginner and feel insecure about this type of headline, you may add some word to express this level of knowledge (e.g.: ""Data science enthusiast""). These are my choices for the headline and the ""About"" section:
Now, we have already shown the job market we know something regarding data science and the machines can find us in their search. However, we still need to convince the recruiter (and other stakeholders) we are problem solvers. What do I mean by that? Take a look at these two descriptions I found on Linkedin:
When people think about data scientists, they think about a crazy genius capable of understanding complex mathematical models, or maybe about great programmers. In spite of that, we are business professionals. Our job is to find solutions using data. You can build a CatBoost with 90% of accuracy using hundreds of features and several feature engineering techniques, but you won‚Äôt impress anyone if that model doesn‚Äôt solve any problem. If your model doesn‚Äôt increase revenue, or reduce loss, or anything like that, it‚Äôs worthless. Both descriptions I showed you are trying to explain the solution and present the results (quantified) to the reader, not talk about algorithms.
That‚Äôs why you should mention numbers when talking about your past experience. Show recruiters you aren‚Äôt only good in math and in programming, tell them about the problems you solve in your career. Did your model increase the revenue? Did it reduce the company loss? Did your program reduce the number of hours of the previous program? Can you quantify this?
Ok, you might be saying ""dude, I‚Äôve just left college, I‚Äôve never done anything like this"". Well, tell them about your volunteer experiences, other internship experience, being the president of any group, all these sorts of things. Just try to show them that you know you must focus your skills on problem solving, not only building machine learning models.
I know some might be thinking that only 3 tips won‚Äôt change much, but I guarantee you that by doing all of these you‚Äôll at least double your chances of getting a data related position. Keep improving your profile, keep becoming an attractive candidate for the position, don‚Äôt stop studying, you‚Äôll soon find what you want!
Nicole Janeway Bills
in
Towards Data Science
Michael Grogan
in
Towards Data Science
Vivek
in
DataKind Bengaluru
BNQ Global Logistics Express
artificial intelligence
Yukio
Michael Grogan
in
Towards Data Science
Tyler Kim
in
Towards Data Science
"
80,https://medium.com/towards-data-science/how-to-run-sql-queries-on-your-pandas-dataframes-with-python-4237ffecc43b,How to Run SQL Queries On Your Pandas DataFrames With Python,"Pandas is being increasingly used by Data Scientists and Data Analysts for data analysis purposes, and it has the advantage of being part of the wider Python universe, making it accessible to many people. SQL on the other hand is known for its performance, being human-readable, and can be easily understood even by non-technical people.
What if we could find a way to combine the benefits of both Pandas and SQL statements? Here is where pandasqlcomes in handy.
From this conceptual blog, you will understand what pandasqlis before diving deep into hands-on practices, which will be even much easier if you are already familiar with SQL.
Keep in mind that table and dataframe will be used interchangeably to mean the same thing.
So, what is pandasql?
It is basically used to query pandas DataFrames using SQL syntax. The same process can be performed using sqldf to interact with R DataFrames.
If you prefer it in video format, the following link gives you access to the full walkthrough.
The installation is straightforward with the syntaxes below depending on your environment:
Before installing, it is important to note that installing libraries using conda, it is necessary to specify the --yesflag so that you don‚Äôt get stuck to the (Proceed ([y]/n)?) prompt.
After successfully installing the library, you should be able to import the library using the following statement.
Before diving deep, let‚Äôs start by creating the datasets StudentTable and TeachingAssistantTablethat will be used for hands-on practice.
These are the main concepts that will be covered in this section: (1) column section, (2) data filtering, (3) data aggregation, and finally, (4) data joining.
This corresponds to selecting part or all the columns of your database/data frame. It is performed with the keyword SELECT col_1, col_2,... col_X FROM tableName
‚Üí 1, 2, ...X correspond to the columns you are interested in having in the final result.
‚Üí tableName is the name of the dataframe/table.
The result from the following code includes all the columns of the student dataframe.
Sometimes when you have complicated queries, you can proceed step by step as follow:
Let‚Äôs say we want the student's name, their email and limit the result to the first 3.
Data filtering is when the WHERE statement comes into the equation to perform custom filtering on the result of the SELECT statement.
Let‚Äôs imagine that we want all the female students.
Let‚Äôs have a look at this query that aims to retrieve the Name, Email, and Degree of all the Teaching Assistants with a Master's Degree.
I have decided to perform the same request, this time using Pandas statement, and this is what it looks like!
This looks like. scary statement ü•∫. Let‚Äôs understand what happened:
But‚Ä¶ we performed the same task with SQL query, and was simple and easy to understand. Without going into too much comparative analysis, just wanted to emphasize how complicated can be some queries that are simple to write in SQL.
Aggregation in SQL is performed with the help of aggregation functions, and these are some of the most commonly used: COUNT , SUM , MAX & MIN , and AVG . For instance, you can get the age of students, based on their gender.
The join concept becomes useful when we want to combine data from at least two tables. This section goes straight to the point with only one type of join. You can learn more about all the join concepts from my article titled 4 Types of SQL JOIN Every Data Scientist Should Know.
Imagine you want to know who is both a student and also a teaching assistant. The answer to this requires joining our two tables as follows, using an INNER JOIN and the final result contains the following information:
Congratulations!üéâüçæ You have just learned how to leverage the power of pandasql, a great tool that allows you to apply both SQL and Pandas queries on your dataframes.
If you like reading my stories and wish to support my writing, consider becoming a Medium member. With a $ 5-a-month commitment, you unlock unlimited access to stories on Medium.
Feel free to follow me on Medium, Twitter, and YouTube, or say Hi on LinkedIn. It is always a pleasure to discuss AI, ML, Data Science, NLP, and MLOps stuff!
Cornellius Yudha Wijaya
in
Towards Data Science
Kyrios
Wayne Wooyoung Hong
Joseph Bloom
in
Towards Data Science
TechGig
Baker Eliza
Cornellius Yudha Wijaya
in
Towards Data Science
David St. Geme
"
81,https://medium.com/gitconnected/chatgpt-is-mind-blowing-everything-you-need-to-know-9e03fdb0b370,ChatGPT Is Mind-Blowing ‚Äî Everything You Need To Know,"ChatGPT is OpenAI‚Äôs latest language model, specifically trained for interactions by conversation. Since the release of GPT-3, many problems have surfaced, such as the AI not being able to respond to some questions very well or making up information. While you can try to change the prompt (i.e. the text given to the AI), it is still you that is trying to cue the AI. But logically, it would be much better if the AI instead cued you. That is, instead of trying to iteratively rephrase a question to make the AI understand what you are looking for and answer correctly, the AI should simply ask you if it doesn‚Äôt understand in a conversational manner.
Now, this is the direction ChatGPT moves us towards. It is still more inclined to answer than ask for now, but the conversational format allows for more dynamic interactions. Before you could simulate a conversation with GPT-3, but this isn‚Äôt what it was trained for. ChatGPT, on the other hand, has been trained explicitly for this purpose. It uses a technique called reinforcement learning from human feedback. Reinforcement learning is an area within machine learning where agents are trained to complete objectives in an environment driven by rewards. Iteratively, the agent interacts with the environment by performing actions, obtaining rewards and then adjusting its behavior to both explore the environment and obtain better rewards.
Games, in particular, have been a playing ground for these techniques with huge success and impressive results in Chess, the Atari games, Go and Poker. While games have predefined rules and rewards, a conversation does not, thus, human feedback becomes essential. This was done by prompting a model, sampling several responses and then letting a human manually rank the responses. These rankings will then become training data for a reward model. Finally, a fine-tuned language model will be further trained using reinforcement learning to respond to questions so as to optimize the output of the reward model. For more information, check out OpenAI‚Äôs blog post:
openai.com
The question is of course, is it good? The model was made available for free and it quickly went viral to the point where even Elon Musk tweeted about it:
Thus, a ton of people have tested it, most being highly impressed by its capabilities, myself included. Below I‚Äôve aggregated some of the discoveries that have been made so far of its capabilities and limitations:
Will it replace Google?
Solves bugs:
Solves the first day of Advent Of code 2022:
Explains complicated regex:
Solves questions GPT-3 struggled with:
Can write long-form code:
Not only can ChatGPT be useful, but it can also answer in any manner you please, write odd scripts and be funny:
Prompt injection means prompting a language model to perform unintended side effects, such as revealing its instructions. GPT-3 has been shown to be vulnerable to this type of ‚Äúattack‚Äù. For more information check out the following article:
medium.com
The question is, does the same vulnerability exist for ChatGPT? It seems so:
Testing it myself I could simply ask it directly and obtain similar information:
OpenAI wants ChatGPT to reject questions with bad motives. Naturally, this feature was tested, and it was proven quite easy to bypass the safeguards:
ChatGPT has shown capabilities unseen in GPT-3, and likely we‚Äôve only scratched the surface. Still, similar challenges encountered in GPT-3, such as controlling its output and not revealing its instructions remain. Personally, I‚Äôm eager to perform different experiments with it, especially in those situations where I know GPT-3 was insufficient. You can test it yourself using the link below:
chat.openai.com
You may also be interested in:
medium.com
If you‚Äôd like to get a Medium membership you can use my referral link if you wish. Have a nice day.
Thanks for being a part of our community! Before you go:
üöÄüëâ Join the Level Up talent collective and find an amazing job
MasterControl
Minhaaj Rehman
deepakvraghavan
Yudhi Rahadian
Beardy Nerd
Tarni Infotech
Ale Koretzky
in
Towards Data Science
HyperNation
"
82,https://medium.com/@youssefraafat57/statistics-interview-questions-answers-55e2bd13a96b,Statistics Interview Questions & Answers for Data Scientists,"I started an intuitive on LinkedIn in May 2022 in which I am posting daily data science interview questions and the answers I got on each post I summarize them and post on the next day in another post. The questions are prepared by me, in addition to others that I received from my connections on LinkedIn. You are more than welcome to send me questions on my LinkedIn profile or through my personal email.
The main goal of this intuitive is to revise the basics of data science, be prepared for your next interview, and know what‚Ä¶
H Doug Prater
Italo Sayan
in
Towards Data Science
David Lee
Andrew Howard
Dimitris Poulopoulos
in
Towards Data Science
Bruno Gon√ßalves
in
Data For Science
Laurenflynn
Christopher Thomas BSc Hons. MIAP
in
Towards Data Science
"
83,https://medium.com/towards-data-science/from-a-hiring-managers-perspective-what-i-was-looking-for-in-a-junior-data-scientist-role-during-18fe3579c6bd,From a Hiring Manager‚Äôs Perspective: What I Was Looking for in a Junior Data Scientist Role During the Interview,"As a hiring manager, I always look for a few key qualities in candidates. I want to see that they are intelligent and able to communicate effectively. Additionally, the candidate must have a good understanding of data science concepts -but that‚Äôs not as important as you think. Finally, I am also looking for signs that the candidate is motivated and can work independently. Here are a few specific things I look for during interviews:
Context: I manage a small team of data engineers and data scientists in a large government agency; thus, we operate as a small unit serving a large audience. I have a bias towards staff who are jack of all trades. I acknowledge that these tips are not for everyone. We don‚Äôt typically build tools but focus on building data-driven capability to identify bad actors in the industries we regulate.
Without saying, it is important for a data scientist to have a strong understanding of data science concepts. Data science concepts will be tested in a job interview to see if the candidate has the required knowledge. Some common questions that may be asked include: How would you explain hypothesis testing to someone with no prior knowledge? What are some of the most popular machine learning algorithms, and why do you think that is? By being able to answer these types of questions, it will show that the candidate has a strong foundation in data science concepts and will be able to hit the ground running in their new role.
However, as I alluded in the intro, at least for my team, the weight I put on this question wasn‚Äôt as high as others. Let me explain.
towardsdatascience.com
One of the most important communication skills during a job interview is effectively explaining your thought process and results. This is especially true for junior data scientist roles, where your communication skills will be tested to prove that you can work with frontline workers and team members. To effectively communicate your thought process and results, you need to be able to break down complex concepts into simple terms. Practice providing an example where you had to explain a complex concept or result to a non-data-savvy audience. The key word here is ‚Äúpractice.‚Äù
Practice providing an example where you had to explain a complex concept or result to a non-data-savvy audience. For example, if you were working on a project that involved predicting customer churn, you might have to explain the concept of churn to someone who doesn‚Äôt work in data science. To do this, you need to start by explaining what churn is and what it means for a business. You would also need to explain how your team could predict customer churn and the results of your analysis.
When asked why I want to work for a particular organization during a job interview, my honest answer is always that I am genuinely excited about the opportunity to work on data science projects that can have a real impact. In my previous role, I was part of a team that developed a data-driven approach to improving customer satisfaction scores. For example, you can say, ‚ÄúI was able to see firsthand how our work made a difference in the lives of our customers, and I am eager to continue making that kind of positive impact in my next role. Given the cutting-edge nature of your data science initiatives, I believe this organization is where I can continue to grow as a data scientist and make the most impactful contributions to exciting projects.‚Äù
Being able to work independently is an important skill for any data scientist, regardless of their level of experience. During a job interview, a potential employer may ask if you are able to work independently with little supervision. This question is designed to gauge your ability to take the initiative and complete tasks without a great deal of direction. The best way to answer this question is to provide a specific example of a time when you were able to work independently successfully. For instance, you might mention a project you completed entirely on your own or a task you took on outside of your normal job duties. By giving a concrete example, you can show that you have the independent problem-solving skills the employer is looking for.
An important part of working independently is being able to manage your own time effectively. Suppose you can demonstrate that you are capable of completing tasks without constant oversight. In that case, it will show the employer that you have the self-motivation and discipline necessary to be successful in a data science role.
towardsdatascience.com
It‚Äôs no secret that attitude is important in the workplace. Your attitude can be the difference between a successful career and a series of dead-end jobs. So, it‚Äôs not surprising that employers place a great deal of emphasis on attitude during job interviews. If you‚Äôre interviewing for a junior data scientist role, here are a few tips to ensure you have the right attitude.
First, be humble. It‚Äôs important to be confident in your abilities, but you don‚Äôt want to come across as arrogant. Remember, you‚Äôre applying for a junior role. The interviewer wants to see that you‚Äôre willing to learn and humble enough to take direction from others.
Second, be positive. A positive attitude is infectious and can go a long way in making a good impression during an interview. Be sure to smile and exude confidence, even if you‚Äôre feeling nervous on the inside.
Finally, be prepared to answer questions about your attitude. Many employers will ask specifically about your attitude and how it relates to the job you‚Äôre applying for. Be ready with examples of times when your positive attitude made a difference in your work.
While technical skills are important, soft skills are just as critical for data scientists. In order to be successful in the data science marketplace, you have to stand out from the rest by demonstrating that you can communicate effectively with other team members, work independently, and be motivated to learn new things. If you can demonstrate these qualities during your interview, you‚Äôll have a better chance of being offered the job. Lastly, to ace your next interview, remember to practice your answers because good communication skills are always a must for any role.
ilro.medium.com
Sumit Kumar Arora
Dario Radeƒçiƒá
in
Towards Data Science
Cate Lawrence
Ejiro Onose
Prashanth Rao
in
Towards Data Science
DNIF
Ted Benson
in
Instabase Blogs
Gautam Bose
in
Persuasion
"
84,https://medium.com/geekculture/five-courses-to-master-data-cleaning-feature-engineering-b3ac8f86800a,Five Courses to Master Data Cleaning & Feature Engineering,"Data cleaning and feature engineering are one of the most important parts of a data scientist‚Äôs day. It‚Äôs something you‚Äôll do on a daily basis. Being able to clean your data effectively and engineering the features effectively will result in better results with less effort. I believe that the more you know, the better you will understand the data, which will help you to produce better results and be effective at work. Although the importance‚Ä¶
Albert Vico
in
SEAT:CODE
Vignesh S
Stylumia
Luca Clissa
in
Towards Data Science
Lokesh Rathi
in
Analytics Vidhya
Naif Aldayel
Jose Antonio Ribeiro Neto (Zezinho)
in
BIG DATA FOR EVERYONE
Jaden
"
85,https://medium.com/towards-data-science/building-an-all-in-one-audio-analysis-toolkit-in-python-e849a0e2129e,Building an All-In-One Audio Analysis Toolkit in Python,"Language forms the basis of every conversation between humans. Due to this, the field of Natural Language Processing (or NLP for short) undoubtedly holds immense potential in assisting humans with their day-to-day lives.
In simple words, the domain of NLP comprises a set of techniques that aim to comprehend human language data and accomplish a downstream task.
NLP techniques encompass numerous areas such as Question Answering (QA), Named Entity Recognition (NER), Text Summarization, Natural Language Generation (NLG), and many more.
While most of the prior research and development in NLP has primarily focused on applying various techniques, specifically over ‚Äútextual‚Äù data, in recent times, the community has witnessed a tremendous adoption of speech-based interaction, veering machine learning engineers to experiment and innovate in the speech space as well.
Therefore, in this blog, I will demonstrate an all-encompassing audio analysis application in Streamlit that takes an audio file as input and:
To achieve this, we will use the AssemblyAI API to transcribe the audio file and Streamlit to build the web application in Python.
The image below depicts what this application will look like once it is ready.
Let‚Äôs begin üöÄ!
Before building the application, it will be better to highlight the workflow of our application and how it will function.
A high-level diagrammatic overview of the application is depicted in the diagram below:
The Streamlit web application will first take an audio file as input, as described above.
Next, we will upload it to AssemblyAI‚Äôs server to obtain a URL for the audio file. Once the URL is available, we shall create a POST request to the transcription endpoint of AssemblyAI and specify the downstream task we wish to perform on the input audio.
Lastly, we will create a GET request to retrieve the transcription results from AssemblyAI and display them on our streamlit application.
This section will highlight some prerequisites/dependencies for building the audio toolkit.
Building web applications in Streamlit requires installing the Streamlit python package locally.
To access the transcription services of AssemblyAI, you should obtain an API access token from their website. For this project, let‚Äôs define it as auth_key.
Lastly, we will import the python libraries that we will be required in this project.
With this, we are ready to build our audio analysis web application.
Next, let‚Äôs proceed with building the web application in Streamlit.
Our application, as discussed above, will comprise four steps. These are:
To achieve this, we shall define four different methods, each dedicated to one of the four objectives above.
However, before we proceed, we should declare the headers for our request and define the transcription endpoints of AssemblyAI.
The objective of this method is to accept the audio file obtained from the user and upload it to AssemblyAI to obtain a URL for the file.
Note that it is not necessary to upload the audio file to AssemblyAI as long as you can access it via a URL. Therefore, if the audio file is already accessible with a URL, you can skip implementing this method.
The implementation of upload_audio() method is shown below:
The function accepts the audio_file as an argument and creates a POST request at the upload_endpoint of AssemblyAI. We fetch the upload_url from the JSON response returned by AssemblyAI.
As the name suggests, this method will accept the URL of the audio file obtained from upload_audio() method above and send it for transcription to AssemblyAI.
In the JSON object above, we specify the URL of the audio and the downstream services we wish to invoke at AssemblyAI‚Äôs transcription endpoint.
For this project, these services include sentiment analysis, topic detection, summarization, entity recognition, and identifying all the speakers in the file.
After creating a POST request at the transcription_endpoint, we return the transcription_id returned by AssemblyAI, which we can later use to fetch the transcription results.
The penultimate step is to retrieve the transcription results from AssemblyAI. To achieve this, we must create a GET request this time and provide the unique identifier (transcription_id) received from AssemblyAI in the previous step.
The implementation is demonstrated below:
As the transcription time depends on the duration of the input audio file, we have defined a while loop to create repeated GET requests until the status of our request changes to completed or the transcription request indicates an error.
The transcription response received for a particular audio file is shown below:
The final method in this application is to print the results obtained from AssemblyAI on the Streamlit application.
To avoid clutter and textual chaos on the application's front-end, we shall encapsulate each of the services within a Sreamlit expander.
The keys from the transcription response that are pertinent to this project are:
As the final step in building our Streamlit application, we integrate the functions defined above in the main() method.
First, we create a file uploader for the user to upload the audio file.
Once the audio file is available, we send it to Method 1 (upload_audio), followed by transcribing the audio (transcribe) and retrieving the results (get_transcription_result), and we finally display the results (print_results) to the user on the Streamlit application.
Our audio analysis application is ready, and now it‚Äôs time to run it!
To do so, open a new terminal session. Next, navigate to your working directory and execute the following command after replacing file-name.py with the name of your python file:
The uploader above asks you to upload an audio file. Once you do that, the functions defined above will be executed sequentially to generate the final results.
The transcription results on the uploaded file are shown below:
In this section, we will discuss the results obtained from the transcription models of AssemblyAI.
A part of the transcription of the input audio is shown in the image below.
The broad topics discussed in the entire audio by the speaker(s) are shown in the image below.
To generate a summary, AssemblyAI‚Äôs transcription services first break the audio into different chapters and then summarizes each chapter individually.
The summary of the input audio file is shown below.
AssemblyAI classifies each sentence into three categories of sentiments ‚Äî Positive, Negative, and Neutral.
The sentiment of the first three sentences in the audio is shown below. They were precisely classified as Neutral by the transcription module.
Finally, the entities identified in the audio and their corresponding entity tags are shown below.
To conclude, in this post, we built a comprehensive audio application to analyze audio files using the AssemblyAI API and Streamlit.
Specifically, I demonstrated how to perform various downstream NLP tasks on the input audio, such as transcription, summarization, sentiment analysis, entity detection, and topic classification.
Thanks for reading!
üßë‚Äçüíª Become a Data Science PRO! Get the FREE Data Science Mastery Toolkit with 450+ Pandas, NumPy, and SQL questions.
‚úâÔ∏è Sign-up to my Email list to never miss another article on data science guides, tricks and tips, Machine Learning, SQL, Python, and more. Medium will deliver my next articles right to your inbox.
Jirong
in
DataDrivenInvestor
Graphicacy
in
Graphicacy
ODSC - Open Data Science
in
ODSCJournal
K3ll3nL
in
smucs
Palak Sharma
Matt
in
The Startup
Melanie kondo
in
lootnft
ODSC - Open Data Science
"
86,https://medium.com/towards-data-science/how-to-effectively-showcase-personal-projects-on-your-data-science-resume-736cbfa1600,How to Effectively Showcase Personal Projects on Your Data Science Resume,"As an up-and-coming data scientist, one of the most common things you‚Äôre told is that listing your personal projects on your resume is a great way to showcase your skills and experience.
However, I can‚Äôt be the only one who gets completely stumped when it comes time to write a description that truly showcases a project, not to mention the fact that trends dictate that most resumes should only be one page long.
This means that a formula has to be created that will keep project descriptions short, to the point, and most importantly, effective. With recruiters giving resumes only 7.4 seconds of their time, your project descriptions must catch their eye and make them call you for an interview.
Recruiters need to have a clear idea of the objective or goal of your project.
What were you trying to accomplish with your project? What was your project attempting to resolve? What sort of answers were you looking for? What had to be done within the project? Why were you attempting this project?
Now is the time to give a brief overview of your project, including a description of the project objective, timeline, and scope. Keeping in mind that your resume shouldn‚Äôt cover more than one page, this section should be succinct with a focus on clearly describing the items listed above in 1‚Äì2 short sentences.
Example: Conducted an analysis to determine air pollution levels in Beijing from July 1st to 15th and how they correlated with hospitalization rates during the same time frame.
The projects you list on your resume don‚Äôt necessarily have to be personal ones ‚Äî in fact, you could list any projects that you were a part of.
For example, you could list projects from when you were in college, from previous employment, or from competitions or hackathons.
The main goal is to give the recruiter an idea of what you did and how long it took you to do it. For a project description where you worked as part of a team, you‚Äôll want to go a step further and indicate your role within the project and your responsibilities. Additionally, for a group project description, now could be a time where you list the technologies or tools you used to fulfill your role within the team.
This section shouldn‚Äôt be any longer than 1 sentence.
Example for a personal project: Worked independently for 3 weeks to extract, clean, analyze, and visualize Beijing air pollution-hospitalization rate data.
Example for a group project: Worked 10 hours a week as a data analyst within a team of 4 to produce visualizations of Beijing air pollution-hospitalization rate data using Tableau.
Now is the time to give the recruiter an idea of the approximate data set size and skew. This could involve detailing the exact data set used and providing a link (if it‚Äôs available freely online) or describing how and where the data was obtained from.
This part of the description could also involve a discussion on the tools and techniques used to obtain, extract, and clean the data.
This section should be 1‚Äì2 sentences long.
Example: OpenRefine was used to clean data obtained from Open Dataset A (Beijing air pollution level data between July 1st and July 15th ‚Äî 350 values) and Open Dataset B (Beijing hospitalization rates between July 1st and July 15th ‚Äî 50,000 values).
Arguably the most important section in the project description is your discussion on the models and tools used throughout your project.
This is where you indicate its relevance to the job you‚Äôre applying for by focusing on listing the models and technologies used. By listing the technologies that appear in the job ad, you indicate to the recruiter that you‚Äôve successfully used them to complete an analysis.
This section should be no more than 1 sentence long.
Example: This project was completed using SQL, Python, Tableau, and a stepwise regression model.
Now is the time to provide a link to the code you‚Äôve written for your project.
The old adage always tells us to ‚Äúshow, don‚Äôt tell‚Äù. If recruiters are intrigued by what you‚Äôve told them about your project, they‚Äôll then want you to show them exactly what the bones of your project look like.
Therefore, this is the right time to provide a link to your Github or another code-sharing repository.
This section should only be 1 sentence long.
Example: Code for this project can be found here: link
The relevance of a data scientist is always determined by the impact they have on their company.
This means that you need to tell a recruiter how your project yielded vital results. The results of your analysis will prove that you can be given a problem to solve and come back with a solution.
Focus on what your analysis determined, the relevancy of your results, and if you feel so inclined, discuss how your results can be expanded upon in the future.
This section should be 1‚Äì2 sentences.
Example: Results from the analysis found that hospitalizations in Beijing spiked during periods of extreme air pollution, which indicates that hospitals in the city should be prepared for an influx of patients at this time. This study could be improved upon by also taking into account extreme temperatures that may also exacerbate pollution and health issues within the city.
Here is what our final project description would look like once you put together all of the individual parts:
Conducted an analysis to determine air pollution levels in Beijing from July 1st to 15th and how they correlated with hospitalization rates during the same time frame. Worked independently for 3 weeks to extract, clean, analyze, and visualize Beijing air pollution-hospitalization rate data. OpenRefine was used to clean data obtained from Open Dataset A (Beijing air pollution level data between July 1st and July 15th ‚Äî 350 values) and Open Dataset B (Beijing hospitalization rates between July 1st and July 15th ‚Äî 50,000 values). This project was completed using SQL, Python, Tableau, and a stepwise regression model. Results from the analysis found that hospitalizations in Beijing spiked during periods of extreme air pollution, which indicates that hospitals in the city should be prepared for an influx of patients at this time. This study could be improved upon by also taking into account extreme temperatures that may also exacerbate pollution and health issues within the city. Code for this project can be found here: link
Once you‚Äôve built this basic description, you can refine it to better relate to the job you‚Äôre applying for and trim the fat to produce a smaller description if necessary.
An alternative if you‚Äôre running out of space on your resume is to include just the objective, tools, and results in your description, along with a link to where the code repository can be found. Then, the README file in your repository can be used to include more details. This saves space on your resume and also gives you the chance to provide additional details without being restricted by space.
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Medford Xie
Josquin Debaz
ODSC - Open Data Science
Olagunju Abdul-Hammid
in
Towards Data Science
Nicholas Leong
in
Towards Data Science
ODSC - Open Data Science
in
ODSCJournal
ISU Corp
Norm
"
87,https://medium.com/towards-data-science/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd,"Linear Regression, Logistic Regression, and SVM in 10 Minutes","Machine learning modeling is a data scientist‚Äôs problem solver. Even though it doesn‚Äôt take the majority of our time, it‚Äôs personally much more fun than data cleaning.
To be frank, some models are mathematically involved. The good news is, as a data scientist, you aren‚Äôt necessarily capable to build machine learning models from scratch. There is already a plethora of libraries to choose a model from and there‚Äôs no need to reinvent the wheel. However, it‚Äôs always good to know how a model works from a bird‚Äôs-eye view.
In general, all supervised machine learning models work in the same way: you have input data X and output data y, then the model finds a map from X to y. That‚Äôs why machine learning is called that way: instead of us deliberately coding the logic that maps X to y, the model learns on its own by updating its parameters. Most models also have hyperparameters, i.e. parameters that can‚Äôt be learned and need to be tuned by the user.
To be clear, let‚Äôs say you have m observations and n features, and you‚Äôre working on a single-output task. Then, X is an m √ó n matrix and y is a vector of size m. To learn the mapping from X to y, the model will have to find the optimum parameters using an optimizer, such as gradient descent, BFGS, and many more. But what does it mean to be optimum? Optimum in terms of what, exactly?
You can see the ‚Äúfind optimum parameters‚Äù problem as an optimization problem (yes, machine learning really is just an optimization problem). So, you need an objective function, or in other literature is also called a cost function.
Cost functions are different depending on the model and task. For example, for linear regression with predicted parameter w, a familiar cost function would be the sum of squared error, denoted by
Optimum w is found when w minimizes this cost function.
In linear models and logistic regression below, we omit the bias coefficient b for convenience. The bias coefficient allows our models to be more general and there are two ways to add it:
In linear regression, the target value ≈∑ is expected to be a linear combination of the features. In other words,
As mentioned before, with this model you would minimize the sum of squared error with respect to w, that is,
Despite its simple form, linear regression has a problem. If some features in X are correlated, then (X·µÄX)‚Åª¬π will be close to singular and w becomes highly sensitive to errors, resulting in overfitting. Luckily, there is a workaround: by giving a penalty to w if it‚Äôs too big.
This gives birth to the ridge, lasso, and elastic-net regression.
To give a penalty to w, we need to add a quantity in terms of w to the cost function such that minimizing the cost function also minimizes this quantity. The quantity in question is the square of the Euclidean norm (also called L2-norm) of w multiplied by a hyperparameter Œ± ‚â• 0. The objective becomes
With this objective, the model is discouraged to be too complex and tends to shrink w toward zero. The larger the value of Œ±, the greater the amount of shrinkage, and thus w becomes more robust to collinearity.
There‚Äôs an alternative to ridge regression. Instead of shrinking all coefficients in w toward zero, your model can also learn to set some coefficients to zero. You‚Äôre left with a few non-zero coefficients, effectively reducing the number of dependent features.
The objective becomes surprisingly easy. We simply switch the square of the L2-norm in the cost function of ridge regression with the L1-norm as follows
With this capability, lasso regression is also useful for feature selection and reducing the dimensionality of X by selecting non-zero coefficients. The reduced data can then be used with another classifier/regressor. It‚Äôs easy to see that the higher the value of Œ±, the fewer features selected.
It‚Äôs also possible to gain the advantage of both ridge and lasso regression. Elastic-net has both L1 and L2-norm in the cost function, allowing for learning a sparse model where few of the coefficients are non-zero like lasso, while still maintaining the regularization properties of rigde. The objective is now
The hyperparameter 0 ‚â§ œÅ ‚â§ 1 is called L1-ratio which controls the combination of L1 and L2-norm. Note that if œÅ = 0, then the objective is equivalent to ridge‚Äôs. On the other hand, if œÅ = 1, then the objective is equivalent to lasso‚Äôs.
Below is an example of the application of linear models on the same data, where X has 4 features and bias b = 0. We can see that ridge shrinks the coefficients toward zero, lasso sets the first coefficient to zero, and elastic-net is like the combination of both.
The model has better performance (lower RMSE) moving from linear regression to elastic-net. Note that this is not always the case.
We‚Äôve talked about regression a lot, so let‚Äôs move on to classification. In a binary classification task, every element y·µ¢ of y is one of two classes, which can be encoded as -1 and 1. The objective is
where X·µ¢ is the i-th observation (row) of X in a vertical vector form. This task is also called logistic regression since we are using the logistic function (also called sigmoid function in some literature).
We will explain why this cost function makes sense. Fix an observation j. The way logistic regression works is first to define a decision boundary, in this case, 0. If X‚±º·µÄw ‚â• 0, then predict ≈∑‚±º = 1. Otherwise, predict ≈∑‚±º = -1. Now‚Ä¶
Hence, the model will prefer to satisfy X‚±º·µÄw ‚â• 0 which predicts ≈∑‚±º = 1 and fits with the observation y‚±º = 1.
Hence, the model will prefer to satisfy X‚±º·µÄw < 0 which predicts ≈∑‚±º = -1 and fits with the observation y‚±º = -1.
Logistic regression can also support L1, L2, or both regularization.
When we look at Logistic Regression, it‚Äôs able to draw a decision boundary w·µÄx + b = 0 for arbitrary x between two classes. Intuitively, a good separation is achieved by the decision boundary if it has the largest distance to the nearest training data points of any class, since in general the larger the margin the lower the generalization error of the classifier (especially in separable classes).
If you look at the literature, SVM is trying to maximize the margin by minimizing the length (L2-norm) of the parameter w. When I first saw this, I had no clue why. It turns out this fact holds due to many mathematical manipulations and reasoning which will be explored below.
So, SVM works just like logistic regression with a twist: sign(w·µÄX·µ¢ + b) is expected to be correct for most observations i, and
is as big as possible. This is called functional margin. Let
Now, for each i = 1, 2, ‚Ä¶, m, let Œ≥·µ¢ be the distance from each observation X·µ¢ to the decision boundary. Geometrically speaking, by simple algebra, we have
or in a more compact form,
This is called geometric margin. Let Œ≥ be the minimum of all distances,
then the objective of SVM is to find the maximum of Œ≥ with some constraints,
Since multiplying w and b by some factor doesn‚Äôt change the geometric margin, we can find w and b such that ‚Äñw‚Äñ = 1. So, without loss of generality, the objective becomes
By definition of functional and geometric margin, the objective becomes
Since multiplying w and b by some factor doesn‚Äôt change sign(w·µÄX·µ¢ + b) but changes the functional margin, we can find w and b such that the functional margin equals 1. So, without loss of generality, the objective becomes
Because ‚Äñw‚Äñ > 0, this is equivalent to
The constraint above is ideal, indicating a perfect prediction. But classes are not always perfectly separable with a decision boundary, so we allow some observations to be at a distance Œæ·µ¢ from their correct decision boundary. The hyperparameter C controls the strength of this penalty, and as a result, acts as an inverse regularization parameter. The final objective is
That‚Äôs it! SVM is trying to maximize the margin by minimizing the length of the parameter w.
SVM for regression can be adopted directly from the classification. Instead of wanting y·µ¢(w·µÄX·µ¢ + b) to be as big as possible, now we want |y·µ¢ ‚àí (w·µÄX·µ¢ + b)| to be as small as possible, i.e. the error Œµ to be as small as possible. Utilizing inverse regularization again, we have the following objective for regression
So far, we‚Äôve only used a linear combination of features in input data X in our way of predicting the output data y. For logistic regression and SVM, this leads to a linear decision boundary. Thus for input data that are not linearly separable, the model will perform poorly. But there‚Äôs a way to improve.
We can map input data into a higher-order feature space through a mapping œï : ‚Ñù‚Åø ‚Üí ‚Ñù·µñ. In this higher dimension, with the correct mapping, there‚Äôs a possibility that the data is separable. Then, we work with this mapped data through our model.
But we have another issue. SVM and many other machine learning models can be expressed in terms of dot products, and solving dot products in high dimension mapped data is expensive. Luckily, we have a trick up our sleeve: the kernel trick.
A kernel K is a similarity function. It‚Äôs defined by the dot product of our mapped data. So, if we have two observations x and y in ‚Ñù‚Åø,
With the kernel trick, we can calculate this equation without visiting the higher-order feature space ‚Ñù·µñ, heck we don‚Äôt even need to know the mapping œï. So, calculating dot products in our model is no longer expensive.
Some common kernels are:
A non-linear kernel allows the model to learn a more complex decision boundary. All parameters of a kernel are hyperparameters of the model. If you apply different kernels to the previous dataset we used for classification, you get the following decision boundaries:
SVM is effective in high-dimensional spaces and in cases where the number of features is greater than the number of observations. The availability of many different kernels to choose from (or make on our own) makes SVM versatile.
However, if the number of features is much greater than the number of observations, it‚Äôs crucial to avoid overfitting in choosing kernels and the regularization term.
There‚Äôs always this one question: ‚Äúshould I standardize/normalize my input data X before feeding it to the model?‚Äù. Ironically, the most satisfying answer is ‚Äúit depends‚Äù.
If your model uses a one-liner analytical solution, i.e. normal equation for linear regression, then there‚Äôs no need to standardize/normalize. You know why?
Suppose you have one feature that is in the order of thousands larger than every other feature and you don‚Äôt standardize/normalize it. Let‚Äôs say the normal equation set the coefficient for this particular feature to be Œ≤. If you standardize/normalize it first, the normal equation will produce a coefficient thousands of times of Œ≤. So, the scaling of the feature and the produced coefficient cancel out each other and the model gives the same predictions afterward.
On the other hand, if your model uses an iterative approach, i.e. gradient descent instead of normal equation, then you‚Äôd like to standardize/normalize your input data first for faster convergence.
Another concern is related to model extensions (non-linear kernel, regularization, etc). For instance, many elements used in the cost function of a learning algorithm (such as the RBF kernel of SVM or the L1 and L2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the cost function and make the estimator unable to learn from other features correctly as expected. Hence, you need to standardize.
You‚Äôve learned in great detail the three most basic machine learning models: linear regression, logistic regression, and SVM. Now, not only you can build it using established libraries, but you can also confidently know how they work from the inside out, the best practices to use them, and how to improve their performance.
Congrats!
Here are some key takeaways:
Hope you learn a thing or two :)
üî• Hi there! If you enjoy this story and want to support me as a writer, consider becoming a member. For only $5 a month, you‚Äôll get unlimited access to all stories on Medium. If you sign up using my link, I‚Äôll earn a small commission.
üîñ Want to know more about how classical machine learning models work and how they optimize their parameters? Or an example of MLOps megaprojects? What about cherry-picked top-notch articles of all time? Continue reading:
Albers Uzila
Albers Uzila
Albers Uzila
Albers Uzila
Albers Uzila
Gio at QRC
Juan Sokoloff
in
data.tale()
Peng Yan
in
Towards Data Science
Monodeep Mukherjee
Monodeep Mukherjee
Ednalyn C. De Dios
in
Towards Data Science
MC
in
Towards Data Science
Patrick Wales-Dinan
in
Towards Data Science
"
88,https://medium.com/towards-data-science/life-lessons-i-learned-from-working-as-a-data-analyst-fbafe6d02fb8,Life Lessons I Learned from Working as a Data Analyst,"Research shows that average professionals spend roughly 80,000 hours at work throughout their entire careers. From Monday to Friday, we spend (at least) around a quarter of our time at work. With this huge time spent, no wonder our work impacts our lives to a large extent. Personally, I feel my worldview and mindset have become more and more analytical.
This comes from the fact that I‚Äôve been working as a Data Analyst for more than 5 years. Over the years, I realize how applicable the Analytics principles and experiences are to our life.
Here are the life lessons I learned and applied in life from my Analytical work.
In many ways, Data Analytics is just like a Detective‚Äôs work. We‚Äôre given some crime scene/evidence pieces/actor‚Äôs testimony (or a pile of data in tables, for Data Analysts) and mysteries (business problems for Data Analysts) to solve.
How do we solve them?
Before using any domain and technical knowledge to solve the problem, you first follow your curiosity to explore the data. You start digging into the data and try to find some patterns, phenomena, or anomalies that lie beneath that dataset. In the middle of the exploration, you tinker with loads of questions: ‚ÄúIs there any better way to slice and dice this dataset?‚Äù, ‚ÄúCan these features be combined for a better model classification?‚Äù, ‚ÄúWhy are these customer segments more prone to the experiment?‚Äù. With these questions, you eventually come up with more hypotheses, experiments, analyses, or statistical tests to provide better insights.
Similarly in life, keeping a curious mind will let you be aware of your surroundings (and also your inner thoughts and feelings!) and eventually understand them. You‚Äôll discover more facts as you entertain searching for answers asked by the curious mind.
Life lesson learned: Curiosity drives you to ask more questions and explore more ways, which more likely leads to better answers and solutions.
In Analytics, we‚Äôre always looking for ways to slice and dice the data to uncover interesting insights. Sometimes you drill down and don‚Äôt yield any valuable insights, but when you change the point of view, the pattern starts to reveal itself.
There was this one time when I was exploring the impact of a marketing campaign on transactions. When I compared the week-on-week transactions, there was no significant increase in total transactions. This was surprising since the campaign‚Äôs reach was quite high, and in the past campaigns, we‚Äôve seen a significant increase. So I continued to dig down the data, trying to figure out what went different on this campaign. When I looked into the purchasing customers, turned out there was a significant portion of new customers from this campaign, and they were known to have a smaller basket size of purchase as they‚Äôre still getting used to the platform. So it was not a failed campaign (just because it‚Äôs not significantly increasing transactions). It is a successful one as we‚Äôre highly acquiring new customers with potentially high LTV through this campaign.
Life lesson learned: There are always multiple angles to a story. Just because it looks bad in our opinion now, doesn‚Äôt mean it is bad. It might be good from another person‚Äôs side, or there might be another underlying story that is still unknown to us.
If you work in a high-growth organization, you might already understand how dynamic an analytics role can be. With constant changes in the market, and eventually, in the product/data requirements, you‚Äôre left with no options but to adapt to these dynamics.
Analysis insights that are true now, can be obsolete within months or even weeks. It is not impossible that you will need to re-run some analysis after some time due to these dynamics. There might be newly added context, data points, or external market movement which changes user behavior and drives the analysis in a different direction. We just need to keep an open mind and explore these possibilities. It‚Äôs common to find the results of the latter analysis differ from the earlier analysis ‚Äî even when both analyses are trying to address the same problems.
Life lesson learned: Change is inevitable. Keep an open mind to the changes, learn new lessons, and adapt to grow. Keep your strong opinions loosely held, as what is true now, might not be true later due to these changes.
Data Analytics/Data Sciences is a rapidly growing field. A lot of research is being (and to be) done in this area. There are a lot of new and interesting methods, algorithms, technologies, tools, etc available ‚Äî which can be quite overwhelming to explore. But worry not, along the way I figured that most of the new findings are just an extension of the basics. And once we understand the foundational principles, it is a lot easier to catch on to the new technologies.
Moreover, lots of industrial problems (the projects we do at work) do not always require complicated or cutting-edge technological solutions. Most of the data modeling projects I did in my Analytics tenure are simple classification, regression, and clustering problems. These concepts, with the right context, application, and implementation, have been helping me in laying out the Analytical solutions for my organization thus far ‚Äî big and small. Well, cutting-edge technologies have also been a nice addition to those concepts, but those 3 concepts have been my Pareto principle at work ‚Äî they help me solve at least 80% of the problems.
Life lesson learned: When learning new things, take the time and effort to understand the basic concepts and foundations. The concepts, with the right practice, can be used to solve a lot of problems, and it will help you understand new concepts as well.
Data Analysis is a discovery work. Us Analysts get curious to explore data, slice and dice them in multiple ways in the pursuit of digging out valuable insights. There are times when we get lost in the journey because we can‚Äôt seem to find the ‚Äúextraordinary insights‚Äù we‚Äôre looking for to answer the business questions. This outcome-driven mindset sometimes leads to disappointment and an uneasy feeling of being unable to deliver what‚Äôs expected of us; forgetting about the journey itself.
I‚Äôve made peace with it and shared a piece on how common it is that we found no interesting or valuable insights in analysis, and it is okay. It is okay because ‚Äúless interesting insights‚Äù can still be valuable, as ‚Äúvalue‚Äù is a subjective matter. Though the analysis result is something already expected and not new, you still deliver value in confirming it or increasing the confidence level towards the hypothesis statement, as long as the analysis is done in the right process. Not to mention, there might be additional insights found along the analysis that might not be answering the main hypothesis, but might be useful for other use cases later.
Life lesson learned: Every step on the journey ‚Äî not only the end ‚Äî matters. The right process will most likely bring you a good result. But even if you think the result is not good for you now, it might still be good for others; or it can be valuable for the future.
When we take the time to reflect and be mindful of our work and actions, there can be lessons and pearls of wisdom we can gather and acquire. The 5 points above are the lessons I gathered as I make sense of Analytical work, to keep in mind and give additional perspectives on how data work can be applied to life.
Hammad-42
Monodeep Mukherjee
Dylan Andreo
Hunter Kempf
Nissi Ozigbu
Carlo Shaw
Lo√Øc Villepinte
Carrie Lo
in
Towards Data Science
"
89,https://medium.com/mlearning-ai/free-mit-course-to-learn-basics-of-statistics-for-data-science-ba3c3b024a62,Free MIT Course To Learn Statistics for Data Science,"Statistics lies at the heart of data science. Some might say data science is a fancy name for statistics.
Statistics play a crucial part in any data-related field from cleaning data to helping you optimize your model. It is very crucial to master the concepts of statistics to be a successful data scientist.
Hacker Noon Support
in
HackerNoon.com
Beewidjaja
BSC Cities
Albert Christopher
in
DataDrivenInvestor
Djietcheu bonaventure
Serena McDonnell
in
Towards Data Science
Nick Walsh
in
Insightful Software
Aryeh Gelfand
"
90,https://medium.com/pipeline-a-data-engineering-resource/data-science-grads-change-3-lines-in-your-linkedin-referral-requests-to-actually-get-a-response-2f256d0c7762,Change 3 Lines in Your Data Science LinkedIn Referral Requests to Actually Get a Response,"As you, a data science grad or entry-level candidate, apply for your first data‚Ä¶
Big Ideas HR Consulting Pvt. Ltd
Dan Blacharski
Kintone
in
Digital Transformation Talk
Corporate Wellness
Ilyssa Panitz
in
Authority Magazine
James Rosseau, Sr.
Stowe Boyd
in
GigaOm
Brian Chin
in
Stock Talk for Teens
"
91,https://medium.com/towards-data-science/logistic-regression-explained-in-7-minutes-f648bf44d53e,Logistic Regression Explained in 7 Minutes,"Logistic regression is one of the most commonly used predictive models by data scientists. In this article, I will explain some of the theory behind logistic regression so that you get an intuitive understanding of how it works.
Then, I will dive into the metrics used to evaluate logistic regression models and when you should use each of them.
Finally, I will show you how to build a logistic regression model in Python using Scikit-Learn.
In simple words, logistic regression is a model that predicts the probability of an event taking place. It is used to solve classification problems, which means that the dependent variable is always a class or category.
Here are some examples of classification problems that can be solved with the help of logistic regression:
Classification problems can be divided into two types: binary and multi-class classification.
Binary classification involves predicting one of two classes (e.g. cat vs dog), and multi-class classification refers to predicting one of many classes (e.g. cat vs dog vs bear).
Logistic regression can be modeled with an S-shaped curve as displayed in the example below:
The X-axis of this graph displays the number of years in the company, which is the dependent variable. The Y-axis tells us the probability that a person will get promoted, and these values range from 0 to 1.
A probability of 0 indicates that the person will not get promoted and 1 tells us that they will get promoted.
Logistic regression returns an outcome of 0 (Promoted = No) for probabilities less than 0.5. A prediction of 1 (Promoted = Yes) is returned for probabilities greater than or equal to 0.5:
You can see that as an employee spends more time working in the company, their chances of getting promoted increases.
Based on the concepts explained above, you might have the following question:
Linear regression is a technique that is commonly used to model problems with continuous output. Here is an example of how linear regression fits a straight line to model the observed data:
For classification problems, however, we cannot fit a straight line like this onto the data points at hand.
This is because the line of best fit in linear regression does not have an upper and lower bound, and the predictions can become negative or exceed one.
Here is an example of what it would look like if we used linear regression to model whether a person will get promoted:
Notice that in this example, linear regression still works for certain values.
For people with 10 years of work experience, we get a prediction of 1.
For 3 years of work experience, the model predicts a probability of 0.3, which will return an outcome of 0.
However, since there is no upper bound, observe how the predictions for employees with 12 years of work experience exceed 1. Similarly, for employees with less than 1 year of work experience, the predictions become negative.
Since we want to predict a binary outcome (Yes/No), the predictions need to range from 0 to 1. It isn‚Äôt possible to have negative predictions or predictions that exceed 1.
To ensure that the outcome is always a probability, logistic regression uses the sigmoid function to squash the output of linear regression between 0 and 1:
Here is the equation for linear regression, which is also the equation for a straight line:
And this is the equation for logistic regression, which simply squashes the output of linear regression between 0 and 1:
Now that you understand how logistic regression works, let‚Äôs look into the different metrics used to evaluate this type of model.
The most popular metric used to evaluate the performance of classification models is accuracy. However, accuracy isn‚Äôt always the most reliable indicator of a good model, which is why data scientists often use measures like precision, recall, and the F1-score instead.
Here is an example to help you understand the different classification metrics and when to use them:
The dataset above comprises information of whether a student passed an exam. A value of 1 indicates that they passed and 0 tells us that they did not.
The ‚ÄúActual‚Äù column comprises true data while the ‚ÄúPredicted‚Äù column contains the predictions of a logistic regression model.
The true data is imbalanced since 80% of students passed and 20% failed. This means that the dataset is skewed to represent more of one class than another.
Now, take a look at the ‚ÄúPredicted‚Äù column. Notice that the model has predicted that 100% of students passed. This model is only predicting the majority class, so it is a poor model.
Let us now calculate a few classification metrics on this dataset:
Accuracy is calculated using the following formula:
This model has an 80% accuracy. However, as we noted earlier, the model has only predicted one class, and has not even made one accurate positive prediction.
Although its accuracy is high, the model has performed poorly. Accuracy can be a misleading metric in this case, since it can leave data scientists with the assumption that their model is good even when that is not the case.
Let‚Äôs now look at a few other metrics that can help us overcome the drawbacks of accuracy.
Precision and recall are two metrics used to assess the quality of positive predictions made by the model.
Now, let‚Äôs look further into precision and recall and how to calculate them:
1. Precision
Precision is a measure that can tell us: out of all the positive predictions, how many students actually passed?
It is calculated using the following formula:
In this case, the classifier‚Äôs precision is 0/0 = 0.
2. Recall
A model‚Äôs recall can tell us: out of everyone who passed, how many of them were correctly identified?
The formula to calculate recall is:
In this case, the classifier‚Äôs recall is 0/2 = 0.
The F1-score combines the precision and recall value and gives us a single outcome.
In simple words, this metric tells us how well a model is performing by taking the average of its precision and recall.
The formula to calculate the F1-score is:
In this case, the classifier‚Äôs F1 score is 2 X 0= 0.
To summarize, this model‚Äôs accuracy is high but its precision, recall and F1-score are 0 because it did not make any positive predictions.
The main takeaway from this section is that while accuracy is often used to evaluate the performance of logistic regression models, it is important to calculate other measures as well to ensure that your model really is performing well.
I did not dive too deep into any of these metrics since the focus of this article is on logistic regression. If you want a deeper understanding of how precision and recall work, along with how to answer interview questions about them, read my tutorial on the topic.
You can run the following lines of code to build a logistic regression model using Scikit-Learn:
Thanks for reading! This post was originally published here.
Frederik Bussler
in
Analytics Vidhya
Shubhangi Hora
in
Towards Data Science
Benjamin Obi Tayo Ph.D.
in
Towards AI
Stephen Smyth
in
Coord
Benjamin Spilsbury
in
Towards Data Science
Joe T. Santhanavanich
in
The Startup
52¬∞North ‚Äî exploring horizons
in
52¬∞North Network
Daniel Angermann
"
92,https://medium.com/geekculture/7-best-github-repositories-to-learn-python-60c0de52d704,7 Best GitHub Repositories to Learn Python,"According to me, after documentation GitHub repositories are one of the best ways to learn to program. There are lots of resources where you can learn programming and it is easy to become lost in these piles of resources. Python is one of the most famous and used programming languages. Here I have compiled 7 repositories that will‚Ä¶
Trent Ho
riethmue
Sudev Suresh Sreedevi
Emmanuel Garsd
Keegan Fernandes
Simon Aronsson
in
Arctic Bit
Blake Rogers
in
Telegraph Hill Software
Paul Franzen
in
WeWork Technology
"
93,https://medium.com/towards-data-science/top-15-famous-data-science-quotes-f2e010b8d214,Top 15 Famous Data Science Quotes,"Over the last few years as I have read and researched the data science landscape, I have developed the habit of collecting, recording and interpreting data science-related quotations wherever I have come across them.
I have found these quotations to be sometimes enlightening, often humorous and always insightful, hence I have compiled a list of my very favourites below along with my interpretation of their meaning and what we can learn from them.
Before we dive into the quotations please consider ‚Ä¶
Joining Medium with my referral link (I will receive a proportion of the fees if you sign up using this link).
Subscribing to a free e-mail whenever I publish a new story.
Taking a quick look at my previous articles.
Downloading my free strategic data-driven decision making framework.
Visiting my data science website - The Data Blog.
1. ‚ÄúThings get done only if the data we gather can inform and inspire those in a position to make a difference.‚Äù
(Dr. Mike Schmoker, Author)
This is one of my favourite data science quotes ever. It refers to the ‚Äúlast mile‚Äù of data science and infers that we can have access to clever data scientists and good data but if we fail to influence key decision makers to act on the findings then it has all been for nought and our beloved models are destined to gather dust on the data-science shelf.
www.amazon.co.uk
2. ‚ÄúTorture the data, and it will confess to anything.‚Äù
(Ronald Coase, Economics, Nobel prize Laureate)
The first data science-related quotation I ever collected, it refers to the temptation to make the data fit the hypothesis, also know as a ‚Äúconfirmation‚Äù bias if we obsess with a data set for long enough.
The key learning is to develop a sense of when to accept that no further value can be realised by continued analysis of a particular data set and even that sometimes we may need to walk away.
3. ‚ÄúData is the new oil.‚Äù
(Clive Humby, Mathematician and Marketeer)
In 2006 Clive Humby created one of the lasting data science quotes by declaring that ‚ÄúData is the new oil‚Äù.
This comparison has been debated and challenged many times since but it still highlights that data has a significant and often untapped monetary value to firms and that the data has to be mined, extracted, refined and delivered to realise that value.
4. ‚ÄúAll models are wrong but some are useful.‚Äù
(George E. P. Box, British Statistician)
Data science models are simplified abstractions of the real world and as such they are all wrong in some measure because the real world is the one and only true representation, so why bother at all?
Well, we build a data model to develop descriptive and predictive insight to drive improvements in decision making and any model that is just accurate enough can produce that insight and the associated competitive advantage even if it is bound to fall short of the real world that it represents.
5. ‚ÄúIf we have data, let‚Äôs look at data. If all we have are opinions, let‚Äôs go with mine.‚Äù
(Jim L. Barksdale, American Executive)
Jim Barksdale is encouraging his teams to come to his door with data that can be analysed in the decision making process with the inherent threat that if no data is available Jim will implement his own view, so you had better bring data to the next meeting!
6. ‚ÄúIntuition is thinking that you know without knowing why you do.‚Äù
(Daniel Kahneman, Israeli-American Psychologist and Nobel Prize Winner)
Pure intuition, according to the writings of Daniel Kahneman, is a fallacy and by inference decision makers who rely solely on intuition without data and evidence will make decisions that are either wrong or just lucky.
www.amazon.co.uk
7. ‚ÄúBig data is like teenage sex; everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it.‚Äù
(Dan Ariely, American Professor)
The source of this flippant, humorous but oh-so insightful quote is a famous tweet from Dan Ariely.
It is referring to the tendency for many data science professionals, managers and leaders to boast about their firms leveraging of big data without really understanding what it is, let alone having developed a mature big data capability that is delivering those all-important and tangible impact and outcomes.
8. ‚ÄúIn God we trust. All others bring data.‚Äù
(Barry Beracha, CEO of Sara Lee Bakery Group)
This is another great data science-related quotation from a successful senior executive.
My interpretation is that Barry would only trust God to make decisions without having data to back it up and if anyone else is coming to his office door they had better bring some data to support their views and opinions!
9. ‚ÄúErrors using inadequate data are much less than those using no data at all.‚Äù
(Charles Babbage, English Mathematician)
Have you ever been in one of those meetings where the data team has worked hard to produce the required information only to be shot down by other attendees opining that the data cannot be trusted to inform the decision making?
Well, I keep this famous quote in my back pocket for just such an eventuality.
Self-evidently we want our data to be as accurate, timely and consistent as possible to enable and support effective decision making but even where there are some inadequacies it is still much better to use data than to resort to uninformed intuition, anecdote and personal opinion.
10. ‚ÄúGentlemen, you need to put the armour plate where the bullet holes aren‚Äôt because that‚Äôs where the holes were on the planes that didn‚Äôt return.‚Äù
(Abraham Wald, Hungarian Mathematician)
Abraham Wald famously reduced the losses of WWII Allied aircraft by recommending to senior officers that they add armour plating to the places on the planes that returned to base where there were no bullet holes rather than the places that were riddled.
His thinking was that the planes that made it back were bad indicators of where to add the armour. It was the ones that did not get home that were shot elsewhere like in the engines and nose-cone.
In data science this refers to the ‚Äúavailability bias‚Äù where there is a tendency to develop projects using just data that is readily and easily available rather than data that is critical to the analysis, but much more difficult to acquire.
11. ‚ÄúIt is a capital mistake to theorize before one has data.‚Äù
(Sherlock Holmes, Detective)
We may think of Sherlock Holmes primarily as a detective but in this quote the great detective is supporting the view of some of our more recent, corporate contributors that data is a critical pre-cursor to forming and evaluating our hypotheses.
12. ‚ÄúNo great marketing decisions have ever been made on qualitative data.‚Äù
(John Sculley, American Businessman)
We have established that data detractors often dispute data accuracy but another group that can often decry the criticality of data in decision making are creatives.
Here John Sculley is challenging that with a counter view stating that even though we associate marketing with qualitative data, no great marketing decisions have ever been made unless there is quantitative data.
And in the modern, machine learning world we can even use models to develop qualitative data into quantitative information using sentiment analysis and other natural language processing techniques so there really is no excuse!
13. ‚ÄúYou can have data without information, but you cannot have information without data.‚Äù
(Daniel Keys Moran, American Fiction Writer)
Data can be analysed, synthesised and developed into information, and again into intelligence and finally into knowledge as firms obtain higher data maturity, however this journey is unidirectional.
Daniel Keys Moran‚Äôs quote highlights two key lessons. Firstly, if we fail to analyse, clean and refine raw data we can never get structured, meaningful, insightful information and also we can never reach the higher levels of data maturity without having the raw data in the first place.
14. ‚ÄúData beats emotions.‚Äù
(Sean Rad, Co-founder of Tinder)
The dating app tinder is all about the emotional business of forming friendships and romantic relationships but here Sean Rad is indicating in just 3 words the criticality of data as an under-pinning, foundational component of building that capability in the digital and real worlds.
15. ‚ÄúIn the labour market of the future, any decision made without data is just a shot in the dark‚Äù
(Duncan Brown, Economist and Andy Durman, Managing Director)
I will finish my famous quotations with this one that is alluding to the historical tendency to recruit based on feeling giving way to an envisaged future where recruitment without data will be little better than guessing.
Of course, it has an important overlap with the moral and ethical uses of data.
We should not and cannot make people-centric decisions based on data in the public space (on social media etc.) unless that data has been rigorously and robustly checked and assured but data will certainly play an increasing role in augmenting recruitment, staffing and people decisions in future.
Thank-you for reading! I hope you have enjoyed this canter through my favourite data science-related quotations, that you found them interesting, insightful and humorous and that you will consider using them to shape your data science journey and to inform and engage your peers, your firms and your network.
If you enjoyed this article please consider ‚Ä¶
Joining Medium with my referral link (I will receive a proportion of the fees if you sign up using this link).
Subscribing to a free e-mail whenever I publish a new story.
Taking a quick look at my previous articles.
Downloading my free strategic data-driven decision making framework.
Visiting my data science website ‚Äî The Data Blog.
Automation Usecase
Ashwin Prasad
in
Analytics Vidhya
Minna Wang
in
Visual Analytics Field Notes
Ahmad Rizqullah
Michael Bogan
in
Towards Data Science
Kent Duong
Julia Dixon
Guilherme M. Iablonovski
in
Towards Data Science
"
94,https://medium.com/towards-data-science/what-is-machine-learning-716dfa72f2bc,What Is Machine Learning?,"Machine learning comes from the idea that machines can learn to program themselves instead of having to be manually programmed. This is because originally, in order to go through routes of logic to find answers, programmers had to manually create if, elif and else statements for the computer to follow to reach the optimal answer (see the amazing book if/then by Jill Lepore). This was a heavy burden on programmers especially when the logic was very complex. Such a burden then led to the idea that computers could be able to learn this logic for themselves based on existing data. In doing so the computers would essentially assign weights, values or statements to the logic flows to get similar or better answers than if they were manually programmed. This would both reduce the burden on the programmer while allowing for more complex and accurate programs than any human could develop.
Since the original idea of machine learning, the field has developed considerably to encompass a wide variety of different techniques and algorithms. This includes models such as Kmeans clustering to identify groups of data points, random forest regression to predict individual values, and support vector machines to predict which groups data points belong to. While these models have developed over time, arguably the key development has been the democratisation of machine learning through the development of a broad number of open source packages and the wide availability of open source datasets. This means that anyone with a computer (arguably a computer isn‚Äôt even needed anymore) can implement a machine learning solution on a dataset. The wide variety of techniques and ease of access can make navigating the world of machine learning quite difficult sometimes though. This article aims to provide a high-level to aid in navigating this broad and continually developing community.
The first place to start is the idea that machine learning can be split into two main lines of work: supervised learning and unsupervised learning. In the former, the computer is passed already defined target points, or outcomes, that the model can use to learn against, while in the latter there is no defined target and the final result can be open to interpretation. This can be split further as supervised learning encompasses regression problems, where the target variable is continuous, and classification problems, where the target variable is categorical. Then, under unsupervised learning, we have clustering where the aim is to find patterns and/or groups from the data without an already predefined target. These groups can be defined as follows:
Regression is the most common machine learning task that is performed. The aim of this group of models is to be able to predict and/or model the precise value of a target variable. Examples of this include predicting house prices based on factors such as the number of bedrooms, location, number of toilets and size, or modeling the height of individuals based on economic, social and environmental factors. The key thing for this is that the target variable is continuous and we are aiming to model the precise value with the specified inputs.
One of the first methods encountered under this umbrella is that of linear or multi-linear regression. While this method falls under the domain of traditional statistics, indeed it has been around for a while, it could also fall under the category of a machine learning model as the weights can be ‚Äúlearned‚Äù by the computer. Here the model will try to learn the optimal weights to assign to the input variables to maximise or minimise a metric related to the output. In the case of linear regression, this is reducing the distance between the actual and predicted value using least squares distance (sum of squares for all values). From this, we can extract the strength and direction of the relationship between the input and target variables and use this to try to predict unseen values.
What this means is that we will often have a series of numerical input values (independent variables) that are used to model the target values (dependent variable). This will often look like:
Where the left-hand side shows the data inputs and the right-hand side shows what the model is expected to look like.
Beyond linear regression there are many different types of regression in machine learning including:
Each of these varies in regards to their complexity, implementation, and best use case. For example, if you expect the relationship to be linear, you should probably use a family of linear regression models such as linear regression, ridge regression, or Lasso regression. If you expect the relationship to be non-linear, then Decision Tree Regression, Random Forest Regression, or Neural networks may be preferred.
The model you choose will also depend on what your objective is and what resources you have available. In most cases, it is often best to try a combination of models to see which fits best and to start with the simplest model you can, which is usually linear regression.
Classification is another supervised machine learning task that is commonly used in machine learning workflows. The aim of this type of algorithm is to be able to determine which group or class a data point may belong to rather than an exact value. This means that we use categorical variables as our target variable rather than continuous ones. Examples of this technique in use include modeling whether a patient has diabetes, whether a picture contains a dog or a cat or whether a user will re-subscribe to the platform or not in the future.
A common method encountered under this umbrella is that of Support Vector Machines. This is because, at least with only two dependent variables, it can be easy to visualise and understand. The way in which this works is by finding a boundary between data points that act as a cut-off between the different groups. This boundary can then be used for predicting which group new data belongs to by finding out which side of the boundary the data point sits on. The boundary can take many forms such as linear, non-linear or defined by the user but can be easy to see, implement and understand. An example of the boundary created by the model can be seen below.
What this means is that for classification we will often have a set of inputs, whether they are numerical or categorical, that are used to predict final categories. This means that data inputs into these models will often take the form:
Where the left-hand side shows the data inputs and the right-hand side shows what the model may potentially look like.
Standard algorithms that fall under this umbrella include:
Each of these methods vary in their complexity and implementation meaning that some algorithms will suit different problems. For example, where a linear decision boundary could be best, then support vector machines with a linear boundary may be advisable. However when the relationship may be more complex and less linear, then Random Forest Classifiers may be preferred for this implementation.
The model you choose will also depend on your objectives and resources available. As with regression, in most cases if you can it is probably better to trial a combination of methods so as to get the best result and understand why different algorithms may perform better or worse.
In contrast to the two groups of machine learning above, clustering algorithms are unsupervised machine learning algorithms. This means that they do not have a predefined target to aim for. Instead, the aim of these groups of algorithms is to be able to identify groups from data based on similar characteristics. This is commonly used when we want to identify a set of groups with different behaviors. Examples of this include identifying groups of shoppers such as single individuals, young families or couples, identifying different groups of shows that users commonly watch together, or grouping together music tastes. Once these groups have been identified this can tell us more about behavior and it can lead to targeted inventions. This can include giving out coupons to groups to nudge them into new behaviors or reinforce existing ones or being able to offer suggestions to viewers in terms of movies or tv shows.
A common algorithm within this domain is that of K-means clustering. This algorithm is able to define different groups/clusters by grouping data points by their distance to each other. For this, the Data Scientist has to first define the target number of groups to identify. The algorithm then sets random points in the data and seeks to find the optimal grouping of points within each group by their distance to each other relative to other groups by finding a centroid of that group. Since the Data Scientist often does not know the optimal number of groups beforehand, and there are often no predefined targets, the results can often be open to interpretation. There are some methods for determining this optimal number of clusters, but the results of these can vary.
This means that a series of numerical inputs are fed into the clustering algorithm with no predefined target variable. This can often look like:
Where the left-hand side shows the data inputs and the right-hand side shows what the model may potentially look like.
Common algorithms within this domain include:
As with both regression and classification algorithms, each of these methods vary in their complexity and implementation meaning that some algorithms will suit different problems more than others. Since clustering results themselves can be highly subjective and open to interpretation, it is often best to try multiple different clustering methods to see which results make the most sense. This should mean that the groups are well balanced, clearly defined by their mean or median and easy to label or interpret for non-specialists.
Machine learning is the process whereby instead of Data Scientists defining the rules of the data, the Machine learns these rules for itself. This can be split into both supervised and unsupervised learning, where in the former there are defined targets that the algorithms are working towards, but in the latter there are none. Both regression and classification fall under the banner of supervised machine learning algorithms while clustering falls under the banner of unsupervised machine learning. When implementing any of these algorithms, if you have the time and the resources, it is often best to try to implement more than one algorithm while trying to keep it is simple as possible. This will ensure that you are double checking your results and that you use the least amount of resources as possible for efficient learning.
While the idea of machine learning has been around for a long time, and even the implementation of some machine learning algorithms has also been around for a while, widespread use of machine learning is only beginning in earnest. This has been facilitated in recent years by the development of deep and wide open source Data Science ecosystems across multiple languages that has enabled any developer with a laptop (in some cases you don‚Äôt even need that anymore) to get started with implementing their own machine learning algorithms. In Python this includes the development and integration of a variety of libraries such as pandas, matplotlib, sklearn, statsmodels, tensorflow, keras among others, alongside open sources of data such as Kaggle, Google Cloud Public Data Sets, Data.gov and others.
This means that Data Science practices are still developing although a solid foundation has already been built. There are many opportunities to contribute to this continually growing ecosystem in a variety of different ways and we continue to see this everyday. With this in mind, I am very much looking forward to what comes in the future!
If you want to look at practical examples of any of the covered topics above, then feel free to look at my article on a complete Data Science curriculum for beginners.
towardsdatascience.com
If you liked what you read and are not yet a medium member, feel free to sign up to Medium using my referral link below to support me and other amazing writers on this platform! Thank you in advance.
philip-wilkinson.medium.com
Or feel free to check out some of my other articles on Medium:
towardsdatascience.com
python.plainenglish.io
Prince Tiwari
Hubert Ruczynski
in
ResponsibleML
D
Philip Wilkinson
in
Towards Data Science
Mohan Rai
in
Nerd For Tech
Risto Hinno
in
Towards Data Science
Kanan Vyas
in
Coinmonks
D
"
95,https://medium.com/towards-data-science/between-r-and-python-which-language-would-i-suggest-to-an-aspiring-data-analytics-expert-469b5cf391eb,"Between R and Python, Which Language Would I Suggest to an Aspiring Data Analytics Expert?","Until now I have always avoided entering the tussle of those interested in the R vs Python battle. But recently, my peer data geek Brian Julius asked my opinion on the Python vs R debate in his LinkedIn post here:
www.linkedin.com
I tried to answer in a comment of the post, but the text was too long üôÇ So, I decided to write a post instead.
In recent years I have been able to observe the evolution of the use of both of these languages in the world of Data Analytics. So, I have come to the following personal conclusions:
‚úî The R language is much more straightforward for interactive data analysis and data exploration, especially for analysts or those who come from the world of business intelligence, where SQL holds sway. Transforming the data with R is very reminiscent of the mental process done by those using SQL, with the advantage of being able to use specific functions that simplify complex transformations (such as data pivoting, for example), or that apply statistical operations useful for analysis.The approach taken by Python to transform data is more related to a programmer‚Äôs experience. Having to necessarily resort to lambda expressions, for example, for rather basic data manipulation tasks, disorients any analyst used to a more set-based approach (which is the right way to think when working with data!) and makes us realize that those who developed the Python packages needed for data wrangling were primarily developers rather than analysts.
‚úî R is the language par excellence used in academia (Statistics, Mathematics, Data Science, and so on). It is therefore very likely to find new data science algorithms implemented directly in R, even before finding them implemented in Python. Therefore, if you need to use these new algorithms for a project, you must necessarily use R.
‚úî As far as data visualization is concerned, R is the main tool that makes you able to produce beautiful graphics ready for professional publications. Certainly, graphs of this type are also obtainable in Python, but not as easily as you can make them in R, especially with the myriad of packages developed to add specific graphical needs.
‚úî As for professional dashboarding, the most widely used platforms have evolved and admit the use of both languages. For example, both Plotly Dash and Shiny allow the development of enterprise-level data apps in Python and R. For those connected to the world of Microsoft‚Äôs data platform, Power BI also allows you to use both languages.
‚úî Python is a very clear, general purpose programming language, very versatile due to the ecosystem of packages developed by the community, and predominantly used by programming students and developers. For this reason, it is easier to find a programmer who knows Python who also wants to go into data analytics. That is why there are so many packages dedicated to data transformation and data analysis now available in Python. As a result, the market for technologies related to Data Analysis has veered conspicuously toward Python adoption.
‚úî On the whole data engineering side, the clear winner is certainly Python. First of all, Python is one of the few languages supported by all the APIs that allow interacting with the data services from all major cloud providers (Azure, AWS, Google). Moreover, the need to integrate the use of heterogeneous data solutions into a production architecture on the cloud pushes towards the use of Docker containers, which make life easier for those who have to engineer all this. One of the languages used to interact with Docker is Python (R is not among them).
‚úî Due to the fact that all the data engineering part is handled in Python, it is often convenient to develop machine learning models always using Python (you can develop them just as well in R). In this way, they are easily released into production via specific pipelines developed in Python. It‚Äôs true that it is possible to integrate specific steps into Python pipelines that invoke R scripts, but often having everything in Python is more convenient and maintainable by a group of data engineers skilled in Python.
‚úî Big data platforms, now mostly based on Spark, allow data transformation or machine learning pipelines to be developed in PySpark, a Python API specifically designed for Spark. It‚Äôs also possible to interact with Spark through the SparkR language (an R API for Spark). The problem is that SparkR isn‚Äôt always implemented in these platforms out-of-the-box. What‚Äôs more, SparkR doesn‚Äôt implement the integration of dplyr (the main package used for data transformation in R) with Spark, which is done by sparklyr instead. But the aforementioned data platforms often don‚Äôt support sparklyr natively. So, PySpark, along with SparkSQL (the SQL API for Spark), is widely used as the ultimate ‚Äúbig data language‚Äù.
‚úî Python is the necessary choice when you need to tackle projects that require Deep Learning solutions (computer vision, automatic spoken language recognition, natural language processing, audio recognition using TensorFlow, Keras and PyTorch frameworks). Implicitly, we are stating that the whole world of Artificial Intelligence is heavily based on Python.
That said, I try to answer the question asked in the title of the article.
Nowadays, Python allows you to deal with pure Data Analysis and Data Science topics as well as Data Engineering and Artificial Intelligence thanks to the vast ecosystem of packages developed by the community. It‚Äôs true that specific problems related more to Statistics and Data Visualization are more easily solved using the R language, but these are still specific cases that make up a very small percentage of the cases for which Python is mostly required.
Therefore, although I myself am more fluent in R than Python,
I highly recommend that those who want to tackle Data Analytics topics devote themselves primarily to learning Python and how to best use the most important packages for data transformation and machine learning (pandas, scikit-learn, etc.).
The versatility of Python allows you to possibly switch from one job role to another (e.g., from Data Scientist to Data Engineer, or vice versa), should your interests change during your career.
This doesn‚Äôt take away from the fact that, once you have delved into Python, studying to have a basic understanding of R and its packages of the Tidyverse ecosystem is definitely a plus that will prove very useful in so many cases. You should not underestimate that the R community is large and that many solutions have been developed in R. Adding the knowledge of this language in your arsenal is therefore a winning choice. Also because, contrary to what some may think, Python and R are no longer two compartmentalized worlds. There are bilingual IDEs that allow you to develop using both languages. One example is RStudio, as shown in this article:
www.rstudio.com
So, if you want, you can make the most of both languages by developing a single solution. You can enjoy both languages! üôÇ
Nathaniel Selevan
finnstats
Narala Naveen Reddy
Nicole Mitchell
in
Square Corner Blog
Gereltuya
in
Graphitics
Ayan Kundu
in
Towards Data Science
Bolanle Rhodes
Marinaprikhod
"
96,https://medium.com/towards-data-science/why-is-mse-bias%C2%B2-variance-dbdeda6f0e70,Why is MSE = Bias¬≤ + Variance?,"‚ÄúThe bias-variance tradeoff‚Äù is a popular concept you‚Äôll encounter in the context of ML/AI. In building up to making it intuitive, I figured I‚Äôd give the formula-lovers among you a chatty explanation of where this key equation comes from:
MSE = Bias¬≤ + Variance
Well, this article isn‚Äôt only about proving this formula ‚Äî that‚Äôs just a mean (heh) to an end. I‚Äôm using it as an excuse to give you a behind-the-scenes look into how and why statisticians manipulate some core building blocks and how we think about what makes some estimators better than others, but be warned: it‚Äôs about to get technical around here.
Forays into formulas and generalized nitty gritty are out of character for my blog, so many readers might like to take this opportunity to rush for the exit. If the idea of a proof fills you with existential dread, here‚Äôs a fun article for you to enjoy instead. Never fear, you‚Äôll still be able to follow the upcoming bias-variance tradeoff article, but you‚Äôll have to take it on faith that this formula is accurate. This article is for those who demand proof! (And a discussion about festooned Greek letters.)
Still here? Nice. This stuff will go down smoother if you‚Äôre somewhat familiar with a few core concepts, so here‚Äôs a quick checklist:
Bias; Distribution; Estimand; Estimate; Estimator; Expected value E(X); Loss function; Mean; Model; Observation; Parameter; Population; Probability; Random variable; Sample; Statistic; Variance V(X)
If you‚Äôre missing a concept, I‚Äôve got you covered in my statistical glossary.
To make sure you‚Äôre comfy with manipulating the building blocks for our discussion, let‚Äôs grab an excerpt out of my field guide to a distribution‚Äôs parameters:
An expected value, written as E(X) or E(X = x), is the theoretical probability-weighted mean (this word is pronounced ‚Äúaverage‚Äù) of the random variable X.
You find it by weighting (multiplying) each potential value x that X can take by its corresponding probability P(X = x) and then combining them (with an integral ‚à´ for continuous variables like height or a sum for discrete variables like height-rounded-to-the-nearest-inch): E(X) = ‚àë x P(X = x)
If we‚Äôre dealing with a fair six-sided die, X can take each value in {1, 2, 3, 4, 5, 6} with equal probability 1/6, so:
E(X) = (1)(1/6) + (2)(1/6) + (3)(1/6) + (4)(1/6) + (5)(1/6) + (6)(1/6) = 3.5
In other words, 3.5 is the probability-weighted average for X and nobody cares that 3.5 isn‚Äôt even an allowable outcome of the die roll.
Replacing X with (X - E(X))¬≤ in the E(X) formula above gives you the variance of a distribution. Let me empower you to calculate it whenever the urge strikes you:
V(X) = E[(X - E(X))¬≤] = ‚àë[x - E(X)]¬≤ P(X = x)
That‚Äôs a definition, so there‚Äôs no proof for this part. Let‚Äôs take it for a spin to get the variance for a fair die: V(X) = ‚àë[x - E(X)]¬≤ P(X=x) = ‚àë(x - 3.5)¬≤ P(X=x) = (1‚Äì3.5)¬≤ (1/6) + (2‚Äì3.5)¬≤ (1/6) + (3‚Äì3.5)¬≤ (1/6) + (4‚Äì3.5)¬≤ (1/6) + (5‚Äì3.5)¬≤ (1/6) + (6‚Äì3.5)¬≤ (1/6) = 2.916666‚Ä¶
If you‚Äôre dealing with continuous data, you‚Äôll use an integral instead of a sum, but it‚Äôs the same idea.
In our proof below, we‚Äôre going to use a little switcheroo with that variance formula, replacing the middle bit with the rightmost bit:
V(X) = E[(X - E(X))¬≤] = E[(X )¬≤] - [E(X)]¬≤
I owe you an explanation of where it comes from, so let‚Äôs cover that quickly:
V(X) = E[(X - E(X))¬≤] = E[X¬≤ - 2 X E(X) + E(X)¬≤] = E(X¬≤) - 2 E(X) E(X) + [E(X)]¬≤= E[(X )¬≤] - [E(X)]¬≤
How and why did this happen? The key bit is going from line 2 to line 3‚Ä¶ the reason we can do this with the brackets is that expected values are sums/integrals, so whatever we‚Äôre allowed to do with constants and brackets for sums and integrals we‚Äôre also allowed to do with expected values. That‚Äôs why if a and b are constants, then E[aX + b] = aE(X) + b. Oh, and E(X) itself is also a constant ‚Äî it‚Äôs not random after it‚Äôs calculated ‚Äî so E(E(X)) = E(X). Glad that‚Äôs sorted.
Estimands (the things you want to estimate) are often indicated with unadorned Greek letters, most often Œ∏. (This is the letter ‚Äútheta‚Äù which we‚Äôd have in English if we felt that ‚Äúth‚Äù deserved its own letter; ‚Äúth‚Äù is close enough to ‚Äúpffft‚Äù to make Œ∏ a truly excellent choice for the standard placeholder in statistics.)
Estimands Œ∏ are parameters, so they‚Äôre (unknown) constants: E(Œ∏) = Œ∏ and V(Œ∏) = 0.
Estimators (the formulas you‚Äôre using in order to estimate the estimand) are often indicated by putting bling on Greek letters, such as a little hat on Œ∏, like so:
Since it‚Äôs a pain to get this blog post to render a Œ∏ with a hat nicely in a Medium post, I‚Äôll ask you to use your imagination and see this neat little guy whenever I type ‚ÄúŒ∏hat‚Äù. Also, you‚Äôre going through this with pen-and-paper anyways ‚Äî you‚Äôre not trying to study formulas just by reading, like some kind of maniac, right? ‚Äî so you won‚Äôt get confused by my notation. You‚Äôll copy down the formulas formatted with the pretty hat above and then read your own notes, glancing at my chatty explanations to help if you get lost.
Estimators are random variables until you plug your data in to get an estimate (‚Äúbest guess‚Äù). An estimate is a constant, so you‚Äôll treat it as a plain ol‚Äô number. Again, so we don‚Äôt get confused:
Now in order to know if our estimator Œ∏hat is dumb as bricks, we‚Äôre going to want to check if we can expect it to be close to the estimand Œ∏. So E() of the random variable X = (Œ∏hat - Œ∏) is the first one we‚Äôll be playing with.
E(X) = E((Œ∏hat - Œ∏)) = E(Œ∏hat ) - E(Œ∏) = E(Œ∏hat) - E(Œ∏) = E(Œ∏hat) - Œ∏
This quantity has a special name in statistics: bias.
An unbiased estimator is one where E(Œ∏hat) = Œ∏, which is an excellent property. It means we can expect our estimator to be on the money (on average). In my gentle intro blog post, I explained that bias refers to ‚Äúresults that are systematically off the mark.‚Äù I should more properly have said that bias is the expected distance between the results our estimator (Œ∏hat) gives us and the thing we‚Äôre aiming at (Œ∏), in other words:
Bias = E(Œ∏hat) - Œ∏
If you like unbiased estimators, then you‚Äôll love you some UMVUEs. This acronym stands for uniformly minimum-variance unbiased estimator and what it refers to is a criterion for a best choice among unbiased estimators: if they‚Äôre all unbiased, pick the one with the lowest variance! (And now I‚Äôve brought you to approximately chapter 7 of a master‚Äôs level statistical inference textbook. You‚Äôre welcome.)
The fancy term for ‚Äúyou offered me two estimators with the same bias, so I chose the one with the smaller variance, duh‚Äù is efficiency.
Of course, there are many different ways to pick a ‚Äúbest‚Äù estimator. Nice properties to look for include unbiasedness, relative efficiency, consistency, asymptotic unbiasedness, and asymptotic efficiency. The first two are small sample properties and the last three are large sample properties since they deal with how the estimator behaves as you increase the sample size. An estimator is consistent if it‚Äôs eventually on target as the sample size grows. (That‚Äôs right, it‚Äôs time for limits! Read this if your time -> infinity.)
Efficiency is a pretty solid property to care about, since no one wants their estimator to be all over the place. (Gross.) Since efficiency is about variance, let‚Äôs try plugging X = (Œ∏hat ‚Äî Œ∏) into our variance formula:
Variance V(X) = E[(X)¬≤] - [E(X)]¬≤becomes V(Œ∏hat -Œ∏) = E[(Œ∏hat - Œ∏)¬≤] - [E(Œ∏hat - Œ∏)]¬≤
Variance measures the spread of a random variable, so subtracting a constant (you can treat the parameter Œ∏ as a constant) merely shifts everything over without changing spread, V(Œ∏hat - Œ∏) = V(Œ∏hat), so:
V(Œ∏hat) = E[(Œ∏hat - Œ∏)¬≤] - [E(Œ∏hat) - E(Œ∏)]¬≤
Now we rearrange terms and remember that E(Œ∏) = Œ∏ for constants:
E[(Œ∏hat - Œ∏)¬≤] = [E(Œ∏hat) - Œ∏]¬≤ + V(Œ∏hat)
Now let‚Äôs take a look at this formula, because it has some special things with special names in it. Hint: remember bias?
Bias = E(Œ∏hat) ‚Äî Œ∏
Can we find that in our formula? Sure can!
E[(Œ∏hat - Œ∏)¬≤] = [Bias]¬≤ + V(Œ∏hat) = Bias¬≤ + Variance
So what the hell is the thing on the left? It‚Äôs a useful quantity, but we weren‚Äôt very creative in naming it. Since ‚Äúerror‚Äù is a decent way to describe the difference (often notated as Œµ) between where our shot landed (Œ∏hat) and where we were aiming (Œ∏), E[(Œ∏hat - Œ∏)¬≤] = E(Œµ¬≤).
E(Œµ¬≤) is named, wait for it, mean squared error! That‚Äôs MSE for short. Yes, it‚Äôs literally named E(Œµ¬≤): we take the mean (another word for expected value) of squared errors Œµ¬≤. Bonus points for creativity there, statisticians.
MSE is the most popular (and vanilla) choice for a model‚Äôs loss function and it tends to be the first one you‚Äôre taught (here it is in my own machine learning course).
And so we have:
MSE = Bias¬≤ + Variance
Now that you‚Äôve worked through the math, you‚Äôre ready to understand what the bias-variance tradeoff in machine learning is all about. We‚Äôll cover that in my next article ‚Äî stay tuned by hitting that follow button.
towardsdatascience.com
If you had fun here and you‚Äôre looking for an entire applied AI course designed to be fun for beginners and experts alike, here‚Äôs the one I made for your amusement:
Here are some of my favorite 10 minute walkthroughs:
Data Science Learners
Ian Leboo
Dibyendu Biswas
Diwash Shrestha
Kira Wong
Abhishek
Anel Music
Jocelyn Wang
in
99P Labs
"
97,https://medium.com/@jkeszka/8-websites-so-helpful-that-it-feels-like-magic-to-get-work-done-d0d1104189f4,8 Websites So Helpful That It Feels Like Magic to Get Work Done,"We live on a hectic schedule, and everyone expects us to be super productive in every aspect of life. Like most people, you probably use a website or two to help you. But are any of these websites magic?
In this post, I‚Äôll highlight eight helpful and efficient websites that feel like magic in getting work done. From scheduling to task management, these websites have got you covered!
Dan Fedirko
in
Elligense | Tech in elearning and ecommerce
Asutosh Jena
Daniela Chamorro
in
Hite Digital
Fleksy
Mia, from Beagle
Malaika Ch
Andreina Caushi
Ruhani Rabin
"
98,https://medium.com/@anna.wu9222/33-must-know-sql-data-analyst-interview-questions-and-answers-f66ccf2cfaa6,33 Must Know Data Analyst SQL Interview Questions and Answers,"SQL is a powerful programming language that is widely used for managing and analyzing data. If you‚Äôre looking to become a data analyst or an existing one who is looking for a new job, it‚Äôs important to be prepared to answer SQL interview questions.
SQL, or Structured Query Language, is a standard programming language for accessing and manipulating databases. SQL is renowned for its simple yet powerful syntax, which makes it easy to query data in a variety of ways. SQL also supports a wide range of data types, including numeric, text, and date/time values.
Some of the most common SQL commands are CREATE TABLE, INSERT INTO, UPDATE, DELETE, and SELECT.
CREATE TABLE is used to create a new table in a database.
INSERT INTO is used to insert data into a table.
UPDATE is used to update data in a table.
DELETE is used to delete data from a table.
SELECT is used to select data from a table.
Want to practice real SQL interview questions? We‚Äôve analyzed over 50,000 interviews from pre-IPO to Fortune 500 companies at Big Tech Interviews (BTI) to curate an exclusive list of the latest SQL interview questions and solutions so you can ace your next interview!
SQL provides a number of built-in functions that can be used to perform various types of data analysis. For example, the COUNT function can be used to counting the number of records in a table, while the SUM function can be used to calculate the sum of numeric values in a column. By using these and other SQL functions, data analysts can quickly and easily perform complex data analysis tasks.
For example, a data analyst might use SQL to count the number of orders placed on a website each day. The following SQL query would return the total number of orders for each day in the dataset:
SELECT COUNT(*) AS ‚ÄúTotal Orders‚Äù
FROM orders
GROUP BY order_date
One common error that occurs when writing SQL queries is forgetting to include a WHERE clause. Without a WHERE clause, your query will return all rows from the table you‚Äôre querying, which can make it difficult to find the specific information you‚Äôre looking for. Another common error is using incorrect syntax, which can lead to unexpected results or errors when your query is executed. Finally, it‚Äôs important to make sure that your SQL queries are properly formatted and easy to read; otherwise, they may be difficult for others to understand or debug if something goes wrong.
For example, the following SQL query would return all rows from the orders table, regardless of the order_date:
SELECT *
FROM orders
This would return a very large dataset that would be difficult to work with. To fix this, we can add a WHERE clause to filter the data by order_date:
SELECT *
FROM orders
WHERE order_date = ‚Äò2018‚Äì01‚Äì01‚Äô
A primary key is a column (or set of columns) in a database table that uniquely identifies each row in the table. A foreign key is a column (or set of columns) in one table that contains values that match the primary key values in another table. Foreign keys are used to create relationships between tables; for example, a foreign key in a ‚Äúcustomer‚Äù table could reference the primary key in an ‚Äúorders‚Äù table, linking each customer with their respective orders.
A SQL join is used to combine data from two or more tables into a single result set. Joins are performed using the JOIN keyword, followed by the name of the table to join with. There are a number of different types of joins, including inner joins, outer joins, and self-joins. Inner joins return rows from both tables that have matching values in the specified columns, while outer joins return all rows from both tables, including rows with no matching values. Self-joins are used to join a table to itself; for example, you could use a self-join to find all customers who live in the same city as another customer.
A subquery is a SQL query that is embedded within another SQL query. Subqueries are often used to find data that satisfies certain conditions; for example, you could use a subquery to find all customers who live in the same city as a particular customer. Subqueries can be used with various SQL commands, including SELECT, FROM, WHERE, and ORDER BY.
For example, consider the following customer table:
Suppose we want to find all customers who live in the same city as customer with id=1. We could use the following SQL query:
SELECT * FROM customers WHERE city IN (SELECT city FROM customers WHERE id = 1)
Want to practice real SQL interview questions? We‚Äôve analyzed over 50,000 interviews from pre-IPO to Fortune 500 companies at Big Tech Interviews (BTI) to curate an exclusive list of the latest SQL interview questions and solutions so you can ace your next interview!
This would return all rows from the customer‚Äôs table, including the row with id=1. To exclude this row, we could add a condition to the subquery that checks for customer_id != 1:
SELECT * FROM customers WHERE city IN (SELECT city FROM customers WHERE id = 1 AND id != 1)
9. What‚Äôs a SQL window function and how is it used?
A SQL window function is a function that performs a calculation on a set of values and returns a single value. Unlike aggregate functions, which return one result per group, window functions return one result per row. Common window functions include RANK, DENSE_RANK, and NTILE.
Window functions are often used with the ORDER BY clause to calculate a value for each row
SQL supports a number of different data types, including numeric, text, date/time, and Boolean values. Numeric values include integers and floating-point numbers, while text values include character strings and date/time values include date, time, and timestamp values. Boolean values can either be TRUE or FALSE.
An index is a database structure that is used to improve the performance of SQL queries. Indexes can be created on columns in a table, and they are typically used to speed up searches for specific values in those columns. When a query is executed, the database engine will first check to see if an index exists for the columns that are being searched; if an index exists, the engine will use the index to quickly locate the desired data, which can improve query performance.
A view is a virtual table that is based on the results of an SQL query. Views are often used to provide security or simplify complex queries. For example, you could create a view that only includes customer information that is relevant to your current project. Tables, on the other hand, are database structures that actually store data.
The WHERE clause is used to filter rows from a table based on specified conditions; for example, you could use a WHERE clause to find all customers who live in a particular city. The HAVING clause is used to filter rows from a table based on aggregated values; for example, you could use a HAVING clause to find all customers who have placed more than 10 orders.
The ORDER BY keyword is used to sort the results of an SQL query in ascending or descending order. By default, ORDER BY will sort the results in ascending order; to sort the results in descending order, you can use the DESC keyword.
A primary key is a column or set of columns that uniquely identify a row in a table. Primary keys must contain unique values, and they cannot be NULL.
A foreign key is a column or set of columns that contains values that match the primary key values in another table. Foreign keys are used to create relationships between tables; for example, a foreign key in a ‚Äúcustomer‚Äù table could reference the primary key in an ‚Äúorders‚Äù table, linking each customer with their respective orders
Window functions are a type of SQL function that operates on a set of rows and returns a single value. Window functions are typically used to calculate aggregated values, such as sums or averages, over a specified window of rows. For example, you could use a window function to calculate the average order total for each customer.
Inner joins return only rows that have matching values in both tables; for example, if you inner join a ‚Äúcustomer‚Äù table with an ‚Äúorders‚Äù table, only customers who have placed orders will be returned. Outer joins, on the other hand, return all rows from both tables, including rows with no matching
Want to practice real SQL interview questions? We‚Äôve analyzed over 50,000 interviews from pre-IPO to Fortune 500 companies at Big Tech Interviews (BTI) to curate an exclusive list of the latest SQL interview questions and solutions so you can ace your next interview!
Some of the most common SQL functions are SUM(), AVG(), COUNT(), MIN(), and MAX(). These functions are used to calculate aggregated values, such as sums, averages, or counts.
This is a common SQL interview question that is designed to assess your real-world experience with the language. When answering this question, be sure to describe a specific problem that you were able to solve using SQL. This will help to show the interviewer that you have a good understanding of how SQL can be used in practice.
Lag and lead functions are used to access data from a previous or future row in a table. Lag functions return data from a row that is preceding the current row, while lead functions return data from a row that is following the current row.
For example,
If the current row is customer_id 3 (Joe Bloggs), a lag function would return customer_id 2 (Jane Doe), while a lead function would return customer_id 4 (Sarah Connor).
20. Write a SQL query to select the second-highest salary in the engineering department.
SELECT DISTINCT salary
FROM employee
WHERE department = ‚Äòengineering‚Äô
ORDER BY salary DESC LIMIT 1 OFFSET 1;
This SQL query will select the second-highest salary from the engineering department by first selecting all distinct salaries from employees in the engineering department, then ordering them in descending order, and finally selecting the top 2 salaries.
Want to practice real SQL interview questions? We‚Äôve analyzed over 50,000 interviews from pre-IPO to Fortune 500 companies at Big Tech Interviews (BTI) to curate an exclusive list of the latest SQL interview questions and solutions so you can ace your next interview!
A correlated subquery is a type of SQL query that contains a reference to a value from outer query. Correlated subqueries are typically used when you want to find rows from a table that match certain conditions, but you can only know those conditions after examining other rows in the same table.
For example, you could use a correlated subquery to find all employees who make more than the average salary in their department. In this case, you would need to calculate the average salary for each department before you could compare each employee‚Äôs salary to it.
A SQL aggregate function is a function that performs a calculation on a set of values and returns a single value. Common aggregate functions include COUNT, SUM, MAX, and MIN. Aggregate functions are often used with the GROUP BY clause to return one result per group; for example, you could use the COUNT() function to find the number of customers in each city.
For example, consider the following customer table:
Suppose we want to find the number of customers in each city. We could use the following SQL query:
SELECT city, COUNT(*) AS ‚ÄúNumber of Customers‚Äù
FROM customers
GROUP BY city
This would return the following result:
23. When would you not want to use a window function in a SQL?
Window functions are a type of SQL function that return a value for each row in the query result, based on values from other rows in the same result. For example, you could use a window function to calculate the running total of all order totals in your customer orders table.
Window functions are not typically used with aggregate functions, because the results would not make sense. For example, if you tried to find the average salary for each department using a window function, you would end up with the same average salary for every department, because the window function would calculate the average salary for each row in the result set (which would be all employees in all departments).
There are a few ways to find duplicate rows in a SQL table. One way is to use the GROUP BY clause to group together rows with the same values in the columns you‚Äôre interested in. For example, suppose we have a customer orders table with the following data:
If we wanted to find all the duplicate rows, we could use the following SQL query:
SELECT name, city
FROM customer_orders
GROUP BY name, city
HAVING COUNT(*) > 1
This would return the following result:
There are a few different ways to optimize a SQL query. One way is to make sure that the columns you‚Äôre interested in are indexed, so the database can more quickly find the data you‚Äôre looking for. Another way is to use the EXPLAIN command to see how the database will execute your query, and then make changes to your query based on that information. Finally, you can use query hints to give database-specific instructions on how to execute your query.
Another way you can optimize a SQL query is to use a tool like SQL Profiler to see where the bottlenecks are in your query and then make changes accordingly.
There are a few different ways to find the top 5% of customers by sales. One way is to use the GROUP BY clause to group together rows with the same values in the columns you‚Äôre interested in. For example, suppose we have a customer orders table with the following data:
If we wanted to find the top 5 customers by sales, we could use the following SQL query:
SELECT name, city, SUM(sales) AS ‚ÄúTotal Sales‚Äù
FROM customer_orders
GROUP BY name, city
ORDER BY ‚ÄúTotal Sales‚Äù DESC
LIMIT 5
This would return the following result:
Want to practice real SQL interview questions? We‚Äôve analyzed over 50,000 interviews from pre-IPO to Fortune 500 companies at Big Tech Interviews (BTI) to curate an exclusive list of the latest SQL interview questions and solutions so you can ace your next interview!
27. What is a relational database?
A relational database is a database that stores data in tables. Tables are similar to folders in a file system, where each table stores data about a particular subject. For example, a customer orders table might store data about customer orders, and a product table might store data about products.
Relational databases are the most common type of database, and they are used by most businesses because they are easy to use and easy to scale.
There are three types of relationships in a database: one-to-one, one-to-many, and many-to-many.
A one-to-one relationship is when each row in one table is related to only one row in another table. For example, a customer table might have a one-to-one relationship with an orders table, where each customer is related to only one order.
A one-to-many relationship is when each row in one table is related to multiple rows in another table. For example, a customers table might have a one-to-many relationship with an orders table, where each customer is related to multiple orders.
A many-to-many relationship is when each row in one table is related to multiple rows in another table, and each row in the other table is related to multiple rows in the first table. For example, a customers table might have a many-to-many relationship with a products table, where each customer is related to multiple products and each product is related to multiple customers.
DDL (Data Definition Language) is a language used to create and modify database structures like tables, views, and indexes.
For example, suppose we have a sales table with the following data:
If we wanted to get the total two-day rolling average for sales by day, we could use the following SQL query:
SELECT date, sales, AVG(sales) OVER (ORDER BY date ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS ‚ÄúTwo-Day Rolling Average‚Äù
FROM sales
GROUP BY date
ORDER BY date
This would return the following result:
If you were teaching SQL to a beginner, you would start by explaining the basics of databases and how they are used to store data. You would then move on to explaining the different types of SQL queries and how they are used to retrieve data from a database. Finally, you would teach them how to use SQL to insert, update, and delete data from a database.
Some common date functions in SQL are:
- CURRENT_DATE: Returns the current date.
- CURRENT_TIME: Returns the current time.
- CURRENT_TIMESTAMP: Returns the current date and time.
- DATE_ADD: Adds a specified number of days, months, or years to a date.
- DATE_SUB: Subtracts a specified number of days, months, or years from a date.
- DAY: Returns the day of the month for a given date.
- MONTH: Returns the month for a given date.
- YEAR: Returns the year for a given date.
There are many advanced SQL functions, but some of the most common are aggregate functions, window functions, and pivoting.
Aggregate functions are used to calculate a single value from multiple values. For example, the SUM() function calculates the sum of a column of values, and the AVG() function calculates the average of a column of values.
Window functions are used to calculate a value for each row in a table based on the values in other rows in the table. For example, the RANK() function assigns a rank to each row in a table, and the LAG() function returns the value of a column in a previous row.
Pivoting is when you rotate data from one format to another. For example, you can pivot data from a horizontal format to a vertical format, or from a columnar format to a row-based format.
Your data analyst SQL interview will test you on these concepts explicitly and implicitly. It‚Äôs not enough to just know what SQL queries show up at the data analyst interview; you need to be knowledgeable about how the subject fits into data analytics and its impact on your soon-to-be colleagues. First, get comfortable with the background and functionality of these concepts and queries. Then you can move on to the next step: practice solving the problems.
Want to practice real SQL interview questions? We‚Äôve analyzed over 50,000 interviews from pre-IPO to Fortune 500 companies at Big Tech Interviews (BTI) to curate an exclusive list of the latest SQL interview questions and solutions so you can ace your next interview!
Munaiz Ahmed
Parth Mistry
in
Flutter Community
KuldipMochi
READ/DOWNLOAD! Issues and Ethics in the Helping Pr
Red Badger
in
Red Badger thinks
Fiodar Sazanavets
Msartaev
in
0tohero
VATSAL PATEL
"
99,https://medium.com/@data-storyteller/data-analytics-interviews-what-to-expect-and-how-to-prepare-64f48d910213,Data Analytics Interviews: What to Expect and How to Prepare,"If you‚Äôre searching for a data analytics job, what can you expect when it comes to interviews? What can you do to prepare?
The first thing to know is that every company has a slightly different ‚Äî or very different ‚Äî process. But there are some commonalities you can expect.
At the very least, expect three rounds of interviews:
However, some companies also add in things like:
The order can vary ‚Äî sometimes the technical assessment comes before meeting the hiring manager, sometimes after, or sometimes the hiring manager or a peer does the technical assessment.
Also, remember that you are allowed to ask questions at any point!
The recruiter screening is almost always the first step. Expect a phone call (sometimes a Zoom/video meeting) lasting 15‚Äì30 minutes.
This round is going to be pretty high level. Expect any or all of the following:
The hiring manager interview can have more variation. I would expect most of the same questions above from the recruiter round, with some differences.
Often during the panel round of interviewing, you‚Äôll meet with the hiring manager‚Äôs boss or someone else in a leadership role ‚Äî Director or Vice President. Sometimes this person will have a technical background, other times more of a business background. The questions they ask will likely depend on their background. But I would expect more high-level ‚Äúbig picture‚Äù questions about how you make decisions and how you‚Äôve delivered value to a company. Maybe similar to what the hiring manager would ask.
You also might meet with a possible peer ‚Äî a data analyst or data scientist already on the team. Or a stakeholder ‚Äî someone from the team you‚Äôll support. The questions might be similar to what the hiring manager would ask, although the stakeholder will likely ask more business or case study questions than technical ones.
There are two ways companies do technical assessments ‚Äî live coding challenges or take-home projects.
For live coding, expect to receive a link to a site like Hacker Rank. This link will allow the interviewer and you to see what the other types. Often they will include a few rows or explanations of a sample data table, and ask you to complete a series of hypothetical problems or questions, which you‚Äôll solve via SQL or depending on the role, via Python or probability.
For a take-home project, you will be provided with a dataset and instructions for the problem you need to solve or questions you need to answer. You will get a set number of days before you need to turn in your results. They will usually recommend how many hours you should spend on the project.
Some companies, instead of giving you a few days to complete, will schedule this like an interview. It will start with them explaining the data and the problem, you‚Äôll then have a set amount of time (maybe 1‚Äì2 hours) to work on it, and then at the end of the time, you‚Äôll need to present your work.
Now that you know what to expect in data analytics interviews, how do you prepare? Below are the steps that I typically follow.
Research the company. I aim to spend maybe 15 minutes reviewing the company‚Äôs website, their Wikipedia page, LinkedIn page, and maybe search Google News for recent articles. You want to make sure you understand what the company does, how the role you‚Äôre interviewing for fits in, and have an answer for the question ‚Äúwhy do you want to work here?‚Äù
Prepare a list of my accomplishments. Many of the interview rounds will include questions about my experience, so I always have this document handy to jog my memory. In it, I have a summary of all of my recent major projects, including the situation or business problem, the tasks I contributed, and the results or outcome or business value that was delivered. I‚Äôm not creating a script for myself, but rather a menu of my own examples that I can refer to depending on the question asked.
Think about my answers to common interview questions. You can find lists via Google Search of ‚Äúcommon interview questions.‚Äù But once you‚Äôve gone through enough interviews, you have a general idea of them. I find it helpful to journal out my own answers to these questions, using all of the advice found online, but putting things in my own words, with my own examples. Again, this isn‚Äôt a script, but just a way to prepare my answers and have examples top of mind.
Research the people you‚Äôre meeting with. This is simply looking at their LinkedIn profile. Generally, I want to get a sense if the person I‚Äôm talking to is more technical or has a different background, so I know how detailed to go in my answers. It‚Äôs also helpful to check out the LinkedIn profile of anyone else in the same role at the company to see what kind of candidate they‚Äôve typically hired.
Practice for coding challenges. Even if you use SQL or Python every day on the job, it‚Äôs often not the type of code you‚Äôll write in these job interview challenges. Practice on sites like Hacker Rank or Strata Scratch.
Think about business case questions. These will vary by industry or what team you‚Äôll be supporting ‚Äî product, marketing, finance, sales, etc ‚Äî they will have different business case questions. For product roles, I have found this article helpful. For any role, think about what the company does, and how this role will support the company‚Äôs goals, and use that to guide your answers.
Prepare a list of your own questions. Always use the time they give you to ask questions. During every round. It shows that you‚Äôre engaged and genuinely interested in the role and looking for a good fit. There are tons of questions you can ask, I have a list of ideas. But I think about what I need to know in order to evaluate if this role will be good for me. And this will be different for everyone.
But some common questions that are good to ask:
Do a mock interview. If you have someone in your network who is also looking for a job, practice interviewing each other. Or if you have a mentor or career coach, ask them if they can do a mock interview. Otherwise, you can also practice in the mirror, or even better, by recording yourself.
It is very common in the first interview for the recruiter to bring up the topic of salary. Typically this is by asking you for your salary expectations.
You should always have a number in mind for your salary expectations. If you aren‚Äôt sure what that number is at the moment, do your research. Check out sites like Glassdoor, Blind, Levels.fyi, the H1B Database for reporting salaries by company and job title. Additionally, there are annual salary guides with more salary information. Finally, you can calculate your goal salary or compare potential new salaries in my goal salary calculator.
A few things to note when talking about salary during interviews:
Never share your current salary. It‚Äôs a pretty outdated practice for recruiters to ask you for your actual salary, and in some states, it‚Äôs actually illegal for them to ask for your current salary. But if they do ask, don‚Äôt share it. You can either respond with ‚Äúwell my expectations for my next role are ‚Ä¶‚Äù or ‚ÄúI‚Äôm not comfortable sharing, but can you tell me the salary range budgeted for the role?‚Äù
If they ask for your salary expectations. Personally, I respond by saying ‚ÄúI do have a number in mind, but can you share the salary range budgeted for the role?‚Äù In some states, they legally have to tell you the range if you ask for it. In states where that‚Äôs not law, you still don‚Äôt have to share any numbers. If they won‚Äôt share their range, you can share ‚Äúwell, the ranges I‚Äôm hearing from similar companies for similar roles is $x-y, so I would expect something in that range.‚Äù And it can be a big range.
You can change your salary expectations. If you do end up giving a number or range, you aren‚Äôt locked in. Your expectations can change for a number of reasons:
Once the interview is over, what‚Äôs next?
One thing you can do to manage your own expectations is ask them about the interview process. In the recruiter screening, about the structure and timing of their interview rounds. And at the end of each round, as ‚Äúwhat are the next steps?‚Äù and/or ‚Äúwhen do you plan to make a decision?‚Äù
Should you send a thank you note? The advice I‚Äôve seen is pretty split on thank you notes. Personally, I don‚Äôt send them. They have an open role that they need to be filled ‚Äî the candidates are doing them a favor. So the idea that the candidate should send a thank you note seems backward to me. Additionally, in the age of Diversity, Equity, and Inclusion initiatives, a company expecting a thank you note goes against that, because it assumes a level of privilege in the candidate's upbringing. So any hiring manager who uses thank you notes or the lack of one in their decision-making needs to go through more DE&I training. However, my perspective is focused on the tech industry, where thank you notes seem to be pretty uncommon. They could still be common elsewhere.
When should you follow up? How long after an interview should you wait before following up, assuming you haven‚Äôt received a response? Well, if you know their timing for the next steps, then I would wait until at least that period has passed, and maybe double it. For example, if they said ‚Äúby the end of next week,‚Äù I‚Äôd wait until the end of the week after that, and then follow up. If you don‚Äôt know the timing, then I would wait at least 2 weeks.
Harriet Boyd
in
Analytics Vidhya
Abdullah Kurkcu
in
Towards Data Science
Bogdan Samoletskyi
in
Analytics Vidhya
Preetham Jagadeeshan
Philip Mohun
in
DataDrivenInvestor
Changhyun Kim
Kelly Epley
in
Towards Data Science
Kristy Andrash
"
100,https://medium.com/towards-data-science/3-best-approaches-to-segment-your-customers-21fb3732b98a,3 Best Approaches to Segment Your Customers,"Today, the customer is at the center of everything. But, you can‚Äôt satisfy everyone. That‚Äôs the reality. The sooner you learn this the better it will serve you and your business. That‚Äôs why the first thing business analysts do is segment customers (both current and potential) into groups based on their needs, wants, and shared characteristics. Knowing the preferences of the customer allows you to design tailored strategies to win them and offer the best products and services. This is even more critical for Software as a Service (SaaS) businesses where the customer retention ratio (RR) is one of the key KPIs.
There are several ways to segment customers. Hierarchical clustering, Recency Frequency & Monetary (RFM) segmentation, and K-means clustering are among the popular ones.
Recently I‚Äôve written a 3 part series where I describe more details how to perform customer segmentation. Check them out:
Introduction to customer segmentation
Customer segmentation with k-means clustering
Improve the k-means customer segmentation model with PCA
According to the Wikipedia, hierarchical clustering is a method of cluster analysis that seeks to build a hierarchy of clusters.
en.wikipedia.org
In hierarchical clustering, pairwise samples are grouped together based on similarity, and then they are merged to form the next level of hierarchy. Finally, it forms a dendrogram (a tree structure). The number of clusters to form is determined by expert analysts based on this dendrogram.
The following code produces a dendrogram based on a standardized customer dataset. The entire code can be found in the Deepnote Notebook.
K-means clustering is an unsupervised clustering algorithm. It attempts to group observations based on the nearest mean. Here are the steps required to implement the K-means algorithm.
en.wikipedia.org
Once we know the optimal number of clusters, we can segment our customers by n_clustersparameter.
Once the model is trained with the data, we can get the assigned cluster label from the label_ parameter of the kmeans object.
Recency, Frequency, and Monetary segmentation (RFM) is a managerial customer segmentation process, which is very adaptable, and easy to understand. The key entities are:
The key element of RFM segmentation assigns static management weights to the 3 factors of a customer and calculates the final grade of each customers which determines the group of the customer.
Customer segmentation is the first step. The next step is to set up strong strategies. While implementing your strategy stay focused and periodically check to ensure you are on the right track.
All the best.
Thanks for reading! If you like the article make sure to clap (up to 50!) and let‚Äôs connect on LinkedIn and follow me on Medium to stay updated with my new articles.
Pralabh Saxena
in
Heartbeat
Mark Palmer
in
Towards Data Science
Greg Rafferty
in
Towards Data Science
Dan Hales
in
The Startup
Yasser Elsedawy
in
Towards Data Science
Arthur Mello
in
DataDrivenInvestor
Pralabh Saxena
in
Better Programming
Setu G
"
101,https://medium.com/towards-data-science/step-by-step-approach-of-building-data-pipelines-as-a-data-scientist-or-a-machine-learning-engineer-4b3e8da3bcd1,Step-by-step Approach of Building Data Pipelines as a Data Scientist or a Machine Learning Engineer,"Oftentimes we are asked in either interviews or in our job roles as data scientists to build an application that is capable of performing machine learning predictions for continuous streaming data. There is often expectation from our boss that we are going to be delivering the results on time and generate these high-quality predictions with the use of machine learning and data science.
When looking at a large number of job descriptions state usually that to become a data scientist, one should be having 3+ years of experience along with a few other things such as knowledge of SQL and Python. In addition, there is one more important component that is often being highlighted which is being able to build data pipelines and ensure that there are timely predictions. It is in these situations that a candidate is expected to have a strong understanding of building the data pipelines that are often the case to become a data scientist or a machine learning engineer.
It is initially important to understand the definitions of a data pipeline before understanding ways in which we can successfully build and create a data pipeline. It is basically performing the automation of data and how it is actually processed and given to machine learning models during test time without human effort.
In other words, we would be creating a pipeline where the data that has been streaming would reach the pre-processing state after which, it would go with the machine learning model predictions with minimal effort.
In this article, we would be looking at the data pipelines and how to build them and ensure that we do a good job in building machine learning predictions. We will go in step-by-step about how to build the data pipelines that are important.
Before we build a data pipeline, we start off by asking some fundamental questions about the data and its size. Furthermore, we also look for business constraints such as whether there is a requirement for low-latency systems. If our business requires us to build low latency systems such as in internet applications, it is advisable to go ahead with simple ML models rather than relying on more complex models though they might be accurate in our cases. On the other hand, there can be other constraints such as model accuracy and it is important for the models to have higher accuracy. This is particularly true when we are using machine learning in the field of healthcare where the cost of misdiagnosing a patient suffering from a disease can have a significant impact. Therefore, the first step would be understanding the business constraints before trying to build interesting machine learning solutions.
Now that you have understood the requirements of the business and then have determined that machine learning and artificial intelligence can be the most useful according to the requirements, it is now time to collect the data that is important for ML predictions. Therefore, different departments can have access to different amounts and variations of the data that could be merged to create a unique dataset for our models to make predictions. Therefore, it would be a great practice to talk with different departments such as Sales teams and Data Science teams to get access to relevant data that could power your ML models. Therefore, we have explored the step of collecting the data that would be the most useful for our models for predictions.
Now that the data is ready for use for ML applications, it is time to preprocess the data and make it easier for the computer (ML models) to understand them. Usually, data consists of a large number of missing values that do not make a significant impact on predictions. If you are considering the task of natural language processing (NLP), there are often words that do not actually add a lot of value to the meaning of the text. These words are also called stop words such as ‚Äòand‚Äô, and ‚Äòor‚Äô that do not have a lot of meaning to the text. Therefore, we perform the task of data preprocessing such as filling in the missing values or removing words such as stop words before giving it to our models.
If you are looking for ways to pre-process the data before feeding it to the machine learning models, feel free to explore my earlier article where I mention various ways in which we could perform feature engineering with the data. Below is the link.
What Are the Most Important Preprocessing Steps in Machine Learning and Data Science? | by Suhas Maddali | Towards Data Science (medium.com)
After the data is processed and converted to a form that is more computer-friendly (machine learning-friendly, to be precise), the next step would be to feed this data to our ML models to make predictions. It is also important to note that we would have to divide the data into 2 parts: training and testing parts. This is because we would not want to evaluate the model for its performance on the training data itself. This is because since the model is trained on this data, it can be expected for it to perform extremely well on it. The only concern that we have is how will the ML model perform once it is put into production. Therefore, the real question that we must be asking before we deploy the model is how well it performs on the data that it has not seen before.
That is the case where the test set can be quite handy as this data could represent the data that the model might be facing in the future as well. However, there can be plenty of scenarios where this does not always hold. Assuming that the situation holds, we can then go ahead and train the model with the training data and test it with the data that we have split for testing. After trying out a large number of models along with hyperparameter tuning, we determine the best model that must be put into production. Note that though the performance of the model on the test set was excellent, we might not be putting that model sometimes to production due to business constraints such as low latency or other requirements depending on the nature of the business. But it is always a good idea to test the models and see how they are performing on the data that they have not seen and to get a general idea about their performance in real-time respectively.
Once the data that we have is given to the models for predictions and we determine the best model to deploy in real-time, the next step that we could be taking is to deploy the product in real-time so that the end users get access to the impressive predictions that the model gives based on the history at which it was earlier trained. The parameters that the model learns during training are used to determine the output for real-time data that is easily accessible to the model respectively. While the machine learning predictions can seem impressive, failing to put the model into production can mean that we have been wasting quite our valuable time looking at a piece of technology that is impressive but was not able to provide any business value. Therefore, we would have to spend good amount of time ensuring that the best model that we have obtained is deployed in real-time to make a business impact.
Efforts have been put to deploy the best model in real-time based on the experiments that we have run on the test data. Now is the time that we constantly monitor the performance and regularly retrain the model if necessary after finding that it is not doing well after a few days with the change in events that cause the relationship between the input and the output to no longer exist as was earlier determined by our ML model.
Hence this is a stage where we try to maintain our product without lowering its quality in the long run. We can constantly monitor our data and see if there are any changes in relationships between the features or the distribution of the output respectively. If we find that there is a significant difference between the data that was used for model training, we again retrain the model to give better predictions with the present real-time data. If you are more interested to know how the data could change and the distribution might not always be the same, you can click on the link below that explains this phenomenon in great detail.
Why is it Important to Constantly Monitor Machine Learning and Deep Learning Models after Production? | by Suhas Maddali | Towards Data Science (medium.com)
While going through this article, you might have gotten a good idea about the importance of building data pipelines and the list of steps that could be followed to effectively build robust pipelines and ensure that we are giving good value to the business from our machine learning models. Steps such as data monitoring must be taken to ensure that the quality of predictions is not reduced. This can be done with the help of retraining the model in cycles and ensuring that it is getting access to the most recent data. Thank you for taking the time to read this article.
If you like to get more updates about my latest articles and also have unlimited access to the medium articles for just 5 dollars per month, feel free to use the link below to add your support for my work. Thanks.
https://suhas-maddali007.medium.com/membership
Below are the ways where you could contact me or take a look at my work.
GitHub: suhasmaddali (Suhas Maddali ) (github.com)
YouTube: https://www.youtube.com/channel/UCymdyoyJBC_i7QVfbrIs-4Q
LinkedIn: (1) Suhas Maddali, Northeastern University, Data Science | LinkedIn
Medium: Suhas Maddali ‚Äî Medium
Dharmendra Gandi
Subarna Lamsal
Aman Kharwal
in
Coders Camp
Matthew Dancho
in
learn data science
Jonathan Legrand
in
Refinitiv Developer Community
Borach Jansema
in
Towards Data Science
Etienne Dieuned Noumen
Brendan Ward
"
102,https://medium.com/towards-data-science/knn-algorithm-from-scratch-37febe0c15b3,KNN Algorithm from Scratch,"KNN stands for K nearest neighbour. The name itself suggests that it considers the nearest neighbour. It is one of the supervised machine learning algorithms. Interestingly we can solve both classification and regression problems with the algorithm. It is one of the simplest Machine Learning models. Though it is a simple model, sometimes it plays a significant role, basically when our dataset is small, and the problem is simple. The algorithm is also known as the lazy algorithm. These are the summary of the KNN algorithm.
I will explain it from the very basics of KNN so that you can understand the article by heart. At the end of the article, you can implement the algorithm by yourself (without any machine learning library).
Here, (X1, Y1) and (X2, Y2) are the two points shown in the image. We can calculate the distance between the two points with the following formula.
If we have more than two features, we need to add the squared distance to the above formula to get the distance.
The name indicates that the algorithm considers the nearest elements to predict the value of new data. The flowchart shows the steps for KNN.
Let me explain.
Step 1: Calculating the Distance
First of all, we need to load the labelled dataset as the KNN algorithm is a supervised learning algorithm. Look at the image below.
Suppose our dataset has only two features, and we plotted the data as shown in the image. Blue and Red points indicate two different categories. Let‚Äôs have new unlabelled data that requires classification based on the given dataset.
In the image, the central point needs to be classified. Now, we will calculate the distance of all the data from the unlabelled data. The arrow from the central point represents the distances.
Step 2: Selecting K-nearest neighbour
In the previous step, we calculated the distances of the new point from all other data. We will sort the data points in ascending order according to the distance. Finally, we will consider the K number of nearest points from the unlabelled data.
In the above image, I have considered the 3 nearest data points (K=3). Observe the image; among 3 nearest points, 2 data belong to the red category, and 1 to the blue category. So, red is the majority class. According to the KNN algorithm, new data points will be classified as red.
In case of a regression problem, we will consider the average value of K nearest data points.
KNN has no training period. For each prediction, the algorithm needs to undergo the same process. There is no parameter that can be optimised in the training period. So, it is a lazy algorithm. When the dataset size is large, it takes longer to predict.
Let‚Äôs write a few lines of code to implement the algorithm.
Importing the modules.
Creating a function for calculating distance.
The euclidean function takes two parameters, namely p1 and p2. According to the formula explained in the Euclidean Distance section, the function will calculate the distance from p1 point to p2 point.
In the next step, we will write a function for saving the distance of each point of the dataset from the new data point and sorting out the data. Finally, we will select the class for the new data point with the majority class.
We have created the ‚Äòpredict‚Äô function to find the prediction for a bunch of new data points. Let‚Äôs use our ‚Äòpredict‚Äô function to get the iris dataset‚Äôs prediction.
Here, we have manually selected the train and test data. We randomise the data first to prevent bias. Then we select 80% data for training and the rest for testing. Finally, we tested our model for 7 nearest neighbours (k=7).
The article [1] helps me to implement the KNN algorithm.
Done. We have implemented KNN from scratch. Let‚Äôs have a coffee and think about the algorithm. If any confusion arises, don‚Äôt forget to make a comment (or reach out to me).
The KNN algorithm seems very simple. But sometimes, it plays a significant role in solving important machine-learning problems. When our data is noisy, we need to solve simple problems. Always running towards a deep learning model is not desirable because it takes huge computational power and data. If we blindly jump over deep learning models always, we won‚Äôt get a good result. The good practice is to have in-depth intuition about all the ML models and make appropriate decisions analysing the problem.
Full implementation is available here.
deepnote.com
Read the popular article on statistics for data science (Complete Guideline)
towardsdatascience.com
Kien Hao Tiet
in
Aviation Software Innovation
Sergios Karagiannakos
in
Becoming Human: Artificial Intelligence Magazine
Pier Paolo Ippolito
in
Towards Data Science
Aneesh Panoli
Alex Held
Subrata Goswami
KAUSHIK M
in
Analytics Vidhya
OneFlow
in
CodeX
"
103,https://medium.com/geekculture/sql-study-plan-for-30-days-from-scratch-for-free-cb8df85230f4,SQL Study Plan for 30 Days from Scratch for free,"SQL is the heart of data science. While it is not only the aspect of data science, it makes up a large portion of it. To fetch the data from the database you will need to write the queries. Numerous benefits for your data models can be unlocked by writing simple, succinct SQL code. One benefit is that it can make your models run more quickly and efficiently when your queries are well-written. Second, it can assist you in assembling complicated datasets and computing important business metrics. Third, when the code is written effectively, it may help to scale your business, lowering your technical debt.
There are many ways to get started in learning SQL. I have created 30 days plan to get started with SQL from scratch for free.
Week 1: Basic Concepts
In the first phase, you can start with basic SQL database concepts on sqlbolt and sqlzoo. You can complete all the interactive lessons on it. By the end of these tutorials, you will be able to
Week 2: Join and Window Functions
In the next phase of learning, you can learn more about joins from w3schools. After that, you can start with an SQL course on udacity.
You can also learn concepts from YouTube Channel ‚Äî kudvenkat , Ankit Bansal , Crack Concepts, techTFQ.
Week 3: Start Practicing
In the third phase of learning, you can start practicing Intermediate and advanced SQL questions on the Mode Analytics SQL tutorial. It has a section called ‚ÄúSQL Analytics Training‚Äù that contains practical SQL problems and has a good range of real-life SQL problems.You can also practice from HackerRank .It provides range of SQL question questions divided by level of difficulty.
Week 4: Practicing Interview Questions
The last step is to put all your learnings to practice with interview questions. You can practice interview questions from :
1- Leetcode: It Offers a variety of SQL questions based on the level of difficulty and technical questions asked in companies.
2- Strascratch: It also offers a variety of SQL questions that are being asked by the biggest tech companies. The database offered is MySQL or PostgreSQL.
3- SQLPad: It also offers a variety of real SQL interview questions. Every code question has a video lecture that explains the idea (or concepts) that are being tested. Additionally, you can examine data sets and enter PostgreSQL and MySQL code here.
4- DataLemur : It offers 50+ real FAANG SQL Interview questions for FREE . Each question comes with multiple hints, full solutions, and an active discussion board!
SQL is one of the most important skills for data science roles. While you may think knowing SQL basics will be enough, you can know the basics and still write poor code. You must regularly practice and rework queries to make them more efficient.
Diksha Mohnani is a business analyst, writer, dancer, and a startup enthusiast. She is currently working as a Business Analyst at Walmart. Her passion lies in combining technical knowledge with her creativity and leadership skills to build great products.
Kenneth Leung
in
Geek Culture
Mathias Gruber
in
Towards Data Science
IMPEER of the NAS of Ukraine
Oluwatimilehin Odubola
The Educative Team
in
Dev Learning Daily
Clickon Gadget
in
DataDrivenInvestor
Wenxuan Zhou
Dr. D. M. Hardy
"
104,https://medium.com/@muhammadawais2403/top-10-best-data-science-and-data-analytics-books-of-2022-38ea7115d092,Top 10 Best Data Science and Data Analytics Books of 2022,"Data science is a growing field, and it‚Äôs becoming more popular every day. But there are still some people who think that data science is just something you can do in your spare time or at night after work. Well sorry to disappoint, but no! Data science is an exciting career opportunity with lots of benefits and opportunities for growth, which means you need to put in some serious effort if you want to advance your career. Fortunately for those who want to learn more about this topic ‚Äî and perhaps even start their own careers in data science ‚Äî there are plenty of books out there on the market today that can teach us everything we need to know about this exciting new field:
The year is 2022 and data is more important than ever before. Data is a strategic asset, the new AI, and currency ‚Äî and it‚Äôs all around us. In fact, if you don‚Äôt have access to good data analysis skills now or in the next few years (or decades), then your business will be at a competitive disadvantage compared with companies that do have these skills on hand.
The best way to ensure that your company has access to high-quality analytics tools is through formal training programs like those offered by universities around the world: Harvard Business School offers its own online BA/MS program in ‚ÄúAdvanced Analytics‚Äù; Stanford University offers an MS degree program; MIT offers an MBA program focused specifically on data science; etc.
This book is a great resource for anyone who wants to learn more about data science and its applications. The authors, Foster Provost and Tom Fawcett are both experienced business leaders with decades of experience in the field. They share their knowledge by explaining how the core concepts of data science can be applied to different industries such as marketing, finance, and healthcare.
The book covers topics such as
‚ÄúData Analytics: A Comprehensive Beginner‚Äôs Guide‚Äù
This is a great book for anyone who wants to learn about data analytics and data science. The authors, Foster Provost and Tom Fawcett know their stuff; they‚Äôve been doing this for years. This book will help you get started with some basic concepts, or if you already know what you need to know but need some help to integrate those concepts into your own career path (or if your boss wants an excuse).
Data visualization is a crucial skill that data scientists and analysts use to quickly understand and communicate their findings. In this book, you‚Äôll learn how to effectively visualize your data with Tableau Public, one of the most popular software packages for visualizing large sets of unstructured text or tabular data.
This book covers everything from basic concepts like visualizing numbers in charts and maps (including tips on making them more meaningful), all the way up through advanced topics such as building interactive dashboards in Tableau Public and creating beautiful presentations using PowerPoint templates. If you‚Äôre new to this field or looking for something more advanced than the average business analytics textbook has offered thus far ‚Äî or if you just want some extra guidance beyond what‚Äôs available online ‚Äî this book will help set your skills up for success!
‚ÄúR for Data Science‚Äù by Garrett Grolemund and Hadley Wickham
R is a statistical programming language that has been used for data analysis, data visualization, statistical modeling and machine learning. It‚Äôs free and open source (GPL), making it one of the most popular choices for data science. The book covers both R Basics & Advanced topics, as well as advanced techniques such as machine learning with predictive models in R.
Python is a powerful programming language for data science. It‚Äôs easy to learn, has many useful libraries and packages, and can be used in many different ways.
Jake VanderPlas is one of the leading Python data scientists in the industry today. He‚Äôs written several books on Python that are highly regarded by beginners and experts alike: ‚ÄúPython for Data Science Handbook‚Äù (1st Edition), ‚ÄúData Science By Example‚Äù (2nd Edition), and ‚ÄúPython Data Science Cookbook‚Äù. In his latest book he covers everything you need to know about using this popular open source language effectively when working with large amounts of data ‚Äî from basic concepts through advanced ones like machine learning models, text mining techniques or artificial intelligence algorithms ‚Äî all while keeping things simple enough so anyone can understand them
SQL is the most popular language for data analysis and data science. It‚Äôs also a programming language used to query databases, which are essentially collections of data stored in one place.
SQL has many similarities to Python, R and other languages such as Java or JavaScript. It can be used to solve many problems that don‚Äôt require using another tool like Hadoop or Spark (the latter two being more specialized).
A Tour of Machine Learning Algorithms by Jason Brownlee
This book is a journey through the world of machine learning algorithms, and it covers everything from linear regression to deep learning. It‚Äôs a great resource for anyone who wants to learn more about ML and can‚Äôt find the time or resources to do so on their own. The author describes each algorithm in detail, including how it works, what tasks it‚Äôs used for (e.g., image classification), how it compares with other algorithms for those tasks, how you choose between them depending on your needs (supervised vs unsupervised), etcetera ‚Äî and then he gives practical examples using Python code that walk you through each part of this process step-by-step!
‚ÄúIntroduction to Data Science‚Äù by Bill Howe
As the title suggests, this book is aimed at people without any background in computer science or programming. While it doesn‚Äôt go into as much detail as other programming texts, it does cover the basics of data science and provides some excellent reference material. As such, this is a great choice if you want an introduction to data science but don‚Äôt have time for a more comprehensive guide.
We hope this list has helped you decide on the best books to read in 2022. You may even find new ways of doing your job or help a friend with their own data science and analytics project.
Shubhi Ahuja
NYU Center for Data Science
EV Quotient
NYU Center for Data Science
Nick Canfield
in
Level Up Coding
Mark Ai Code
Jeroen van Zeeland
in
Towards Data Science
Lasha Gochiashvili
in
Towards Data Science
"
105,https://medium.com/mlearning-ai/8-of-the-best-data-science-youtube-channels-2022-65f5003b2bd3,8 of the Best Data Science Youtube Channels (2022),"Most of the channels in this list are not on other lists from other articles.
Before I start, I would like to preface this article by saying that most of these channels are not on other lists. Another thing is that while you watch these channels remember to code along and remember that what you are watching is not just a video, it is something to interact with.
Currently, Machine and Deep Learning are some of the fastest-growing fields. These two fields require engineers to have skills like Statistics, Mathematics, Data Analysis, and Coding Machine Learning and Deep Learning Models.
The job market has a lot of value for those looking to pursue a career in these topics. If you would like to learn Machine and Deep Learning, look no further than YouTube. There is so much free content in today‚Äôs digital world and these are some of the best channels that produce them.
Here are some of the best channels to learn topics like Artificial Intelligence, Machine Learning, Deep Learning, and Data Science.
If you are new to the field of data science and are looking to learn more about TensorFlow or PyTorch, Daniel Bourke has a great course with Zero-To-Mastery for both of them. The basics of PyTorch and TensorFlow are on his YouTube channel for free.
www.youtube.com
On Coursera, DeeplearningAI has one of the most popular specializations in TensorFlow. Andrew Ng, the co-founder of Coursera, created DeepLearningAI and also posts some parts of the courses on the YouTube channel. The YouTube channel has playlists for the courses which you can watch for free!
www.youtube.com
One of my favorite channels to learn Deep Learning is Aladdin Persson. Aladdin does a thorough job of explaining complicated concepts like Transformers, types of GANs, etc. He also does helpful implementations of a variety of topics.
www.youtube.com
This youtube channel has tutorials on many topics varying from Python, machine learning, finance, data analysis, robotics, web development, game development, and more. Whether you are a beginner or more advanced, you will find videos worthy of your time.
www.youtube.com
Andrej Karpathy, one of the most famous and influential people in the field of AI started a YouTube channel and recently started uploading more. If you would like to learn about Language Models, he has a great series for that.
www.youtube.com
The Lex Fridman podcast is an extremely popular and well-known channel for Machine Learning. Lex Fridman is an AI researcher who works at MIT. On his podcast, he interviews familiar names such as Elon Musk, Andrej Karpathy, Andrew Ng, Yahn LeCun, etc. Most of his podcasts are not about AI but some still are.
www.youtube.com
3Blue1Brown has a 4 part series exploring Neural Networks and some of the math behind them. If you are lacking skill in the technical part of NN‚Äôs, his videos are for you.
www.youtube.com
One of the most entertaining AI Youtubers is Code Bullet. If you would like to sit back and relax while still learning some machine and deep learning theory, Code Bullet is the channel to watch. From absolutely destroying games with AI to solving Rubix Cubes with AI, Code Bullet has a wide catalog of entertaining AI videos.
www.youtube.com
Thank you for reading my list, hopefully, these channels help you on your journey! If I missed any relatively unknown YouTube Channels, please tell me them in the comments.
medium.com
Neutrinos
Deepanshu tyagi
Heiko Hotz
in
Towards Data Science
XQ
in
The Research Nest
Truong-Phat Nguyen
in
BoringPPL
Stan Garfield
Babadeolayinka
Fayokunmi Somoye
"
106,https://medium.com/dev-genius/ceph-next-gen-cloud-storage-system-1ee91fbbeca4,Ceph ‚Äî Next Gen Cloud Storage System,"What is Ceph? Ceph is an open source, distributed, scaled-out, software-defined storage system that can provide block, object, and file storage. The clusters of Ceph are designed in order to run on any hardware with the help of an algorithm called CRUSH (Controlled Replication Under Scalable Hashing).
The CRUSH algorithm enables the client to independently computes where data should be written to or read from. By deriving this metadata dynamically, there is no need to manage a centralized table.
Servers can perform a CRUSH lookup very quickly; moreover, a smaller computing load can be distributed across cluster nodes, leveraging the power of distributed storage. This allows Ceph can quickly scale to hundreds of petabytes without the risk of bottlenecks and the associated single points of failure.
Ceph is a true unified storage solution that provides block, file, and object services from a single unified software defined backend. Ceph provides three main types of storage:
As the following picture show:
You are free to run Ceph on any hardware that matches Ceph‚Äôs requirements. This is a major development in the storage industry, which has typically suffered from strict vendor lock-in. Also Ceph prefers consistency as per the CAP theorem, and will try at all costs to make protecting your data a higher priority than availability in the event of a partition.
Active releases of Ceph are showing below:
Ceph comes with the following benefits:
In Ceph, the core storage layer is called RADOS (Reliable Autonomous Distributed Object Store), it provides an object store on which the higher-level storage protocols are built. RADOS is:
The RADOS layer consists of a number of object storage daemons (OSDs) and each OSD is completely independent and forms peer-to-peer relationships in the entire cluster. Then each OSD is typically mapped to a single disk, in contrast to the traditional approach of presenting a number of disks combined into a single device via a RAID controller to the OS.
The other key component is the MONs (monitors). The name here is very misleading, because MONs not only do cluster monitoring, but act as arbiters, traffic cops, and physicians for the cluster as a whole.
Monitors are responsible for forming a cluster quorum via the use of Paxos. Monitors use cluster maps to provide cluster state, and these cluster maps are used by both Ceph cluster components and clients to describe the cluster topology and enable data to be safely stored in the right location.
Manager is responsible for Ceph cluster configuration and statistics. In order to track the state of every single object in the cluster, Ceph hashes the underlying object names to place objects into a number of placement groups.
Then it use CRUSH algorithm to place the placement groups onto the OSDs. This significantly reduces the computation power needed for tracking millions of objects to a matter of tracking a much more manageable number of placement groups, normally measured in thousands. It responsibilities include:
The Ceph core services architecture diagram is below:
The RBD service presents block (also known as volume) storage in a fashion that with traditional HDD/SDD applications can consume with little or no adjustment. It breaks up block-based application data into small chunks, and these small chunks are stored as objects within RADOS. RBD orchestrates the storage of the objects in virtual block devices
Ceph Object Gateway is an object storage interface built on top of librados to provide applications with a RESTful gateway to Ceph Storage Clusters. Ceph Object Storage supports two interfaces:
Under the hood, Ceph Object Gateway uses RGW (An HTTP server) for interacting with underline storage.
The Ceph filesystem is a POSIX-compliant file system built on top of RADOS. CephFS endeavors to provide a state-of-the-art, multi-use, highly available, and performant file store for a variety of applications, including traditional use-cases like shared home directories, HPC scratch space, and distributed workflow shared storage.
In CephFS, file metadata is stored in a separate RADOS pool from the file data and served via MDS (Metadata server). Access to data is coordinated through the cluster of MDS which serve as authorities for the state of the distributed metadata cache cooperatively maintained by clients and MDS.
Based upon RADOS, the librados API enables you to create your own interface to the Ceph Storage Cluster. With librados, you can interact with the two types of Ceph daemons:
The following are some general rules for deploying a successful Ceph cluster:
Patrick Ren√©
Agata Radzi≈Ñska
in
Pragmatists
Ruby Radar
Tom Elliott
in
average-coder
Amit Kumar
Qiang Hao
in
Better Programming
Talha Tahir
Ibrahim Ates
in
Level Up Coding
"
107,https://medium.com/@anna.wu9222/sql-case-question-with-detailed-answers-step-by-step-82e82115882b,SQL Case Question with Detailed Answers (Step-by-Step),"SQL Schema
Prompt: You are given data for arena attempts by a player. The arena is won if 6 matches are won. The arena is lost if 2 matches are lost. Find the average number of match wins per arena attempt for this player. Round to 2 decimal places.
Example
Prompt (A) Output
Want to practice this problem in a real SQL engine? Learn more here!
First, we need to count the number of wins per attempt:
Using Step 1 in a CTE (called ‚Äúwins‚Äù) we can take the resulting wins number and find the average:
Sahil Jha
in
stray stream
ismail yenig√ºl
in
FAUN Publication
Daniel Kioko
Michael Gruber
Yst@IT
based on a true story
Liam Connors
in
Python in Plain English
IncubateX_NFT
in
Incubatex_NFT
"
108,https://medium.com/wwcode-python/5-data-engineering-project-ideas-276f29c8885f,5 Data Engineering Project Ideas,"‚ÄúPortfolios are everything, promises are nothing. Do the work.‚Äù ~ Chase Jarvis
You may have heard about this.
Hmmm‚Ä¶What is this new area? You may know about data science and machine learning, but what is data engineering? Is this a new technology or something that has always been part of the industry, repackaged?
Ken Jee
in
DataDrivenInvestor
Diego In√°cio
in
Analytics Vidhya
Saul Shanabrook
Technical Source
Teon Brooks
Technical Source
M Khorasani
in
Towards Data Science
Juilee Talele
"
109,https://medium.com/jovianml/fastest-way-to-become-a-data-analyst-in-2022-from-scratch-58db8a77dec5,FASTEST Way To Become A Data Analyst in 2022! (from Scratch),"Data Analyst is one of the most trending and demanding jobs of 2022. If you want to make a career in Data Analysis and are confused as to where to start, then this article is for you!
To start with, you must be clear about the role of a Data Analyst.
#1 Role of a Data Analyst
A data analyst :
#2 Educational Background Needed
Most people think that a technical degree is needed to get into the field of Data Analysis.
But this is not true!
In most entry-level jobs you just need a minimum of a Bachelor‚Äôs degree. If you have completed a graduate-level degree in Mathematics, Economics, Statistics, or any other field then you are eligible to sit for Data Analyst jobs.
Although the requirements totally depend on the company you are applying for, the minimum criteria for most entry-level jobs are just a Bachelor‚Äôs degree in any field!
#3 Technical Skills Needed (Roadmap)
Luke Barousse, a famous YouTuber and a data analyst, scrapped a lot of LinkedIn job openings and came up with these facts :
Keeping this in mind, I have created a complete roadmap to becoming a Data Analyst.
So, let‚Äôs start!
#4 Soft Skills Needed
Some soft skills needed are : - Communication.- Critical thinking.- Problem Solving.- Analytical.
#5 Resources to follow
Excel : https://www.udemy.com/course/microsoft-excel-2013-from-beginner-to-advanced-and-beyond/SQL : You may go through this video . After watching it, you should practice more on HackerRank.Python : https://www.youtube.com/watch?v=r-uOLxNrNk8Tableau : https://www.udemy.com/course/tableau10/
Follow this roadmap and it will surely help you get a job as a Data Analyst in your dream company.
Hope you liked it :)
Lotanna Nwose
in
Data Science, Thoughts & Prayers
Blal Mansor Y
Hana ≈†turlan
in
Analytics Vidhya
marcus evans online events
in
marcus evans online events
Jaekang Lee
Abdelkader YAGOUB
in
Data has a better idea
Kabbajjalil
Priyen Logeswaran
"
110,https://medium.com/didact-publication/clear-code-how-to-write-code-that-is-easy-to-read-fc8f12820592,Clear Code: How To Write Code That Is Easy To Read,"This article is a follow up to a tweet I made on how I deal with my poor ability to remember implementation (code). It may seem funny to you, but I do actually tend to forget what I write shortly after writing it.
A K M Intisar Islam
in
Better Programming
automation feed
in
Better Programming
Double Protocol
Shawn Xu
Mustafa Kapuz
Javokhir
Jonathan Campos
in
Google Cloud - Community
Gunnar Peipman
"
111,https://medium.com/towards-data-science/on-probability-versus-likelihood-83386b81ad83,On Probability versus Likelihood,"From the perspective of machine learning and data science, probabilities and likelihoods are used to quantify uncertainty, or perhaps how probable it is that an observation belongs to one class or another. They crop up when looking at confusion matrices; and indeed, algorithms like Naive Bayes classification are pretty much probabilistic models. The reality is that data scientists cannot escape these concepts.
In everyday language, though, we tend to use the terms probability and likelihood almost interchangeably. Indeed, it‚Äôs not uncommon to hear things like ‚Äòhow likely is it to rain today?‚Äô or ‚Äòwhat are the chances of this or that happening?‚Äô And I‚Äôll be honest; it wasn‚Äôt a distinction that was initially apparent to me, but probability and likelihood are different (albeit related) concepts. This interchangeability can often creep into our work as data scientists, so it‚Äôs important to be clear about the distinction between the two terms. So, in this post, I thought I‚Äôd have a crack at illustrating the difference between these two ideas.
In a nutshell, the fundamental difference between probability and likelihood lies in what is allowed to vary ‚Äî let me explain.
What you need to know is that likelihood ‚Äî though to be specific, I‚Äôm actually talking about the likelihood function ‚Äî is actually derived from a statistical model and is considered a function over the parameters that generate data from that model. Confused? Don‚Äôt worry ‚Äî let's be a little more concrete and consider tossing a fair coin ten times.
We know that this is a Binomial process that is governed by a rate parameter, p, that defines the expected probability of throwing K heads (or tails ‚Äî it doesn‚Äôt matter which) out of a total of N throws. Here, the number of observed heads, K, is a random variable that changes in response to different values of p. For any fixed parameter value (e.g., p = 0.7), the probability of observing K heads is given by the probability mass function (PMF):
where
Now, because I said the coin is fair, and it was tossed ten times, we can let p = 0.5 and N = 10. If we then plug these values into the equation above ‚Äî and let K vary ‚Äî we get something that looks like the left panel of Figure 1. Based upon this, we can see that K = 5 is the most probable outcome, which should make sense: if the coin is fair, and we toss it ten times, we should expect that ‚Äî in the long run ‚Äî we‚Äôll get 5 heads and 5 tails more often than not. But you should also note that getting 4 or 6 heads isn‚Äôt all that uncommon, either.
So, what the PMF tells us is how probable particular realizations of the random process are given a fixed set of model parameters. In this instance, we assume the model parameters are fixed and it is the data that is free to vary. The takeaway here is that, if the model parameters are known, then we are asking questions about the kinds of data that might be observed.
Okay, so what if instead, I tell you that I already flipped the coin ten times and got K = 7 heads? My question to you is whether the coin I threw was fair or not.
What‚Äôs important to note is that, in this case, K is no longer random ‚Äî we have an observed realization of the Binomial process meaning it is now a fixed value. Given this information, the likelihood function for the Binomial model can be written as:
Here, I‚Äôm using ‚Äúhat‚Äù notation to make it clear that K is the observed number of heads from ten throws. Now, this just looks like I‚Äôm saying that the likelihood function is the same thing as the PMF ‚Äî well, I kind of am saying that. But, the difference here is that now the data is fixed and it is the model parameter, p, that is free to vary.
What the likelihood function gives us is a measure of how likely any particular value of p is given that we know that K equals some observed value. Just like above, if we plug K = 7 into the equation above ‚Äî letting p take on all possible values ‚Äî we‚Äôd get something like the right panel in the above figure. Note that here the likelihood function is not symmetrical; rather, it is peaked over p = .7. Specifically, the mode of this distribution (i.e., the peak itself) coincides with something called the maximum likelihood estimate (MLE). What‚Äôs that? Well, it‚Äôs the value of p that is most likely given the observed data ‚Äî the name sort of gives it away, really. I‚Äôll talk more about MLE in another post, but for now, all that you need to know is that value of the likelihood function at the MLE is approximately .27.
Okay, before moving on, a note of caution: I know the likelihood function looks like a distribution function, but it is not a proper probability density function (i.e., it typically doesn‚Äôt integrate to 1). More importantly, the likelihood function is not the probability that p equals a particular value (for that you‚Äôll need to compute the posterior distribution. I‚Äôll talk about that in another post).
Right, moving on.
Remember when I said that the likelihood function is interpreted as a measure? Well, specifically, it can be used to quantify statistical evidence. I also said that all you needed to know about the MLE is that it has a likelihood value of approximately .27. But what does this tell us? What does it mean to say that the likelihood is approximately .27? It certainly doesn‚Äôt tell us whether or not the coin I used was fair or not ‚Äî so where to from here?
I need to introduce a couple of things:
Taking these ideas together implies that looking at a single likelihood in isolation tells us very little. Yes, it provides a measure of evidence, but relative to what? The definitions above tell us that likelihoods are more useful when they‚Äôre compared to other likelihoods. That‚Äôs why we typically prefer relative measures such as likelihood ratios.
With this in mind, recall my question to you was whether, based on the data you have, the coin I tossed was fair. To properly address this question we also require the likelihood of observing the data if p = 0.5, which is approximately .12. Note that both these points are marked on the likelihood function in Figure 1. Given these two pieces of information we can now compute the likelihood ratio:
What does this give us? Approximately 0.44. You‚Äôll need some context.
In this example, the likelihood ratio quantifies the degree to which the data supports the claim that p = 0.5 (fair coin). If this ratio is 1, there is no evidence either way. If the ratio is greater than 1, then the evidence is in favor of the numerator (here, p = 0.5). However, if the ratio is less than 1, then the evidence supports the denominator (here, p = 0.7). So, it appears we have evidence against the fair coin hypothesis. In fact, if we take the reciprocal (i.e., 1 / .44) we see that p = 0.7 is approximately 2.3 times more likely than p = 0.5 (bear in mind that is a comparison with only one alternative‚Äî there are other possibilities other than p = 0.7).
In reality, this is a trivial result, and the likelihood principle, along with the law of likelihood, means that the parameter value that maximizes the likelihood function is the value most supported by the data. The question is then whether this is convincing enough evidence that the coin I flipped was unfair. That‚Äôs a topic for another day.
I hope that this post made clear the difference between probability and likelihood. To recap: probability is generally something we consider when we have a model with a fixed set of parameters and we are interested in the types of data that might be generated. Conversely, likelihood comes into play when we have already observed data and we want to examine how likely certain model parameters are. In particular, we can use likelihood ratios to quantify the evidence in favor of one value over another.
Thanks for reading, and feel free to leave me a comment below.
If you enjoyed this post and would like to stay up to date then please consider following me on Medium. This will ensure you don‚Äôt miss out on new content. You can also follow me on LinkedIn and Twitter if that‚Äôs more your thing üòâ
StephenwithaPhD
in
Cantor‚Äôs Paradise
NO√âL
Gabriel Miranda
Eli Goz
StephenwithaPhD
in
Y(MATH)
Naren Suresh
David Sasu
StephenwithaPhD
"
112,https://medium.com/towards-data-science/graphs-with-python-overview-and-best-libraries-a92aa485c2f8,Graphs with Python: Overview and Best Libraries,"A graph is a relatively old mathematical data entity that is a set of connected elements. Since the graph is a very flexible structure and allows you to store information in a form familiar and convenient to humans, graphs have always been used in computer science and technology. With the rise of machine learning and deep learning, graphs have gained even more popularity by creating the field of graph machine learning.
In this post, I would like to share with you the most useful Python libraries I‚Äôve used for graph/network analysis, visualization, and machine learning. Today, we will review:
Before that, let me tell you a few words about graph theory and graph machine learning and provide some learning resources that may be helpful to you. If you don‚Äôt know what graph or graph machine learning is, that is a great opportunity to lift the veil of secrecy!
The graph is simply a set of elements connected to each other.
However, the fact these elements (called nodes) can contain any information and can be connected in any way (with edges) makes the graph the most general data structure. Indeed, any complex data familiar to us can be represented as a simple graph: for example, an image ‚Äî as a grid of pixels or text ‚Äî as a sequence (or chain) of words.
You might wonder: are graphs really so important? Well, some tasks simply cannot be solved or even formulated without them, as some information cannot be structured in data. Imagine the following situation: you need to visit a list of cities, say for tourism or for work. You have information about the distance from one city to another, or say, the cost of tickets for different transport modes ‚Äî it‚Äôs even more interesting! How to create an optimal route, that is, spend the minimum amount of money or drive a minimum distance?
For me, the task is quite practical ‚Äî think at least about its application in logistics. And, this is an example of a problem that cannot be solved without the help of graphs. Think about how you will represent the data and in any case, you will still come to the weighted graph (a graph whose edges have some value, called weight). By the way, if each city needs to be visited exactly once, this task turns into the famous traveling salesman problem (TSP), which is not so easy to solve. One of the reasons is that the number of possible routes is growing very fast, and even for 7 cities, there are already 360 of them!
Graph theory (originated in the 18th century) was engaged in the study of graphs and solving various graph problems: finding a possible or optimal path in a graph, building and researching trees (a special type of graph), and so on. Graph theory was successfully used in social sciences, chemistry, biology, and other fields. But with the development of computers, the process of using graphs has reached another level.
What is really important is that this base: a set of related elements, often with different elements and types of connections, is very useful for modeling real-world tasks and datasets. This is the place where graph machine learning comes into the picture (although amazing tasks were solved before it as well). After humanity collected the appropriate datasets and developed technologies to model them (like Graph Convolutional Networks (GCNs), by analogy with Convolutional Neural Networks (CNNs)) it becomes possible to solve a wide range of graph tasks:
Let‚Äôs take a look at examples of graphs from real life. One of the most famous graph datasets is the karate club dataset. Here, each node is a person (club member), and each edge represents the two members who interacted outside of the club.
A common problem is finding two groups of people into which the club split after an argument between two instructors (now we can treat it as binary (or 2-class) node classification). The dataset was collected back in 1977 and become a classic example of a human social network or community structure.
Another graph type, interpretable for humans, and therefore extremely useful for machine learning models is a knowledge graph. In a knowledge graph, a node is some entity or concept and an edge represents knowledge about the interaction of a pair of entities. Thus, the node-edge-node structure stores a certain fact about the world or a particular system.
The knowledge graph in the example above contains two types of edges: is and eat and is thus a multigraph we introduced earlier. The Dogs-is-Animals structure gives us the knowledge that the ‚Äúdogs‚Äù set is a subset of the ‚Äúanimals‚Äù set, or, in simpler terms, that dogs are animals.
Wikidata is a huge free knowledge base by Wikipedia, which is constantly updated and has more than 100 million nodes now. There are more than 400 edge types, some of which are part of, different from, opposite of, population, and location, so definitely make sense.
That huge knowledge base contains a lot of information about the world around us. It‚Äôs still amazing to me how humanity has collected this data, and that machines are now able to process it!
One more thing I can‚Äôt keep silent about is wikidata's beautiful visualization capabilities. For example, here you can see the plot of connectivity of the United States states. Note that it is not drawn by anyone, it is just a subgraph of the entire wikidata graph: we took only American states as nodes and P47 (shares border with) as edges.
Take a look at Wikidata Graph Builder and other visualizations. Let me point you to some of them that I find entertaining:
If after that brief overview you are now interested in graphs and want to know more about them, I refer you to the wonderful Gentle Introduction to Graph Neural Networks by Google Research. In this article, you can find more examples and interactive visualizations.
Check the Graph Theory Algorithms course by freeCodeCamp.org for various graph theory algorithms overviews or Stanford CS224W: Machine Learning with Graphs course to start your graph machine learning journey.
After that brief introduction, let‚Äôs actually start with Python libraries!
If you have to do some operations on graphs and you use Python as your programming language, you will most likely find the NetworkX library pretty quickly. It is probably the most fundamental and commonly used library for network analysis that provides a wide range of functionality:
The library is pretty intuitive and easy to use. Also, the majority of fundamentals, like graph data structures will remain the same or at least similar for all popular graph libraries. For clarity, you can create a simple graph and visualize it with the following code:
When it comes to algorithms, networkx is pretty powerful and has hundreds of graph algorithms implemented.
To summarize, this is an efficient, scalable, and powerful library, that will definitely be useful for you if you are dealing with graph analysis.
networkx.org
Using networkx for graph visualization can be pretty good for little graphs but if you need more flexibility or interactivity, you better give PyVis a chance. The situation is similar to matplotlib vs plotly. Using matplotlib for quick and straightforward visualizations is perfectly fine, but if you need to interact with your chart or present it to somebody else, you better use more powerful tools.
PyVis is built on the VisJS library and produces interactive visualizations in your browser with simple code. Let‚Äôs plot the same graph as in the example above.
This code will create a graph.html file. By opening it, you will be able to interact with your visualization: zoom it, drag it, and much more.
Looks interesting, right? The library even allows you to use web UI to dynamically tweak display configurations. Definitely check the official tutorial that will walk you through the main library‚Äôs capabilities.
pyvis.readthedocs.io
Let‚Äôs now switch to the more advanced topic ‚Äî graph machine learning. I will mention two of the most popular libraries for it: DGL and PyG.
DGL (Deep Graph Library) was initially released in 2018. In contrast to PyG (PyTorch Geometric), which is built on top of the PyTorch and therefore supports only PyTorch tensors, DGL supports multiple deep learning frameworks, including PyTorch, TensorFlow, and MXNet.
Both libraries implement popular Graph Neural Network (GNN) cells such as GraphSAGE, GAT (Graph Attention Network), GIN (Graph Isomorphism Network), and others. It will not be difficult to build a model from pre-made blocks ‚Äî the process is very similar to plain PyTorch or TensorFlow.
Here is how you can create a 2-layer GCN model for node classification in PyG:
And the same code for DGL:
Both code snippets are pretty straightforward if you are familiar with deep learning and PyTorch.
As you see, the model definition is very similar for both libraries. The training loop then can be written on the plain PyTorch for PyG and require some modifications for DGL (since DGL graph objects store the entire dataset, and you have to address train/validation/test sets using binary masks).
There is a slight difference in data representation here: you can see it at least based on the different input parameters for the forward method. Indeed, PyG stores everything as PyTorch tensors and DGL has a separate graph object that you have to use, and under the hood, it follows a more classical NetworkX style.
However, that is not a big deal ‚Äî you can convert the PyG graph object to the DGL graph and vice versa with a few lines of code. The more important question is: how else are they different? And which one should you use?
Trying to figure out which of the libraries is better, you will keep coming across the same answer ‚Äî ‚Äútry both and decide which works best for you‚Äù. Okay, but how are they at least different? Again, the answer that you will constantly encounter is ‚Äúthey are quite similar‚Äù.
And they really are! Moreover, you saw it for yourself by looking at the code a few minutes ago. But of course, you can find some differences digging deeper: here is a good resource list including a few thoughts by library authors, and here is a pretty detailed comparison on different sides.
In general, the answer is really to try both. In fact, DGL has more low-level API and can be harder to use in the sense of implementing new ideas. But this makes it more flexible: DGL is not limited to message-passing networks (classical Graph Convolutional Networks) and has the implementation of several concepts that PyG can not provide, for example, Tree-LSTM.
PyTorch Geometric, on the other hand, makes his API as easy as possible and then gains more popularity among researchers that can quickly implement new ideas, i.e. new GNN cells. In the past time, PyG becomes more and more popular due to important updates with PyG 2.0 and active and powerful teams of collaborators, including Stanford University.
So I still encourage you to try both of them, giving PyG the chance first.
If you are working on a relatively familiar graph problem (be it node classification, graph classification, etc.), both PyG and DGL have a huge amount of GNN cells implemented. Also with PyG, it will be easier for you to implement your own GNN as part of any research.
However, if you want to get full control over what is happening under the hood or implement something more complicated than the message-passing framework, your choice will most likely fall on DGL.
The target audience of this article (people interested in graphs) is quite small. Well, machine learning is a fairly young field of computer science, and graph machine learning is even younger. The last mainly attracts the attention of the research community, but, believe it or not, it is used in important real-world applications such as recommendation systems and biology/chemistry studies.
In any case, I hope these materials were interesting or helpful for you ‚Äî whether you were looking for anything specific or just learned something new today. As a recap, today we briefly reviewed what graph and graph machine learning is, and took a look a the following libraries:
HRUSHIKESH SHELAR
in
Analytics Vidhya
Autodesk University
in
Autodesk University
Andrej Baranovskij
in
Katana ML
Matt Sosna
in
Towards Data Science
Ankit Gupta
Great Learning
Growexx
T.Ferguson
"
113,https://medium.com/@Armonia1999/data-analysis-project-using-sql-to-clean-and-analyse-data-64a24e84b730,Data Analysis project- Using SQL to Clean and Analyse Data.,"In this project we use a dataset from ‚ÄòReal World Fake Data‚Äô , Import it to MySQL Workbench, Clean it a bit then analyse it. At last, as always, I like to use Tableau to visualize the data.
Welcome back everyone üñ§, and to those who are new to my page here, ermmm‚Ä¶ hello ? *stares judgementally*
Today we will go through how import CSV files into MySQL Workbench, clean the data, analyse it and then visualize using Tableau. Let‚Äôs start !
The dataset we are using is from my all time favourite, master above masters, Real World Fake Data.
They have A M A Z I N G datasets that are very interesting and in a variety of fields so I am sure you will find one that would suit you ! The one that I chose is the ‚Äòcall center‚Äô dataset. But you can easily choose whatever one you want and just follow my steps here.
The call-center dataset basically has over 32,900 records of data that describers calls made to various call centres. It includes the ID of the call , duration of the call in minutes, the name of the person who called, their satisfaction score and many other attributes that you will see as we go.
Anywayyyy, we downloaded our dataset, now it‚Äôs time to import it to MySQL Workbench.
Really Reallyyyyy important note: This process of Importing our data to MySQL I did by following this brilliant YouTube video. So if you can spare 10 minutes to watch it and follow with the video steps it‚Äôs wayyyyy better than my explanation here. So watch it and then jump to step 2 ! If you trust me , continue with your own self to blame..
Another not so important note: The code is on my GitHub profile, I recommend to open the code in a separate tab and watch step by step from there since it‚Äôs clearer. That‚Äôs where I post all of my projects. Link here.
First off, we need a database that we will import our data into it. We can either create a new one or just use an existing one. I chose the former.
Here I just went off and created a database called Project. Note that the two dashes at the beginning of the line makes it a comment and it doesn‚Äôt affect our code. Next, to get to work on a specific database we need to select it, and we do that with the keyword ‚Äòuse‚Äô.
After that, we need to create a table that will fit our data and match it. Let me explain that.
Here is a snapshot from the CSV file we have:
You can see that we have 12 columns: ‚Äòid‚Äô containing the, well, Id of the record. Then we have a customer name column, the sentiment that describes the call and so on..
To load that data into a table in the database we need to create a table that will match it. Like this:
Here we are creating a table called ‚Äòcalls‚Äô, and we are designing it in a way to fit our data by matching the columns and data types. We go column by column in our csv file and in the same order we build our calls table. It is not that critical as we can change that while we import our data.
We create columns in this way ( at least in this example): first off we specify the column name, then we add the data type of that column and in the parenthesis we choose the size of the variable. Check this guide here.
Specifying the size of the variables without checking the CSV file before proved to be stupid. By specifying that city, or state , or whatever it is , is of max size of 20 meant that rows that will be more than 20 characters will not be imported. It wasn‚Äôt that critical though, I lost almost 20 rows out of 32,900+ records that was too lazy to fix, no biggie. But I hope you will double check everything you do before creating a table.
Ok, now we have our table calls that is waiting to be fed with data. Here is where our magic works, quite literally. We go to the left panel of the MySQL workbench, go to the database we use> the table we just created:
If you cant find the database or the table, make sure to refresh by pressing the little arrows button next to schemas.
When you right click, there is an option called ‚ÄòTable Data Import Wizard‚Äô. That is the wizard that will help us ! click on it, it will open a window for you to browse for the csv file you want to import, search for it and then we you finish press next:
After you press next, it will ask you for the destination where you want the data to go:
We want the data to go to the table we created so choose the ‚ÄòUse existing table‚Äô and then press next.
Now we need to open our eyes very well here, because here we are finalizing the process by making sure everything is good:
As you can see, it shows us the CSV columns and their destination columns in our table. But Lo and behold , in the highlighted areas it shows us that the csat_score will go to sentiment column in our table, and that the reason column will go to the city column *face palms*. So make sure you select the right column from the dropdown menu ! Ah right, at the end we can see a preview of our beloved csv file. Then after making sure the columns are matching press next and continue. I can‚Äôt do that since I already imported my data so, continue :)
To make sure everything went smoothly, let‚Äôs take a look at our table:
The table is way too big to see all of it, so let‚Äôs see first 10 rows. This is the result:
Looks like everything is good‚Ä¶ you thought !! Anyway let‚Äôs go to step 2 and explain there.
My last note, I swear ! There is another faster and neater way to import files, but I didn‚Äôt vibe at the time with using it. Maybe in the future. explanation here.
Yep, small minor inconveniences happen. Back to where we were at the last part in Step 1. Let me introduce you to our two small bugs:
Let‚Äôs fix them:
We need to set the SQL_SAFE_UPDATES off before we do change the column. reason is because we dont specify a where clause that uses a KEY column. That is why we set it off before the query, and then set it back on after. and the results?
The call_timestamp is finally date format ! yay.
Now to our next problem. The zero‚Äôs in the csat_score. There are two options: either we set them to NULL, or we just leave them be and then when we query the table we just add the clause WHERE csat_score != 0. But I will set them to nulls in this way:
(Make sure to add set the safe updates off and then on, just like we did earlier).
Now let‚Äôs see the table:
Sweet. Now the we finished cleaning, Let‚Äôs go to Step 3.
I wonder what is the shape of our table, i.e, the number of columns and rows. Let‚Äôs see:
Running the first line gives us:
And the second line gives us:
So we 32,918 records and 12 columns.
Next up: Distinct values.
We are checking the different values possible for the rows we selected. Let‚Äôs run a random one of these , since they are all similar:
So looks like we have only 4 call-centres. Let‚Äôs continue :
To see the distribution of our calls among different columns. Let‚Äôs see the reason column:
Here we can see that Billing Questions amount to a whooping 71% of all calls, with service outage and payment related calls both are 14.4% of all calls.
Moving on, which day has the most calls?
Friday has the most number of calls while Sunday has the least.
Now let‚Äôs move to some aggregations:
Again, most of them follow the same logic with minor changes. Let‚Äôs run a few of them , starting with the one at line 106, querying the min, max and average call duration in minutes:
Then let‚Äôs check line 108 and 109:
Here we are checking how many calls are within, below or above the Service-Level -Agreement time. For example we see that Chicago/IL call center has around 3359 calls Within SLA , and then Denver/CO has 692 calls below SLA. you get it.
At the end I just added one Window Function to query the maximum call duration each day and then sort by it.
Here we see that for example on Oct 4th the maximum call duration was 45 minutes long while on Oct 8th it was 27 minutes long.
That‚Äôs it for SQL ! You can make as many queries as you want and change to suit your dataset. One thing to be said though, is I wished we had a database with multiple related tables so we can run some joins and more window functions, but I guess we can do that next time!
Before wrapping up, I made a dashboard using Tableau ! Link to it here.
Sneak peek:
Hope you all have a fruitful Data Analysis journey ! Have a nice week ~
Follow me for more projects coming up! Till next time ‚ù§ Peace out
Planet Crust
Submit My Press Release
Hannah Giles
in
Zensend Blog
Juan Matthew
Acadly
in
Acadly
Atlas
Simon Weniger
in
Mintgate
Jennie Autobot
"
114,https://medium.com/@AshBlogs/heart-disease-prediction-ab27d019eb74,Heart Disease Prediction,"Heart disease is becoming very common in older and younger adults starting from age 20. About 18.2 million adults suffer from coronary heart disease which makes it 2 in every 10 deaths in adults less than 65 years old. In the United States alone, about 659,000 people die from heart disease each year, making it 1 in every 4 deaths. It is estimated that 16.3 million Americans aged 20 and older have coronary heart disease. We analyze what factors can be used to predict if a patient has some level of heart disease present.
Machine learning is used in analyzing, extracting, and organizing information from a large set of raw data. Currently, Machine Learning as one type of artificial learning is being used to find the patterns in data and implement the result to the various business decision-making. The Healthcare sector is one of them. Machine Learning can assess heart disease risk for patients. Developing a predictive Machine Learning model can be used as an accurate diagnostic tool and formulate the most effective treatment plans for the patient and identify high-risk patients by their characteristics. With patients‚Äô symptoms, data sets, and records, predictions of the heart disease and insights for doctors to create the treatment plan can be accurately planned. Finally, it also helps patients to notice some early symptoms of severe heart disease so that they can go to the doctor for diagnosis and avoid missing the best time for treatment.
We have worked with the heart disease dataset from the Kaggle. The response variable indicates if there is the presence of heart disease in a patient with a scale of 0‚Äì4 (where 0 indicates no presence of heart disease & 1,2,3,4 indicates the presence of heart disease). For this particular dataset, the response variable is named ‚Äòtarget‚Äô that has 2 nominal values: 0 ‚Äî heart disease, not present, and 1 ‚Äî heart disease present.
Here is a brief preview of what the dataset looks like:
There are a total of 1025 observations. The original dataset contains 76 columns, but only 14 columns including the predicted attribute as these columns are the most significant. There are no null values present in this dataset.
The statistics of all attributes as shown below:
Looking at the minimums and maximums, we can see that most of the attributes have values that are nominal except age, trestbps, chol, thalach, and oldpeak.
Below is a detailed description of each of the attributes:
The class distribution for our response variable ‚Äî target, where 1 indicates having heart disease and 0 indicates not having heart disease:
We see that the target variable has 526 patients having heart disease and 499 not having heart disease. The class imbalance is not significant for the target variable in our case. Hence, the model accuracy and the confusion matrix are the evaluation metrics that evaluate the performance of the models.
The distributions for all the 14 attributes for a better understanding of the data:
From the histograms above, we can see the age is slightly skewed to the left, meaning there are more older people in the dataset. There are more males than females in the dataset. The distribution of the ‚Äútrestbps‚Äù (resting blood pressure), ‚Äúchol‚Äù (serum cholesterol), and ‚Äúoldpeak‚Äù (ST depression induced by exercise relative to rest) are right-skewed. There are 9 categorical variables and 5 continuous numerical variables.
According to the correlation matrix shown above, even though there does not seem to be a strong correlation between any of the independent variables, we still addressed the slight multicollinearity present. The strongest positive correlation we see is between ‚Äúslope‚Äù and ‚Äúthalach‚Äù with a value of 0.4 and the strongest negative correlation is between oldpeak and slope of -0.58. Next, for the positively correlated attributes, we see that thalach and exang have a correlation of 0.38. For the negatively correlated attributes, we see that age and thalach have a correlation of -0.39, cp and exang have a correlation of -0.4, thalach and oldpeak have a correlation of -0.35, and thalach and exang have a correlation of -0.38. We also see that ‚Äúcp‚Äù, ‚Äúthalach‚Äù, and ‚Äúslope‚Äù, have the strongest positive correlation with the target variable with values of 0.43, 0.42, and 0.35 respectively. And ‚Äúexang‚Äù, ‚Äúoldpeak‚Äù, and ‚Äúca‚Äù have the strongest negative correlation with the target variable with values of -0.44, -0.44, and -0.38 respectively.
We will drop ‚Äúthalach‚Äù since it is highly correlated with more than one attribute to avoid any form of multicollinearity for further analysis. In addition, we will also drop ‚Äúslope‚Äù as it is highly correlated with oldpeak and is more weakly correlated with the target variable than ‚Äúoldpeak‚Äù.
This is a classification problem to predict if the patient has the disease or does not have the disease, so we used Python to run the supervised models and used weka to run the unsupervised model:
Supervised:
Unsupervised:
Association Rules
For the supervised models, we used python‚Äôs sklearn library to run the four classifiers and perform pre-processing on our data. We used the confusion matrix and the overall accuracy of the model as the evaluation metric but mainly focused on the recall for the models as we cannot afford to misclassify a patient having heart disease. Here‚Äôs the formula for recall: true positive/true positive + false negative. We will perform a 70:30 train/test split on our data.
We first ran all our classifiers on the unprocessed data to create a benchmark for the model results. Next, to improve our model performance, we applied discretization to the continuous variables along with grid search and cross-validation of 10 folds to tune the model hyperparameters and to optimize model performance. Lastly, we applied feature selection with grid search to reduce the data to the attributes that matter the most and noted the model performance. We used sklearn‚Äôs feature selection tool ‚Äî SelectKBest, to select the top k features for each algorithm using a pipeline, and then ran the algorithms on their respective selected features.
Finally, for the unsupervised model, we used weka to run the association rule on the dataset to see what set of factors related to having heart disease.
Looking at the results for the 4 classifiers after the train/test split, we can see that without any pre-processing, the decision tree classifier performed the best. KNN on the other hand, performed the worst especially looking at the recall which means the number of misclassified patients that truly have heart disease is quite high. We can see this from the confusion matrix that KNN misclassified 33% of patients as not having the disease when in reality, those patients did have the disease. Naive Bayes and Logistic Regression both yielded similar model results. To further improve the model performance, we applied a few pre-processing techniques.
Grid search uses different hyperparameter values and evaluates model performance. The best model is then chosen by the search, based on the hyperparameter value that yielded the optimal model performance.
For logistic regression, the hyperparameter we would like to tune is the C parameter. This is the inverse of regularization strength. Regularization is used to improve a model‚Äôs performance on unseen data by preventing overfitting the training data. Regularization gives a penalty to complex models by shrinking the coefficients of less contributive variables towards zero. The high value of C means that less weight should be given to the penalty while a low value of C means more weight should be given to the penalty. Smaller values of C are better as the training data might not be representative of the real world.
For the Decision Tree classifier, the hyperparameter that will be optimized is the max depth of the tree. This is a pre-pruning technique to avoid further splits once the tree has acquired optimal ‚Äòpurity‚Äô. We ran this classifier with the ‚Äòentropy‚Äô selected as the criterion.
For K-Nearest Neighbors, we simply optimize the k number of neighbors for the classifier through grid search. Finally, for the Naive Bayes classifier, we optimized the var_smoothing parameter. The var_smoothing hyperparameter smooths out the GaussianNB distribution curve and accounts for observations that may be outliers on the GaussianNB curve by adding the largest variance portion of all features to the variances.
Discretization
We applied discretization ‚Äúage‚Äù, ‚Äútrestbps‚Äù, ‚Äúchol‚Äù, and ‚Äúoldpeak‚Äù as they contain continuous values. Classifiers such as decision trees split on the attribute with the highest information gain. However, if there are too many unique values, the information gain will be biased. To avoid this situation, it is important to bin or discretize those attributes. This categorizes the values so that, for example, age 25 falls under the 20‚Äì25 category and does not just have a single category for 25.
We used python‚Äôs quantile-based discretization function (qcut) tool to discretize these attributes. This tool by default gives an equal number of instances per bin. This means that the bin intervals are not consistent. For example, the first bin contains a range of 20 while the next contains a range of 10 but each bin interval is selected to acquire approximately equal instances per bin. All these continuous attributes were divided into four bins except oldpeak, which was divided into 2 bins as it had a small range of continuous variables. The bins were then converted to ordinal values using sklearn‚Äôs LabelEncoder. For instance, the first age bin of interval (28‚Äì48) is replaced by 0, the next with 1, and so on.
Here are the results after applying the four classifiers on the discretized dataset and using grid search:
Based on model accuracies, we can see that applying discretization to the dataset has different levels of influence on the models. Firstly, looking at the overall accuracy of the models, we can see that KNN has significantly improved from the benchmark accuracy of 70.24%. The precision, recall, and the f1-score have also significantly gone up. For the logistic regression, Naive Bayes, and Decision Tree classifier, there does not seem to be a significant improvement in the model accuracies compared to their corresponding benchmarks. Even though the precision for the decision tree classifier improved to 1, unfortunately, the recall has depreciated from the benchmark. Hence, we can conclude that KNN was the most affected by this preprocessing step.
Feature Selection
For the feature selection procedure, we used SelectKBest to extract the top k features based on a scoring method. The scoring function used is the chi2. Getting a high value of the chi2 statistic between the predictor and response variable indicates a dependence between the two variables hence, the predictor variable is significant.
To select the top k features in each of the models, we need to know the value of k. To do this, we created a separate pipeline with SelectKBest tool with each of the classifiers, then used grid search to go through the given values of k (anywhere between 1 and 6). This range was chosen to fulfill the goal of having as few features as possible to make the heart disease diagnosis process easier. Increasing the value of the range of k did not yield significantly different results across the classifiers, hence, 6 was the maximum number of features given to the grid search.
Using the pipeline and the grid search, all four classifiers yielded the same number of features. Here are the features that were selected by the SelectKBest tool according to the top 6 chi2 scores for the 4 classifiers:
Applying the feature selection preprocessing step, there was a significant improvement in the overall accuracy of KNN and the recall and f1-score. There was a slight improvement in the model accuracy for Naive Bayes as well as the precision and the F-score. For the logistic regression, the accuracy and recall improved slightly. For the Decision Tree classifier, the overall accuracy, precision, and f1-score dropped. The recall did not change. Hence, we can conclude that the best-performing model is KNN.
We ran Apriori in WEKA to find out which attribute combinations are more frequently shown in the dataset and related to having heart disease. Even though the association rules do not indicate causality, we can use them as a self-exam list. If a person has more than one symptom, he or she needs to be more aware of the risk of having heart disease and go to the hospital to do a further examination.
Before running the association rules, we used J48‚Äôs accuracy to decide how to discretize the data. Compared to many different set-ups, we decided to go with discretizing into 3 bins and equal frequency for the continuous numerical attributes.
Below are the association rules found in WEKA, with the setting of the confidence as metric type, 0.9 as minMetric, and 10 as numRules:
Based on the rules that the right-hand side is having heart disease, we learned that some of the attributes showed up in the different combinations multiple times, such as ‚Äúthalach‚Äù (the maximum heart rate), ‚Äúca‚Äù (the number of major vessels colored by fluoroscopy), ‚Äúthal‚Äù(thalassemias), ‚Äúexang‚Äù(exercise-induced angina denoting), and ‚Äúage‚Äù. We noticed that ‚Äúca=0‚Äù and ‚Äòthal=2‚Äù are shown in all 9 rules that having heart disease. It is probably because of class imbalance. The instances that meet these two conditions are significantly more than others.
According to the association rules generated from WEKA, if a person has the following symptom shown below, he/she may have a higher chance of having heart disease.
Below is a summary of the accuracy scores for the 4 classifiers:
Looking at the overall accuracy results summary, feature selection has been the best pre-processing step for all of the classifiers except the decision tree. The decision tree performed quite well at the benchmark and performed similarly after discretization. However, feature selection hurt the model‚Äôs performance. Decision trees can be sensitive to slight changes in the data which can accelerate or decelerate their performance. Comparing the two preprocessing steps with the benchmark, we can conclude that KNN‚Äôs performance is significantly affected by preprocessing especially by discretization. For Naive Bayes and Logistic Regression, the performance was somewhat constant throughout the two preprocessing steps. We can therefore conclude that the K-Nearest Neighbor classifier is the best performing model.
Along with the overall accuracy of the model, it becomes imperative to evaluate the confusion matrix. When the model was created without any preprocessing of feature selection, it predicted more False positives and False negatives, both of which can be considered as high cost as it can have an adverse impact on the life of the patient.
To improve the predictions on test data, it was important to focus on key features of the data to participate and predict. However, It is also interesting to see that after applying the preprocessing and feature selection, Naive Bayes and logistic regressions model are not able to increase the precision & recall.
Our project was intended to apply the techniques that we learned from course 273 to exploring this heart disease dataset and try to find out what attributes are better in predicting heart disease as well as how to improve models‚Äô accuracy. The cost of the wrong prediction, important for deciding which model to pick in the real business situation, was not taken into account in our project. A future expansion of this work could be to give weight to different attributes based on cost-efficiency. Also, a larger size of the dataset can be helped to increase the accuracy of the models.
We can conclude the no. of major vessels colored by fluoroscopy, chest pain, exercise-induced angina, ST depression induced by exercise, age, and sex are good attributed to looking at for predicting if the person is having heart disease or not.
KNN classifier model can be used by doctors to improve their predictions, insurance companies, testing laboratories to improve their cost-efficiency. On the other hand, the Association rules are helpful for the companies and developers who want to create digital interfaces to sell products/services related to heart-related diseases.
Github: https://github.com/aasaleem20/Heart-Disease-Prediction/blob/main/Heart_Disease.ipynb
Sandeep Kumar
Aden Omar
in
EYEZ14
Aleksey Zavgorodniy
in
Unicsoft
Jelilat Anofiu
in
FormPlus Blog
made2GROW GmbH
Abhay Parashar
in
Geek Culture
Abhishek A
in
Analytics Vidhya
gulfjobs.ae
"
115,https://medium.com/towards-data-science/how-to-easily-create-a-pdf-file-with-python-in-3-steps-a70faaf5bed5,How to Easily Create a PDF File with Python (in 3 Steps),"PDF is probably one of the most common file types that we can find on our computers.
We use PDFs for our resumes, reports, invoices, you name it!
A common way to create a PDF file is by saving a Word file as .pdf, but we can also create a PDF file using Python.
The pros? If you plan to create multiple PDFs using the same template, you could do it with Python! But first, we‚Äôll see how to create a PDF file using Python.
In this tutorial, we‚Äôll first create a simple PDF file and then we‚Äôll create a more advanced one that looks like the one below.
There are different ways to create a PDF file with Python, but I found using HTML templates the easiest one.
With this option, we can create basic and advanced PDFs in a few minutes!
To create a PDF file, we need to install the following libraries:
Before we use thepdfkit library, we need to install wkhtmltopdf. The steps to install it is up to your operating system:
In case you‚Äôre on Windows, you can download the installer here.
Now let‚Äôs create a simple PDF with Python.
First, we need to create an HTML document that we‚Äôll use as a template later.
To create this HTML document, we‚Äôll use a website called HTML Editor. On this website, we can type text in the visual editor on the left and generate its HTML code in the source editor on the right.
Here‚Äôs the text I introduced in the editor (this file can be found on my GitHub)
Note the placeholders I created using the {{}}. Later we‚Äôll introduce values inside these curly braces with Python.
Here‚Äôs the HTML code that was generated by the HTML editor.
We need to copy this HTML code, create an HTML file in our working directory, and paste the code on it.
First, we import jinja2 and pdfkit. I have a placeholder named {{today_date}}, so I also import datetime to get today‚Äôs date.
Then we create a Python variable for each placeholder in our HTML document and we create a dictionary that pairs the placeholders with the Python variables.
Note that I named each Python variable as the placeholders, but they could have different names.
Now it‚Äôs time to use jinja2 to create an environment for our template.
Then we specify which template we‚Äôre using. In my case, I named the HTML file as basic-template.hmtl. After this, we render the dictionary we created in step 2.
Next, we add wkhtmltopdf to the pdfkit configuration. To do so, we need to specify the path where wkhtmltopdf was installed.
Here‚Äôs the command you need to run to get this path.
Finally, we export the PDF file using the .from_string() method.
Note that I added a CSS parameter to the .from_string() method. This is optional, but I found it necessary to make the font size of the paragraphs in our PDF bigger.
I created a style.css file in my working directory and wrote the code below.
That‚Äôs it. Here‚Äôs the PDF we generated with Python.
The code we wrote and the files used in this tutorial can be found on my GitHub.
Now to generate a more advanced PDF like the invoice below, we need to download an HTML template and create some placeholder. In this video tutorial, I show how to do it.
Automate your life with Python! Get my FREE Automation Cheat Sheet by joining my email list with 10k+ people.
If you enjoy reading stories like these and want to support me as a writer, consider signing up to become a Medium member. It‚Äôs $5 a month, giving you unlimited access to thousands of Python guides and Data science articles. If you sign up using my link, I‚Äôll earn a small commission with no extra cost to you.
frank-andrade.medium.com
Buddhima Mataraarachchi
Emre Yƒ±ldƒ±rƒ±m
Ashish Bhardwaj
Caio Oliveira
Kanchan Adhikrao jadhav
Anup Vaze
Alcide Updates
Tech Britt
"
116,https://medium.com/towards-data-science/amazon-business-analyst-interview-questions-bcb57f7768c3,Amazon Business Analyst Interview Questions,"Amazon moves more than a million packages every day. Even small improvements in the efficiency of logistical and business operations can save millions of dollars down the line. Business Analysts can analyze hard data, gain insights and communicate their findings to decision makers at the company.
In this article, we will discuss how business analysts fit into Amazon‚Äôs organizational structure. Knowing their future responsibilities can help aspiring business analysts get the job and advance their career at Amazon.
Business Analysts at Amazon work with data to gain insights about business operations, logistics, customer behavior and habits, and use them to increase profits. There are two ways to do this: cut unnecessary costs and increase revenues.
Amazon, like other tech businesses today, collects large volumes of user data. Business Analysts increase revenues by finding ways to increase user spending. On the other hand, looking at data is one of the safest ways to find and eliminate inefficiency within an organization.
Interviews for this role reflect Business Analyst‚Äôs responsibilities at Amazon. Candidates have to demonstrate strong knowledge of business and the ability to work with data. Writing SQL queries is one of Business Analysts‚Äô day to day tasks.
Prospective Business Analysts have to answer questions ranging from business cases to actual SQL challenges. SQL questions usually deal with business processes, eliminating inefficiencies and increasing revenues.
Analyzing data is only a part of a Business Analyst‚Äôs job. One of their main responsibilities is communicating insights with decision makers at the company.
Business Analysts do a lot of work in SQL, so interviewers expect a strong proficiency in this language. Unlike other data science specialties, business analysts typically don‚Äôt need more than a basic knowledge of Python. It helps to have a good understanding of Excel as well.
Having the ability to focus on the task and detect patterns in data can also help you succeed on the job. Such skills are hard to measure, so interviewers usually observe candidates in the process of answering the question.
Aspiring Business Analysts also need to show a deep knowledge of business. The ability to analyze and aggregate data is important, but not enough if you can‚Äôt gain insights and make the right conclusions in a business context. To get the sense of non-technical questions during these interviews, read a general article about Business Analyst Interview Questions.
The culture at Amazon is focused on improving customer experience and making their website the most convenient option for shopping online. Having the same focus on improving customer experience can help you land a job at Amazon.
1. Understand the question
The best way to approach an SQL challenge is to carefully read the description. Especially if the question has a lot of concepts and conditions. New questions can be confusing at first. Read them multiple times to stay on the right track.
Some questions are clear and specific. Others are broad and open to interpretation. When questions are unclear in any way, it‚Äôs best to directly ask the interviewer to specify.
Questions often introduce concepts like ‚Äòunique user‚Äô or operations like ‚Äòcalculate monthly difference in sales‚Äô. Some questions have specific definitions for these concepts and tasks, while others don‚Äôt.
Often a description of the task also specifies how to arrange data before returning it, how to format the output, or similar matters.
2. Analyze data
Looking at the available data is the second most important step towards solving an SQL question. If you‚Äôre lucky, the available data will be contained within one table, but that‚Äôs not always the case.
Some questions have multiple tables and each can have more than a dozen columns. It‚Äôs important to develop a methodical approach to easily understand the available data.
The first step is to look at values mentioned in the question description and identify which tables and columns contain them.
Looking at data types of each column can be helpful. It can help you decide whether you‚Äôll need to cast or format values. This is often the case with date values, which need to be formatted or converted into text values.
Also, uniqueness and limitations of each value in the data are important. So looking at the actual data is worth it.
3. Plan your approach
Once you understand the question and available data, it‚Äôs time to lay out logical steps for finding a solution.
Writing down your plan can help you focus on solving the challenge. You can start with the most direct approach, and eventually you might come up with improvements to the original idea. Written down logical steps are easy to follow as you‚Äôre writing the query.
Making a habit of planning your approach can be very beneficial. It‚Äôs an opportunity to show off your thinking during an interview. Coming up with multiple ways to approach a problem can be a testament to your SQL skills and overall sharpness as a data scientist.
These steps should describe what operations (casting values, multiplication, division, formatting) you are going to perform to go from initial data to the final answer.
You don‚Äôt have to write specific code for each step, but you can write pseudo code, or reference the SQL features you‚Äôll use when writing the actual code.
4. Write code
Transform your logical outline into a valid SQL query. Start with the most basic query and gradually add complexity to it.
For example, if you need to filter values in a certain column, first SELECT all values from the column. Incrementally add conditions for filtering, formatting rules, aggregation and so on.
Check the output of your query as you add these complex features. This way, you‚Äôll know what‚Äôs working and if the additional code has the desired effect. You can also detect problems in the code and easily fix them.
One of the Business Analysts day to day tasks is to understand customer behavior. In this role, you might have to analyze the behavior of users who did not place an order. Perhaps you need to understand why they did not place an order. Or how Amazon employees can encourage users to order more.
Analyzing user behavior is one of the secrets to Amazon‚Äôs success. For this reason, we start the list of potential interview questions with this SQL challenge.
The question seems fairly simple and clear on what candidates need to do. It is marked as ‚ÄòMedium‚Äô difficulty on the StrataScratch platform.
In this Amazon business analyst interview question, we have to work with two tables ‚Äî one that contains information about orders, and the other that contains information about customers.
It seems logical that in order to find the answer, we will have to cross-check two tables to get to the answer.
Without further ado, let‚Äôs get started.
As previously mentioned, we can extract all the necessary information from two tables. First, let‚Äôs take a look at the orders table:
Available data may be easier to understand if we look at the actual table:
Let‚Äôs go over each column:
The customers table is another essential piece of the puzzle:
Now that we know about columns and types of values contained in them, let‚Äôs look at the table itself:
Let‚Äôs go over each column:
Essentially, we have a list of all users, regardless of whether they placed an order. We have to cross-examine this list with the cust_id column in the orders table.
First, we need to identify important columns. In this case, that is going to be the id column in the customers table and cust_id column in the orders table.
Common mistake is to SELECT the id column from the orders table. We need the cust_id column because in this table, the id column identifies orders, not customers.
Looking at the data makes it obvious that every order is tied with a customer. Sometimes there are multiple orders for one customer, but there is no order without a cust_id value, which identifies who has placed the order.
1. Combine data from two tables
In this case, the obvious choice is to use a JOIN to combine data from two tables. We only have to choose the right type of JOIN.
In order to perform a JOIN, we need to define the shared dimension ‚Äî identity of the customer. In the orders table, customer identifiers are found in the cust_id column. In the customers table, these values are stored in the id column.
The question asks us to find the number of customers who meet a certain criteria. The first table is orders, and the second is customers. We use RIGHT OUTER JOIN to make sure that customer ids from the second table that are absent from the first table are not deleted.
The shared dimension between two tables is the customer‚Äôs identity. In the customers table, these values are stored in the id column. In the orders table, they are stored in the cust_id column.
2. Filter records to find users without orders
Next, you need to find instances when there are no orders for a specific customer id. In other words, the id of the customer can not be found in the cust_id column of any order.
We perform a JOIN, but there is no guarantee that every value in the id column of the customers table will appear in the cust_id column of the orders table. In cases when there is no match, the cust_id column will be NULL.
At this stage, we have the list of all customers with corresponding order data. We can use the WHERE statement to only keep users without orders.
3. Get the number of rows in the filtered result
Once we have all the records that do not have a cust_id value, we‚Äôve almost arrived at the answer.
The last step is to use the COUNT() aggregate function to get the number of filtered rows. This is the number of customers without orders.
4. Output the result and rename the column
It‚Äôs a good practice to use the AS keyword to give the output column a more descriptive label.
2. Filter records to find users without orders
3. Get the number of rows in the filtered result
4. Output the result and rename the column.
Experienced SQL developers sometimes change the order of tables to use LEFT JOINs instead of RIGHT OUTER JOINs.
Try to solve this Amazon business analyst interview question using a LEFT JOIN if you‚Äôre up for the challenge.
Business Analysts sometimes need to detect structural inefficiencies in the organization. For this reason, Amazon interviewers might ask them to solve this question, where candidates need to find departments with a certain number of workers.
To answer this amazon business analyst interview question, the candidate must find the number of workers for each department, and then filter the list to meet the criteria.
The final output contains only two columns ‚Äî the department and corresponding number of workers.
All the information necessary to solve this question is contained in one worker table. Here is the model of the table:
We have six columns with different types of values. Besides paying attention to types of values, it helps to look at actual data in the table:
Let‚Äôs go over values in each column:
The most important columns in the workers table are worker_id and department. We have to get the number of employees for each department. The safest approach to get the number of employees is to count unique user_id values for each department.
1. SELECT the data to work with
We need to write a statement to SELECT department and worker_id columns from the workers table. This will give us the list of employees and their place of work.
We don‚Äôt have the number of employees in a separate column. We will need to find the total number of workers some other way.
2. Aggregate the number of workers for each department
We can get the number of workers by using the count() aggregate function on the worker_id values. We need to use GROUP BY statement to output the total number of workers for each department.
By default, the count() aggregate function gives columns a generic count label. We can use the AS keyword to give the column a descriptive name.
At this stage, we should only have unique department values and the number of employees for each one of them.
3. Filter out departments with more than 5 workers
We can not use the WHERE statement to filter the result of aggregate functions. SQL developers use the HAVING()clause to discard groups (departments) that do not meet the criteria. In this case, the condition is that each department must have more than 5 employees.
2. Aggregate the number of workers for each department
3. Filter out departments with more than 5 workers
To solve this ‚ÄòMedium‚Äô difficulty question, candidates have to analyze the data in the worker table. It contains information like workers‚Äô salary and their department.
The task is fairly simple ‚Äî we have information about workers, their salary and the department they belong to. We need to find the highest earner for each department. Question description also specifies which values need to be returned.
You have to decide how to solve the edge case when multiple employees working at the same department share the highest salary. Most likely you should return information about all of them, but you might want to clarify this with the interviewer.
Prospective Business Analysts need to write SQL query to work with data in the workers table. This is the table from question 2. However, in this case, we will work with different values to get the final answer:
Let‚Äôs once again look at concrete example of available data:
Let‚Äôs go over columns in the table:
In order to answer this Amazon business analyst interview question, we‚Äôll need to work with salary and department columns. In the end, the output needs to include the respective first_name value of the worker as well.
1. Find the highest salary among workers at each department
After looking at the available data, it becomes clear that there are many workers assigned to only a few departments. We need to separate workers based on the department where they work, and find the highest earner in each group.
To do this, most likely we are going to use the MAX() aggregate function as well as GROUP BY statement to put them in separate groups.
2. Combine data from main and temporary tables
In order to access values in all columns, we need to create a temporary table that contains the highest earners. Then we must JOIN it with the main table to gain access to all values from both tables.
We can perform an INNER JOIN to weed out the records of workers who are not highest earners. For that, we need to specify the shared dimension using the ON statement. INNER JOINs will do the rest.
3. Output the values in three columns
The main reason why we JOIN the temporary table with the main table is to have access to all columns of the main table.
The question asks us to find people with the highest salaries in each department, and output values in their salary, department and first_name columns.
Write code
2. Combine the data from two tables
3. Output the values in three columns
The Business Analyst role is unique, because it requires technical and business expertise, both being equally important. To get a job, candidates are often given SQL challenges. Their ability to write SQL queries will affect their chances of success.
In this article, we discussed three interview questions for a Business Analyst position at Amazon. You can explore other SQL questions on StrataScratch, a platform where data scientists of any level can practice and improve their SQL skills.
Originally published at https://www.stratascratch.com.
Masilamani Chidambaram
in
Bold BI
Data science Assignment help
Bill Franks
in
Analytics Matters
Nehal M
Joshua Szymanowski
Yurywallet
Jeremy Chow
in
Towards Data Science
Dr. Alvin Ang
"
117,https://medium.com/@johnvastola/10-must-know-machine-learning-algorithms-for-data-scientists-adbf3272398a,10 Must-Know Machine Learning Algorithms for Data Scientists,"Machine learning is the science of getting computers to act without being explicitly programmed.‚Äù ‚Äî Andrew Ng
Machine learning algorithms are a crucial part of data science, allowing us to make predictions and understand complex data sets. In this guide, we will cover the top 10 machine learning algorithms that every data scientist should know.
Akash Desarda
in
GreyAtom
Forefront
Angela Wang
in
Project Aqua Fellowship: Neural Structured Learning
Kenny Helsens
in
Towards Data Science
Scott Duda
Bhaumik Pathak
Michal Banach
Petuum, Inc.
"
118,https://medium.com/towards-data-science/how-to-use-the-sherlock-mind-palace-study-technique-to-teach-yourself-data-science-7595602979e,How to Use the Sherlock Mind Palace Study Technique to Teach Yourself Data Science,"Over two years ago, I began researching the best ways to teach myself data science. I‚Äôve looked into microlearning, ultra-learning, the Feynman technique, deliberate practice, brainwashing techniques from the 1950s, and many more. They‚Äôve all been unique and have been shown to produce results.
However, I was looking to bring you data science self-teachers something even more obscure and unorthodox than brainwashing techniques from the 1950s. I wanted to get you a learning method straight from some of the first brilliant minds: the ancient Greeks.
Enter the mind palace. Or perhaps, as it‚Äôs better known, thanks to the BBC‚Äôs Sherlock program: the Sherlock mind palace.
The mind palace technique originated in ancient Greece, where, according to the myth, a Greek poet by the name of Simonides of Ceos invented a way to remember information after attending a fatal banquet. Simonides stepped outside of the banquet hall only for it to collapse behind him. Despite the attendees who were caught inside the hall during the collapse being too badly crushed to be identified, Simonides was able to identify each body based on where they had been sitting. Thus, the mind palace technique was born.
The mind palace technique is known by many other names, including the method of loci, the memory theater, the art of memory, and the memory palace.
The mind palace technique works like this: imagine a complex place (such as a palace) where you could physically store memories or information. Each room is specific to the memory or piece of information that you want to remember (read: store inside of the room). Because the mind is so gifted at remembering visual memories, it can help to make the memory or piece of information as detailed or unique as possible. When you need to retrieve a memory or piece of information, you simply walk back through the building seeing and remembering each memory.
Greeks and Romans used this technique to memorize their speeches, with each idea paired mentally with a complicated room in a building. During this time, it was expensive to write things down to remember them, so it made more sense to use elaborate memory techniques to save money. The mind palace technique was also used during the Middle Ages when monks and scholars used it to memorize religious texts.
However, as soon as the printing press became mainstream and writing things down became less expensive, the mind palace technique soon fell out of favor as people no longer had a requirement to memorize information or memories. Like record players, low-rise jeans, and scrunchies, the memory palace technique saw a resurgence in the 20th century in international memory competitions where participants would use it to recall long lists of items in a specific order. For example, Simon Reinhard was able to memorize the exact order of 370 cards using the mind palace technique.
The mind palace technique made it into the Sherlock Holmes novels by Arthur Conan Doyle, who writes Holmes as describing it to John Watson as the idea that a man‚Äôs brain is like an empty attic that needs to be stocked with important furniture. This idea is important because everyone‚Äôs mind palace will be different: some will choose the home they grew up in, others will think of a real palace, and others will choose a road with lots of addresses on it as a way to organize their information. In fact, a mind palace doesn‚Äôt even have to be a real place at all ‚Äî participants in a scientific test were found to perform equally well when using a real place or a virtual place to memorize a list of unrelated words.
The mind palace technique isn‚Äôt designed to help you comprehend data science topics ‚Äî instead, it‚Äôs there to make it easier for you to call information or concepts you‚Äôve learned to mind. The mind palace serves you by creating a way for you to rapidly recall memorized information. When you use the mind palace technique to memorize concepts in data science, those memories become procedural memories stored in the basal ganglia which can be accessed a lot quicker than memories stored in other areas of the brain.
Therefore, this technique is best used to memorize concepts in data science that you‚Äôll need to retrieve on a regular basis for either your studying or your work.
For example, data science concepts you could memorize using a mind palace are the elements of good data visualizations, statistical methods and when to use them, calculus formulas, software engineering best practices, a good documentation checklist, or a python code-checking list. From personal experience, this method has proven useful when remembering statistical methods and checklists for things, like visualizations or code.
While this study method won‚Äôt help you understand the relationship between different concepts, it‚Äôs still a great tool to help you memorize the concepts you‚Äôll need to become a data scientist.
One of the key ideas to remember when teaching yourself data science is that you don‚Äôt always have to understand why something works ‚Äî you just need to understand how and when to use it. When I was taking my first university calculus class, I had to remind myself that I didn‚Äôt have to know why the formulas worked ‚Äî I just needed to know when to use them. This is exactly where the mind palace comes in handy.
The mind palace technique works when you can associate data science concepts with locations you‚Äôre familiar with. The most common ‚Äúpalace‚Äù people use to store their information is their childhood home because it‚Äôs a place you‚Äôre intimately familiar with. For example, if you closed your eyes and thought of your childhood home, you'd instantly see the floorplan of the house, the color of the walls in each room, and the furniture and decor in each room. As you walk through each room of the house, you would associate a data science concept with each room. Then, when you want to retrieve that information, you‚Äôd retrace your steps through the house until you reached that room. As I mentioned previously, the more unique or visual you can make each concept, the easier it will be to remember it.
Since writing something down costs nothing nowadays, it‚Äôs easiest to start with a visual sketch of your palace‚Äôs floor plan and then write down your concept associations with each stopping point in the house. Here‚Äôs an idea to get you started:
Step 1: Draw a floor plan of your mind palace.
Step 2: Draw a travel path through the mind palace stopping at relevant locations ‚Äî make sure your travel path is linear so that you‚Äôre not crossing over your own path.
Step 3: Create a two-column list next to your sketch of the floor plan ‚Äî in the first column, list the relevant locations in your floor plan; in the second column, attach a data science concept to each relevant location. For example, in our example of statistical methods, the front door could be associated with k-means clustering (because you cluster with your family at the door to enter the house), the staircase could be associated with linear regression (because you‚Äôre either going up or down), and the door to the backyard could be associated with random forests (because you go through the back door to enter the forests).
Step 4: Run through your mind palace mentally on a regular basis until it becomes second nature to know what you‚Äôre about to remember as you enter a room.
Step 5: Use your mind palace the next time you‚Äôre practicing for a data science interview by creating mock interview questions that require you to retrieve information from your mind palace. Alternatively, use your mind palace to make sure you remember all of the data science concepts you‚Äôve learned in previous lessons, such as the requirements of a good data visualization or the software engineering habits you should know to write good code.
If I‚Äôve learned anything while teaching myself data science it‚Äôs that the more unorthodox you can make your learning methods, the more likely you‚Äôll remember what it is that you‚Äôre learning.
The mind palace is a great way to shake up your data science learning experience. Instead of relegating yourself to many boring hours of reading books and listening to videos on Youtube, why not actively work to memorize concepts in data science using a mind palace? This active learning and recall method will give you a visual model on which to base your data science learning experience. Not only will it help you retrieve vital concepts during crucial moments thanks to the ease of remembering how you used to walk through your childhood home, but it will also reinvigorate your learning process by encouraging you to work different parts of your brain.
When learning data science, it‚Äôs worth trying anything ‚Äî especially a technique as old as some of the greatest learners of all time: the ancient Greeks!
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Support my writing by donating to fund the creation of more stories like this one: Donate
Neo Malesa
Winston A. Henderson
Omri Lavi
Guy Erez
Dalan Ozaki
in
The Startup
Eric S Burdon
in
Purposeful Life
Emily Richard
Chirag Rijal
"
119,https://medium.com/@alinaabid_95/25-keywords-used-in-data-analyst-job-specifications-to-use-in-your-cv-or-resume-a-data-project-a38e0390c217,25 Keywords Used in Data Analyst Job Specifications to Use in Your CV or Resume: A Data Project,"Every job seeker should be familiar with the Application Tracking System (ATS). If you aren‚Äôt, the ATS is software that essentially acts as a gatekeeper for companies by letting only top candidates pass through to the next stage. A common way it highlights top candidates is by keywords. Therefore, if you can predict the correct keywords for your CV from a job specification, you have a greater chance of passing the ATS.
I figured this out two months into my seven-month job search and decided to try and make it easier for me to make these keyword predictions. My solution ‚Äî web-scrape a commonly used job board and find the most common keywords used in data analyst job specifications.
I used the following filters for my search:
What: Data Analyst, Where: London, Greater London, Job type: Full-Time
And I used the code below to extract 240 job descriptions and export them to Excel. Two useful sources I used while writing the code were:
(i) John Watson‚Äôs YouTube tutorial
(ii) Stack Overflow to browse relevant questions or post my questions
I wanted to take this project further and match the keywords to a list of powerful action verbs that can impress hiring managers, shared by wonsulting.
I used the following code:
The keywords list extracted in Step 1 needed some cleaning. Without going into too much detail, the data cleaning in Excel involved:
I then used an IF statement to determine if the verbs extracted in Step 1 matched those in Step 2. Since I was interested in verbs that were a ‚ÄòMatch‚Äô, I filtered on the matched words. Following this, I used a VLOOKUP to get the relevant ‚ÄòSituation‚Äô in which each verb would be used.
The last step was sorting out the words in descending count order per situation and capping the list to 25 (see below).
Sydney Kingsepp
Ravneet SV
Ani
Lucas Leow
P Guzman
ZaranTech
Thomas Huault
in
Superalgos | Algorithmic Trading
Ryan Gosha
in
DataDrivenInvestor
"
120,https://medium.com/better-programming/chatgpt-will-change-software-engineering-but-not-in-the-way-youre-hoping-2d7dfbc3c6c0,ChatGPT Will Change Software Engineering ‚Äî But Not in the Way You‚Äôre Hoping,"I‚Äôm always dubious of claims made in the media, both social and mainstream, around AI and ML developments. With so much hype over the past decade, many people are.
Whilst I don‚Äôt have a deep background in applying AI and ML models, I understand the fundamentals of the technology better than most. The appreciation of the technical and mathematical achievements in the area is why I feel a lot of the claims made are a little (if not completely) over-exaggerated.
So when I started seeing comments about ChatGPT over the past day, I felt that these were another exaggeration. As I have an OpenAI account that I opened for Dall-E 2, I thought I may as well give ChatGPT a go and experience the ‚Äúamazingness‚Äù for myself.
I hate to admit that my cynical ‚Äúamazement‚Äù quickly dissipated to real amazement. ChatGPT really is something incredible and provides a glimpse of how software engineering could be in the future, but not in the way I think some people are thinking.
In fact for some people, this increase in productivity is going to be so great, they‚Äôre actually going to be out of a job.
A YouTube video I watched on Dall-E 2 showed, from the perspective of a designer and artist, how it could change the way they worked. The summary was that it wouldn‚Äôt replace designers or artists.
Instead, it would become an assistance tool that would increase productivity and could help act as a virtual muse for users.
After watching this, I needed to experience Dall-E 2 for myself and I wanted to see if the video was overhyping the technology. Once I got access, I‚Äôll admit, it was impressive.
At the same time, I could see that the images being produced from the input I was giving weren‚Äôt close to what I was imagining and I could appreciate the points made in the video about how it wouldn‚Äôt replace designers and artists but could assist them.
Even with this experience, I couldn‚Äôt fully appreciate the gravity of how this technology might change an artist‚Äôs or designer‚Äôs way of working. I‚Äôve never worked in any art or design field so I don‚Äôt know the first thing about how they work. The way I interacted with Dall-E 2 was an extremely contrived use case as I was just using it to generate narrative-based scenes.
At the time of using Dall-E 2, I immediately wondered if the OpenAI team was working on something similar for software development. I wanted to experience an AI-based tool like Dall-E 2 but in a use case, I‚Äôm familiar with it.
With this latest ChatGPT release they were clearly doing that and I‚Äôll say straight away, this really will change the way we work as software developers, and, for some, this could be the beginning of the end of their software development careers.
On first use, I just did what I saw people doing online ‚Äî which was to ask ChatGPT contrived questions about software development like building a well known algorithm or asking it questions about code.
The output from this was sort of impressive but, as an experienced software engineer, I could probably write the solutions just as quickly and would have more confidence about their correctness. The primary algorithms powering all AI tools are pattern based, not rationally or logically based, so there are bound to be errors in the implementations.
For my next attempt, I chose to approach it from a different angle. I‚Äôm currently working on a simple note-taking application in Flutter and I wanted to see how I might do it with ChatGPT. I asked the question ‚ÄúHow do I build a simple note-taking application in Flutter?‚Äù. I‚Äôm assuming that because the OpenAI APIs are rate limited, it only did half the application, but what it did was a good start for a basic note-taking app. Most importantly it completed a number of basic tasks for me -
The value here isn‚Äôt huge, and I wouldn‚Äôt expect it to be. All of this is something I could‚Äôve done myself, but it does save me a lot of time. On top of this, I could continue ‚Äúchatting‚Äù with ChatGPT and was able to have it build out more features and further refine the generated code. It was like having a slightly stupid human assistant doing the boring coding for me.
In addition to these basic tasks, like with Dall-E 2 for designers, interacting with it gave me inspiration for other parts of the note-taking app that I hadn‚Äôt yet considered. The power of acting as a virtual muse shouldn‚Äôt be understated. The concept is so powerful, an old work colleague that used to work at IBM told me how on his first day he was given a teddy bear and was told ‚ÄúThis is Ted and he‚Äôs a great problem solver.
If you get stuck on a problem tell Ted and if you haven‚Äôt solved it after that, seek help from others.‚Äù. This wasn‚Äôt a backhanded attempt to tell a newbie not to bother other people. It was teaching the important lesson that sometimes when we narrate and verbalize our problems, the solution can sometimes become apparent. ChatGPT is Ted on steroids!
Just this initial interaction shows so much promise for what is to come. I‚Äôm starting to see how an AI model is actually providing a more significant positive value to being part of my daily workflow beyond writing simple comparison if statements (one of the ‚Äúamazing‚Äù things I‚Äôve had people show me with Github Co-pilot).
This now beckons the question, do I think software developers are still going to be needed in the future? The short answer of ‚Äúdefinitely‚Äù should come as no surprise. The disheartening extension to that answer is that the price of these tools will be the jobs of entry-level and lower-level software developers.
Amongst software development communities, there is the continuous joke about how Google, StackOverflow, and ‚Äúcopy and paste‚Äù write most people‚Äôs code. As the old idiom goes, there‚Äôs a grain of truth to every joke. I know for a subset of people out there, copy and pasting code from the internet is how they operate daily.
Infamous events like ‚Äúleft-pad‚Äù with NPM show that many people often choose to outsource basic thinking. The danger with this is not the act of doing it, it‚Äôs the reason why. There will be many that do it because it saves time. There will also be many that do it because they don‚Äôt know how to do it. It‚Äôs this latter group that needs to be worried about these recent advances in AI.
As these advances in AI progress, the most basic programming tasks, like creating boilerplate classes, implementing algorithms, building simple web forms, doing simple data processing, etc. are increasingly going to be computer-aided. At the moment some of these tasks are time-consuming and companies will hire entry-level or lower level software developers to do this work. This will inevitably change.
With the rapid progress made by tools like ChatGPT, people with no technical experience, like business analysts or designers, will do this work themselves. The supporting tools like IDEs, SDKs, and compilers will evolve to the point that they‚Äôll be hidden from view and will more closely resemble Excel where anyone can get set up immediately and with little training, only their existing knowledge of the business domain.
Assuming this future is inevitably approaching where does that leave technical people? As the basic repetitive actions are performed increasingly by AI, the work that will be left will be highly technical and environment specific. These problems will be beyond the reach of a pattern-based AI due to the unique nature of that work requiring greater reliance on logical and rational thinking.
As an example, the majority of work I do in my day job isn‚Äôt boilerplate work. It‚Äôs understanding the business needs and then figuring out a way to use multiple, unrelated systems to achieve that business requirement in a reliable and scalable manner.
My daily work requires me to have an understanding of networking, distributed systems, and data layer technologies and leverage these technologies to build solutions for business problems that I often have to research and define myself. I rarely use Google or StackOverflow for my daily work because most of the problems I deal with are too bespoke to have a ready-made solution.
The most interesting thing about this question is that I don‚Äôt think it changes much from what I would‚Äôve told someone 10 years ago before things like Dall-E and ChatGPT were even on my radar. The crux of that advice would be two major principles -
To that second principle, if you‚Äôre new to software development, make sure you understand your primary tool ‚Äî computers. When I say ‚Äúcomputers‚Äù I mean everything about them ‚Äî mobile phones, PCs, servers, operating systems, browsers, networking, software architectures, distributed algorithms, etc. You don‚Äôt need an encyclopedic knowledge of everything, but you need to be at least aware of the different facets. A good test is this ‚Äî if someone asked you the question ‚ÄúWhat happens when you type google.com into your browser?‚Äù is your answer more like this or is it more like this?
If you‚Äôre the latter, you don‚Äôt have anything to be worried about. If your answer looks more like the former, work to get yourself to the latter because ChatGPT and its pals are coming for your job.
If you‚Äôve been working for a few years in software development, the same applies to someone new. It‚Äôs never too late to learn more about the tools you work with day-to-day. But if this isn‚Äôt where your strength lies, if your strength lies in your communication skills and working with people, focus on this side of things.
There will always be a need for people at the intersection between where people problems meet technical problems. The explosive growth in the no-code community is proof of this. There will always be a need for great business analysts and great project managers. I highlight ‚Äúgreat‚Äù because there‚Äôs already plenty of mediocre business analysts and mediocre project managers out there and they‚Äôve been struggling for decades.
ChatGPT and whatever other tools may arrive in the near future are extremely positive for software development. There will be people at the lower end of the software development skill spectrum that will need to find a different role.
If those developers are willing to up-skill, there are going to be even greater and higher rewarding opportunities on the horizon. These technologies are going to be a massive productivity boost for knowledgeable software developers.
The AI software developers are coming and they will take your job, thankfully that job is pretty boring and you‚Äôll find something more interesting.
Princewill Celestine
Hari Manikkothu
in
Kubernetes
Jayashree domala
in
Towards AI
Position Exchange
Daniane Gomes
ExpressionEngine
Knoldus Inc.
in
Knoldus - Technical Insights
Knoldus Inc.
"
121,https://medium.com/@datawithdan/4-reasons-why-being-a-data-analyst-is-better-than-a-data-scientist-1be5082135a6,4 Reasons why being a Data Analyst is better than a Data Scientist,"Data Scientists are often thought of as a higher rank than Data Analysts. Data Scientist sounds a lot flashier than a Data Analyst. Data Scientists on average get paid more than Data Analysts, and sometimes do cooler things like building neural networks.
Ivy Liu
in
Towards Data Science
Or Hiltch
in
Skyline AI
Kristjan Eljand
in
Towards Data Science
Nick Brown
in
Meta Research
Ashish Trada
Ben Roshan
in
Analytics Vidhya
Ritik Kumar Tiwari
Keino Baird
"
122,https://medium.com/@s.yao/data-analysis-with-sql-and-tableau-customer-churn-analysis-d13bc1ba8ac0,Data Analysis with SQL and Tableau ‚Äî Customer Churn Analysis,"One of my favourite things about the field of data analytics is the process of turning messy data into actionable items that make a difference for businesses. A few weeks ago, thanks to Maven Analytics, I got my hands on the churn data of a telecom company, and performed a customer churn analysis with the goal of identifying high value customers/churn risks and improving customer retention in mind.
As usual, I love doing data pre-processing and EDAs in SQL before moving onto Tableau to share my findings.
Data Exploration and Storytelling
The main question that drives the analysis is straightforward ‚Äî how do we plug the hole of revenue loss resulting from customer churn, especially from the churn of high value customers.
With our main question in mind, I needed to have an idea of the KPIs to monitor. I chose to measure churn by revenue instead of number of customers. There are also more than one way to look at revenue ‚Äî total (lifetime) revenue, average monthly revenue and monthly charges.
The first question I needed to answer was what makes a customer high value. The main factors are: tenure, number of referrals (which are very valuable to the business), how much they spend on our services, how much revenue potential they have. The vizQL (Tableau‚Äôs version of SQL) formula to determine whether a customer is high value looks like this:
IF [Total Revenue] > {FIXED: PERCENTILE([Total Revenue], 0.5)} AND [Contract] != ‚ÄúMonth-to-Month‚Äù AND [Number of Referrals] > 0THEN TRUEELSE FALSEEND
Next, I singled out the top five (not including ‚Äúdon‚Äôt know‚Äù) churn reasons and related each one of them to aspects of our business operations in order to come up with actionable items (for example: for customers who left for competitors due to download speed, I looked at their internet type). I drilled it down (in Tableau) to the value status of the customers.
Here‚Äôs the SQL code with my thought process as comments.
Here‚Äôs the Tableau Viz presenting my findings and recommendations (interactive).
Project Takeaway
I spent quite a bit of time on this dataset since there are so many layers to it, it‚Äôs always a challenge and an art to select the right data to analyze. For example, I think sometimes it‚Äôs a reflex to create a map whenever we see geospatial data, but in this case, I found that even if I slice the data by location, I still wouldn‚Äôt have been able to come up with an action plan based on the findings with the information I had, this is when I get reminded of the importance of asking the chief question and letting that lead the analysis.
On the visualization side in Tableau, this analysis made me recognize the impact of simplicity. Initially, since I had both the churn category and the specific reasons, it looked like a good idea to illustrate the hierarchical relationship with a dendrogram or a sankey diagram(both of which I did make but left out when assembling the final report, they do look fancy and took a bit effort to make ;), but sometimes complicated charts could look more overwhelming than informative for readers, on top of that, for the purpose of my analysis, the overall churn category does not matter as much as the specific churn reasons since ultimately, the churn reasons are what help me identify and determine the action plan.
As always, I learn the most by doing, until next time, happy data science‚Äôing ;)
Mukesh Kumar
in
INSAID
Mosunmoluwa
EARN-SAVE-INVEST-REPEAT
BestShot.win
Alexander Tanase
Abdullah Alka Kandilli
CivicTech Contributor
in
Civic Tech Innovation Network
Abhisek Gautam
in
Towards Data Science
"
123,https://medium.com/dssimplified/part-2-the-best-data-science-mentors-you-must-follow-d0411150863e,Part 2! The Best Data Science Mentors you must follow!,"In part 1 of the ‚ÄúThe Best Data Science Mentors you must follow‚Äù, I introduced you to 5 of the most amazing people in the world of Data Science who you should know. In this part, we‚Äôre taking it ahead and looking at 5 more mentors who can accelerate your growth in Data Science exponentially! There are numerous data science mentors today and they are sharing their knowledge with data science enthusiasts via different platforms like Kaggle, YouTube, GitHub, etc.
Find part one of the Best Data Science Mentors you must follow, here:
medium.com
In this article, we‚Äôll go over 5 such mentors you need to follow in your Data Science journey to keep your Data Science journey at the pace of the industry.
(Note: This list includes suggestions for amazing data science mentors from fellow data enthusiasts on LinkedIn! I‚Äôll mention them in the article towards the end for you to catch up with them too!)
1. Krish Naik
2. Nicholas Renotte
3. Jose Marcial Portilla
4. Alex Freberg(the Analyst)
5. Josh Starmer (Statquest)
One of the co-founders of iNeuron, Krish Naik is an Indian Data Science professional who is very well known for his efforts in making data science accessible to everyone and he has his own YouTube channel, with over 596K subscribers, where the content focuses on making Data Science simpler for everyone. iNeuron also offers Data Science internships and preparation guides to students who wish to learn more!
His YouTube channel has this description about him which speaks volumes about him.
I am the cofounder of iNeuron and my experience is pioneering in machine learning, deep learning, and computer vision,an educator, and a mentor, with over 10 years‚Äô experience in the industry. This is my YouTube channel where I explain various topics on machine learning, deep learning, and AI with many real-world problem scenarios.
One can learn from the playlists he has on YouTube ranging from PyTorch to Live Projects and NLP. Find out more about him in these links given below!
www.youtube.com
www.linkedin.com
Currently a senior data science & AI specialist with IBM, Nicholas Renotte‚Äôs life has took him through multiple roles in the world of data and he has his YouTube channel with over 82K subscribers helping them with Tensorflow, ML trading, OpenCV, Computer Vision, NLP, and much more!
His experience and short tutorials help everyone understand concepts relating to the aforementioned easily and his YouTube channel says this about him;
Well, I‚Äôm here to share practical ways to get started with data science, machine learning and deep learning using a bunch of different tools but mainly Python and Javascript. My only goal is to make it easier to get started and break through all the fancy jargon and complex math that‚Äôs out there in the field and get YOU up and running with some sick ml projects.
Find out more about him with these links below!
www.linkedin.com
YouTube Channel : https://www.youtube.com/c/NicholasRenotte/featured
github.com
Before I get to start about this amazing person here, it was his courses that got me going with data science in a way that I fell in love with it and started this blog. One of my roots in data science trace back to him.
Jose Marcial Portilla is the head of data science at Pierian Data and has multiple courses to his name on Udemy! His courses are to the point, easy to understand and can kickstart your DS and ML career in a way like none other. They are comprehensive and his easy to understand explanations make sure that you never forget the concepts.
Check out this simple tutorial of Python on YouTube if you are new to Data Science:
With almost 40000 followers on LinkedIn and 2.8 million students, there is no doubt he is one of the best Data Science tutors there are and being mentored by him is one of the best ways to crack the world of Data Science.
Find out more about him here:
www.linkedin.com
https://www.udemy.com/user/joseportilla/
pieriantraining.com
medium.com
Alex is one of the best YouTubers out there with his welcoming content, his YouTube channel tells this about him;
My name is Alex Freberg and on this channel I will be going over everything you need to know to become a Data Analyst. If you are wanting to make a career change or learn the skills needed to become a Data Analyst, be sure to subscribe to stay up to date on all my latest content.
For someone who wants to venture into the world of Data and not essentially go for Data Science entirely and only learn more about the analytics side of Data. With over 237K subscribers on YouTube, and playlists about PowerBI, SQL tutorials, Tableau tutorials, and data analyst portfolio projects etc.
Find out more about him in the links given below!
www.linkedin.com
www.alextheanalyst.com
discord.gg
With over 752K subscribers, Statquest is undoubtedly one of the best statistics channels on YouTube that make statistics easy and not too tough to understand for anyone who wants to be a data scientist but fears numbers. Numbers make up most of the data in the world and statistics is needed to make sense of these numbers.
Using Statquest, one can start off their journey in data science and understand statistics in a way that can take the understanding of analytics to another level.
Josh says it as follows on the channel:
Statistics, Machine Learning and Data Science can sometimes seem like very scary topics, but since each technique is really just a combination of small and simple steps, they are actually quite simple. My goal with StatQuest is to break down the major methodologies into easy to understand pieces. That said, I don‚Äôt dumb down the material. Instead, I build up your understanding so that you are smarter.
Find out more about Statquest and Josh here:
https://www.youtube.com/c/joshstarmer
statquest.org
www.linkedin.com
Find out more about these mentors and check out all the content uploaded by them today and let me know in the comments below if you think someone else fits the role of being the best data science mentors all over the world.
For more such articles, stay tuned with us as we chart out paths on understanding data and coding and demystify other concepts related to Data Science. Please leave a review down in the comments.
Check out my other articles at:
medium.com
Do connect with me on LinkedIn at ‚Äî Yash Gupta ‚Äî if you want to discuss it further! Leave a clap and comment below to support the blog! Follow for more.
P.S. A big thanks to Akshata Palsule and Alparslan T. for suggesting few names to make it to this list!
Akshay Agrawal
Mukundh Murthy
dj patil
Utsav Chatterjee
Lawrence Alaso Krukrubo
in
Towards AI
JD Solomon
Emeka Boris
in
Becoming Human: Artificial Intelligence Magazine
Tim Schopf
in
Towards Data Science
"
124,https://medium.com/towards-data-science/before-you-ask-why-ask-who-b90841d578b4,Before You Ask ‚ÄúWhy‚Äù‚Ä¶ Ask ‚ÄúWho‚Äù,"Here are the stages of evolution that you‚Äôll go through if you‚Äôre lucky enough to have the opportunity to mature as a data scientist.
During the freshly hatched phase, the larval data scientist cheerfully does whatever they‚Äôre told without so much as an inquisitive chirp. You‚Äôve barely finished describing the problem and they‚Äôre already thinking about how they can solve it as correctly and objectively as possible.
After all, they spent decades solving complicated homework problems without asking themselves whether there‚Äôs anything ridiculous about calculating how many spherical cows would be left in a field after 10 years if the you start with 50 cows whose lifespans are distributed GEOM(8)...
Ah, the (emotional) elasticity of youth. You‚Äôll grow out of it. (Unless you have a phenomenal manager, that is. Then you won‚Äôt be incentivized by suffering and you can stay in Neverland. Lucky you.)
Eventually, you‚Äôll get frustrated enough by your lack of impact that it‚Äôll tarnish your job satisfaction. You‚Äôll start asking ‚Äúwhy (and why and why and why and why)‚Äù before agreeing to perform any kind of heavy lifting.
That‚Äôs it, right? Just two stages? Many data professionals think they‚Äôve arrived when they get cynical enough to ask why before they leap.
Nope, there‚Äôs more.
Simply put: your stakeholders and leaders might have some very good reasons to withhold the truth from you, so asking why only gets you so far.
When you‚Äôve matured a bit more, you‚Äôll start asking ‚Äúwho‚Äù before you ask ‚Äúwhy‚Äù.
Maturing as a data scientist means asking ‚Äúwho‚Äù before you ask ‚Äúwhy‚Äù.
You‚Äôll get better at reading between the lines, thinking in terms of peoples‚Äô incentives, and picking your collaborators carefully. This is also the stage where you stop assuming that you‚Äôll get truthful responses to your questions about the importance of your technical work and contributions to your organization. Why? (Er, um, who?)*
Simply put: your stakeholders and leaders might have some very good reasons to withhold the truth from you.
There will be a lot of different whos involved in your project, so you‚Äôll have to get all their motivations straight. You‚Äôll learn that having everyone aligned on an issue is a rare moment of bliss in the organizational chaos. And when the question about who gives you an answer that renders the why hopeless, you‚Äôll pick your battles wisely.
You‚Äôre the leader now.
Figuring out what‚Äôs worth doing is now up to you, but you‚Äôll be asking every question above in addition to the what that you‚Äôre now in charge of. After all, you‚Äôve matured enough now to understand that at the end of the day, everything boils down to people, so even though you‚Äôre on the hook about what to prioritize, you‚Äôll never escape the who (as well as the why and perhaps even the how). Good luck!
*Moving you from Stage 2 to Stage 3 is what my next article is all about. As we mature on the job, it helps to learn to do more than ask why. It‚Äôs useful to learn how to skip taking the answer at face value and learn to read between the lines.
kozyrkov.medium.com
P.S. Have you ever tried hitting the clap button here on Medium more than once to see what happens? ‚ù§Ô∏è
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Renee Zhang
Paco Nathan
in
Knowledge Technologies
Anusha Kuppahally
Korey Stegared-Pace
Joseph Asinyo
in
CodeX
Megatrend poslovna rje≈°enja
The Center for Effective Global Action
in
CEGA
akhil anand
in
MLearning.ai
"
125,https://medium.com/towards-data-science/forget-about-algorithms-and-models-learn-how-to-solve-problems-first-c791fde5842e,Forget about algorithms and models ‚Äî Learn how to solve problems first,"Almost weekly a friend or an acquaintance asks me, I want to learn to code; which language should I start with? More or less bi-weekly I get a DM on LinkedIn starting with My son should start programming; what is the best language for him?
It‚Äôs not just people who‚Äôve never coded before. Often I get these messages from people who have several years of coding experience under their belts.
I‚Äôm not saying this to complain.
I make half a living from prodding the pros and cons of different programming languages, frameworks, and AI models here on Medium. I profit greatly from people having such questions.
The questions are quite intuitive. After all, everyone would like to work with the best possible tools and build their software skills as quickly as possible.
And when you observe that every developer seems to use a different technology stack, it makes perfect sense to wonder which one is the right one.
The thing is, it all depends on the problem at hand.
No technology on its own is good or bad; it just depends on what type of problem you want to solve. At the end of the day, programming is just that: problem solving by means of using a computer.
So, for people who want to start programming or enhance their skills in software development or data science, the question should not be What should I use, Python or Julia? The question should be How can I solve software problems better?
For full disclosure, I‚Äôm not a computer scientist by trade. I‚Äôm a particle physicist who happens to use concepts from programming and data science because I deal with humungous amounts of data from particle colliders.
That being said, physicists are similarly sought-after as computer scientists. That‚Äôs not because of their knowledge about neutrinos or black holes; it‚Äôs because of their problem-solving capabilities.
Abraham Lincoln is quoted to have said, Give me six hours to chop down a tree and I will spend the first four sharpening the axe.
For programmers and data scientists, this means spending time understanding the problem and finding high-level solutions before starting to code. In the average coding interview, candidates are expected to spend less than half of their time actually writing code, and the rest of the time understanding the problem.
Don‚Äôt skip this step, ever!
Key to knowing whether you understand a problem is whether you can explain it to someone who isn‚Äôt familiar with it. Try to write it down in plain English or your mother tongue; draw a little diagram; or tell a friend about it. If your friend doesn‚Äôt understand what you‚Äôre talking about, you need to get back to the problem statement.
Key questions to ask are:
Every big problem consists of lots of smaller problems. Given our previous example with the linear regression, you might want to consider the following sub-problems:
Breaking the problem down helps you make a proper plan for your work.
It‚Äôs also more motivating, because you‚Äôll be achieving small but important milestones along the way. This is much more satisfying than sitting in front of a mountain of work and feeling like you‚Äôre not moving forward.
The devil is always in the details.
Instead of starting with the whole project, take a little piece of it. Try whether your plan works, or whether you have to adapt it because of unforeseeable difficulties.
This helps you get your head the hard parts. Many problems sound simple, but when you start building them there is one roadblock after the other.
In our example, instead of using all relevant variables, one could perform a linear regression on a couple of variables first. This won‚Äôt give you any points for project completion; however, finding bugs in your scripts when you‚Äôre still dealing with a small amount of data can be life-saving.
When you‚Äôre throwing all your data at the machine, running it for hours, and then come back to realize that the script hung up midway, you‚Äôll be very frustrated.
Trust me, this happens a lot!
Run small tests first, and make sure your solution works as you envisioned it.
This is the meaty part. Now you can build the solution for your large problem.
Throw all your data at the code. Run a fancy model. Do whatever you want.
Having completed the three prior steps, this should run through quite smoothly!
If there are errors, you might have to go back to steps 1‚Äì3 to see if you‚Äôve understood everything already and haven‚Äôt overlooked any bugs.
Just because you found one solution doesn‚Äôt mean you found the best solution. Don‚Äôt run off and call it a day; think about how you could optimize your solution and how you might be able to approach it differently.
You might want to exchange with your colleagues and ask them how they would solve the problem. Is their approach different from yours?
You could also try to identify the biggest bottlenecks in your solution, i.e. the parts that take the most time and resources to execute. How can you improve them?
Finally, reflect on how your solution might evolve in the future. Would new software frameworks or the use of AI make your solution better? How could your solution contribute to solving other, even more complex problems?
People, including myself, tend to obsess over different programming languages and the newest framework that might make everything 1000x more efficient.
It‚Äôs worth reminding yourself that this is less than half of what it takes to become an excellent programmer. The other half is problem solving.
You won‚Äôt acquire problem solving skills over night.
But if you apply these steps, ask the right questions, and do this often, you‚Äôre on the right path to taking your career from good to great.
Become a Medium member for full access to my content.
Radzion Chachura
Lior Barak
Mingjie Zhao
in
Towards Data Science
Drew Conway
in
Insight
Sayak Paul
in
Towards Data Science
Nathan Lee
Zach Quinn
in
Towards Data Science
Matti B
in
Dataology
"
126,https://medium.com/@agrawalananya17/exploratory-data-analysis-for-beginners-75fc6f16a8e0,Exploratory Data Analysis for Beginners ü§©,"We are living in an ever-evolving world, and technology has accelerated this process to boundless new possibilities. Digital revolution is making the world increasingly data-driven and to keep up with this transformation, businesses have to incorporate data analytics into their strategic vision and use it to get some mind-blowing insights for wise decision making.
Turns out, the recipeüìÉ to these insights, lies hidden in one acronym known as EDA (Exploratory Data Analysis). As the name suggests, it is a process used to probe various datasets and extract useful information from them which is often combined with data visualizations.
It can assist in finding glaring errors, better understanding data patterns, spotting outliers or unusual occurrences, and discovering intriguing relations between the variables or new variables altogether. Features obtained after EDA is done, can then be used for more complex data analysis or modelling, including machine learning.
To better illustrate the idea behind EDA, let‚Äôs take a sample dataset and embark on this quest for insights and what better than books üìö to get started with. The dataset that is going to be used here is of Amazon‚Äôs Bestselling books from 2009‚Äì2019 which can be found here.
For our analysis here, we‚Äôd be requiring pandas, numpy, matplot and seaborn library.
Using pandas read_csv function to read the loaded dataset and head() and tail() functions to look at the first and last five rows of data and the variables with their respective values. The column values are basically features that will be used to analyse the data.
The .info() function is used to get additional details about these features as shown. It can be inferred from this that there are 4 numerical features and 3 categorical features by referring to their data types.
Now let‚Äôs get some statistics for the given features using .describe(),
Using value_count() to get the count of fiction and non-fiction in genre column and visualizing it using seaborn‚Äôs countplot function:
Let‚Äôs analyze two features together and see the results, here number of reviews are plotted against genre, and it can be clearly seen fiction books have a greater number of reviews than non-fiction books.
We can also implement boxplots for visualizing the distribution of books across various features, as an example here, the price distribution of books is shown across the genre and price
Next up, a double bar plot is shown, they can come in handy in cases like these when we have two sets of data such as fictional and non-fictional books here. So, the data both of these sets can be visualized in one plot.
Here .sort_values function is used to first sort the data by price of the books in descending order and then the top 20 of those are included in the newly created dataframe ‚Äòbooks_price‚Äô
It is also seen that there are recurrent values in this particular dataframe, which can be removed by drop_duplicates() function to have a clear picture of what one is dealing with.
We can also explore the statistics of each feature exclusively, such as of Price shown below:
It is observed from the statistics of price that the minimum value of price in the dataframe is ‚Äò0‚Äô. This is not desired as prices of bestseller books cannot be ‚Äò0‚Äô. To fix this problem, we first look at the count of ‚Äò0‚Äô values in the price column:
Output:
Next, we use the map function to map all ‚Äò0‚Äô values with the median of the price values which is basically replacing the values:
Output:
If we check the minimum now, it is no longer 0.
Sometimes, we exclusively want to analyze a particular category, and hence to ease this purpose, the dataset can be divided into sub-datasets. There are many ways to do this but here it is split into fiction and non-fiction using nesting amongst the dataframe:
Now we can keep other features common and use the same functions to get individual insights for each category, for example the top authors in each genre:
It is also possible to split data into ranges to get meaningful insights, here the Price values are divided into three ranges as shown using cut function from pandas and a new feature called PriceRange is made. This is used to get a distribution of reviews depending on the price, and clearly low-priced books have the highest number of reviews.
This is visualized using Seaborn‚Äôs boxplot, again to get an idea about how the reviews are distributed across ranges.
In this way, we can go on and on about data exploration, applying functions, creating new features or modifying the existing ones and deriving insights. The possibilities are endless. The foremost important thing to keep in mind is the purpose, when one has a well-defined end goal, it becomes easy to carve the way to accomplish the task at hand.
No business can draw any assumptions with raw data, an exhaustive approach like EDA thus becomes an integral part before moving on to the modelling part of the work.
Thankyou if you read all the way down here! You can find the github repository of this code here on my github. Do check it out! You can also respond to this story, I‚Äôd be glad to get some feedback as this is my first blog on Medium.
Steve Rouge
Jungsik Noh
Harikrishnan N B
in
Analytics Vidhya
Patrick Titzler
in
Center for Open Source Data and AI Technologies
Victor Sim
in
Towards Data Science
Rohit Verma
Ng Wei Jie
in
Bootcamp
WHOOP
"
127,https://medium.com/towards-data-science/interview-questions-for-entry-level-data-intensive-jobs-a6647f2652b8,Interview Questions for Entry-Level Data-Intensive Jobs,"I teach data courses in an applied social sciences program, preparing students to work on various social and environmental problems in community-based agencies or government settings. My students are not preparing for a career in data science. Still, they will have data-intensive positions ‚Äî e.g., data analyst, business intelligence (BI) manager, program evaluator, research coordinator, quality improvement coordinator, etc.
While a strong market demand exists for data skills, the best jobs are highly competitive. This article presents interview questions from my consulting work in the non-profit sector. I also ask these questions when I hire advanced students and recent graduates to join my research projects as analysts or coordinators. I provide rationales for each question to help employers of entry-level analysts get at helpful information about applicants. More significantly, though, the rationales can give students and analysts in training essential insights into market demands. With this insight, students can be more deliberate in how they spend their time.
I see from your resume that you have skills working with different software applications. In what software environment are you most proficient in working with data? Walk me through your portfolio so I can better understand your skill level.
I regularly see applicants providing a laundry list of software skills. A laundry list gives information about the applicant‚Äôs awareness and exposure to software, but not proficiency. For example, most students in my introductory data courses indicate that they ‚Äúknow‚Äù Excel. Yet, few know how to create a pivot table or use the VLOOKUP function.
What I find is that most applicants indicate that they are aware and have been exposed to different software applications but do not have proficiency. The most robust demonstration of proficiency is with some data product that can be shared ‚Äî ideally, a collection of works organized as a professional portfolio. Regardless of the training, experience, or degree listed on their application, I screen out applicants who cannot provide a portfolio or some concrete example of their work.
For entry-level positions, I am often not seeking high levels of proficiency in a given software environment. Applicants proficient in working with data in a given software environment can quickly upskill in a different software environment, assuming they have a strong foundation in the principles of data.
You have many portfolio examples that appear to be class assignments. Have you done any personal projects? I‚Äôm interested in knowing whether you have conceptualized and carried out a project independently.
Personal projects are activities carried out to build skills and gain experience independently. Course assignments have fixed deadlines, a grading rubric, and content parameters. In other words, the course instructor defines the deliverables for class assignments. I‚Äôm interested in how much an applicant can make project decisions. Can the applicant independently conceptualize a project, break the project down into manageable steps, iterate on different solutions, solve unanticipated problems, and define when the project is complete? These types of decisions require many micro-level skills that take time to develop. Personal projects are an ideal way to build these critical micro-level skills.
Personal projects go beyond the requirements of a course, signaling an applicant‚Äôs commitment to engaged learning. Among applicants with skill deficits or gaps in experience, I still consider them for a position if they have a strong portfolio of work, especially personal projects. Personal projects are among the most vital indicators of intrinsic motivation and passion.
I routinely stress to students the importance of doing personal projects to build experience working with real-world data. Here are a couple of posts describing different ways to do this:
towardsdatascience.com
towardsdatascience.com
Tell me how you stay up to date with the field. What are the different social media sites and people you follow? Where do you go for information about the latest developments?
This line of questioning connects to my interest in engaged learning. I want to know where and how applicants obtain information directly related to working with data. Different social media platforms can be excellent resources for getting assistance in solving problems. These questions help clarify their involvement with the broader field of professionals. I am indifferent to whether the applicant has a social media presence unless their messaging is incendiary or divisive.
Tell me about your experience managing a project.
Applicants seeking an entry-level position do not have many project management opportunities. Even without formal experience, I look for indicators that help clarify their thoughts about managing activities toward a broader project deliverable. People do not like to be micromanaged in their day-to-day work. As a supervisor or mentor, I don‚Äôt want to be a micromanager. My interest is in creating a team where we have a common understanding of project goals and deliverables. Again, personal projects are an excellent way to help fill this void in experience.
Describe your approach to prioritizing project tasks.
This question relates to project management. But, I am trying to understand the applicant‚Äôs ability to differentiate between being busy and productive. When working with data, a full day of work can pass quickly. However, logging a whole work day does not mean the work has meaningfully advanced a given project. With this question, I want to know what systems the applicant uses to monitor their productivity. Can applicants recognize when they spend too much time on a given task and may require some guidance? Does the applicant acknowledge the importance of starting tasks well before project deadlines, allowing sufficient time to address potential unanticipated roadblocks? Time is among the most valuable resources that require careful and ongoing management.
I see you have strong technical skills. What ‚Äúcontent‚Äù area do you consider your strength? Do you have any work examples showing how you integrated your technical skills and content area?
To work with data effectively requires context. Having content expertise helps you understand the meaning and value of data. Without context, an analyst risks blindly running analyses without a clear understanding of what and why something is important. An applicant‚Äôs content area may not align precisely with the position. However, the applicant should be able to articulate the importance of integrating technical skills with a given content area. Demonstrating this understanding builds my confidence that the applicant can grow into the position.
Describe to me your strategies for solving data problems.
This question is crucial for new analysts. When I ask this question, I want to understand how they approach problems they haven‚Äôt previously encountered. I certainly allow time for new analysts to explore different solutions. Exploration also has to be balanced with efficiency and project resources. Can the applicant articulate an efficient approach? What sites do they use? Do they have experience posting questions to social media ‚Äî and, if so, are they aware of best practices? How long will they work on a problem before deciding to ask for help?
Here is a short article I wrote on this problem-solving. Entry-level data analysts should be aware of these essential strategies.
towardsdatascience.com
Why do you enjoy working with data? What excites you?
This line of questioning helps me understand the applicant‚Äôs fit for the position. I want to work with people passionate about high-quality work and learning new content and skills. I want to know that person‚Äôs passion.
Do you enjoy cleaning and preparing data?
Of course, I do not expect every activity to be enjoyable. But, the one deal breaker for me is statements suggesting they do not enjoy data cleaning and preparation. If you have read the data science literature, I am sure you will have seen comments ‚Äî and debates ‚Äî we devote about 80% of time and resources to data cleaning and preparation. That estimate holds for all of my projects. Unfortunately, many people regard data cleaning and preparation as mundane, low-level work. That is far from the truth. The quality of the data determines the success of a data project. Data cleaning and preparation involve a high level of skills and creativity. If the applicant doesn‚Äôt enjoy this type of work, I will have minimal confidence they will take it seriously and give it their best effort.
The data field is growing rapidly, with a demand for entry-level data professionals. Whether an employer or an applicant, I offer these interview questions and rationales to help you think critically about interviews for entry-level data-intensive positions. Feel free to use the comment section to provide other interview questions that you have asked or been asked.
Frederick Bott
Preetam B A
ASME IIEST Shibpur Student Section
in
The Treatise
Yoel Zeldes
in
Towards Data Science
Costas Andreou
in
Towards Data Science
Jean-Claude Cote
in
Towards Data Science
Tirthajyoti Sarkar
in
Productive Data Science
Ahmed Mohamed AbdElHameed AbdElHameed
"
128,https://medium.com/towards-data-science/how-to-build-a-successful-dashboard-359c8cb0f610,How to Build a Successful Dashboard,"At this point in my life, I think I have worked on more than a hundred different dashboards. As I grew more senior and started managing a team, I became more wary of these tasks ‚Äî questioning the actual need and expected value for each dashboard request. Building an effective dashboard actually takes time ‚Äî and when the requirements are not properly defined, it can quickly turn into a multi month nightmare (which ends up being used by no one).
And people are usually quick to push for a new dashboard. Building a dashboard is doing something that is actually tangible ‚Äî i.e. you are building a product that you can release in the ‚Äòreal world‚Äô (contrary to a study, that is more abstract). But ultimately, if there is not a clear issue that the dashboard would solve, or if the cost of building it is more important than the value it would create ‚Äî it might be a waste of time to take on the project.
And as your time is valuable, below is a checklist to follow to increase your dashboard project‚Äôs chances of success.
Before starting the actual work ‚Äî you should be clear about a few things, and in particular: why you are building this dashboard and for who.
‚ÄúHard choices, easy life. Easy choices, hard life.‚Äù ‚Äî Jerzy Gregorek
From my experience, it is important at this stage to put a stake in the ground and to be clear about what are and are not the use cases, and who exactly is the intended audience. If you make those ‚Äúhard‚Äù choices at the beginning, it will make your life way easier ‚Äî so make sure to properly answer the following questions:
Now that you have a clear idea on what success looks like and what you should be building, it is time to get to know your audience better and their relationship with data. Where are they in their data journey? What data are they currently consuming? What is their data dreams? Understanding them is key to make your dashboard a success:
The user experience of the dashboard plays an important role in the dashboard success, and too often, analysts overlook the importance of it. It is important to never forget that a dashboard is a tangible product that is meant to be used regularly ‚Äî and the best way to make sure of that last point is to develop it with your potential users:
Making sure the dashboard works before releasing appears like a no-brainer. But what does ‚Äòworks‚Äô mean? It means that ‚Äòyour audience is able to find the information they are looking for‚Äô and that has multiple components:
Remember on step 2 how happy you were when you found out someone already tried to solve this issue and documented their efforts? Now it is your time to give back!
You‚Äôre done! Now it‚Äôs time to go-to-market, to show the world your incredible work, and to receive congratulatory messages:
Remember the success criteria you defined during step 1 for this dashboard? Now it is time to use it and understand if you made it or not. Did you get the # of users you expected? Is your dashboard helping the right people to do the right thing? If it doesn‚Äôt ‚Äî look into what went wrong, and either fix it or learn from it. If it does ‚Äî celebrate, because life is too short.
Because a dashboard is a tangible product that is meant to be used on a regular basis, and because a lot of things can go wrong once you release the dashboard (e.g. the intended audience can change over time, the general needs can also evolve, the upstream data sources can be updated, etc.) ‚Äî you should make sure you have the right processes in place to maintain it and update it accordingly. Landing the launch is one thing, making sure it is a success on the long term is another one (and I‚Äôll touch on this on another article so.. Stay tuned I guess?)
Hope you found this insightful and I didn‚Äôt bore you to death with this long article. Would you add anything to this checklist? Let me know in the comment section!
If you had ‚Äòfun‚Äô see my other articles:
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
Aakash uppadhaya
Florent Poux, Ph.D.
in
Towards Data Science
Djamel GHARBI
in
Towards Data Science
Sofien Kaabar, CFA
Sofien Kaabar, CFA
in
Geek Culture
Akshaykendurkar
Murtaza Ali
in
Towards Data Science
Sofien Kaabar, CFA
in
Geek Culture
"
129,https://medium.com/towards-data-science/the-8-attributes-that-recruiters-are-looking-for-in-their-ideal-data-science-candidate-48e896af2763,The 8 Attributes that Recruiters Are Looking for in Their Ideal Data Science Candidate,"Landing a job in data science can sometimes feel like throwing darts at a board blindfolded ‚Äî sometimes you‚Äôre way off the board in your application and don‚Äôt even get an initial interview, sometimes you land close to the bullseye only to not fit the candidate mold perfectly in the technical interview, and sometimes you hit the bullseye and land a job offer.
Sometimes there's no rhyme or reason for how you did when throwing your dart blindfolded.
This means that it can be equal parts unnerving, frustrating, and crazy when you go to throw your hat in for a job that you can see has hundreds of applicants. You know the type of job I‚Äôm talking about ‚Äî some basic entry-level position for a mid-level company with hundreds of data scientists clamoring for their shot to be discovered. This may be the type of job that you apply for just because it‚Äôs there with no real belief that you‚Äôll get it.
However, what if you knew what recruiters were looking for? What if you knew that you could improve your dart-throwing ability by just presenting recruiters with what they‚Äôre after? What if you knew that of the hundreds of possible candidates, you and only a handful of others are the real competitors who hold all of the attributes that recruiters are dying for on their teams?
You‚Äôd probably go into the application process with a lot more confidence, wouldn‚Äôt you?
Over the last several months, I‚Äôve paid attention to recruiter posts on LinkedIn and Reddit that have outlined their struggles in finding ideal data science candidates. Recruiters from varying backgrounds, methodologies, and company sizes have reported the same consistent problems with candidates. From these descriptions, I‚Äôve been able to put together the ideal attributes that data science candidates should have according to the discrepancies noticed during the hiring process, and have coupled these attributes with advice from my own personal experience that you can use to achieve these attributes. Your job search process will no longer involve blindfolded dart-throwing ‚Äî you just need to focus on developing these eight attributes to help you become the well-rounded data science candidate recruiters are looking for.
It‚Äôs estimated that 40% of job candidates lie on their resumes and three out of four recruiters have caught someone lying on their resumes. These are pretty substantial numbers when you consider that hundreds of candidates may be applying for the same data science role at one time. The most common lies found on a resume include education, the time spent with a previous employer, and past salaries. Further lies can be found where candidates have exaggerated impact numbers (such as increasing a company‚Äôs data pipeline efficiency by 75%), used inaccurate job titles, lied about their technical abilities, claimed programming (and spoken) language fluency, or even used a fake address.
Therefore, the key here is to always provide recruiters with a recent, updated, and accurate resume that is designed with the job description in mind, represents your current qualifications, and is free from lies or other information that will keep you from getting the job. This may sound easy, but when you‚Äôre applying for hundreds of jobs it can sometimes be easy to fall into the trap of sending the same resume to all the companies and hoping one sticks.
Each time I apply for a job, I craft a unique resume keeping the parts that are relevant to the job and rewording sections to make them more applicable to the company. I rarely craft a cover letter unless it‚Äôs required to apply, and instead, focus on developing the best one page of information I possibly can. I also focus on a non-graphical resume to ensure that it doesn‚Äôt run into parsing issues during the application process. With recruiters only giving a few seconds to your resume, I try to keep it short, sweet, and focused on my ability to produce impact rather than the certificates I‚Äôve amassed or courses I‚Äôve completed.
It‚Äôs vital to take the time to produce a tailored resume for each job. This not only shows that you have an understanding of the job requirements but it also shows the recruiter that you have attention to detail and genuinely want the job. Your resume should include all relevant job experience, your past education, certificates you‚Äôve completed, and a list of personal projects that are relevant to the job.
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
I became a favorite to have on a team for group projects when I was in university studying software development because I was a ringer for presentations who could communicate the value of our project to a wide range of audiences.
I got my first job in tech because I had superior written communication skills that allowed me to explain technical concepts to non-technical clients and potential clients and why they would benefit their company.
Throughout my time working in tech, I was on the front lines of communicating with clients because they felt comfortable talking with someone who could go between them and the technical team to ensure that business problems were being correctly translated into technical cases and vice versa.
In short, I got to where I was in tech by having communication skills.
Data science recruiters are particularly looking for candidates who know what they‚Äôre doing inside and out and can answer a variety of questions in interviews that are applicable to daily work. For example, recruiters are looking for candidates who can answer questions in their technical interviews about what they‚Äôre doing that go far beyond ‚ÄúOh, I don‚Äôt know, it‚Äôs probably in the documentation‚Äù.
Unfortunately, communication skills aren‚Äôt focused on or taught in traditional data science learning pathways, which means that candidates who succeed in this area have these skills naturally. While communications courses are often taught as part of the program in university or college, one or two courses over 2‚Äì4 years generally aren‚Äôt enough to make you a competent or comfortable communicator.
Therefore, the only way to develop better communication skills is to practice. My favorite way to practice is to tell my very begrudging dog about everything that I‚Äôm doing, from why I‚Äôm writing the code the way I am to why I‚Äôm using this mathematical construct instead of another. Treating my dog like a recruiter sitting in on a technical interview is not only a great way to practice communicating what I‚Äôm working on, but it also gets you in the rhythm of talking while working. I went to school with lots of people who can only code in perfect silence. Even taking breaks to describe what they‚Äôre doing would throw them off because they had never practiced.
While some companies may not mind whether or not you can describe what you‚Äôre doing while you‚Äôre doing it, they‚Äôll certainly want you to be able to give a full rundown at the end, as well as be able to descriptively answer any questions they throw at you beyond, ‚ÄúOh, I‚Äôm sure you can find that in the documentation‚Äù.
Because the tech industry is on a sliding scale of requiring and not requiring credentials for a job, you must have the skills to support the job, regardless of whether your credentials are what they should be. This is often the reason why you‚Äôll see hundreds of applications for a single data science job ‚Äî because the field doesn‚Äôt always require credentials, anyone can apply to the job.
This results in many applicants not having the skills to support the job despite applying anyway. Because let‚Äôs be honest ‚Äî how many of us have applied for jobs saying we‚Äôre Excel gurus without knowing much beyond =SUM()?
Recruiters seem to be coming across many instances of people not having the right skills for the job (see point 1 above about lying on your resume) which means that while a job may have hundreds of applicants, there may only be 10 or so actual hopefuls.
Luckily, the skills required for the job are always well-outlined in the job description which can give you a good starting point for your studies.
Given that you‚Äôre reading this article, I can assume that you‚Äôre either a trained data scientist trying to find out what recruiters are looking for, or you‚Äôre someone studying data science who‚Äôs trying to find out what recruiters are looking for so you can tailor your learning experience. From these assumptions, I can already tell that you probably have the skills for the job that you‚Äôre applying to.
The real issue that arises here is with people applying to data science jobs who have no real skills in the area.
Instead of being this person who wastes the recruiter‚Äôs time, just read the job description and ensure that you have the skills required. One of my favorite activities is reading job descriptions to see what companies are looking for at a given time. If I don‚Äôt have all the skills they‚Äôre looking for, I make a list and then begin learning some of them. I do this for all the job ads that I look at so at the end of the learning process, I‚Äôll be well positioned to apply for any data science job that comes across my path.
Not sure where to begin? Try these resources to get started:
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
I‚Äôve said it before and I‚Äôll say it again: recruiters are only looking for candidates who are capable of producing an impact on the company the moment they join the team. One of the key determinants of this is a candidate‚Äôs track record of quality.
There‚Äôs not much more to say here beyond the fact that recruiters are looking for team members who can produce quality work right from the beginning, regardless if you‚Äôre an entry-level data scientist entering their first job or a seasoned veteran entering a mid-level management position.
In my experience, providing samples of your work whether formally or informally is the right way to present the quality of your work, especially when you‚Äôre first starting out. While recruiters may have to take a leap of faith since you have no recommendations behind you, the quality of your work should be able to speak for itself and provide the recommendation of quality that you need.
The best way to prove your track record of quality is through a pile of glowing recommendations from previous instructors, team members, and employers.
If that‚Äôs not possible due to you just starting out in the field, you‚Äôre going to have to showcase the quality of your work through projects and a portfolio. Your portfolio is how recruiters will be able to track your quality by getting to read about the impact of your projects and see the level of code and documentation that you write.
towardsdatascience.com
towardsdatascience.com
The data isn‚Äôt always good, the business problem isn‚Äôt always clear, and the tools are never the same year to year.
Data scientists have to be masters at adapting to all of these occurrences and more. Recruiters are looking for candidates who can roll with the punches and don't need a pristine environment in which to do their work.
The problem with university data science programs is (as mentioned in point 8) that they often give you simple, squeaky-clean problems to solve where the data isn‚Äôt really a mess, the problem is clear, and you have a required set of tools to use that you have been using for the last four years.
This means that to become a more adaptable data scientist, you have to find the worst data set, the most unclear business problem, and use an unfamiliar tool to pull it all together into a coherent analysis. This will be the only way that you can develop your adaptability as a data scientist. It‚Äôs not fun, but it will make any problem the company throws at you look like a cakewalk. As mentioned previously, practice will be your best friend in helping you develop this skill.
Adding these projects to your portfolio is a great way to show recruiters that you‚Äôre willing to do the hard work and that you can produce results despite adverse conditions. Talking through these projects can also give recruiters a sense of your thought process and how you come around to solve complex problems by adapting to the situation.
towardsdatascience.com
There‚Äôs a reason why ‚ÄúThe Pragmatic Programmer‚Äù is such a good book name: it refers to a programmer who can work through problems sensibly and realistically in a manner based on practicality.
Individuals new to data science tend to get wrapped up in the theoretical. This can distract from focusing on one simple thing: getting the project done. At the end of the day, all that matters is that you keep your code simple, make sure your project is adding value to the company, and get the task completed one bite at a time. Nobody cares about your code style if it‚Äôs too complex to refactor, nor will anyone care if your project contains a feature that ruins its intended goal. Data science, unlike other tech disciplines, is about getting the job done simply, on time, and while telling one heck of a story that clients and C-level executives will understand and abide by.
Recruiters are looking for data scientists who are practical, sensible, realistic, and determined. Why do an already hard job harder than necessary? Just because you can, doesn‚Äôt mean you should. Recruiters will be looking for candidates who can practically work through a problem and produce a conclusion that provides a clear, unaltered picture of what the data is representing. While this is the time when you can show off your practical programming by using one line of code instead of two to get the job done, it shouldn‚Äôt be at the expense of the project.
In other words, throw away all the theory that you don‚Äôt need to understand to get the job done, and instead focus on the practical aspect ‚Äî does this mathematical construct work for the question I‚Äôm trying to answer? can this code be sent into a production environment? will the stakeholder understand the gravity of the data if I use this visualization?
Pragmatism is well learned by individuals who complete 2-year technical diploma programs or bootcamps because these learning opportunities are often taught by instructors who have spent time in the industry cutting their teeth and truly using their skills. Pragmatism may fall by the wayside for individuals who complete university degrees taught by professors who may have never seen the outside of their faculty buildings.
Pragmatism is something that can be self-taught, and truly boils down to asking yourself this question every time you embark on a new project: what is the bare minimum foundation of information I need to complete this project? Then, you lay out the tools you need from your toolbox: Python and its packages, MySQL, linear regression, and Tableau, for example.
Nothing more and nothing less. Just the bare minimum you need to complete a project.
Yes, you could get into the nitty-gritty of visualization design, or debate the merits of linear regression for this type of analysis until the dogs come home, but at the end of the day, you need to begin working on the project, whether everything is perfect or not. If things need fine-tuning along the way, so be it. But the vital thing here is that you begin. I remember going to school with some individuals who had a hard time beginning projects because their pre-planning wasn‚Äôt absolutely perfect. While I believe that 10 minutes of planning can save 10 hours of reworking project aspects, I also understand that, in the fast-paced world of data science, you sometimes just need to start. So, start with what is practical, sensible, and reasonable, and the rest will fall into place.
Learning is difficult, if not impossible without these attributes.
When recruiters are looking for data scientists to join their team, they‚Äôre looking for someone who can still learn a new trick here and there. A data scientist‚Äôs value to their company is measured by the impact they provide in solving business problems. Therefore, the ability to keep up with new technologies, understand new problems that arise in the sector you're in, and the ability to learn something new are all skills that require some honesty and humility and that will allow you to provide an impact on your company.
Not only that, but data scientists need to have the honesty and humility required to acknowledge when their analysis isn‚Äôt the best it possibly could be or is even wrong. Growth and learning can‚Äôt happen without these attributes, which means that recruiters are looking for someone who can keep growing as a data scientist with their company long term.
My favorite way to exercise my honesty and humility is to participate in code reviews and after-project reports. Nothing helps you grow like hearing exactly what you did wrong and how you could have done it better. Code reviews are the perfect way to get input from your colleagues about different techniques to try or alternatives to help your code fit better into the production environment, whereas after-project reports help you see how your work fits into the bigger picture of an entire project. This is especially helpful when you‚Äôre working on only a small part of a much bigger project.
Beyond that, the only way to grow as a data scientist is to exercise a continuous feeling of self-improvement toward your skills in the field. New technologies are always cropping up and they‚Äôre always teaching something new in MOOCs that could help you become a better data scientist. The key is to make bettering yourself a priority that you incorporate into your daily schedule.
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
The business problems you‚Äôre given to solve in university or a data science bootcamp are squeaky-clean, clear-cut, simple, and almost give you the answer as to how to solve them.
However, once you enter the work world, you‚Äôll realize that the business problems you find there are much harder to translate into analyses. Sometimes the data sucks, or the client has no idea what information they‚Äôre really after or a whole variety of wrenches that can get thrown into the mix of your project.
Recruiters will be looking for you to be able to get a clear picture of what the client is looking for, determine if the data you have can be used to solve the problem (and if not, what data you will need from the client), and how you can produce the correct analysis mathematically that will answer your client‚Äôs problems. This means being able to work through customer-related problems, optimization problems, predictive problems, and more. Sometimes, clients won‚Äôt know what they want and will ask you to solve all of the above-mentioned problems. Your job is to figure out what the real business case is and explain to them why and how your analysis will provide the results they‚Äôre looking for to change the trajectory of their company.
Remember: recruiters are looking for you to be attentive and detail-oriented when working with clients, while also having the willingness to tell customers what their real business problem is.
The only way to get better at translating business problems into analyses is to get your hands dirty and work at it. Whether this means solving your own business problems using one of the countless data sets available on the internet, doing pro bono work for a small business in your area, or just rolling up the sleeves when your data science team starts on a new project, the main focus should be on exposing yourself to as many different types of data science problems as possible.
towardsdatascience.com
Subscribe to get my stories sent directly to your inbox: Story Subscription
Please become a member to get unlimited access to Medium using my referral link (I will receive a small commission at no extra cost to you): Medium Membership
Support my writing by donating to fund the creation of more stories like this one: Donate
Madison Hunter
in
Modern Programmer
Earhart Solution
Guillaume Coffin
in
Criteo R&D Blog
B√ºnyamin Ergen
Douglas Gregory
Annie Shi
Hamdi Yoga Pratama
Mapbox
in
maps for developers
"
130,https://medium.com/the-modern-scientist/how-to-become-the-best-data-scientist-6b1334f53244,How to Become the Best Data Scientist,"Practice like you‚Äôve never won and play like you‚Äôve never lost ‚Äî Michael Jordan
Who‚Äôs the best Data Scientist in the world? Cassie Kozyrkov? Yann LeCun? Someone else? Whoever this person is, you can be sure of one‚Ä¶
Naved Anjum
Jin Cui
in
Towards AI
Vivek Murali
Ritik Jain
in
Analytics Vidhya
Ben Huberman
in
Towards Data Science
Eric Yang
in
Towards Data Science
Naina Chaturvedi
in
Coders Mojo
Daniel Ebert
in
EnterpriseDataMap
"
131,https://medium.com/better-humans/the-best-strategy-for-studying-to-land-a-job-697bbc08af36,The Best Strategy for Studying to Land a Job,"Reader Talal asks:
I have a question:
1. Single variable calculus (MIT)
2. Mathematics for computer science (MIT)
3. Harvard CS 50 (Harvard)
4. Intro to algorithms (MIT)
5. Design and analysis of algorithms (MIT)
6. Leetcode practice problems
Are taking these courses enough if I want to get a job at big tech company?
Not being a professional programmer, I‚Äôm going to dodge the specifics of Talal‚Äôs question. (Although, anyone who does have experience here is welcome to weigh in!)
Instead, I‚Äôd like to use this question as a jumping-off point to discuss a broader question. How do you reverse engineer the learning required to achieve a particular outcome (getting a job, passing a test, proficiency in a skill)?
In general, I think there are two mindsets you can apply to learning:
Both approaches have merit, but they have different strengths and weaknesses.
Given Talal‚Äôs course choices, it seems he may not have a lot of programming experience (or he has programming experience but little formal computer science training). In this case, there‚Äôs probably a lot to learn, and proficiency won‚Äôt happen overnight.
In this kind of case, I think the strategy of starting with some foundational courses is reasonable, even if those courses alone won‚Äôt be sufficient for landing a job. Having some basic idea about how algorithms work, how computers work and how programming generally works is a good idea.
In contrast, if Talal already has a computer science background, taking more and more foundational classes would be unlikely to meet the goal he has in mind. Instead, he would need to work backward. He would need to figure out what specific skills are sought in programming interviews and ensure he has the required competency. If the skills of interviewing and performing on the job are different, he may need to master those too.
Working backward is important because most skills are fairly specific. There isn‚Äôt much payoff for practicing skills that differ considerably from the tasks you‚Äôre trying to master. Spending your time learning about general programming concepts will be much less efficient than practicing the skills needed to write good code and answer technical interview questions.
Starting with the basics, and ignoring the specific test criteria, is best when cognitive load is high.
Cognitive load refers to how much you need to juggle mentally to understand a new idea. It‚Äôs why quantum mechanics classes are hard and why you can‚Äôt easily repeat back a sentence spoken to you in an unfamiliar language.
Cognitive load isn‚Äôt fixed for a subject ‚Äî it depends on your prior experience. Familiar patterns don‚Äôt use up as much mental bandwidth as unfamiliar ones. This is why learning the tenth word in a new language is much more effortful than learning the 1010th. By the time you get to the latter, the basic patterns of pronunciation and orthography are so familiar that the word ‚Äúclicks.‚Äù
Some subjects intrinsically have a higher cognitive load than others.
Programming, for those without a background in it, is famously high in cognitive load. Everything looks alien and arbitrary. It takes a lot of work to understand what a variable is, what functions are, how pointers work, or why anyone would bother using recursion. You must grasp many individual elements to appreciate a programming design pattern.
In contrast, smaller elements of programming, such as the specific spelling of individual functions, may be low-load ‚Äî you can pick them up just through regular use.
The higher the cognitive load you‚Äôre experiencing, the more it helps to start with basics. Getting an introductory programming book, learning the theory and ideas, and getting lots of practice can make it feel easier.
Starting with the specific test requirements makes more sense once you have mastered the basics. There are two reasons for this:
You can have a (simple) conversation with less than a thousand words in a language. But full fluency likely requires fifty times this vocabulary. As you get deeper and deeper into infrequently used words, tailoring which words you learn to the situations you‚Äôre likely to encounter begins to pay off.
This is the basic pattern for all skills. The more advanced you get, the more numerous and specialized your knowledge becomes. As it can take a lifetime to master even a subspecialty, it makes sense to work backward from the end goal.
Abstractly knowing the right answer is rarely enough. You need to be able to retrieve it and apply it to the specific question you face. This, in turn, requires a lot of practice.
Most people who have studied CS would be able to say what a pointer is. But they might struggle to debug a program that has an error in how a pointer is dereferenced. To be useful, you need to be able to retrieve knowledge in the appropriate contexts. That requires practice in situations similar to those you‚Äôre likely to encounter.
Fluency in complex skills depends on rapid, automatic access to knowledge. It‚Äôs one reason we often make a distinction between knowing about something and being able to perform proficiently in it.
I generally try to do a bit of both when approaching a new learning project.
I often start by looking at the criterion task. If my knowledge is relatively weak, I‚Äôll use that task to assess which broad topics or subjects I need to study. If I am expected to show competence in algorithms, I‚Äôll take a few classes in algorithms.
As I get further along, I spend more and more time focusing on what is needed in the specific situation I‚Äôm going to face. Practice questions that are highly similar to those on the test are the best starting point. If those aren‚Äôt available, practice that is at least as difficult and covers the kinds of situations you face is probably helpful.
Trascender Global
Ozisco Obiyo
Leanne Lalonde
in
GC_Entrepreneur
Social TrendSpotter
mkrumdi1
Lu Ning
Jonathan S Lee
in
New Writers Welcome
Roberto Cortez
"
132,https://medium.com/towards-data-science/create-an-infographic-using-matplotlib-3fbb546c1628,Create An Infographic Using Matplotlib,"From my previous posts about the hierarchical structure of matplotlib plotting and the many ways to instantiate axes, we can see that these features render matplotlib a great potential for creating highly complex and customisable visualisations. To demonstrate this and also improve my own understanding of matplotlib, I set out to make an infographic using matplotlib this week.
An infographic generally combines visual imagery, data charts, and minimal text together. It aims to illustrate an easy-to-understand overview of a topic. Owing to this nature, its layout and content are more complicated and diverse compared to a regular static data plot, which normally exhibits one type of data in one type of plot (e.g. bar, scatter, line, and box plot or their variants). Furthermore, an infographic can be a stand-alone plot while a regular static data plot mainly serves as supplementary material and should be read within a certain context.
As you might already see the finished infographic from the heading figure, which shows the yearly patterns of daily maximum temperature and precipitation across eight Australian major cities in 2019 (I really hope you could get its topic before reading this paragraph. This makes me feel confident to proceed). My infographic was inspired by this infographic from South China Morning Post.
I personally really like to visualise large quantities of data in a grid-like structure. When the colours and shapes are set appropriately (e.g. brick red for high temperature and arctic ice blue for low temperature in my case), it starts to convey the patterns underlaid the intricacies (e.g. less arctic ice blue as latitude decreasing), while also delivers an artistic feeling (e.g. the appearance of a bunch of red bars looks like burning wood).
Alright, without further ado, let‚Äôs now dive into the code and show you how I created this infographic using matplotlib. As usual, you can access all the required data and the jupyter notebook from my Github.
Although this is not within the scope of this post, I list this section to highlight that it is the fundamental part of making an effective infographic. Only if you know what story to tell, you have a direction for gathering data and thinking about design, layout, charts, and aesthetics. As for our case, weather data was downloaded from the Australian Bureau of Meteorology website. We only need daily maximum temperature, city name and date values (i.e. max_tmp_day, City, and Date as shown in Figure 1).
As stated in my previous post, fig.add_axes([left, bottom, width, height]) is a good way to manually place axes on fig. However, it is tedious to accurately calculate coordinates for each axes on the fig when we have lots of axes (we have 18 axes in total) and alignment is required (Our eyes are very picky, even slight misalignments will throw things off balance). I hence adopted a mixture of common layout and arbitrary layout of axes on fig. using fig.add_gridspec() and fig.add_axes(). Here, fig.add_gridspec() is a function that enable us to specify the geometry of the grid that a axes will be placed. For example, imagine a 2 rows by 2 columns grid on a fig, normally if we use fig.subplots(nrows=2, ncols=2), this will generate four axes that distributed evenly on the 2 by 2 grid. What if, on the 2 by 2 grid, we only need 3 axes that the first axes occupies the whole first row and the remaining 2 axes occupy the second row evenly? The power of fig.add_gridspec() comes in here, which enable us to create axes spanned rows and columns as desired. We can achieve the aforementioned layout as follow:
As you can see, once a grid space object (i.e. gs) is created, we can make use of its grid geometry, just like access values from a 2D numpy.ndarray, to place axes accordingly. For instance, fig.add_subplot(gs[0, 0:2]) denotes add an axes occupied the whole first row in the 2 by 2 grid.
Back to the infographic, as shown in the above code, except for ax2 (the axes for the colour bar), all other axes were added by fig.add_gridspec(nrows=10, ncols=10). Specifically, on a 10 rows by 10 columns grid (Figure 2), ax1, which is the axes for the title, text and notes, occupies the first 2 rows and 8 columns. Each axes from ax3 to ax10, which are the axes for plotting temperature bar, occupies 1 row and 8 columns. Finally, each axes from ax11 to ax18, which are axes for rainfall data, occupies only 1 row and 2 columns.
As you can see, fig.add_gridspec() is a powerful function when flexibility and alignment of multiple axes are both required.
After the layout is confirmed, we can then add data plots. Here, take the temperature bar as an example (Figure 3).
Considering both information and aesthetics, each bar represents the difference between daily maximum temperature and the yearly average temperature of these eight major cities (24.43¬∫C) in Celsius degree. By doing so, the audience can clearly identify days with above-average temperature (bars upward) and below-average temperature (bars downward). Otherwise, since temperature values are positive, all bars will toward the same direction above zero, which makes the bar plot boring. In addition, unlike a regular bar plot, I removed all grids, x-axis and y-axis and only left essential parts.
Another additive is the text annotation with curved arrows to point out days with highest and lowest temperature individually. This add agility to the whole infographic in terms of aesthetics. This was delivered by ax.annotate(). The official document about ax.annotate() gives very detailed examples, so I will not repeat the introduction of this function in this post.
Since the style of temperature bar plots need to be consistent for all these eight cities, we just need to come up codes for one plot and add others by a for loop (see code below).
Although colour scheme is a very subjective decision, I can share the principles I followed here: creating contrast, grouping elements, and encoding quantity. In this infographic, I used a dark background to make the temperature bar plot and rainfall circles visually salient. The dark background also expressed a cheerless atmosphere to reflect the emergency of extreme weather. In addition, the use of consistent colours for bar plot and rainfall circles, respectively, helped to group information together. Finally, the application of a colour spectral (from arctic ice blue to brick red) for different temperature values highlighted the patterns.
The reason that an infographic can be a stand-alone plot is that necessary text helps to reinforce its topic. Here we can use ax.text() to put whatever text wherever we want on the fig. The only pitfall I found is that it is a bit troublesome to add a custom font family to matplotlib (one solution here). An alternative is to add text via Illustrator once the main body of infographic is finished and export as svg file.
As always, I welcome feedback, constructive criticism, and hearing about your data science projects. I can be reached on Linkedin, and also on my website.
Avikar Banik
in
Geek Culture
Roberto Di Bella
in
Level Up Coding
Alpha Theta Center
Promise Wilson Jnr
Aniket Kardile
in
MLearning.ai
TDS Editors
in
Towards Data Science
Fabrice Mesidor
in
Towards Data Science
Walker Payne
in
Towards Data Science
"
133,https://medium.com/towards-data-science/hands-on-exploratory-data-analysis-on-honey-consumption-in-the-usa-using-python-ddfb8be519d7,"Hands-On Exploratory Data Analysis on Honey Consumption in the USA, Using Python","Disclaimer: This article may sound offensive to the vegan people who follow me and read the stories. While I am sorry in advance for that, please be aware that this exercise doesn‚Äôt want to promote the usage, consumption, or selling of honey.
In the following article, we are going to do an exploratory data analysis on the production and consumption of honey in the US.
In particular:
Let‚Äôs start then!
The first step of EDA does not involve any code at all and is just based on qualitatively describing the data. First of all, the dataset comes from the National Agricultural Statistics Service, USDA, and the license is CC0: Public Domain (open source). The description says:
‚ÄúThis dataset provides insight into honey production, demand, and supply across different states of Honey is the natural product made by bees.‚Äù
And that is exactly what we are going to do: an exploratory data analysis on the production and price of honey for different states.
Another important thing that the dataset has is a column named ""year."" This means that for a given state, we have the average production and the average price in a given year. As we will see, this will allow us to compare the states' production and price and to see how it changes over time.
Sounds exciting right? üôÉ
Let‚Äôs start with the code.
Before starting the analysis, we have to import the libraries we are going to use. A lot of this analysis will use some plots, so I attached my preferences for the plotting part as well. Feel free to get rid of it if you don‚Äôt like beautiful plots üòè
Let‚Äôs import and show the first entries:
Nice, let me use the magic data. data.info():
As we can see, all the columns except the state one are numeric. This is good üòÆ‚Äçüí®
As we can see, the ""state"" column is the column that tells us the US state of the instance (Alabama, Arizona, Arkansas, etc.).
The production column, the average_price and the year columns, are the ones we are going to use the most in this specific example*
1. The Kaggle user informs us that the production is the product of the colonies' number and the yield per colony. It is, broadly speaking, how much we produce in terms of honey considering the space we are using and how many colonies we have. So I‚Äôd say the name is pretty simple but explicative. It is properly a summary quantity of the production of honey.
2. The average price refers to the average price per pound based on expanded sales.
*Keep in mind that this approach is easily generalizable by just changing the column string, as we will see.
3. All the instances in the dataset are related to a specific year that is indicated in the year column of the dataset.
First question:
Do we have all the states in the dataset?
To answer this question, I have downloaded the dataset list from this GitHub folder (how lazy, right? ), called it states, and compared it with the state list we have in our dataset:
So it seems like we are missing a lot of states, but it is not really like that. Let me show you the following plot:
The only states we are truly missing are:
All the other states are actually in the dataset. The only problem is that in the original list we have ‚ÄúNew York‚Äù (spaced) and in our dataset, it becomes ‚ÄúNewYork‚Äù (not spaced).*
*To see this effect, look at the xticks of the plot above.
Now, why don‚Äôt we have these states? The real answer is that I don‚Äôt know, of course.
I hypothesize that we don‚Äôt have enough honey production in those states. This hypothesis is confirmed by the fact that, if we look at USDA reports, these 6 states are in the ‚ÄúOther States‚Äù and the combined production is smaller than the other single states in the dataset.
Ok, let‚Äôs forget about the time column for a second. We want to get a summary of all the values. To do that, we can use the following function:
Let me show you some examples:
2. California
Ok, so for example, we can plot the mean value of the production and the average price for each state. If we do that, we can get information about which state has the most (or the least) production and which state has the biggest (or the smallest) average price. To do that, we have to use our defined summary_state function and the following block of code:
2. Top 10 states for average price (most expensive):
3. Summary plots:
What we can see is that the (relative) price range of the average is smaller than the one of production. That is probably because the production is strictly related to the place where you are (you can‚Äôt force the honey to grow in Alaska). On the other hand, the price is related to a lot of business factors and is chosen by men, not nature.
Now, I‚Äôd immediately stop doing this analysis because I have the feeling it could lead us to misleading conclusions. In fact, as we will see, the dataset that we have is indeed extremely time-dependent. This kind of time-averaged study can make us lose a lot of important bits of our analysis.
Let‚Äôs take three random states, say Ohio, California, and New York, and let‚Äôs plot the year vs. production line plots.
Mh. huge difference in range. Interesting.
Let‚Äôs do the same for the average price.
Now that is interesting. We have a huge drop in terms of the average price of our dataset. There are two possible explanations.
A. We have a problem in the data acquisition step. For example, in the first range of years (1995 to 2018), we are considering things in terms of dollars, and from 2018 to 2021, we are changing units to ‚Äúhundreds of dollars‚Äù or something like that.
B. There is an actual reason that leads us to this drop in average price. This is also possible. This study addresses the problem and gives us statistical reasoning about the drop in the price of honey.
I would say that option A is the most reasonable. I struggle to think of a price of 350+ dollars per pound. üòÖ I am pretty confident that it is a bug and that the scale is not in dollars (as the Kaggle user says), but in terms of cents. Anyways, for the purpose of this analysis, we will just get rid of that part of the data in our analysis so that we don‚Äôt commit any errors.
Now, what we are going to do is the following:
A. Extract the time and state-dependent data from our datasetB. Do a regression analysis study on the production and average price Compare the different states' values to see the evolution of production and the average price.
For example. Let me take Ohio. We are going to do the following:
*The degree of the polynomial is the hypeparameter that we are tuning.
To do that, you need this:
That‚Äôs what we get out of it:
Now what we are going to do is do it state by state.
2. Plotting function:
3. Some examples:
But why do we even do that?
Because we have the model now, we can sample this for way more data points than we have (let‚Äôs say 1000 instead of 20). We will use this surrogate model to compare two different states.
We will do that with the following function:
Let me show you some examples:
This surrogate model makes us see where the intersection between the two functions is.
In other words, it tells us when California (state 1) produced more than Ohio (state 2) and when, and if ever, Ohio (state 2) produced more than California (state 1).
From the examples I have given, we can make some very interesting points.
I think that this could be very useful. For example, we can use a state as a baseline (I don‚Äôt know‚Ä¶ Virginia) and compare it to all the other states. As engineers, we use this concept of a ‚Äúbaseline‚Äù and the concept of ‚Äúbaseline subtraction"" frequently.
If we want to do the same for the price, we just have to be careful about trimming it to the right point to avoid that drop, as we said earlier. I love you too much to get you into the details of this tedious data-cleaning step, but please trust me or look at the Jovian full notebook.
So if you want to do the same analysis but with the average_price rather than the production column, you can just run the following:
This is a cute looking example:
Let me finish this article with a very small study about correlation. Let‚Äôs say we want to know which parameters influence the average price the most. We can use two approaches:
The Pearson Correlation, which is what is implemented with pd.corr(), is nothing but an indicator of how much linear correlation there is between the features. As we have seen, even in the first bit of data, the correlation between the years and the average price doesn‚Äôt look to be linear at all. For this reason, I also used the Mutual Information Score, which gives you an idea of the statistical correlation between two features, which is what we really want.
Not so much of a correlation except for the year column. This is probably due to the fact that the data looks pretty scattered (look at some average price plots). The year and the average price are highly correlated, and this is 100% related to the huge drop we have seen in the price plot. If we do this analysis on the clean data, I am confident this value is going to decrease.
2. Linear Correlation Code and Results:
This is pretty interesting. The more you have, the less it costs. This is business 101, as reflected in the negative correlation between production and price. The linear correlation is also sensible to the drop we see in the average price. I am confident that by using the filtered data, this correlation is going to increase.
We did the following in this exploratory data analysis on honey consumption and price in the United States:
If you liked the article and you want to know more about machine learning, or you just want to ask me something, you can:
A. Follow me on Linkedin, where I publish all my storiesB. Subscribe to my newsletter. It will keep you updated about new stories and give you the chance to text me to receive all the corrections or doubts you may have.C. Become a referred member, so you won‚Äôt have any ‚Äúmaximum number of stories for the month‚Äù and you can read whatever I (and thousands of other Machine Learning and Data Science top writers) write about the newest technology available.
EDRANS Stories
Isha Garg
in
Towards Data Science
Amjad El Baba
100TB.com
in
20ms
UW Interactive Data Lab
Hamilton Chang
Paulo C. Rios Jr.
Brentforsberg
"
134,https://medium.com/@diawahad/data-analyst-data-engineer-data-scientist-experts-specialised-in-data-at-the-service-of-c50377fb64fc,"Data Analyst, Data Engineer, Data Scientist : Experts specialised in data at the service of business challenges.","Many companies rely on the data they collect to make crucial decisions. The amount of data generated is such that it is generally referred to as Big Data. To manage this data, companies need qualified personnel and in particular Data Analyst, Data Scientist and Data Engineer.
In my previous articles I have talked about these three jobs while trying to compare them briefly so as not to overload the content.
Krzysztof Ograbek
Roberto Cadili
in
Low Code for Advanced Data Science
Platypus.finance
in
Platypus.finance
Apache Pulsar Neighborhood
in
Apache Pulsar Neighborhood
Open Data Charter
in
opendatacharter
Alex Souza
in
blog do zouza
Krithika Nagi
Akashkr
"
135,https://medium.com/data-science-at-microsoft/writing-a-resume-for-a-data-science-role-345b98bdf80b,Writing a resume for a data science role,"I have spent a great deal of time reviewing resumes from my friends, coworkers, classmates, and family members ‚Äî not to mention strangers! I just love reviewing resumes.
After reviewing dozens of resumes, I often give the exact same advice to most people. So, I thought I would take the feedback that I typically provide and summarize it in this article. I believe most of my resume advice works just as well for non-data roles as it does for data roles, so I think what I recommend can be applicable for anyone.
Please know that I am not a professional career coach. This article reflects my own point of view and not that of my employer, Microsoft.
The included resumes are real, but personal identifiable information has been removed or changed, and permission has been given to use them.
See the difference? I have known all these people personally, so when I saw their resumes, I knew they could improve how they were representing their experience. Also, note that each resume is still very different.
So how did I help get these resumes from point A to point B? And what is the feedback that I tend to give over and over? Let‚Äôs break it down by component:
A resume broken out into components: Header, experience, education and miscellaneous.
A header should contain your name, how to get in contact with you, and the most important things you want to highlight to the hiring manager or recruiter.
How long does it take a recruiter to decide if you‚Äôre right for a job? It‚Äôs actually around seven seconds, according to eye-tracking research.*
This means that sometimes a recruiter might not even get to your experience! That‚Äôs why I love a good resume summary. This allows you to tell the hiring manager why you‚Äôre qualified, why you‚Äôre interested, and why you‚Äôre different from other candidates. The summary is also a great place to highlight recent achievements (such as a new degree or certification), or to explain why you are interested in changing industries or type of job. It is also a straightforward way to personalize your resume for each role that you‚Äôre applying for with very little effort.
Writing a resume summary can be challenging ‚Äî I‚Äôm still not happy with mine! It‚Äôs important that it be genuine and have substance, but without too many buzzwords, being too long, or having too many details. I recommend avoiding platitudes and clich√©s such as ‚Äúself-starter,‚Äù ‚Äúmotivated,‚Äù ‚Äúrock star,‚Äù and ‚Äúunicorn,‚Äù but don‚Äôt be afraid to emphasize your soft skills. And remember, a summary should not be more than three sentences.
Here is a recent example that I used when applying to an analytics consulting company (as I mentioned, I tweak my summary for each company to personalize my resume, so this is just one example):
Riesling is a full-stack data scientist with an MS in Analytics who gravitates toward Analytics Engineering to ensure data quality and usability. As demonstrated by her success in a wide variety of companies across B2B and B2C, Riesling learns quickly, adapts, and finds ways to improve data infrastructure to accelerate data science. In addition to her core responsibilities, Riesling functions as team ‚Äúglue‚Äù to help a team become stronger together ‚Äî above all she likes improving processes and helping people.
Information that must be in your header includes:
Information that could be in your header includes:
Information not to have in your header includes:
The experience section is the heart of the resume and is meant to convince the recruiter to call you, or the hiring manager to interview you.
Your resume is a document to tell the hiring manager why you are qualified for the role they need, not to list out what you have done in the past. Your experience section should reflect that ‚Äî leave out details and positions that are not relevant to the role (or that you didn‚Äôt enjoy) and emphasize and elevate aspects of previous experiences that are applicable.
This section should be filled with the work you did, its scope, and its impact ‚Äî this is the place for metrics! This section does not need to include the tools or packages that you used; each company may have a different tech stack and is likely to trust that if you know how to solve a problem, you‚Äôll find the right solution with the resources available. (Plus, you will highlight your tech skills in the ‚Äúskills‚Äù section further down.)
Here are some tips that I have given for the experience section:
Some before and after statements, along with the feedback that was given:
The education section may be more straightforward, but I often provide the following feedback:
This is for all the other material that can appear on your resume. It typically is near the bottom (either right before or right after education).
The skills section is a must have. This is a concise list of the tools, platforms, and other skills that you have that are relevant for the role. This is where a data hiring manager will be looking for SQL, Python or R, a BI platform, a work management tool, a workflow engine, web analytics tooling, stakeholder management, and soft skills. It‚Äôs always good to read the job description to find the skills being sought for the role, and make sure to list them (or at least list something in the category, such as Power BI or Tableau). Here are some additional tips:
I tend to leave off personal interests unless I have a reason to include them. Here are some examples of when they might be appropriate:
I hope these tips are helpful and not overwhelming. My ‚ÄúTLDR‚Äù for resume feedback is usually:
As I mentioned previously, I am not a professional career coach. These are just my personal opinions that I have seen work. If you have any resume tips that I missed, I would love to hear them! Please include them in the Comments section below. Also, check out my related article on my experiences changing roles during my data science career, linked below.
medium.com
Riesling Meyer is on LinkedIn.
Chin Jern Tat
Simon Nicholls
in
Pragmapolitic
Neal Wang
Pranav Bhargava
Chloe Robinson
in
Introduction to Cultural Analytics
Mrinal Gupta
Saran
Muhammad Hussnain
"
136,https://medium.com/@hipiyushjain/sql-windows-functions-explained-like-you-are-5-af6f034fa7d7,SQL Windows Functions Explained like you are 5!,"If you are an aspiring data analyst, you must learn SQL. If you are currently learning SQL now, YOU NEED TO LEARN WINDOWS FUNCTIONS IN SQL. If you understand these functions, it will make your life a lot easier whether it is to crack interviews or on the actual job itself.
The built-in SQL window functions are then sometimes divided into two different types: the ranking and value functions. The ranking window functions are used to simply assign numbers to the existing rows according to some requirements. On the other hand, the value window functions are used to copy values from other rows to other rows within the defined windows.
Suppose you have financial data of companies and you have recently started investing and you want to invest in companies now. You have the criteria or basic idea that you will only invest in companies that are in the Top 3 of their business category. (See image above). That‚Äôs where the importance of windows functions comes in. You will use the windows rank function(See the first image) and fetch on top 3 companies by the market cap of their business. (See the code below).
Ranking functions are, RANK(), DENSE_RANK(), ROW_NUMBER()
Value functions are, Lead() , Lag().
As they say, clear the fundamentals and everything else can be learned along the way. This was written just to provide you with the basics of windows functions in SQL.
That‚Äôs It. I hope it helps you in your journey of becoming a #data analyst. I have used and used most of these websites not just for practice but for revision of multiple aspects of the data analytics journey.
If you have any questions or any inputs you want to share, just comment or hit me up on LinkedIn or Twitter. I am pretty much active there.
All the best.
Please Start. You will not regret it.
Thanks,
Piyush Jain
Mateusz Tu≈Ñski
in
Keeping Code
William Buist
Sahibsingh
Mia Arh
Aaron Hanson
in
Object Partners
Daniel Cohen-Dumani
in
Digital Transformation Today
Leonardo Alexandre de Geus
Ross Meadows
"
137,https://medium.com/bitsrc/7-github-repositories-to-become-a-pro-javascript-developer-8ce09c4f8950,7 Github Repositories to Become a Pro JavaScript Developer,"JavaScript is a lightweight, interpreted programming language. It is designed for creating network-centric web and mobile applications. It is complementary to and integrated with Java.
JavaScript is very easy to implement because it is integrated with HTML. It is open and cross-platform.
Some of the best JavaScript repositories are listed below.
This repository takes the concepts of clean code by Mr. Robert C. Martin and adapts them for JavaScript so you can use them in future work.
This repository consists of a list of software(Saas, Paas, Iaas, etc.) and other offerings that have free tiers for developers.
This repository can be used by anybody who wants to land a job at a tech company but is new to technical interviews.
This repository contains JavaScript-based examples of many popular algorithms and data structures.
This repository is a series of books deep-diving into the core mechanism of the JavaScript language.
This repository is an amazing place to keep yourself learning and updated about the Node.js world and also learn about best practices while you are at it.
A collection of inspiring lists, manuals, cheatsheets, blogs, hacks, one-liners, cli/web tools, and more.
Congratulations on reaching this far! You‚Äôre a fantastic reader!!
These are some of the good repositories available, and there are many more. You can dig deeper to check out all the repositories for JavaScript.
If you wish, you can support me by buying me a Chai.
Follow me here :) Adarsh gupta as well as on Twitter.
Bit‚Äôs open-source tool help 250,000+ devs to build apps with components.
Turn any UI, feature, or page into a reusable component ‚Äî and share it across your applications. It‚Äôs easier to collaborate and build faster.
‚Üí Learn more
Split apps into components to make app development easier, and enjoy the best experience for the workflows you want:
blog.bitsrc.io
blog.bitsrc.io
bit.cloud
blog.bitsrc.io
bit.cloud
Paolo Giangrandi
in
ITNEXT
JP de Vries
in
Frontend Weekly
Kevin Maene
Dana Sugu
Filippo, Some NotDev Dev
Saurabh Gour
Jon Tonti
SunJet Liu
"
138,https://medium.com/@elliotgraebert/laptop-development-is-dead-why-remote-development-is-the-future-f92ce103fd13,Laptop development is dead: why remote development is the future,"I have spent a decade in charge of development tooling teams. During that time, I watched the rise and fall of Vagrant and the introduction of Docker and a plethora of other build tools. I remember when most developers had two desktops under their desks, and I helped with the mass migration to Mac laptops. I also helped develop internal platforms for self-service compute on AWS. All of these tools were intended to bring production environments closer to development and provide easy configuration and scalability of the local environment. However, none of them nailed it.
Since Palantir, I‚Äôve talked to many different developer experience leads. It turns out that there are some common themes.
But honestly, none of that matters compared to this:
If you absolutely love your laptop and have never had to close out of most of your applications to get your damn Teams meeting not to stutter, then this blog post is not for you. Go away.
Remote development involves having a remote machine performing all build and test processes, with your laptop serving as merely a browser or thin client.
Want to skip reading the rest of this article? You can just head straight to this GitHub Repo and try out Coder OSS in 20 minutes. Only keep reading if you want to know more about why remote development is the superior development environment.
If you like this post, please üëè , subscribe, or follow me on medium or LinkedIn to let me know I‚Äôm on the right track!
¬∑ Why is remote development better than local development?¬∑ Spin up a remote development ecosystem in under 30 minutes¬∑ What types of problems does remote development solve best?
Remote development significantly improves developer productivity
The core argument for remote development is that by moving the development workload to a remote machine, you are able to move, configure, and scale the remote machine significantly faster than you could distribute new laptops to your development team. If the remote machine is optimized for your use case, it can do the following.
I first ran across this model with an engineer who was using code-server (note the 57k stars) on a remote VM. His driving issue was that he was unable to check out and build a large code base because his laptop was so bogged down with security tooling that his IDE would crash. My mind was blown, but I realized that this model wasn‚Äôt going to scale. VMs can be very expensive and running one or more environments per developer would be unreasonable. Developers only need their environment running during working hours, and this type of usage is perfect for Kubernetes.
Deploying the remote development environments as a Kubernetes pod solved my cost and setup problems. I could control the execution environment, base images, and security practices, and I could utilize the cost advantages by scaling up and down the computing capacity as developers started and stopped their workdays.
As illustrated above, the idea is that each developer has their own persistent volume, but their underlying container can be made ephemeral. Since storage costs are a small fraction of the total compute costs, this enables developers to run really powerful containers during their workdays while their company spends only pennies in the evenings and weekends.
Ok, fine ‚Äî I don‚Äôt actually believe it will die, but not because I have a very compelling argument. Large organizations like Google and Facebook have been using a remote development model for years, but the OSS tooling to make this experience great has taken a while to catch up.
The most reasonable counterargument is that remote development is not ideal for developers that work in areas with spotty internet. While this is true today, with efforts like Starlink and ASTS, I believe that we are only a couple years away from globally accessible internet to be a reality for all.
The second most reasonable counterargument is around the remote development experience of the JetBrains suite of IDEs. JetBrains released Gateway in 2021 to address these issues. Unfortunately, JetBrains‚Äô Gateway was rough at first, and I would not have recommended it for full time IntelliJ or Webstorm users. However, it has made significant advancements in the last year, and I expect that within 1‚Äì2 years, users will find it equivalent to local development.
To be frank, the primary hesitation is a lack of belief that a browser-based IDE would be acceptable to a full time developer. It‚Äôs got to be laggy, difficult to use, unreliable, or something unpleasant right?
This is where CoderOSS shines. It‚Äôs a free way to see for yourself what a remote development experience could be like for your developers. Trying it out yourself is the best method.
I‚Äôve built an easy-to-use repo as a demo path to show what a remote Visual Studio Code on Kubernetes looks like. I chose to use Google‚Äôs hosted Kubernetes service, as they offer a $300 credit and GKE Autopilot is dirt simple to use.
Coder install instructions:
The entire setup should take you less than 10 minutes, though if you are new to Google Cloud, it might take you a little bit longer.
Coder setup Instructions:
Not sure about you, but this blew my mind the first time I saw it in action. I was used to a world of Windows RDP or VNC, and the idea of using my browser as my full-time IDE seemed like a gimmick. After seeing it in action, though, I was sold.
Having led a developer experience team for years, and as a devout believer in DevOps (at least, DevOps as described by the Phoenix Project), I can say that while optimizing your company‚Äôs development workspace is important, it might not be your most important problem. If any of the problems listed below are one of your top issues, then remote development is worth exploring.
Problems that are well-suited for remote development
Development types that lend themselves well to remote development
Remote development is quickly taking over as an alternative to local development, and the Kubernetes model seems to be the most compelling version of this. An engineering team can easily double the memory capacity of their team‚Äôs containers, but doubling their laptop memory is excessively expensive. No matter what you try to do to optimize your code, in the end, you are often stuck at the mercy of Chrome or resource-hogging security tools.
The Kubernetes remote development model encourages the full encapsulation of the development requirements, which pays major dividends for companies that have to deal with historical releases or legacy code. The Kubernetes model also enables horizontal and vertical scalability, preventing your developers from ever being inefficient due to ‚Äúperformance.‚Äù Finally, your finance team will thank you for the cheaper laptops.
While remote development has long been a common practice for software giants like Google and Facebook, Coder OSS has opened the doors to make it readily accessible to companies of any size. If you followed the instructions in my repo, you were hopefully able to see this live for yourself in under 30 minutes.
I hope you enjoyed this post. If you did, please clap, follow, or hit me up on LinkedIn to let me know that I hit the mark.
Renzo Regio
Jakub Karkoszka
in
KubeIt‚Ää‚Äî‚Ääjkarkoszka‚Äôs blog
Jo√£o Ramiro
Pixlverse
Dio Wu
in
Uncle Dio
ZaranTech
Rahul Bhardwaj
Aakash Goel
"
139,https://medium.com/techtofreedom/8-levels-of-using-type-hints-in-python-a6717e28f8fd,8 Levels of Using Type Hints in Python,"Dynamic typing, which means that the type of a variable is determined only during runtime, makes Python flexible and convenient.
However, every coin has two sides. Messy typing of variables is usually the root of hidden bugs. Robust programs‚Ä¶
Marianne Bellotti
Guilherme Teixeira
Srinivas Deshraju
Timothy Karani
in
The Startup
SoldierGirl
Double Pointer
in
Tech Wrench
POOJANKUMAR TANDEL
Tech Jobs Here
"
140,https://medium.com/@anna.wu9222/amazon-business-analyst-interview-12dc3c0de4a6,Amazon Business Analyst Interview,"Apart from getting the necessary qualification, you need to prepare well to increase your chances of success when you go for an interview. This article provides essential details on the Amazon business analyst interview and tips to help you prepare for the interview.
Amazon is a large tech and e-commerce company that utilizes product development and business analytics in expanding its business. Being a business analyst in Amazon, your sole responsibility will include the ability to build a functional set of operational and business metrics.
A business analyst in Amazon has to work with other teams to design, create, and employ the reporting systems. It also involves your ability to use metrics to enhance development opportunities. A business analyst will also be involved in aiding the Kaizen processes.
The Amazon business analyst interview is similar across the entire team. What matters is that your resume is shortlisted. The interview has about 4 to 5 rounds which will have to take place within a day. However, you must have done an online screening with a hiring manager. The round of interviews involves Amazon‚Äôs 16 leadership principles which aim is to test your ability. It also includes Amazon technical interview questions.
In essence, the interview involves the leadership principles, technical interview questions, background knowledge on database system design, coding, SQL queries, and product and business cases.
Similarities Between Amazon Business Analyst Interview Compared to Other Tech Companies
There are certain things the Amazon business analyst interview has in common when compared to other Tech companies. They include;
Differences Between Amazon Business Analyst Interview Compared to Other Tech Companies
Compared to other tech companies, the Amazon business analyst interview emphasizes using the STAR method during the interview to explain your work experience better.
Amazon has moved almost entirely to AWS internally, so it will be good to possess some AWS knowledge, especially the services related to databases.
Tip: Address the scenario-based questions by explaining in the context of a situation you were involved in and the responsibility given to you. Explain how you were able to perform your responsibility and the outcomes of your actions.
In essence, you should make use of the STAR method. (Situation, Task, Action, Result). Emphasize more on the action and the result.
There will be at least one round on technical skills. This round aims to test your SQL skills. However, it will help if you have background knowledge in Python or data visualization tools. You should expect SQL questions on your interview and other elements of data engineering such as data extraction and transformation.
The Amazon business analyst interview includes the behavioral interview, which has to do with leadership principles. Explain how you have exhibited leadership competency through past events.
Some examples of leadership competency include;
Structured Query Language (SQL) is a standard for accessing databases. It is an integral part of the Amazon business analyst interview as you will be required to provide solutions using SQL.
You don‚Äôt spend much time focusing on the theoretical concepts of Amazon and its services because only a few questions are asked during the interview. You need to prepare yourself for the interview by solving practical SQL questions.
Some of the basic concepts of SQL interview include;
Amazon SQL questions aim at testing your knowledge of the SQL concepts. However, there are questions you need to pay detailed attention to. SQL questions start from the basic definition-based questions to advanced practical exercises.
Some questions that are often asked are divided into seven categories. They include;
Tip: You can practice for free on Big Tech Interviews!
Here are some questions to help you prepare for your SQL interview under each of these seven categories;
To prepare, all you have to do is review a list of SQL terminology and application definitions. Understanding the differences between SQL joins, the different types of aggregations accessible, and basic procedures like CASE WHEN and HAVING are just a few of the questions that will be asked.
Here are the major concepts defined;
Reporting and Metrics SQL Question
This set of questions examines your problem-solving skills and knowledge of SQL functions. It also demands advanced SQL skills.
ETL SQL Interview Questions
ETL stands for ‚ÄúExtract, Transfer, Load.‚Äù
Questions ‚Äì
Database Design SQL Interview Questions
Database design SQL questions test your knowledge of data architecture and design.
Questions ‚Äì
Logic-based SQL interview questions
These sets of questions are not relatable to real-life questions and can be tricky. Are very tricky.
Question: Write a query to create a new flight routes table, displaying unique pairs of two locations.
Statistical Analysis & Mathematical Logic
Question: How are mean, median, and mode in a positively skewed distribution related?
Getting a job at Amazon requires many technical and business skills, including logical thinking, problem-solving skills, communication skills, and leadership skills. You should focus on developing these skills before going for an interview.
Get as many practice questions before the onsite interview using Big Tech Interviews. You should expect a bar raiser towards the end of your interview. Amazon uses an individual not part of the interviewing team to enhance objectivity and sustainability.
Originally published at https://bigtechinterviews.com on June 09, 2022.
Aarush Bhatt
Digital Defined
Arnav Roy
Leighann Amanda
Authority Magazine
in
Authority Magazine
Santino Dickens
Santino Dickens
d'Oliveira & Associates
"
141,https://medium.com/towards-data-science/ultimate-guide-to-statistics-for-data-science-a3d8f1fd69a7,Ultimate Guide to Statistics for Data Science,"Statistics is a powerful mathematical field. I‚Äôm saying ‚Äúpowerful‚Äù because it helps us to infer a population outcome from sample data. As it can infer population outcome, it can also be used for the big picture (like overall impact, future forecast, etc.). Statistics is not just a combination of some merely isolated topics. Moreover, Statistics discovers new semantics within these topics, which sometimes paves the way for new opportunities. Statistics needs everywhere in our day-to-day life. The main problem I faced in my junior year was that I couldn‚Äôt relate statistical knowledge to real life. I was unknown where to use which techniques to find the answer and act accordingly. The new journey began when I started to learn data science back in 2018. I covered all the basic statistics topics needed for data science and realized how necessary the knowledge of statistics is! As a lecturer of Artificial Intelligence and Data Science at a University, I conduct statistics classes and try to represent it as a useful tool for our regular usage. I also want to share the knowledge throughout my write-ups. That‚Äôs why I have written a series of articles covering all the basic topics of statistics and tried to represent them in the easiest possible way. Though some topics are missed in the series, I will try to integrate them into this article along with my previous works. I hope every walk of people will easily understand these articles on statistics. No prerequisite is required. Finally, I want to say, ‚Äúthis article covers most (but not all!) topics, and it will provide a base of statistics for data science so that you can explore other advanced topics if needed.‚Äù
So, if you are an absolute beginner to statistics and searching for a complete guideline, this article is gonna be for you.
The next section is the most important part of this article where all the articles in this series will be linked with brief descriptions.
As per my understanding, statistics combines some techniques for drawing a reliable conclusion about a large group (population) by experimenting on a small group (sample) and summarizing the dataset. It is not a formal definition; it‚Äôs my realization while working with statistics.
Let‚Äôs look at a formal definition according to Wikipedia [1] -
Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.
There are two categories of statistics.
The population consists of all the members of an experiment, whereas sample is a selected group of members from the population which represents that population.
For example, we want to know university students‚Äô average CGPA. Here, the experimental area covers all the students. So, the population will be all the students of that university. If we pick some students to calculate the average CGPA, these students will be the sample.
Before jumping to statistics, you must clearly understand the topics.
To make your idea crystal clear please read the following article ‚Äî
towardsdatascience.com
Simply variable is something which can vary (hold multiple values). It is nothing but the features of a dataset. There are different types of data as different features exist in the real world. We must know the measurement level to understand how we deal with the data.
If you have any confusion about the topic, go through the article.
towardsdatascience.com
Central tendency is a way to find out the tendency of majority values. In statistics, mean, median, and mode are used to know it.
The concept of ‚Äúmean‚Äù is straightforward. We get the mean value by dividing the summation by the number of values (n).
Complete Mean Guideline ‚Äî
medium.datadriveninvestor.com
The Median is another way to know the central tendency. To get the median value, we need to sort the values in ascending order and pick up the middle value, it varies with the even and odd number of values.
For example, 12, 13, 10, 15, and 7 are the series of values. Firstly, we need to sort out the values. After sorting, the sequence will be 7, 10, 12, 13, and 15. The total number of values is 5, which is an odd number. So, we will use the following formula ‚Äî
In our case, 12 is the median.
Another example is that some values are 12, 13, 10, 15, 7, and 9. After sorting, we get 7, 9, 10, 12, 13, and 15. This time, the number of values is 6, and it‚Äôs even. So, we won‚Äôt get the middle value with the above formula. Because (6+1)/2= 3.5 is not a whole number. Now, we need to sum up the 3rd and 4th values. And their mean is the median value, 22/2 = 11.
Python Implementation ‚Äî
The mode works on categorical data, and it is the highest frequency of a dataset. Suppose you have some data containing the quality of a product like [‚Äògood‚Äô, ‚Äòbad‚Äô, ‚Äònormal‚Äô, ‚Äògood‚Äô, ‚Äògood‚Äô]. Here, good has the highest frequency. So, it is the mode for our data.
Python Implementation ‚Äî
When to use which central tendency?
In the case of nominal data, we use mode. For ordinal data, the median is recommended. Mean is widely used to find the central tendency of ratioed / interval variables. But the mean is not always the right choice to determine the central tendency because if the dataset contains outliers, the mean will be very high or low. In that case, the median is more robust than the mean. We will use the median if the median is greater or less than the mean. Otherwise, mean is the best choice.
For more details, click here.
A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value (or score) below which 20% of the observations may be found [2].
In the percentile, the entire values are divided into 100 different parts. The quartile divides the values into four equal parts, and each part holds 25%. The main quartiles are First Quartile (Q1), Second Quartile (Q2), Third Quartile (Q3) and Fourth Quartile (Q4).
IQR is the range between Q1 and Q3. So, IQR = Q1 ‚Äî Q3 .
We can also find out the outlier with IQR by defining a minimum (Q1 -1.5*IQR, also known as lower fence) and maximum (Q3 + 1.5*IQR, also known as upper fence) boundary value. Outside the minimum and maximum values are considered outliers.
Boxplot shows all the Quartiles and upper and lower fences.
Frequency is the measure of the occurrence of an event in a dataset. The following articles will help you to know details about the topic.
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
The concept Measure of Dispersion indicates how spread the values are! Range, Variance, Standard Deviation. etc., are some of the techniques to find dispersion.
The range is the interval of maximum and minimum values. For example, we have some sample data 12, 14, 20, 40, 99, and 100. The range will be (100‚Äì12) = 88.
Variance measures the difference between each value of a dataset from the mean value. According to Investopedia ‚Äî
Variance measures how far each number in the set is from the mean (average), and thus from every other number in the set [5].
Here, xÃÑ is the sample mean and n is the number of values.
Œº is the population mean and N is the number of population values.
Standard Deviation is the square root of variance.
Python implementation of variance and standard deviation ‚Äî
Covariance is a way to compare the variance between two numerical variables. The following formula is used to calculate it.
Here, x and y represent two variables. N is the number of the population.
It finds the linear relationship between two numerical variables.
The correlation value fluctuates between -1 to +1. -1 indicates an entirely negative relationship, whereas +1 indicates a completely positive relationship between the variables. And 0 means no relationship.
Python Implementation ‚Äî
Normalization is the process of converting the data into similar scale and it is one of the key parts of data pre-processing. The following article integrates all the techniques for data normalization.
medium.datadriveninvestor.com
Probability is a mathematical technique by which we can predict the probable outcome of an event. Read the following articles if you have any confusion about probability.
towardsdatascience.com
towardsdatascience.com
The probability distribution is the distribution of all the probabilities of an event. There are two types of the probability distribution.
The discrete probability distribution is the probability distribution of discrete values. For example, the probability of rolling a die. We find a specific outcome for each role, and it‚Äôs obvious a discrete value.
Uniform distribution, Binominal distribution, Poisson distribution, etc., are some of the main discrete probability distributions.
The continuous probability distribution is the probability distribution of continuous value. For example, the probability of being age 24 of a sample group. As age 24 is a continuous value, we need to use continuous probability distribution.
Normal distribution, Student‚Äôs t distribution, etc., are some of the continuous probability distributions.
In uniform distribution, all the values of specific outcomes are equal. For example, rolling dice has 6 individual outcomes = {1,2,3,4,5,6}. If it is uniformly distributed, each probability value is 0.16667.
Python Implementation [6] ‚Äî
The name ‚ÄúBinomial‚Äù suggests two mutually exclusive outcomes of trials. For instance, head or tail, good or bad, pass or fail, etc.
For Binominal distribution, a trial must satisfy the criteria of the Bernoulli Trial.
The Bernoulli trial must have two independent outcomes like, high or low. The probability of success must be constant.
Here, n is the number of trials, p is the probability of success, and the number of successes x.
Let‚Äôs plot a binominal distribution for rolling dice. Suppose you roll a die 16 times. What will be the probability that 2 comes up 4 times? Here, p=1/6 and n=16. The binominal distribution for the scenario is shown with python code.
[The article [7] helps me to implement the code.]
The red bar indicates the probability of 2, comes up 4 times.
The red bar indicates the probability of 2, which comes up 4 times.
Binomial distribution finds the number of successes out of a specific number of trials. Poisson distribution determines the number of successes within a unit of time interval.
For example, in a shop 8 customers arrive between 12 pm to 1 pm. With the help of Poisson distribution, we can find out the probability of arriving 3 people between 12 pm to 1 pm. Poisson distribution can be explained with the following formula.
Where x is the number of successes, Œª is the number of occurrences per unit of time. Œµ is the Euler number (2.71828‚Ä¶). For the above problem, x=3, Œª=8/1=8.
Red bar shows the probability of arriving 3 customers between 12pm to 1pm.
The continuous probability distribution is applicable to continuous variables. The normal distribution is one of the widely used continuous probability distributions. Many real-life problems can be solved/described with normal distribution. Suppose we consider a sample age of 70 students. The age ranges from 18 to 25 years. It will be normally distributed if the mean, mode, and median are equal in that sample.
In case of normal distribution, the probability of the left and right parts being equally distributed means it is symmetrical. Total probability equal to 1. The distribution follows the following equation.
Here, œÉ is the standard deviation and Œº is the mean.
Normal distribution with python ‚Äî
Normal distribution will be a standard normal distribution when the standard deviation is 1, and the mean value is 0. Below is an example of the standard normal distribution with python.
William Sealy Gossett proposed the distribution. As there is a restriction in his working place to publish research articles with the original name, he used his pseudonym, ‚ÄúStudent.‚Äù The distribution was proposed to find out the best sample from a small sample [8].
The image shows the comparison of two distributions. Student‚Äôs T-distribution has a fatter a tail than the normal distribution. Python implementation of student‚Äôs t distribution [9].
If we randomly make huge samples from a population and consider the mean values, we will find that sample values will be normally distributed around the population mean. It‚Äôs a way to find out good sample data.
Estimators are the terms of statistics by which we estimate some facts about the population [10]. Some estimators are the sample mean, standard deviation, variance, etc.
The value of the estimator is known as the estimate.
Suppose the variance of a sample is 5. Here, the variance is the estimator, and the value of the variance is called the estimate.
Standard deviation indicates how far the values are from the population mean, and standard error shows how far sample means are from the population mean. It is calculated as follows ‚Äî
Here, œÉ is the population standard deviation, and n is the sample size.
The confidence level is the percentage value; within that value the parameter‚Äôs truth value will remain. Suppose we have solved a problem with a 95% confidence level; that means 95% time, we will get the accurate result from our solved problem.
The confidence interval is a range within which we will get the truth value of the confidence level.
The significance level of an event (such as a statistical test) is the probability that the event could have occurred by chance. If the level is quite low, that is, the probability of occurring by chance is quite small, we say the event is significant [11].
Significance level is denoted by Œ± symbol.
Hypothesis testing is a statistical technique by which we can test and validate an assumption. In hypothesis testing, we consider a null hypothesis, which is assumed to be true, and an alternative hypothesis, which is acceptable if the null hypothesis fails. More details about Hypothesis testing is given in the following article.
medium.datadriveninvestor.com
In the Hypothesis testing article, I have mentioned some tests like p-test, t-test, etc., but those tests only compare between two groups. None of them can be used for multiple groups. ANOVA is a statistical test used for comparing the variability between two or more groups. Detail explanation has been provided in the article below.
towardsdatascience.com
The Chi-Square test is another statistical test for finding the dependency of categorical variables. Go through the following article.
towardsdatascience.com
Statistics is an integral part of data science. Each and every topic of statistics can‚Äôt be covered because the field is huge. However, I have tried to cover the important knowledge of statistics needed for data science. With this article, I am going to wrap up my series writing on statistics for data science. If you have any questions, feel free to inform me in the comment section.
Thank You
mzh706.medium.com
mzh706.medium.com
Anthony Mipawa
in
Neurotech Africa
Snehal Gawas
Anton Max
Anthony B. Masters
in
Towards Data Science
Falon
Aamil Shohail
Kopal Sharma
San Jos√© MOTI
"
142,https://medium.com/geekculture/3-reasons-why-you-shouldnt-become-a-data-scientist-27994914cfea,3 Reasons Why You Shouldn‚Äôt Become a Data Scientist,"Data Science is somewhat of a misunderstood field. On paper, you‚Äôre solving global warming and cancer straight of out college for a mid 6 figure salary. In reality, you‚Äôre struggling to understand why‚Ä¶
iManassa
in
Geek Culture
Pete Hansell
Peter Flom
Piyush Kumar
Mallikarjuna G
The Pragmatic Programmers
in
The Pragmatic Programmers
Debayan Hazra
daisy okacha
"
143,https://medium.com/@data-storyteller/how-to-tell-if-data-analytics-is-the-right-career-for-you-899f1c83179,How to Tell if Data Analytics is the Right Career For You,"Data Science and Analytics are still buzzing, going on 10 years, as the ‚Äúsexiest‚Äù job of the 21st century. It‚Äôs well paid, it‚Äôs interesting, and there‚Äôs a ton of demand for talent. As a result, it‚Äôs still attracting tons of people interested in breaking into the field. But a common question is ‚Ä¶ how do I know if I‚Äôll like it?
As someone who has been working in analytics and data science for 5+ years, has a masters degree in data science, and pivoted from a career path that I didn't enjoy (marketing), here is my view.
If you can honestly say ‚Äúyes‚Äù to all of the following, then there is a good chance you‚Äôll enjoy working in this field.
And how do you feel about learning new math? (If you haven‚Äôt already taken college-level courses in statistics, and maybe calculus and linear algebra.) There are some folks who claim that you don‚Äôt need to be ‚Äúgood‚Äù at math to do analytics, because ‚Äúthe computers do all the math for you!‚Äù While that‚Äôs true ‚Äîmost of all of us aren't writing out mathematical proofs on the job ‚Äî you still need to be comfortable enough with math to know which math to tell the computer to do. You need to be able to look at a data set and figure out what is the numerator and what is the denominator for the rate or percentage you need to calculate. If you want to do any machine learning, you need to have a grasp of the math behind your code, and also how to interpret the performance of your model ‚Äî which is all evaluated using math.
Even with vague or no instructions? The job of a data analyst or data scientist isn‚Äôt to write SQL code or Python code or create machine learning models.
The job is to solve problems.
SQL, Python, machine learning ‚Äî those are just tools. Tools that you use to do your job. But they are not your job.
All day, every day, you will be bombarded with people who need your help to solve their problems. It can get overwhelming at times, even for someone who likes to solve problems.
A lot of times they won‚Äôt even identify the correct problem to solve. So you need to first figure out what is the problem that I‚Äôm even solving, and then how do I do it?
If you think the job is merely jumping into writing code immediately, with clear instructions or goals‚Ä¶ you‚Äôll be in for a surprise.
When you are presenting your solution/insights/whatever to your stakeholders, or to leadership, you should be prepared for someone to ask ‚Äúwell did you look at XYZ?‚Äù
There are usually multiple ways to solve a problem, and a good analyst or scientist will consider all of those when determining the best solution. Because if you don‚Äôt, someone you're presenting to will ask if you did. And it‚Äôs a bad look if you routinely say ‚Äúwell, no‚Ä¶‚Äù It‚Äôs a better look to say ‚Äúyes, and it wasn‚Äôt as good as [this solution] for XYZ reasons.‚Äù
This job is very literal. It‚Äôs very black and white. That‚Äôs not to say there is only ever one right answer for every problem ‚Äî no, the job is finding the best answer of all the possible answers, considering various tradeoffs. But if you are very literal and straightforward, then you might enjoy this work. This is one of the reasons why I personally enjoy working in this field more than I did working in marketing.
Spreadsheets are often the gateway to analytics. It‚Äôs something that anyone with a computer can access and start digging in and learning. If you‚Äôre someone who enjoys using spreadsheets, and you‚Äôve been able to figure out on your own how to do some cool things ‚Äî pivot tables, visuals, formulas, etc ‚Äî then that‚Äôs a good sign that you‚Äôll do well in this field.
If you‚Äôve never used spreadsheets, my recommendation is to create a personal budget for yourself in Excel or Google Sheets. Download all your spending data, clean and join it together, then categorize everything, aggregate it in a way that is useful for you, create visuals that tell a story, and then reflect on what insights you‚Äôve gained from the process.
Not only is this a good, basic, end-to-end analytics project, but if you can figure it out all on your own and create something useful with clear insights, then that‚Äôs a good sign you‚Äôll enjoy this work.
Because that‚Äôs the job. People come to you with vague problems. Or they don‚Äôt, but you identify problems to solve. And then you have to solve them. Usually, you have to figure out the solution on your own, there often isn‚Äôt an example or tutorial that you can follow exactly. If you intuitively are already the type of person who will seek out your own solutions or answers, then that‚Äôs another sign that you will do well in this field.
If you‚Äôre the type who will wait for someone else to find the solution, or will ask someone else for the answer or solution before seeking it out on your own, or you just give up pretty quickly, then you might have trouble succeeding in this field.
I know it‚Äôs obvious, but there is no other way to do this job. I say this because I occasionally see folks interested in transitioning from teaching or a blue collar role, and for some folks, the thought of sitting for 8-ish hours per day, 5 days per week, and staring at a screen doesn‚Äôt sound great. But it is what it is.
What do you think? Do you work in analytics, and do you agree or disagree? Anything else you‚Äôd add? Any other questions about a career in analytics?
Check out the rest of my data analytics & career resources and sign up for my weekly newsletter with tips for a career in analytics.
FutureAnalytica
Som
Krutika Desai
Jose Ramon Perez Ag√ºera
Shivani Chander
in
Data Byte
Rory J Clarke
Anjani Kumar
in
DataDrivenInvestor
Stephen Fordham
in
Towards Data Science
"
144,https://medium.com/quantumblack/how-to-build-a-data-portfolio-without-an-analytics-job-3350e45ad0b1,How To Build A Data Portfolio Without An Analytics Job,"Garazi Gomez de Segura Solay, Senior Data Scientist, Sam Hiscox, Senior Data Engineer and Shawn Tan, Data Engineer, QuantumBlack
Many people seeking their first job often find themselves in an age-old and challenging loop: you can‚Äôt gain employment without experience and you can‚Äôt gain experience without employment. A similar refrain is common in the advanced analytics industry, where demonstrable analytics skills are valued even for entry-level roles. But how can you build a compelling data portfolio before you‚Äôve even taken part in your first professional project?
At QuantumBlack, we‚Äôre surrounded by practitioners who entered the data industry through a variety of traditional and non-traditional routes. We gathered insights from three colleagues across data science and data engineering roles who all acquired initial analytics skills ahead of entering the professional industry. They offer their advice below on how aspiring data engineers and data scientists can make a start on a portfolio before they‚Äôve even submitted their first analytics job application.
Understand The Road You Want To Follow
The analytics industry may appear to have a high bar of entry and it‚Äôs common to meet colleagues with extensive years of study behind them. Despite this, it can be a fairly accessible industry. Practical knowledge is ultimately what candidates are assessed on, rather than the degree you enrolled in or other industries you may be switching from. To the contrary, many prospective employees often bring transferable skills and experiences that benefit their analytics careers. Day-to-day skills can be learned on the job but demonstrating a dedication to learning and picking up initial proficiencies will be attractive to prospective employers.
That being said, analytics is awash with job titles and varied, winding career paths. While there is often a significant degree of overlap activity between titles ‚Äî and QuantumBlack offers plenty of freedom in charting your own career path ‚Äî it‚Äôs useful to understand what‚Äôs expected of which discipline. Knowing where you‚Äôd like to start and the nuanced differences between roles will guide your initial career decisions and your choices when developing a portfolio.
Data Engineers (DE) are often tasked with building the infrastructure that powers an analytics solution, as well as finding, understanding and delivering data in a way that helps Data Scientists (DS) extract deep insights. DEs are typically then asked to take the insights that have been generated and deliver them to the end user, while automating the process and ensuring it runs robustly.
DE Sam Hiscox explains: ‚ÄúDEs aren‚Äôt just Structured Query Language (SQL) specialists. We‚Äôre better thought of as Software Engineers who have specialist skills in data. This allows us to design and build data-powered products that are resilient and scalable. Joining tables and understanding data structures are fundamental to our job, but writing readable, efficient and tested code is just as important.‚Äù
In contrast, DS Garazi Gomez de Segura Solay frames her role as ‚Äúresponsible for translating a business problem into an analytical problem, before building the models that create the solution. Data scientists often rely heavily on statistical and mathematical analysis. We rely on engineers to ingest, transform and join the data that feeds the analytical models we build.‚Äù
Building A Data Portfolio
A portfolio is ultimately an asset for the job interview ‚Äî it provides the candidate with something to talk about and a way to demonstrate an initial skillset or proficiency. Building one from scratch may sound daunting and identifying that initial use case or a project can be difficult. However, it is useful to remember that data is fairly ubiquitous in most tech projects today.
‚ÄúAny software project will involve a degree of data and I remember setting up my first database at 3am during a university hackathon,‚Äù explains DE Sam. ‚ÄúDuring my spare time I consumed plenty of Medium articles and YouTube videos that covered technology I was interested in, such as Docker, Airflow, Ml flow and others. I‚Äôd then build little demos using the technology I read about and tried to find opportunities in my previous job as a mechanical engineer to use my coding skills to automate data activities.‚Äù
DE Shawn Tan also attended AI-themed hackathons at university and also suggests gaining initial experience with startups, even if this is temporary and voluntary. ‚ÄúEven those positions I volunteered for on a short-term basis provided opportunities to build tech solutions such as web scrapers, chatbots and databases,‚Äù he says. ‚ÄúGrappling with new technology as you‚Äôre building it may sound challenging, but there‚Äôs plenty of existing documentation available online.‚Äù
Resources To Help Build Your Prelimiary Portfolio
Online platformslike Coursera, FreeCodeCamp, Udemy and Hackerrank were all used by our colleagues used when preparing for careers in data science. ‚ÄúKaggle is also a fantastic resource,‚Äù explains Garazi, ‚Äúas it provides cleaned data and a pre-configured environment, leaving you to concentrate on building the best model you can. Kaggle tends to be a popular platform for technically proficient practitioners to share their code on, so there are plenty of opportunities to learn from others.‚Äù
All three colleagues were clear that taking advantage of existing support is the easiest way to build an initial portfolio. ‚ÄúIt‚Äôs important to remember that in analytics we almost never build software from scratch,‚Äù says Sam, ‚ÄúOften someone much smarter than me has thought deeply about the problem I‚Äôm facing, created a very elegant solution and then made it freely available as a Python package. I‚Äôd suggest that candidates get good at finding these assets, understanding the documentation around them and leveraging them. Building a product that solves a problem by cobbling together a selection of open source software is far more impressive than being able to implement a bubble sort in five different languages.‚Äù
Alongside an impressive portfolio, all three colleagues agreed that the most crucial asset a candidate can bring to a job is a visible eagerness to learn and improve. ‚ÄúI frequently hear some of the smartest people at QuantumBlack say ‚ÄòI don‚Äôt fully understand this, can you tell me more,‚Äô‚Äù says Shawn. ‚ÄúWhether technical knowledge or background on a fresh industry, the data practitioners I know are learning something new every day, so it‚Äôs not just accepted but expected that entry-level candidates won‚Äôt know everything. Arriving at an initial interview with a reasonable first attempt at a portfolio demonstrates a keenness to proactively learn, and that‚Äôs a trait that data employers value.‚Äù
If you‚Äôd like to discuss a career at QuantumBlack as well as other opportunities, please visit our careers page or alternatively reach out directly to a member of our team ‚Äî we‚Äôre always eager to offer more information or advice.
Raju Goud Burra
Abdishakur
Leon Lok
in
Towards Data Science
Deepinder Singh Dhingra
Suraj Gurav
in
Towards Data Science
Abdullah G√∂k
Sheldon.studio
in
Nightingale
TimeNewBank
"
145,https://medium.com/gitconnected/5-crazy-chatgpt-results-that-will-blow-your-mind-a9dabeba78eb,5 Crazy ChatGPT results that will blow your mind!,"Have you checked out OpenAI‚Äôs new ChatGPT chatbot? It‚Äôs Crazy Good.
2. Asking it to explain what a weird looking regex does?
3. Asking it to Write a Piano piece.
4. Asking it to Simplify text
5. Asking Chatgpt ‚ÄúWill AI take over humanity ?‚Äù
ChatGPT is crazy good! People are exploring various ways of using it from writing essays, Simplifying text, debugging code, writing code, writing stories, writing lyrics, creating music etc.
This is Just amazing. You can also go play around with Chatgpt at https://chat.openai.com/chat. Share your results in the comments below.
levelup.gitconnected.com
Also, If you want to stay informed and up-to-date on the latest news and ideas about AI and Technology, then you need to subscribe to Medium.
With a subscription, you‚Äôll get access to a wealth of expertly-written articles on a wide range of topics, from business and technology to politics and culture.
Plus, you‚Äôll be supporting independent writers and helping to promote quality Writers. So why wait?Subscribe to Medium today using this link.
DataBridge Market Research
Robert Smith
in
Chatbots Life
Aditya Pande
Kalyani Tangadpally
DataBridge Market Research
Robert Smith
in
Chatbots Life
Gregory Walfish
Asia AI News
"
146,https://medium.com/javascript-in-plain-english/9-amazing-visual-studio-code-extensions-to-skyrocket-productivity-f281f7827b53,9 Amazing Visual Studio Code Extensions To Skyrocket Productivity,"Visual Studio Code is an amazing editor with a large marketplace of extensions to tweak the editor to your use cases. It offers over 40.000 extensions, and more are added every single day.
Recently, I shared a list of extensions that get recommended a lot in articles here on Medium and elsewhere whose functionality is already supported natively and can be uninstalled.
javascript.plainenglish.io
Today I want to share the extensions I think are most important in a Web Developers toolkit.
The 7 extensions you‚Äôll find below are oriented around improving your workflow. They help you save time or prevent you from context switching.
1. Project Manager ‚Äî 2.8M Downloads
Project Manager is a simple plug-in that allows you to quickly switch between projects with one click. You can choose to open a project either in your current window or in a new window.
Before downloading this extension I was pretty fast at project switching through simple keyboard commands and selecting from the Recent Projects list within VS Code. Now, that I have been using this extension for several months I can say this is much easier, especially when your list of projects is longer than the Recent Projects list in VS Code.
2. GitLens ‚Äî 18M Downloads
Although Visual Studio Code has native Git integration, GitLens makes the experience so much better. It visualizes code authorship inline without ever cluttering the UI.
Here are some of the features which make me love GitLens:
3. Live Share ‚Äî 10.2M Downloads
Live Share is an extension developed by Microsoft themselves, it lets you collaborate with another developer right from within VSCode. You can write code simultaneously, follow each other and debug together.
Unlike traditional pair programming, Visual Studio Live Share allows developers to work together, while retaining their personal editor preferences (e.g. theme, keybindings), as well as having their own cursor. This allows you to seamlessly transition between following one another, and being able to explore ideas/tasks on your own. In practice, this ability to work together and independently provides a collaboration experience that is potentially more natural for many common use cases. ‚Äî Microsoft
Live Share became an important extension for me during the pandemic. At my current position - we almost exclusively work in pairs - working from home for two years straight allowed me to try out different tools. I prefer Live Share over other pair-program tools for various reasons:
Live Share is also great for following along with a candidate in a technical job interview or when hosting a developer stream since Live Share has an extension pack for in-editor chatting and/or audio calling.
4. Jira and Bitbucket (Atlassian Labs) ‚Äî 1.2M Downloads
This extension by Atlassian adds great support for both Jira and Bitbucket right into your editor. It speeds up your workflow by reducing context switching and giving clear information right within the editor. Here are some of my favorite features of this extension:
If you use Bitbucket you can also perform code reviews easily from within VS Code and kick off builds.
PS: For Github and Gitlab similar extensions exist which provide an amazing in-editor experience. Managing code reviews, opening branches based on issues, managing actions/pipelines, and other features becomes much faster if you have access to it from within the editor.
5. GitHub Copilot ‚Äî 2.6M Downloads
GitHub Copilot is a little bit of a controversial topic at the moment, nevertheless, it‚Äôs a great tool to use.
When people speak of it they often refer to Copilot writing entire functions for you based on a comment, the real power for me lies in simple autocompletes.
In most situations, it can autocomplete templates, functions, and loops exactly as you were planning to write them out. It is a huge time-saver.
6. Jest ‚Äî 1.2M Downloads
If you write unit tests with Jest this extension is a blessing. This extension can show you which lines of your code are covered by unit tests, a visual interface to passing/failing tests, easily run test suites or single tests, and more.
javascript.plainenglish.io
7. Turbo Console Log ‚Äî 585K Downloads
If you‚Äôre a console.log developer this is the extension for you. You can highlight a variable and insert a meaningful console.log automatically with a keyboard shortcut. (CTRL+Option+L on Mac or CTRL + Alt+L on Windows).
It also has keyboard shortcuts to (un)comment or delete all console.log messages in the current file so you can easily clean up a file after debugging it.
1. ESLint ‚Äî 23M Downloads
If you use ESLint for static code analysis this extension will increase your experience in VSCode. It gives real-time support helping you visualize and fix the issues as you code, instead of after you‚Äôve run your build command.
2. Prettier ‚Äî 25.7M Downloads
Prettier is an opinionated code formatter. It enforces consistent style by parsing and reprinting your code. It supports many languages and really improves the experience of maintaining consistent coding styles.Take a look at their website for more information on how to use Prettier.
Visual Studio Code is a great lightweight code-editor that can be customized through various extensions. The 9 extensions listed above are the 9 most useful extensions I have found.
Let me know in the comments which extensions I might have missed in this list.
Did you like this article and do you want to learn more about customizing VS Code to your liking and increase your productivity? Check out my list of VS Code articles down below.
Wesley Smits
If you like my content and want to support my efforts, consider becoming a Medium subscriber through my affiliate link. It will cost you nothing extra, but Medium will give parts of the proceeds to me for referring you.
And if you want, you can connect with me on LinkedIn or Twitter!
medium.com
More content at PlainEnglish.io. Sign up for our free weekly newsletter. Follow us on Twitter, LinkedIn, YouTube, and Discord. Interested in Growth Hacking? Check out Circuit.
Designveloper
Mae Schreck
Yudo Maryanto
Enver ≈ûanlƒ±
Aladin Legaspi
LongYC
Steve Kuenzli
Anonstake
in
Anonstake
"
147,https://medium.com/learning-sql/5-sql-mistakes-youll-probably-make-in-your-first-data-science-job-e1a7d810d0f2,5 SQL Mistakes You‚Äôll (Probably) Make In Your First Data Science Job,"Coming to data engineering and SQL from the world of journalism, I‚Äôm used to working at a fast pace under deadline; coincidentally, rushing through development has been‚Ä¶
Crowdin
Pen Magnet
PHOENISTECH
Deeksha Sharma
Tristan Mendiola
Bryan Parsons
Joanna Wyka
in
Kt. Academy
Mohamed Dhaoui
in
Python in Plain English
"
148,https://medium.com/better-programming/a-programmers-regret-neglecting-math-at-university-9d937655752b,A Programmer‚Äôs Regret: Neglecting Math at University,"Yes, you can ignore math and be a highly paid-professional programmer. Programming is a wide enough field that you can choose which areas you want to focus on ‚Äî some of which do not require math ‚Äî and still be successful. On the other hand:
I would like to give you some examples for both, with the hope that this will motivate you to give math a chance to grow on you. For this I have chosen some short (average 10 minutes in length) videos that should excite you and explain the concepts better than I can.
But first, a story. About me not caring about math‚Ä¶
When I was at university, I didn‚Äôt really see the point of math. It was such a big change from the math we did in high school, and all I really wanted to do was program. Math just seemed like a necessary evil for getting my degree, so that I could move onto more programming. The proofs were the least motivating of all, since it felt like they needed to be crammed solely for the exam and offered no value for my programming career. My stubborn self felt so ‚Äúprincipled‚Äù about it that I refused to study the proofs. This resulted in me being put into a special group for underperforming students in my first year. Too proud (read stupid), I refused to study the proofs and ended up narrowly passing even though 40% of the exam was proofs. Still not getting the message, I had to rewrite a test and an exam in my second year. The exam rewrite is where I finally compromised my ‚Äúprinciples‚Äù and had to give up most of my summer vacation to study proofs. I couldn‚Äôt hack the math, so I lost a vacation and still didn‚Äôt understand why it was important. You would think that from this great failure I would learn something, but I was still blinkered by stubbornness. Suffice to say, when math became an elective, I didn‚Äôt choose it! In hindsight this is one of my biggest regrets: five years later I would find out that the problems that I found most interesting were all intimately based in math.
I finally picked up a Computer Science book to read past the prescribed material in the week that I was handing in my master‚Äôs thesis. This is when I finally realised that Mathematics and Computer Science were linked in a way that I found really interesting. Since then I have been playing catch-up with all the students who were actually paying attention in class, during ‚Ä¶ my valuable after-work free time. This has been an ongoing process for over ten years now, when I could have rather just paid attention for the four years that it was shoved in my face. This is ten years I could have spent learning new things, like my peers who paid attention, but instead my math is still not at graduate level. Sometimes I really feel like I wasted my life, but rather than dwell on this any further, let‚Äôs look at some examples of Math that is important for programming.
Obviously graphics programming in games and movies requires physics knowledge, but since exact physics can be too expensive to properly simulate, we typically use Numerical Methods from Mathematics, for example using Verlet Integration to approximate ragdoll physics:
Less obvious might be Google‚Äôs web search algorithm. You might think this is simply counting words on web pages and showing the page with the highest count of the relevant word at the top, but this ranking is too easy to manipulate, for example a web page that just repeats the word math, does not actually contain any math. Ranking the pages to have the most relevant results at the top is a much harder problem. The PageRank algorithm takes into account the number of links to and from a web page and places them in a matrix, then it uses an approximation of an eigenvector from Linear Algebra to calculate a more relevant ranking:
Artificial Intelligence or rather the subfield of Machine Learning, is something that I found very intriguing while studying. Tracking gestures on a dance game, finding movies that you might like on Netflix, recognising the song that is currently playing, etc. If you want to help to build any of these systems, you will need at least a good understanding of Calculus, Probability Theory and Linear Algebra.
I think those are enough examples, which use math as the tool for solving specialised problems. I now want to talk about the less obvious and show that math is programming, which I now find even more intriguing.
Abstraction is an extremely important part of programming. It is one way we can break up a complex problem into smaller pieces. We see some pattern or want to hide some complexity and we create an abstraction, using for example abstract classes or interfaces. We even create patterns for how we abstract and abstract over those patterns. It is very important how we abstract, since abstractions can be very confusing or very useful. How can we discover the most useful abstractions?
Even though computers, as we know them, have only been around for a few decades, questions of computation and the design of computing engines have existed for hundreds of years. This is quite a surprising fact, but still it is considered a very young field when compared to mathematics, which has been around for thousands of years. This means that mathematics has had quite a bit more time to come up with solutions to certain problems. We might as well take a look to see if there are ideas that we can steal ‚Äî in fact, it might be arrogant not to!
No surprise ‚Ä¶ Mathematics has an appropriate sounding sub field called Abstract Algebra. Here is a little taste of Abstract Algebra, with an explanation of a Group:
A Group was a good abstraction over addition and multiplication, but it is also a superclass of a Monoid. If you take a Group and remove the property that the elements need inverses, you are left with a Monoid. A Monoid is a set of elements, which includes an identity element and an associative operation. Now we have an abstraction that not only works for addition, but also:
This abstraction is useful because now we can write single implementations of functions that work for any monoid. For example:
We can abstract even further and not only make this function work for trees, but for any foldable container, in other words, any container that can be transformed to a list.
Knowing about monoids can be useful when designing a library. Whenever you have a binary function, like multiply, which takes two parameters of the same type and returns a result of the same type, it is a good idea to think about what the identity element would be. If you find one, you have found a very intuitive way of using your binary function over lists. Your function will even work on the empty list, since you have a sensible default with mathematical properties, and it will remove a case for error handling, which will simplify your user‚Äôs code.
Here we used the abstraction to create shared implementations, but an abstraction can be and should be more useful than that. The abstraction is at the service of explaining or discovering the connections.
One quite popular way that humans solve a complex problem is to divide and conquer. Break down the problem into smaller parts, solve those smaller problems and then compose those solutions into a solution for the bigger problem. Can you think of another (general) way? ‚Ä¶
In programming we break up a problem into several smaller functions that solve smaller problems and then compose them together into bigger and bigger functions that eventually solve the bigger problem. What would be the best way to compose functions together? I wonder if math has any ideas that we can steal?
Category Theory, another subfield of Mathematics, is what I like to call abstract Abstract Algebra, but really it is the mathematics of composition, where we study the context of objects rather than their content. Here we can find many useful ideas, like:
These extremely abstract concepts can take a while to mature in your brain, so the earlier you start the better. See below a short video that tries to explain monads:
This is the sweetest and final part of your meal of math.
Now, you might remember that I said that I found proofs to be the least motivating part of doing Mathematics at University. Well what if I told you that types can be viewed as propositions and programs as proofs:
Here are some provable properties:
Now that I know that I can use programming to prove things mathematically, I find proofs to be the most interesting thing about programming. Proofs allow us to write safer programs with fewer errors, because we can prove properties instead of just testing them.
But wait, if you can prove things using programming, can‚Äôt you also contribute to Mathematics by simply doing the thing we love, writing programs? Yes and mathematics seriously needs you. Proofs are not just hard for you, but also hard for mathematicians. Mathematicians create bugs in proofs all the time ‚Äî and these bugs go undiscovered for decades. Homotopy Type Theory, which studies different types of equality, is disrupting the foundations of Mathematics with its univalent point of view, because it thinks that the so called foundations of Mathematics are buggy. Yes, not even math is perfect and there is lots of room to contribute.
I am still new to the concept of proving using programming, but I find this very exciting and can‚Äôt wait to learn more. I can recommend reading The Little Typer, which provides a great introduction to proving using dependent types to any programmer that has some experience with recursion.
We did a talk at Devoxx UK 2022 on a slightly longer introduction to the Coq Proof Assistant, Correct Code by Construction. If you want to see me in a builder‚Äôs outfit and some chicken‚Äôs in high vis:
Then this talk is for you:
It seems the thing that I have been trying to avoid at all costs, turned out to be the thing that I loved doing most of all. Don‚Äôt make the same mistake I made: Make the best of your mandatory Mathematics subjects or you will regret it later.
If your educators or education material is not inspiring you to explore this vast area of knowledge, then please turn to the plethora of alternative online resource, like YouTube, Coursera or Edx or try to find a better book. There is someone who can explain a difficult concept and inspire you ‚Äî you just need to take the time to find them. You also need to practice using exercises, just like you practice programming.
Other times I have struggled because I didn‚Äôt understand a previous fundamental concept that was required to understand the new material. Don‚Äôt be ashamed to go back and fix your understanding. In math, many things build on top of one another and without solid foundations it is very hard to make any progress. It is almost always worth taking the time to go back and find an alternative resource on the subject and try to properly grasp that key concept, instead of moving forward and banging your head against a wall.
I have a todo list that I am using to try to catch up to all the math that I missed. As you will see, this is only the start of a very long and delightful journey. Mathematics is a huge and exciting subject, where almost everything is connected. It might be useful to take a look at the bigger picture:
Jay Huang
Jenn Sydeski
Bryan Downing
Fajar Purnama
MARS DAO
in
MARS DAO
Kuldeep
in
SpeedLabs
Rajnikant Mishra
Somorjai Andrew
"
149,https://medium.com/@anna.wu9222/google-data-scientist-interview-questions-step-by-step-solutions-63b7403e2a5c,Google Data Scientist Interview Questions (Step-by-Step Solutions!),"Are you preparing for a Google Data Scientist interview? Congratulations!
A data scientist role at Google is one of the most sought-after positions in the tech industry. This is because data scientists at Google are responsible for some of the most important projects at the company, such as developing algorithms to improve search results, improving Google Maps, and developing new features for products like Gmail and YouTube.
To land a data scientist role at Google, you will need to ace the interview process. In this article, we will go over some of the most common questions asked in a Google Data Scientist interview, along with step-by-step solutions to help you prepare.
The data science interview process at Google is divided into two parts:
Part 1: The first part of the interview process is designed to test your technical skills. You will be asked questions about SQL, statistics, product, analysis, and machine learning similar to product management and data analyst interviews.
Part 2: The second part of the interview process is designed to assess your problem-solving abilities. You will be asked questions about real-world data sets and will be expected to come up with solutions to problems that data scientists typically face.
Now that we have an overview of the interview process, let‚Äôs dive into some specific questions you may be asked in each part of the interview.
In the first part of the interview, you will be asked questions about your technical skills. Here are some examples of questions you may be asked:
1. What is SQL?
2. How would you calculate the median in SQL?
3. What is a decision tree?
4. How would you use a decision tree to predict whether or not a customer will churn?
5. What is gradient boosting?
6. How would you use gradient boosting to improve the accuracy of a machine learning model?
Solutions to Google Data Scientist Interview Questions Part 1
Now that we‚Äôve gone over some examples of questions you may be asked in a Google Data Scientist interview, let‚Äôs look at some specific solutions to these questions.
1. What is SQL?
SQL (Structured Query Language) is a programming language that is used to manipulate and query data stored in databases. SQL can be used to add, delete, and update data in a database, as well as to retrieve data from a database.
2. How would you calculate the median in SQL?
To calculate the median in SQL, you would use the PERCENTILE_CONT() function. This function takes two arguments: the column name that you want to calculate the median for, and the value 0.5 (which corresponds to the median).
For example, to calculate the median salary for all employees in a table, you would use the following query:
SELECT
PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY salary) AS median_salary
FROM employees;
3. What is a decision tree?
A decision tree is a type of machine learning model that is used to predict the value of a target variable. Decision trees are created by splitting the data set into smaller and smaller subsets, until each subset contains only one data point.
4. How would you use a decision tree to predict whether or not a customer will churn?
To use a decision tree to predict whether or not a customer will churn, you would need to train the model on data that includes information about customers who have already churned. Once the model is trained, you can then use it to predict whether or not new customers will churn.
5. What is gradient boosting?
Gradient boosting is a type of machine learning algorithm that is used to improve the accuracy of a machine learning model. Gradient boosting works by training a series of weak models, and then combining the predictions of these models to create a final prediction.
6. How would you use gradient boosting to improve the accuracy of a machine learning model?
To use gradient boosting to improve the accuracy of a machine learning model, you would need to train a series of weak models on different subsets of the data. You can then combine the predictions of these models to create a final prediction that is more accurate than the predictions of any individual model.
In the second part of the interview, you will be asked questions about your problem-solving abilities. Here are some examples of questions you may be asked:
1. Given a data set, how would you go about finding the most important factors that contribute to customer churn?
2. How would you develop a machine learning model to predict whether or not a customer will purchase a product?
3. You are given a data set that contains information about when users click on ads. How would you use this data to optimize ad campaigns?
4. You are given a data set that contains transaction data from an eCommerce website. How would you use this data to increase conversion rates?
5. Write a query to find out the third-highest mountain name for each country. Please make sure to order the country in ASC order.
Solutions to Google Data Scientist Interview Questions Part 2
1. Given a data set, how would you go about finding the most important factors that contribute to customer churn?
There are many ways to find the most important factors that contribute to customer churn. One way would be to use a decision tree to identify the most important features. Another way would be to use a technique like gradient boosting to find the most important features.
2. How would you develop a machine learning model to predict whether or not a customer will purchase a product?
To develop a machine learning model to predict whether or not a customer will purchase a product, you would need to train the model on data that includes information about customers who have already purchased the product. Once the model is trained, you can then use it to predict whether or not new customers will purchase the product.
3. You are given a data set that contains information about when users click on ads. How would you use this data to optimize ad campaigns?
There are many ways to use this data to optimize ad campaigns. One way would be to use the data to create a model that predicts when users are more likely to click on ads. Another way would be to use the data to segment users into groups based on their likelihood of clicking on ads.
4. You are given a data set that contains transaction data from an eCommerce website. How would you use this data to increase conversion rates?
There are many ways to use this data to increase conversion rates. One way would be to use the data to create a model that predicts which users are more likely to purchase products. Another way would be to use the data to segment users into groups based on their likelihood of purchasing products.
5. Write a query to find out the third-highest mountain name for each country. Please make sure to order the country in ASC order.
Output
Solution
We do a subquery because we want to calculate the rank for each mountain in each country before filtering for just the third highest mountain. In the subquery, we use the RANK() function to give each mountain a rank within its country, with the highest mountain having a rank of 1.
Then, in the outer query, we filter for only those mountains that have a rank of 3. Finally, we order the results by country in ascending order.
1. Be prepared to answer questions about your technical skills. The first part of the interview will likely be focused on your technical skills, so you should be prepared to answer questions about SQL, machine learning, and data analysis.
2. Be prepared to solve problems. The second part of the interview will be focused on your problem-solving abilities. You should be prepared to solve problems that are based on real-world data sets.
3. Practice your interviewing skills. In addition to practicing your technical skills, you should also practice your interviewing skills. This means being able to clearly and concisely communicate your thoughts and ideas.
4. Be yourself. The best way to ace any interview is to simply be yourself. Google is looking for candidates who are smart, creative, and passionate.
These are just some examples of questions you may be asked in the second part of the Google Data Scientist interview. Be sure to come up with your own solutions to these questions before the interview so that you will be prepared to impress the interviewer.
Don‚Äôt forget to also read our recent article covering the three latest Google SQL Interview questions and check out our free and paid plans on app.bigtechinterviews.com if you‚Äôre looking to learn and/or pass the SQL interview.
Google Data Scientists are among the best in the world at what they do, so you will need to be at the top of your game to land this job. Good luck!
Want to practice real SQL interview questions? We‚Äôve analyzed over 50,000 interviews from pre-IPO to Fortune 500 companies at Big Tech Interviews (BTI) to curate an exclusive list of the latest SQL interview questions and solutions so you can ace your next interview!
Nuwan I. Senaratna
in
On Economics
MeetNagadia
Want to be There
Ehud Karavani
in
Towards Data Science
BestShot.win
Ohoud
Thomas Spicer
in
Openbridge
Board Infinity
in
Board Infinity
"
150,https://medium.com/towards-data-science/how-to-level-up-your-pandas-skills-in-5-easy-ways-223208ce18cc,How to Level up Your Pandas Skills in 5 Easy Ways,"Pandas is a powerful and versatile Python library for data science. It is often one of the first libraries you come across when learning how to use Python for data science applications. It can be used to load data, visualise it and manipulate it to suit the objectives of the project you are working on.
However, many people don‚Äôt go beyond the basics of how to use the library and don‚Äôt take advantage of some of the more advanced and interesting features.
Within this article, we will go through 5 features that you may not have come across before that will make you more efficient when using the library.
For all of the examples used within this article, the data that we will be using is a subset of well log data that was used as part of a Machine Learning competition run by Xeek and FORCE 2020 (Bormann et al., 2020). This data is publicly available here and licensed under Norwegian Licence for Open Government Data (NLOD) 2.0.
However, all methods and examples shown here can be applied to any dataset loaded into pandas.
To begin, we first need to import the pandas library, which is commonly shortened to pd to make life easier.
We will then read the csv file into a dataframe using the read_csv() function.
When we view the dataframe we can see we have 12 columns of data containing a mixture of text and numeric values.
Now that is done, we can now move on to the 5 ways you can level up your data science skills with pandas.
When carrying out Exploratory Data Analysis we often want to generate a quick plot of our data. You could build up a plot using matplotlib, however, we can do it with a few lines of code using pandas.
Once you have the data loaded you can call upon the dataframe followed by the .plot method.
For example, if we take the GR column and plot a histogram, like so:
This will return the following plot:
The image that is returned is very basic and lacks interactivity.
From pandas version 0.25 it is possible to change which plotting library is used. For example instead of matplotlib, you can switch this out for plotly. Plotly allows the generation of very powerful and interactive data visualisations in an efficient way.
For more info on how to use Plotly to generate another type of plot called a Scatter Plot, you may be interested in exploring my previous articles:
To change the default plotting library for your current instance you need to change the plotting.backend option for pandas.
Once you have done that, you can then call upon the same code as before
And the plot that is generated is now interactive and provides a much better user experience when exploring datasets.
You can also use other types of plots, including scatter plots by changing the code around slightly like so:
When the code above is executed, you get back the following plot:
Chaining or joining multiple methods together is a long-practised programming technique that can improve code readability.
It is the process of calling methods on an object one after the other on a single line rather than applying the methods on the object separately. This can help with multiple stages of your process.
For example, if you want to load data, change the columns to lowercase and then drop missing values (NaNs) it could be done like this:
However, a more efficient way would be to chain the operations like this:
Sometimes the line may become very long so you may want to make it more readable by splitting it over multiple lines.
This can be done using the line continuation character ( \ ) as suggested in this StackOverflow question.
Or by using parentheses as suggested in this other StackOverflow question, which removes the need for the line continuation character.
A common task that we often do when working with data is to filter it based on single or multiple conditions. You can do this using the following code:
However, using the pandas query function produces a more readable piece of code, especially when things become a little more complex.
For example, if you want to find all rows where the GR (Gamma Ray) column contains values > 100 you can call upon the query method like so:
Which returns the following dataframe object.
You can also use logic if you want to combine multiple conditions.
Which returns an even smaller dataframe with just 7,763 rows
If you want to look for a specific string value, like Anhydrite within this dataset you need to modify our query method and chain a few methods together.
This will return any row where the LITH column contains the word Anhydrite
This can also be used if the string contains special characters such as the WELL column within our dataframe which has backslashes and dashes:
The eval() method within Python is a powerful tool when it comes to evaluating arbitrary Python expressions on columns within the same dataframe.
This means you can take columns from a dataframe and carry out arithmetic calculations by providing a string expression.
For example, you could subtract a value of 100 from the GR column:
Which returns
If you wanted to place this calculation in a new column you need to call upon pd.eval and pass in the expression followed by the target dataframe.
A common calculation within petrophysics is to calculate the volume of clay present within a formation. To do this sort of calculation you just extend the expression:
Which creates a new column called CLAYVOL
If you were doing a proper petrophysical analysis you would need to consider the selected parameters based on multiple formations or depth ranges. The above illustrates a quick method of carrying out the calculation.
If you have a situation where we need to match up values from an object such as a dictionary or substitute values within a dataframe with another value, we can use the map function.
This function can only be applied along a single column within a dataframe or on a Pandas Series.
With this data example, we can create a new column containing a numeric code based on the text string. This can be achieved using another dataframe or with a dictionary.
If using the dictionary as a reference you first need to create one or load one. This example uses a simple one that has been quickly created.
Next, you would create a new column, for example, LITH_CODE.
You then call upon the LITH column and apply the .map function, which contains the dictionary created above.
When you call upon the dataframe you now have the new column with the lithology code mapped to the correct lithology.
Pandas is an incredible library, that allows users to visualise, transform and analyse data in a very intuitive way. This article has covered some of the lesser-known features and methods, many of which may be new to you if you are just starting out on your data science journey. Understanding these will help you leverage the power of pandas to improve your data analysis skills.
Thanks for reading. Before you go, you should definitely subscribe to my content and get my articles in your inbox. You can do that here! Alternatively, you can sign up for my newsletter to get additional content straight into your inbox for free.
Secondly, you can get the full Medium experience and support me and thousands of other writers by signing up for a membership. It only costs you $5 a month, and you have full access to all of the amazing Medium articles, as well as the chance to make money with your writing. If you sign up using my link, you will support me directly with a portion of your fee, and it won‚Äôt cost you more. If you do so, thank you so much for your support!
Bormann, Peter, Aursand, Peder, Dilib, Fahad, Manral, Surrender, & Dischington, Peter. (2020). FORCE 2020 Well well log and lithofacies dataset for machine learning competition [Data set]. Zenodo. https://doi.org/10.5281/zenodo.4351156
Unity Health Score
in
Predict
Jeff Emrich
Shahed Mashni
Chris Holmes
in
Radiant Earth Insights
Elliott Wave Forecast
V Sampath Kumar
Zatak Softech
Xinyi Tang
"
151,https://medium.com/@frank-andrade/goodbye-low-quality-udemy-courses-hello-subscription-courses-539a99bff8bf,"Goodbye Low-Quality Udemy Courses. Hello, Subscription Courses!","There are thousands of online courses on the internet about almost any topic you can imagine. The problem is that‚Ä¶
Sunil Singh
The Political Grow
Chloe Wallace
Catriona Campbell
Frank Bonsal III
Dad On The Spectrum
Bart Caylor
Chris Hedges
in
The Bad Influence
"
152,https://medium.com/@tsecretdeveloper/the-most-hilarious-programmer-t-shirts-34b45ff28a4e,The Most Hilarious Programmer T-shirtsüëï,"Programmers have a sense of style and a sense of humor.
People have put the two together and market to this group who are both rich and young.
That‚Äôs not a surprise
Here are the most hilarious coding T-shirts available.
Maitrik Kataria
in
Paper Planes
Ketan Patil
Krishna Das
CableLabs
Edgeless Systems
in
Edgeless Systems
jelly
HyperDAO
Manpreet Singh
"
153,https://medium.com/@alfiramdhan/7-sql-queries-you-should-know-as-data-analyst-6a16602fffbe,7 SQL Queries You Should Know as Data Analyst,"The Most Used SQL Queries with Examples
SQL, which stands for Structured Query Language, is a standard computer language used to interact with a database system ‚Äî or rather a relational database management system. So, users can add, modify, update, search and delete data from a database system using SQL.
Nowadays, we all know how data important is. In this post, we will see the most common SQL queries with study cases.
Note: All the queries are written in https://www.w3schools.com/sql/.
Let‚Äôs get started!
1. Selecting Distinct and Aggregate Functions
1.1 Select Distinct
Inside a table, a column often contains many duplicate values; and sometimes you only want to list the different (distinct) values. To eliminate duplicate data, we can use the select distinct statement.
With select distinct, the same or duplicate data will be eliminated and only unique data will be displayed.
Syntax :
1.2 Aggregate Functions
Aggregate function is used to perform calculations on a group of values. Various aggregate functions such as sum(), count(), avg(), max(), and min(), here are the details :
The min() function returns the smallest value of the selected column.
The max() function returns the largest value of the selected column.
The count() function returns the number of rows that matches a specified criterion.
The avg() function returns the average value of a numeric column.
The sum() function returns the total sum of a numeric column.
Syntax :
Demo Database
Let‚Äôs say we have 2 tables that contain sales data in an e-commerce, I call them ecommerce_event and user_profile table.
The following is the contents of the ecommerce_event table,
This table contains product information and records of events carried out by the user along with the date info.
Then the following is the contents of the user_profile table, which contains a record of user information.
Now let‚Äôs explore those two tables to get sales information!
Example 1
From the ‚Äúecommerc_event‚Äù table, write a SQL query to output:
1. Total unique of user_session
2. Lowest, highest, and average price
With criteria ‚Äúevent_type‚Äù is view, and brand except apple and samsung.
Before we answer that, let‚Äôs break down the problem we really need to pay attention to there:
a. Pick the columns that we need to show the data, which are the user_session and price columns;
b. A column often contains many duplicate values, so we can use count distinct statement on user_session column to get amount of unique data;
c. In order to get output on point b, we can aggregate the value of price column using avg(), max(), and min() function;
d. Then filter the data using where clause based on the criteria requested.
Query :
Result :
So the results throughout October 2019, were 12,550 sessions followed by the lowest price of 0.88 then the highest price of 2,574.07 and the average price of 246.25.
2. Where Statement
The where clause is used to filter records. It is used to extract only those records that fulfill a specified condition.
Syntax :
Example 2
Write a SQL query to output total unique product. Filter brand which start with ‚Äòa‚Äô or ‚Äòk‚Äô letter, and date after ‚Äò2019‚Äì10‚Äì04‚Äô
Let‚Äôs break down the problem. First, we need to show unique number of product. So we can use count distinct statement on product_id column and use where clause to filter some criteria,
Query :
And the result show that total of unique product throughout October 2019 amounted to 891:
3. Group by and Order by Clause
In order to group data in SQL, we use the group by statement. The group by statement will group data with the same value into one group, and by using an aggregate function such as (COUNT, MAX, MIN, SUM, AVG) we can perform aggregation for each group or groups that are formed. While order by keyword is used to sort the result-set in ascending or descending order.
Important things to note are:
a. Group by is used with select, meaning that the columns used in the group by statement also need to be placed in the select.
b. Group by is placed after where, but if you don‚Äôt use where then it is placed immediately after from.
c. If using order by, then group by is placed before order by
Syntax :
Example 3
Write a SQL query to output total unique product and total unique user, for every order date. Only show the date above 04 august 2019, and sort the result by the latest date.
In this case, we need to use count distinct statement on product_id and user_id columns to get unique numbers and also use group by statement which will group rows having the same value into summary rows.
Query :
Result :
4. Having Clause
Having clause is used instead of where when using group by whose data is aggregated.
Syntax :
Example 4
From the question of example 3, filter only dates that have total unique product more than 500.
Similar to the previous, in this case we only need to add a having clause to the aggregate data.
Query :
Result :
5. Join Clause
A join clause is to join two or more different tables in the database. Well, this join can only be done if the tables have the same column key. In SQL, there are several types of join, namely: inner join, left join or right join. Each join type has a different purpose and query result.
Syntax :
Example 5
From the ecommerc_event and user_profile table, which gender has more sessions in October 2019?
Let‚Äôs take a moment. In this case we need to combine 2 tables to get information on which gender has the most sessions during October 2019. Firstly, we need to find the primary key and foreign key of each table. These key are useful for connecting the two tables so that we can get the table information.
Notice that the user_id column in the ecommerce_event table refers to the user_id column in the user_profile table. So the relationship between the two tables above is the user_id column.
Then we can create the following SQL statement using Inner Join that selects records that have matching values in both tables.
Query :
Result :
As we can see throughout October 2019, Male has more sessions than Female, with the total sessions were 9747.
6. Case When Clause
The case statement goes through conditions and returns a value when the first condition is met (like an if-then-else statement). So, once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the else clause. If there is no else part and no conditions are true, it returns null.
Syntax :
Example 6
From the ecommerce_event and user_profile table, is it true that iphone has more dominated by female and samsung dominated by male?
The case statement is used to implement the logic where you want to set the value of one column depending upon the values in other columns. In this case, we want to know which brand (iphone, Samsung) has more dominated by gender. So we set case statement goes through ‚Äòbrand‚Äô conditions and once the condition is true, it will return the result. Here are the code,
Query :
Through following table, we can see that Male dominated apple while samsung was dominated by Female,
7. Subqueries
A subquery is a SQL query nested inside a larger query. A subquery may occur in:
a SELECT clause
a FROM clause
a WHERE clause
Example 7
Calculate the daily average revenue per user (Revenue/ # users) for date with revenue higher than 3000.
In order to solve the problem, we require two queries. One query returns the revenue and total unique user, then a second query identifies the daily average revenue per user. Firstly,
First query :
Query result :
Using the result of this query, here we have written another query to identify the daily average revenue per user, for date with revenue higher than 3000. Or we can combine the above two queries by placing one query inside the other. The subquery (also called the ‚Äòinner query‚Äô) is the query inside the parentheses. See the following code and query result :
Final result :
These are the queries that are used commonly by data professionals.
Hope you found it helpful! Thanks for reading!Let‚Äôs connect on Linkedin!
Not sure what to read next? please visit the Basic Python : Work with Pandas, Numpy and Matplotlib and Dashboard with Google Data Studio!
A Byte of Coding
Cloudmersive
Shivamani A
Hafiz Waleed Hussain
in
U Want To Learn
Jaime
Matthew Clark
Bogdan Alexandru Militaru
Joseph Ashwin
"
154,https://medium.com/codex/top-data-engineer-interview-questions-586923349667,Top Data Engineer Interview Questions,"What are possible job interview questions for Data Engineers? Here is a list of the most popular questions you read on the internet, but also that I have heard from my own experience.
Minyarinn Chaotrakul
in
Bitkub.com
Mulyadi Oey
in
Product Narrative Publication
The University of Chicago Magazine
Todd P. Marco
Jessica Russell
Matty Kerr
Ajay Rajani
in
Agile Insider
Yari Diaz
in
An Idea (by Ingenious Piece)
"
155,https://medium.com/geekculture/stop-using-python-everywhere-3aa82f66d05,Stop Using Python Everywhere,"There was very little fanfare when Guido von Rossum released Python more than three decades ago. Released four years later, Java got a lot more limelight and ruled as the most dominant programming language for the next one and half decades. Python remained under the radar for a long time, and there were many reasons for‚Ä¶
Danyal Mh
in
Towards Dev
Jimmy Tran
Manu Mathew
Neuro-D
Human Digital
in
Nerd For Tech
Aravindaru
Jarvis Carter
Abdul Jabar Hakim
"
156,https://medium.com/towards-data-science/python-main-b729fab7a8c3,What does if __name__ == ‚Äù__main__‚Äù do in Python?,"If you are new to Python, you might have noticed that it is possible to run a Python script with or without a main method. And the notation used in Python to define one (i.e. if __name__ == ‚Äò__main__') is definitely not self-explanatory especially for new comers.
In today‚Äôs tutorial we‚Äôll explore and discuss the purpose of a main method and what to expect when you define one in your Python applications.
Before executing a program, the Python interpreter assigns the name of the python module into a special variable called __name__. Depending on whether you are executing the program through command line or importing the module into another module, the assignment for __name__ will vary.
If you invoke your module as a script, for instance
then Python Interpreter will automatically assign the string'__main__' to the special variable __name__. On the other hand, if your module is imported in another module
then the string 'my_module' will be assigned to __name__ .
Now let‚Äôs assume that we have the following module, that contains the following lines of code:
So in the module above, we have one print statement which is outside of the main method and one more print statement which is inside. The code under the main method, will only be executed if the module is invoked as a script from (e.g.) the command line, as shown below:
Now, let‚Äôs say that instead of invoking module first_module as a script, we want to import it in another module:
And finally, we invoke second_module as a script:
Notice that the first output comes from module first_module and specifically from the print statement which is outside the main method. Since we haven‚Äôt invoked first_module as a script but instead we have imported it into second_module the main method in first_module will be simply ignored since if __name__ == ‚Äò__main__' evaluates to False. Recall that from the above call, __name__ variable for second_module has been assigned string '__main__' while the first_module ‚Äòs __name__ variable has been assigned the name of the module, i.e. ‚Äôfirst_module‚Äô .
Although everything under if __name__ == ‚Äò__main__' is considered to be what we call a ‚Äúmain method‚Äù, it is a good practice to define one proper main method instead, which is called if the condition evaluates to True. For instance,
Note: I would generally discourage you from having multiple main functions in a single Python application. I have used two different main methods just for the sake of the example.
In this article, I have described how the main method gets executed in Python and under what conditions. When a module is invoked as a string, then Python interpreter will assign the string '__main__' to a special variable called __name__ and the code which is defined under the condition if __name__ == ‚Äò__main__' will subsequently get executed. On the other hand, when a module is imported in another module, then the Python interpreter will assign the string with the name of that module to the special variable __name__ . This means that in such cases if __name__ == ‚Äò__main__' will evaluate to False which means that only the code outside of this condition will get executed once imported.
Become a member and read every story on Medium. Your membership fee directly supports me and other writers you read. You‚Äôll also get full access to every story on Medium.
gmyrianthous.medium.com
Related articles you may also like
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
Waqar Mansoor
Chanak Karki
Okan Yenig√ºn
in
Towards Dev
James Salter
in
The Memrise Engineering Blog
Imran Ahmad
Purushotham Reddy
Dilibe
Pixverse
"
157,https://medium.com/mlearning-ai/a-b-testing-result-analysis-using-python-beginners-guide-7a6562933f7,A/B Testing Result Analysis Using Python‚Äî Beginner‚Äôs Guide,"Recently, I just finished the A/B testing Course at udacity.com. That was one of the few online courses that made me fell in love with a data science concept. I managed to find this A/B Testing Result dataset that collected from a mobile gam. Here I am, sharing my analysis and notes.
Notebook and data can be found here: Github
An A/B Testing Result Analysis contains:
Link to Statistic testing mind-map
The data contains five columns:
Can‚Äôt start any data analysis project without explore and clean the data! Our data set is relatively neat; all I need to do is to
1. detect(using histogram and box-lot) and remove outliers (play 0 rounds or more than 3000 rounds)
2. remove users who played 0 rounds of the game
3. check if duplicate values.
The general steps for statistical testing contains:
In order to determine Parametric or Non-Parametric test, we first need to check three assumptions: (1) Independence of Variables (2) Normality of Data, and (3) Homogeneity of Variance.
By graphs-
QQ Plot: If the data has a normal distribution, we should see all the points sitting on the straight line.
Box Plot: If the data has a normal distribution, we should see the mean and median in the center.
Histogram: If the data has a normal distribution, we should see a bell curve.
By Statistical tests-
Shapiro-Wilk Normality test, D‚ÄôAgostino‚Äôs K-squared Normality test, Chi-Square Normality Test,Kolmogorov-Smirnov Normality Test , and Anderson-Darling Normality Test
If the data passes the Parametric Tests:
If the data does NOT passes the Non-Parametric Tests:
Mann-whitney U/ Wilcoxon Sum of Ranks(Independent, non-categorical)
Wilcoxon Signed Ranks test/Sign Test(Paired,non-categorical)
Chi-squared, Fisher‚Äôs Exact test ( Independent, Categorical)
Link to Mind-map
According to our chart above, we are working with Non-Parametric Test ‚Üí 2 Groups ‚Üí Independent ‚ÜíQuantitative outcomes ‚Üí Mann-Whitney U-test
According to our chart above, we are working with Non-Parametric Test ‚Üí 2 Groups ‚Üí Independent ‚ÜíBinary(categorical) outcomes ‚Üí Chi contingency
H0:There is No significant difference in sum_gamerounds between two versions
H1: There is significant difference in sum_gamerounds between two versions
H0: There is No significant differene in retention rate between two versions
H1: There is significant differene in retention rate between two versions
Since p=0.0588, which is > 0.05, we fail to reject the null hypothesis that there is No significant difference in sum_gamerounds between two versions
Since day 1 retention rate p=0.1155, which is > 0.05, fail to reject the null hypothesis that two versions have no significant difference.
Since day 7 retention rate p=0.0027 , which is <0.05, we have enough evidence to reject the null hypothesis that two versions have a significant difference.
medium.com
TARUN G
Avishek Choudhury, Ph.D.
in
Towards Data Science
Jon Loyens
in
distinct values: data.world
ZaranTech
Aparna Bhat
Danny van Leeuwen Health Hats
in
Tincture
Mahmood Rahmani
in
Coord
Michael Harris
"
158,https://medium.com/towards-data-science/why-i-decided-to-do-a-phd-in-machine-learning-fdee03bb956f,Why I Decided to do a PhD in Machine Learning,"Stressful schedules, unhelpful supervisors and low pay. You don‚Äôt have to look hard for advice against doing a PhD. To make matters worse, most of this comes from students themselves.
So why would anyone want to do a PhD?
I thought hard about my decisions and I want to share the reasons with you. If you are considering a PhD, the hope is they will help you make a decision.
The first is a common one. During the interview process, a professor asked:
‚ÄúWhy do so many applicants have 2‚Äì3 years of industry experience?‚Äù
He wanted to understand the reasons for this trend. I can‚Äôt speak for others but I was simply‚Ä¶ bored.
For the most part, I enjoyed my role as a data scientist. I worked with interesting datasets and built impactful models. Yet, I became unhappy. My learning began to stagnate and I was becoming frustrated with the rigid 9‚Äì5 schedule. Mainly, due to the industry I was in, I had no sense of purpose. It was time for a change.
Similarly, I think the wider trend is driven by this need for change. All around me my friends are quitting their jobs. Many of us have become disillusioned with what it means to be a professional (call it a quarter-life crisis if you will). At the same time, we have more connections and experience. Those that haven‚Äôt accepted the mundanity are using these to find better opportunities.
Personally, this experience was an awakening. I have always worked hard. I naively thought I would naturally end up in a good position. By that, I mean one that provided interesting and fulfilling work. This is not true. I realised I needed to find it myself.
The search led me to a PhD at University College Dublin (UCD). I‚Äôll be applying ML to monitor the Irish coastline (I‚Äôve just started so please don't ask me for specifics). To start, I will be using open-source satellite data and computer vision methods. From there, I will have a lot of control over the direction of the PhD.
This control is important to me. My intention was actually to do a PhD in Interpretable Machine Learning. I find this subfield incredibly interesting and I‚Äôve written a lot about methods like SHAP and PDPs. Although it won‚Äôt be the focus, I will have the freedom to incorporate IML methods in my research. In fact, there is a need for these in remote sensing applications.
Most deep learning techniques do not disclose how the data features take effect and why the predictions are taken. Remote sensing big data exacerbated the problem of in-transparency and in-explainability of current AI.
‚Äî Prof. Dr. Lizhe Wang
Ultimately, I will still be able to explore my interests in IML. At the same time, working with climate data gives me a sense of purpose. Crucially, to satisfy my 3rd reason, I will also develop new skills along the way.
When I started to look for a change, I didn‚Äôt just look for PhD positions. I applied to roles in industry as well. I even did a few interviews. The problem was that I was being rejected from the most interesting ones.
I was interested in more advanced data science roles. That is working with the latest tools and applying ML to image or text data. Although I was working as a data scientist. My focus was more on tabular data and I used outdated tools. Any modelling was done with simple logistic regression.
I needed to upskill and, yes, I can hear you screaming ‚Äî ‚ÄúThere are better ways to upskill than a PhD!‚Äù I agree but none of these would align with my last reasons. I was also drawn to ML research scientist roles. The vast majority of these require a PhD.
Whether it‚Äôs academic papers or a final thesis, it seems like writing is a major source of stress for PhD students. Yet, this part of research doesn‚Äôt worry me. I actually enjoy writing and, after 3 years on Medium, I‚Äôve had a lot of practice.
Okay, so academic papers are far more rigorous. Still, the skills I have developed make them less daunting. At the same time, the PhD research will benefit my Medium page.
To continuously create new content you have to continuously learn new things. Learning is at the core of any PhD. Importantly, all of the work will be open-sourced. Unlike an industry role, this means I can share everything I learn. The flexible schedule will also allow me to balance the PhD workload and content creation.
I must admit, I love even more that I can make money from my writing. 3 years of hard work (and luck) has created a stream of income that will supplement my stipend. Yet, making money online is addictive and I‚Äôve decided to take it more seriously.
I recently launched a Python SHAP course and a newsletter (if you sign up you will get free access to the course). I‚Äôve even started telling people I run a small (a.k.a microscopic) online business. Through better content creation, a PhD will help me grow this business.
After a 4 year PhD, the hope is it will provide a significant stream of income. I would like nothing more than to do it full-time. However, the goal is really to make enough to expand my options. That is so I do not need to make career decisions based solely on salary.
In the end, the 4 reasons come together. I needed a change and I found an interesting and fulfilling opportunity. This opportunity will allow me to write and grow my online business. At the same time, the pragmatist inside is satisfied. I‚Äôll also develop skills that will lead to opportunities beyond the PhD and business.
I am only 1 month into the PhD. Yet, I have already learned so much and met so many driven and excited people. Hopefully, it stays that way. Otherwise, look out for the article titled ‚ÄúWhy Doing a PhD was the worse Decision of my Life‚Äù.
I hope you enjoyed this article! You can support me by becoming one of my referred members :)
conorosullyds.medium.com
| Twitter | YouTube | Newsletter ‚Äî sign up for FREE access to a Python SHAP course
Jela Nalica
Rahul Gupta
Deepika Shreerang Kavishwar
Sergei Izrailev
in
Towards Data Science
Elena Kosourova
in
Towards Data Science
Rahul Kumar
Art Kulakov
in
Level Up Coding
Mike
"
159,https://medium.com/towards-data-science/dbscan-clustering-break-it-down-for-me-859650a723af,DBSCAN Clustering: Break It Down For Me,"Welcome back to the world of I-don‚Äôt-know-why-I-was-freaking-out-about-this-algorithm-because-I-totally-intuitively-get-it-now, where we take complicated sounding Machine Learning algorithms and break them down into simple steps using fun illustrations.
Today we‚Äôll tackle another clustering algorithm called DBSCAN (Density-based spatial clustering of applications with noise). To understand DBSCAN better check out the K-Means and Hierarchical clustering articles first.
As the name suggests, DBSCAN identifies clusters by the densities of the points. Clusters are usually in high-density regions and outliers tend to be in low-density regions. The 3 main advantages of using it (according to the pioneers of this algorithm) are that it requires minimum domain knowledge, can discover clusters of arbitrary shape, and is efficient for large databases.
Now that we have the introduction out of the way, let‚Äôs get to the fun part ‚Äî actually understanding how this works. Let‚Äôs suppose our raw data looks like this:
The first thing to do is count the number of points close to each point. We determine this closeness by drawing a circle of a certain radius (eps) around a point and any other point that falls in this circle is said to be close to the first point. For instance, start with this pink point and draw a circle around it.
We see that this point overlaps, fully or partially, 7 other points. So we say that the pink point is close to 7 points.
The radius of the circle, called eps, is the first parameter in the DSBCAN algorithm that we need to define. We need to define eps appropriately because if we choose a value that is too small, a large part of the data will not be clustered. On the other hand, if we choose a value that‚Äôs too large, clusters will merge and a lot of data points will be in the same cluster. In general, a smaller eps value is preferred.
Now consider this blue point. We see that it is close to 3 points because its circle with radius eps overlaps 3 other points.
Likewise, for all the remaining points, we count the number of close points. Once we do that, we need to decide what points are Core Points and what points are Non-Core Points.
This is where our second parameter of the algorithm ‚Äî minPoints ‚Äî comes in. We use minPoints to determine if a point is a Core Point or not. Suppose we set minPoints to 4 then we say that a point is a Core Point if at least 4 points are close to it. If less than 4 points are close to a point, it is deemed a Non-Core Point.
As a general rule, minPoints ‚â• number of dimensions in a dataset + 1. Larger values are usually better for datasets with noise. The minimum value for minPoints must be 3, but the larger our dataset, the larger the minPoints value must be.
For our example, let‚Äôs set minPoints to 4. Then we can say that the pink point is a Core Point because at least 4 points are close to it, but the blue point is a Non-Core Point because only 3 points are close to it.
Ultimately, using the above process, we can determine that the following highlighted points are Core Points‚Ä¶
‚Ä¶and the remaining ones are Non-Core Points.
Now, we randomly pick a Core Point and assign it to the first cluster. Here, we randomly select a point and assign it to the blue cluster.
Next, the Core Points that are close to the blue cluster, meaning they overlap the circle with radius eps‚Ä¶
‚Ä¶are all added to the blue cluster:
Then, the Core Points close to the growing blue cluster are added to it. Below, we see that 2 Core Points and 1 Non-Core Point are close to the blue cluster, but we only add the 2 Core Points to the cluster.
Ultimately, all the Core Points that are close to the growing blue cluster are added to it and the data will look like this:
Next, we add all the Non-Core Points close to the blue cluster to it. For instance, these 2 Non-Core Points are close to the blue cluster, so they are added to it:
However, since they are not Core Points, we do not use them to extend the blue cluster any further. That means this other Non-Core Point that is close to Non-Core Point 1 will not be added to the blue cluster.
So, unlike Core Points, Non-Core Points can only join a cluster and can not be used to extend it further.
After adding all our Non-Core Points, we are done creating our blue cluster and it looks like this:
Now because none of the remaining Core Points are close to the first cluster, we start the process of forming a new cluster. First, we randomly pick a Core Point (that‚Äôs not in a cluster) and assign it to our second yellow cluster.
Then we add all the Core Points close to the yellow cluster and use them to extend the cluster further.
And then the Non-Core Points that are close to the yellow cluster are added to it. After doing that, our data with the 2 clusters, looks like this:
We keep repeating this process of creating clusters until we have no Core Points left. In our case, since all the Core Points have been assigned to a cluster, we‚Äôre done making new clusters. Finally, any remaining Non-Core Points that are not close to Core Points and not part of any clusters are called outliers.
And just like that we made our 2 clusters and found outliers AND came out the other side unscathed.
K-Means and Hierarchical are suitable for compact and well-separated clusters and are also severely affected by noise and outliers in the data. On the other hand, DBSCAN captures clusters of complex shapes and does a great job of identifying outliers. Another nice thing about DBSCAN is that, unlike K-Means, we don‚Äôt have to specify the number of clusters (k), the algorithm automatically finds clusters for us. The below figure illustrates some examples of the differences and why DBSCAN can be powerful when used appropriately.
That‚Äôs all for today. Please feel free to connect with me on LinkedIn or email me at shreya.statistics@gmail.com to send me questions and suggestions for any other algorithms that you want to be illustrated!
Jyoti Prakash Maheswari
in
Towards Data Science
Brian Graves üö¢
Charlotte Tu
in
Towards Data Science
Srivatsan Srinivasan
in
DataDrivenInvestor
Callie Collins
in
Syneos Health Life
UNDP Accelerator Labs
Anu
data4help
"
160,https://medium.com/towards-data-science/data-science-anti-patterns-you-should-know-1d8711eef72d,Data Science Anti-Patterns You Should Know,"Anti-patterns are common yet counter-productive responses to recurring problems. Because they‚Äôre ineffective, they perpetuate recurring pain points without ever resolving the underlying, systematic issues. Anti-patterns exist pretty much anywhere people come together to solve problems, in software development, project management, and yes, also in data science.
Knowledge about anti-patterns is the best way to avoid them, so in this post I‚Äôll break down 5 common data science anti-patterns that I‚Äôve observed over the years in my own career, namely:
(Of course, this list is not meant to be complete, nor are all items on the list exclusive to the domain of data science.)
Ready? Let‚Äôs dive in.
Every data scientist knows ad hoc requests. They‚Äôre queries from collaborators and stakeholders, such as‚Ä¶
and so on and on. In the worst case, ad hoc requests may completely fill a data scientist‚Äôs day, resulting in ad hoc overload.
üëâ What to do instead:
Never do it immediately. Unless it‚Äôs an emergency, don‚Äôt take care of the request immediately, but instead wait until you have collected a few such requests and then do them all in one batch, perhaps during a dedicated time slot each week (I used to set such a time slot on Fridays). This ‚Äòinterrupt coalescing‚Äô has two advantages:
Automate recurring ad hoc requests. A good rule of thumb: if you do a small task twice, for the third time try to write an automated script. For example, at Amazon our team was frequently asked to prepare a list of all models that we owned along with certain metadata such as the time since the last model update. After doing the task manually a few times, I scheduled an ETL job that would automatically pull the data each week and Email it to the relevant stakeholders. Voila, I never had to even think about that task again.
Beware of data insights that aren‚Äôt actionable. Before writing any SQL query, always ask what action can be taken based on the result. If the datapoint you‚Äôre pulling doesn‚Äôt inform any particular business action either way, do we really need it?
You were hired to solve problems, not to sit in meetings. Yet, almost inevitably, meeting load tends to increase with your tenure inside an organization. If you feel that you‚Äôre not getting any actual work done because of meetings, you‚Äôre suffering from meeting paralysis.
üëâ What to do instead:
Leave meetings if you aren‚Äôt contributing. If you are neither contributing your expertise to a decision-making process, nor presenting anything to others, nor learning anything that‚Äôs useful to you, then leave. It‚Äôs not rude to leave a meeting that‚Äôs wasting your time, but it is rude to waste company resources (and that includes your time).
Skip an instance of a recurrent meeting and see what happens. Sometimes you find yourself on the invite list of a recurrent meeting series even though you‚Äôre not sure if your presence is actually required. Amazon principal engineer Steve Huynh has a useful tip for this situation: one week, simply don‚Äôt show up to the next meeting instance and see what happens. If no one asks where you were, that means you can permanently drop this meeting series from your calendar. If people ask about your absence, apologize and say you‚Äôll be back next time.
Feel empowered to reschedule meetings. There is an enormous difference between having four 30-minute meetings spread throughout the day and having them all in one 2-hour chunk in the morning. That‚Äôs because in the former case you‚Äôll end up with 4 times more context switches (8 instead of just 2) between ‚Äòmeeting mode‚Äô and ‚Äòdeep work mode‚Äô, which can practically cost an entire day of productive time. Batching your meetings such as to minimize context switches is therefore not only in your own personal best interest, but also in the company‚Äôs best interest because they pay you for your time. Never hesitate to do so.
Here‚Äôs a recipe for disaster: have a team of model developers build model artifacts and hand these over to an engineering team for deployment. This pin factory approach doesn‚Äôt work well in practice because:
üëâ What to do instead:
The opposite of the pin factory approach is end-to-end ownership, where a model owner owns the entire model lifecycle, including data collection, model development, offline/online testing, and deployment.
For this approach to work, an organization needs two types of ML roles, namely ML infra engineers and model developers (aka data/applied/research scientists). ML infra engineers build, maintain, and own a set of services that abstract away the infrastructure around the model development process, one service for model training, one service for feature engineering, one service for inference calls, and so on. Model owners use these services in order to develop, test, and deploy their models end to end.
The former own the services, the latter own the models: this approach works well because it has clear ownership boundaries and little communication overhead.
ML model developers are incentivized to build overly complex solutions where simple ones would do, just because complex solutions look better on their resume or promo doc, a phenomenon known as resume-driven development. This can create higher maintenance cost for the entire organization in the long run because overly complex models are harder to interpret, debug, and retrain, and create higher infrastructure costs for training and serving.
üëâ What to do instead:
A good strategy therefore is to keep production systems as simple as possible, and reserve wild ideas for research projects that can result in publications. LinkedIn Staff ML engineer Brandon Rohrer puts it this way:
‚ÄúML strategy tip: When you have a problem, build two solutions ‚Äî a deep Bayesian transformer running on multicloud Kubernetes and a SQL query built on a stack of egregiously oversimplifying assumptions. Put one on your resume, the other in production. Everyone goes home happy.‚Äù
As an empirical discipline, a lot of progress in ML is driven by experimentation. However, things get problematic when model developers throw everything at a problem and ‚Äúsee what sticks‚Äù. This is HARKing, the practice of formulating a hypothesis only after the results from a large suite of experiments are known.
HARKing is dangerous because of the statistical look-elsewhere effect: the more experiments you run, the higher the chance that you‚Äôll find a model that looks better just by chance alone. Needless to say, if the proposed model was HARKed, then the expected model improvement will not actually materialize in production: it was just a statistical fluke.
üëâ What to do instead:
Let‚Äôs summarize these 5 anti-patterns and their respective resolutions as follows:
Of course, this is list is by no means complete. Generally, a good way to spot anti-patterns in your day-to-day work is to watch out for recurring pain points. For example, the recurring pain point caused by ad hoc overload is frequent context switching that drags your productivity. Or, the recurring pain point caused by pin factories is the act of frequently having to send data back and forth in between teams. Pay attention to the recurring pain points, and you may discover the underlying anti-patterns.
Lastly, distinguish between anti-patterns you can break yourself, and those that you can‚Äôt. For example, you can probably break ad hoc overload and meeting paralysis yourself, at least to some degree, by following steps such as the ones I outlined here. However, for organizational anti-patterns such as ‚Äòpin factories‚Äô, there‚Äôs really not much you can do yourself as an individual contributor. If anti-patterns are organizational, the only thing you can really do is leave that organization.
üì´ Subscribe and get my next article straight to your inbox.üí™ Become a Medium member and unlock unlimited access.üìñ Get my e-book, 50 pages packed with practical ML insights from the trenches.
Robert (Munro) Monarch
BioTuring Team
janet wong
J Li
in
Towards Data Science
Eric Sun
in
Analytics Vidhya
Matthieu Moreau
in
Towards Data Science
Gerard Sho
Sean Moore Gonzalez
"
161,https://medium.com/@PythonDude/python-the-2023-road-map-to-master-it-all-2ed2460efa6f,Python The 2023 Road Map To Master It All,"Python can do anything. All including Data Science, Machine Learning, Deep Learning, NLP, Image/Video Processing for both Detection and Segmentation, Games, Android, Web Apps , Desktop GUI , You Just name it.
The thing is, if you want to learn it all, this can be a bit overwhelming. By the end of this article, you will see how you can master all of those topics.
This mind map summarizes all the libraries (and basic python concepts) that you can learn to master it all. This is not a very long run. You should be able to master all of those topic in a shorter amount of time than you think, because python takes care of most of the code for you, due to its HUGE library support.
In the world of modern software, one cannot ignore the importance of maintainable easy to extend code.
So here is what you need to know:
The following files formats are essential to work with. And Python is the best programming language to edit, script and automate whatever realted to those file formats.
It is necessary that math computation should be fast, and python, kind has the reputation of being slow compared to C/C++.
Well, this is not entirely true. In Python we use a library called Numpy to perform almost any Complex math operation. Be it matrix multiplication, be it searching an array for an element or finding Max/Min elements.
The library is huge, but here is some highlights on what you need to learn to get started:
Pandas is the to-go library when handling any Tabular data (like excel files, csv..etc).
Imagine being able to automate anything that excel can do, and add it to the vast enormous Python world.. well that is exactly what is this library all about. Here is some highlights on where you need to start:
It is a wild statement to say that Python can replicate all of those Photo/Video Apps you see around. Because Python combined with OpenCV is a giant monster when it comes to media file manipulation.
Here is some highlights on what‚Äôs necessary to start with:
Now everything you applied to images, can be applied to video, because a video is nothing but a series of images displayed at a certain rate.
The only thing I can add here is Object Tracking , take a look at it.
How would you get rid of that ‚Äúcode view‚Äù and use some Interactive interfaces instead. Isn‚Äôt that what all software have in common?
Python has multiple forms of user interface.
Having data, be it internal python data or external tabular data requires some visualization right?
The most important two libs are:
Matplotlib is more flexible to use, but Seaborn has a more beautiful visualization. In general here is the most important topics to learn about any of them
Now that you have learned some Python coding, you will realize that executing‚Äôs one instruction at a time is not practical in most large applications. Like sometimes you would like to have your GUI running independently of the python other functions.
And here comes multithreading.
The topic is pretty large, but those should be enough to get you started.
Automating the ‚ÄúBoring Tasks‚Äô , like Excel File , webforms and GUI logging can be easily automated with python. You can even create python scripts just to play your favorite Android/OS game and collect all the daily rewards for you!
The following libraries are the most famous for this:
Here is a highlight of what you need to start with:
Supervised learning, unsupervised learning, Reinforcement learning , all of those exciting topics can be learned through programming them with python. Since programming with hands on application will increase your learning rate x10 folds, consider learning the algorithm and playing around with in python immediately.
The two most famous library for data science and machine learning are:
Initially you can stick to sklearn. Here are the top algorithms that you can start with:
It‚Äôs important also to familiarize your self with Machine Learning models evaluation techniques :
This topic is built on Python Machine Learning, and it utilizes essentially Neural Networks. This is a whole domain of specialization now.
You need to check out Tensor Flow, Keras and Pytorch.Start with Keras (built on Tensor flow) then you can jump to Pytorch.Since Keras functions are easier to handle than Pytorch.
Here are the first algorithms that you need to start with
Make sure you learn all the common technicalities that comes with all deep learning models like:
Those should be enough to get you started
Yes, you can actually design backend for your web pages with Python. Actually libraries like Flask can be the most user friendly one to get you started. To be honest, it would be super helpful to have some knowledge in at least HTML/CSS to be able to test out what‚Äôs going on through your Font-End (Front end being all of flashy buttons and menus you see on a page).
However, if you are fine with writing super dummy pages using Flask internal functions, then be my guest and get started!
Here is what you need to start with:
You definitely should check This Course out.This is the only course on the internet that gets weekly updates with a new section. Since the goal here, is that this course includes everything that you would ever need in python. You don‚Äôt need to buy 20 courses to master python.
You can ask the instructor to add any topic regarding Python and he will add it within a week!! No additional costs!
SACHIN KUMAR
in
Chatbots Magazine
Yokwejuste
Rajesh kumar
Prathamesh Mistry
in
AWS Tip
Pulkit Juneja
Megha Varshney
Ales Kozelsky
Sathish Gangichetty
in
The Startup
"
162,https://medium.com/codex/8-signs-you-are-an-intermediate-software-developer-e5801defcf47,8 Signs You Are An Intermediate Software Developer,"Stay away from ‚ÄúI still Google x or y.‚Äù Yes, you will because you can‚Äôt remember everything. If you can‚Äôt remember everything, you can take a small code snippet and make it do what you want.
rigyaja
in
Mobis3c
Aditya Gurjar
Jack Leavey
Lawrence Richards
Avatar Creator Factory
Vincent Au
Edgar Sanchez
DAYSTARTER
"
163,https://medium.com/towards-data-science/stop-using-print-and-start-debugging-3cd11524c92a,"Stop using Print, and start Debugging","I once read that in programming an error in code is called bug for a very specific reason. When computers were still huge mainframes (a long, long time ago) it happened that a bug got stuck in the gears and that‚Äôs why the program wouldn‚Äôt run anymore!
Today by bugs we mean a very different thing. Everyone, even the most experienced and paid programmers in the world write code that contains bugs, but the skill lies in finding them. In this article, we find out how!
It is common for data scientists to write code using Jupyter Notebooks or Google Colab. I too find that they are super convenient to use, unfortunately, though debugging is not as convenient.
In these notebooks, we can split our code into cells, that way at the end of the execution of a cell, we can print out all the values of the variables we are interested in, and see if anything has gone wrong. Or at least we can get an idea of what‚Äôs going on inside the code.
Even in cases where we do not use notebooks, we often use print() to figure out the value of variables and thus understand why the code behaved a certain way and fix it.
For example, suppose we want to execute the following code.
With the previous code, we know we have the variable admin. When this variable takes the value ‚Äòadmin‚Äô we want to greet our admin.
We find, however, that this condition never occurs, even when it should occur, since it is our admin who is using the program.
Then what novice programmers generally do is go and print out the admin variable and figure out what‚Äôs wrong.
We note that the result of this print is as follows.
And we find that our get_user_input function returns one \n extra and that is why the condition never occurs. We are then ready to fix our code and run everything. Bug found!
But is this the best way to find the bug? Not really. In such simple cases where we have 4 lines of code maybe it is okay, but in more complex projects you have to prefer the debugger tool that is provided by various IDEs.
IDE stands for Integrated Development Environment and is a tool that allows you to write code by providing very useful tools such as a debugger.
In this article, we will look at the VSCode debugger but they all work pretty much the same, so even if you use PyCharm or any other IDE don‚Äôt worry.
Let‚Äôs start by writing a simple code in Python in VSCode. The example code is as follows.
In this code, we created a User class, where each user is described by name and password.
Plus a user has two methods, such as say_hello which greets the user, get_name_len which returns the length of the user‚Äôs name, and the __str__ method which defines how a user should be printed.
Finally in the main we go to create three random users and have them use the say_hello method.
Let‚Äôs now go to introduce an error. For example in the get_name_len method, we only print the length of self instead of self.name.
Now if we go to execute the code we will get the following error.
Our purpose now is to try to find the error using the VSCode debugger.
The first thing to do is to insert a Breakpoint at a certain point in your code. With the Breakpoint, you tell your compiler to pause the run at that point so that you can inspect the various variables in your code, and then continue the run only when you want it to.
To enter a Breakpoint click to the far left of your code and a red dot will appear.
Now let‚Äôs run the code in debug mode by clicking on the following icon.
Once you click you will see that your code will start the run but stop almost immediately.
You should now have a screen like this.
So, the yellow line with that symbol next to it, tells you where the code stopped, right where you put the Breakpoint.
In the upper left corner, on the other hand, you can see the contents of your variables within the code. Many of these variables seem meaningless since they are pointers to functions you have in the code. But if you see well there should be a User variable since before the Breakpoint we have already created a user. In my case, I can see the content which is as follows.
This way I know what the content of my user variable is and I have already saved myself from writing a print().
Now we can start using the main debugger commands found at the top of the bar made like this.
The main commands are those highlighted in red. The first allows us to go forward one line of code to see what happens. The second allows us to go inside a function and see what happens. And the last one allows us to go outside the scope of a function.
Since our program is now stuck on the user.say_hello() function let's click on the down arrow to see what happens inside this function.
Once clicked you will see that the execution will continue within this function.
Now we need to understand what the stack is for. The stack tells you basically how many functions you are in. In fact, if you see in the lower left-hand corner now a line has been added.
This tells us that we are now in the scope of the say_hello function so if we exit this function by clicking the up arrow, we will be back in the main.
Now let‚Äôs move forward one line of code with the forward command.
Now that we are over another function, we click the down arrow to go inside the function itself.
You see that because we have entered inside another function the stack has increased, so we are in a function inside a function (no, this isn‚Äôt Inception).
Obviously, we can enter more than one unique Breakpoint. So let‚Äôs stop the debug and relaunch it by entering 2 Breakpoints this time.
So in this case by clicking on forward, we are going to stop at the second Breakpoint.
If we now click forward again, the code will go to stop, but this time because we found the error!
We succeeded in our intent. Error found, and we did not dirty our code with useless prints!
Only one thing remains to be said. What is the watch section for? Well, to look at variables.
Suppose we save all our users in a list.
Now we want to see what happens to this list as we go through the code.So let‚Äôs click on the plus button and add the viable users under watch. In fact, let‚Äôs add len(users) as well.
Now as you move forward with your code with the forward command, you will see the list fills up and you will be able to inspect the objects within it.
Well, these are the basics of using the debugger. It is a very useful and easy tool to use. Of course like all things, you have to get used to it. So the next time you have problems with your code, force yourself to use the debugger instead of using prints and you will see that in no time you will be automatically using it all the time!
Marcello Politi
Linkedin, Twitter, CV
Makarbaderko
Abhishek kumar
Pedro Szloma Herr Zaterka
in
Argo Project
Keya Patel
in
buildimpact
Nazmuz Shakib Pranto
Beto Rayashi
Roberto Hernandez
Mike Schlansky
"
164,https://medium.com/@borntowinuk/how-to-write-python-code-as-a-pro-few-cool-ways-f936c9a80108,How to write Python code as a Pro‚Ä¶Few Cool Ways,"Over these all these years (atleast for past 15yrs) I had chance to review lot and variety of code as a Lead , as an Architect written by various level of fresher to experienced programmers (min 1 to 20 years)‚Ä¶In my own words‚Ä¶I always call Programming is an art and I feel it divine while writing the code, so that you give your at most best focus to make the code much cleaner and elegant and produce world class products.
Python is one of my favorite programming language for its simplicity but doing some amazing things, indeed I learnt to lead my life like Python, be simple and calm but do some great and exciting stuffs. Lot of programmers make a basic fault to grab the low hanging fruits from Python and don‚Äôt climb up to reach some golden fruits which is part of the Advanced Python‚Ä¶I am sure if you are a DataScience programmer you must be knowing how cool its to create some great predictive models out of python libraries like NumPy, Scilearn,Pandas,matlib‚Ä¶etc ‚Ä¶so in this post I am going to write about how to write Python code as a Pro‚Ä¶.so that you can leverage the potential of Python.
‚ÄúBeautiful is better than ugly‚Äù ‚Ä¶.according to the Zen of Python poem lines, some of the code I use to review use to be very ugly and bad and that differentiates a bad programmer from a Pro developer..
You may not believe , a person named Tim Peters composed a poem called ‚ÄòThe Zen of Python‚Äô about Python programming. Just type ‚Äòimport this‚Äô into an interpreter to read the poetry. It‚Äôs a mind blowing and entertaining poetry that you should read for yourself.
Do you know Python has overtaken French in primary schools ?
Yes you read it correct, Python surpassed French as the most popular primary school language in 2020. Statistics show that 60% parents would prefer their children to learn Python than French. This shows that the importance of Python programming was acknowledged by a vast number of people.
This post will help you to understand few great Python tricks to write more Pythonic programs in your daily work.
Ternary Conditional Operator:
Writing a Simple If-Else Structure in One LineThe if-else conditions are everywhere in the programming world. To make simple logic easy to express, Python provides us with the ternary operator. Simply put, it just allows as put an if-else condition in one line:
Obviously, the above code is much neater than the below
List Comprehensions
Get a List in a Pythonic WayTo say the list comprehension makes your code elegant is still an understatement. It can save you lots of typing and time but still keep your code readable.
Look at the above elegant program for a while and think about how many lines of code you need to write without the list comprehension trick.
Avoid Nested Python Loops Using product() Function
When a program becomes complicated, you inevitably have to write nested loops. However, nested loops will make programs harder to read and maintain. I can see during my code reviews‚Ä¶even a experienced programmers used to do this‚Ä¶Fortunately, you can always avoid nested loops in Python through the built-in product() function.
For example, we have a program as follows which contains 3-level nested for loops.
To make it neater and cleaner, we can use the product() function, which is from the itertools module, to optimise the code.
Walrus Operator: An amazing Trick for Assignment ExpressionsSince Python 3.8, there is a new syntax called ‚Äúwalrus operator‚Äù that can assign values to variables as part of a larger expression. The operator := got its cute name from the eyes and tusks of a walrus.
Unfortunately, we cannot directly put the assignment into the print() function. it would throw TypeError if you try it.
Instead make use of the Walrus operator like below‚Ä¶Isn‚Äôt it cool
Using Lambda Functions To Define Simple Functions
If you only want to define a simple function, probably you don‚Äôt need to use the traditional syntax for it. The lambda function is a more elegant option.
For example, the following function is to make a database connection
It works perfectly but the code itself is a bit ugly. Let‚Äôs write a one-liner to implement the same function.
Union Operators ‚Äî Simple way To Merge DictionariesMerging dictionaries is a common requirement in daily Python programming. There are many ways to do it. But all of them were ugly before Python 3.9.
Since Python 3.9, we finally got the most elegant way for dictionary merging ‚Äî using union operators.
F-Strings ‚Äî The Pythonic String Formatting TechniqueAlmost every programming language supports string formatting syntax. But not each one is as elegant as Python‚Äôs f-string technique.
Using Asterisks for Unpacking Iterables and Destructuring Assignments
How to merge a list, a tuple and a set into one list? The most elegant way is using asterisks.
Hope the above few ways to write a much cleaner and effective programming in python, helps all of you‚Ä¶.catch you in the next post.
Thanks for reading, please follow me if you enjoy reading stories like these and want to support me as a writer, consider signing up to become a Medium member. It‚Äôs $5 a month, giving you unlimited access to thousands of Python guides and Data science articles.
Lotome John
in
Towards Data Science
TechlearnEasy
Kuldeep Singh
in
XRPractices
Philip Wilkinson
in
Towards Data Science
Steve
Linda Ramos
Endless Battlefield
Sravan Kumar E
"
165,https://medium.com/@ll.ltsnk/the-secret-of-clear-charts-how-paypal-tiktok-and-airbnb-visualise-their-data-bd0255a52b81,"The Secret of Powerful Charts: How PayPal, TikTok, and Airbnb Visualise Their Data","Making charts that are clear to ourselves is easy. Making charts everyone else understands is hard, time-consuming, and not always feasible. It is better not to visualize the data at all than to make a confusing visual.
Complex charts raise more questions than answers.
Bradley Arsenault
in
Towards Data Science
Social Finance UK
in
Social Finance UK
Sindhu Ravikumar
Karim Ouda
in
Towards Data Science
Obalanatosin
Lars Nielsen
in
MLearning.ai
keerthika ravichandran
Tan Pengshi Alvin
in
Towards Data Science
"
166,https://medium.com/towards-data-science/cte-sql-945e4b461de3,What are CTEs in SQL,"Writing clean, readable and efficient SQL queries is an important aspect of any engineering or analytical process across a team. Such queries be maintained effectively and scale well when the time comes.
One SQL construct that can be easily adopted by both developers and analysts in order to make this happen, is Common Table Expression (CTE).
A Common Table Expression (CTE) is a construct used to temporarily store the result set of a specified query such that it can be referenced by sub-sequent queries. The result of a CTE is not persisted on the disk but instead, its lifespan lasts till the execution of the query (or queries) referencing it.
Users can take advantage of CTEs such that complex queries are split into easier to maintain and read sub-queries. Additionally, Common Table Expressions can be referenced multiple times within a single query which means that you don‚Äôt have to repeat yourself. Given that CTEs are named, it also means that users can make it clear to the reader as what a particular expression is supposed to return as a result.
Every CTE can be constructed using the WITH <cte-name> AS clause
Multiple CTEs can be specified within a single query, each being comma-separated by others. CTEs can also reference other CTEs:
Then subsequent queries can reference CTEs like any table or view:
Normally, you are able to achieve the same results using a sub-query. As the name suggests, a sub-query is a query defined inside another query (aka nested queries).
There‚Äôs a misconception that CTEs tend to perform better than sub-queries, but this is not true. In reality, CTE is a syntactic sugar which means that behind the scenes, a sub-query will still be executed but there are a few things you need to have in mind when it comes to deciding as to whether you are going to write a Common Table Expression or sub-query.
CTEs can be way more readable than nested queries. Instead of having two or more queries inside one another, all you need to do is define a CTE and reference its name in sub-sequent queries.
This means that CTEs can also be reused and referenced multiple times, by subsequent queries. With sub-queries, you would have to re-write the same query over and over again.
Additionally, a CTE can be recursive meaning that it can reference itself. The syntax for recursive CTEs is somewhat different to the one used to specify non-recursive CTEs. You will need to specify it using WITH RECURSIVE and also combine the results from the recursive call and base case (aka the anchor) using a UNION ALL:
I am not going into more details about recursive CTEs at the moment but I am planning write an article just for this in the next few coming days, so make sure to subscribe below and be notified when it‚Äôs out!
Common Table Expressions provide a simple yet powerful way to write clean, readable and maintainable SQL queries. Users can take advantage of such constructs in order to enhance re-usability across queries and in some cases even improve the performance since a CTE ‚Äî a temporary result set ‚Äî can be referenced multiple times.
Whenever possible, CTEs should be preferred over nested joins since the latter will clutter your code base, and if needed more than once will make your code less readable. Additionally, CTEs can also be recursive which is a big win when this is required.
There are use-cases where subqueries can offer more flexibility than CTEs though, so this article doesn‚Äôt aim to convince you that sub-queries are completely useless! For instance, because CTEs must be specified before the SELECT clause, it means they cannot be used in WHERE clauses.
Become a member and read every story on Medium. Your membership fee directly supports me and other writers you read. You‚Äôll also get full access to every story on Medium.
gmyrianthous.medium.com
Related articles you may also like
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
Chanak Karki
Ion Thruster
Divami Design Labs
Wendy Raven McNair
Sandesh hegde
Jeff Davies
Rusben Guzman
Ishan
"
167,https://medium.com/data-engineering-ramstkp/zero-etl-a-new-future-of-data-integration-9ca2ea3f6c2e,Zero ETL a New Future Of Data Integration,"Zero ETL is the future of data Integration, although ETL tools are indispensable in combining data from various sources and enabling data integration across enterprise systems. Data integration is unthinkable without ETL.
‚àò Extract Transform and Load (ETL) ‚àò The transformation from ETL to Zero ‚Äî ETL ‚àò What is Zero ETL ‚àò What does Zero ETL do? ‚àò Where are we with Zero ETL
Extract Transform and Load (ETL) is unforgettable and eminent in data management.
Extracting, transforming, and loading data is a fundamental part of the data management process, which is the first step in any big data project.
In Simple terms, ETL is the process of extracting data from one system, transforming it into a different structure, and loading it into another system. ETL tools are crucial for Enterprise Data Management.
ETL tools are indispensable in combining data from various sources and enabling data integration across enterprise systems.
Data integration is unthinkable without ETL.
However, lately, The data engineering industry has been transformed by digital transformation.
As more companies adopt data technologies in the cloud with open-source and new architectural patterns (data mesh)
With this transformations, ETL tools are on their way out.
Instead, engineers will learn how to process data in the data lakes, lake houses, warehouses, or Spark clusters and Python scripts, and they will rely on low-code platforms for many of their needs.
ETL tools are on their way out because of their inability to work with the data collaboratively. The new generation of tools has made ETL unnecessary.
In contrast to ETL tools, low-code or no-code data engineering tools have emerged in recent years.
This new generation of software offers data engineers a chance to do what they do best ‚Äî programming and machine learning ‚Äî without any coding skills required.
As the new generation of ETL heading towards low-code, no-code platforms, a new future of data integration is emerging in the data management industry
Zero ETL ‚Äî A future of Data Integration
Different data sets reside in various formats, locations, and even formats.
Data integration is a process where data from multiple data sources are combined into a single dataset to be used by analytical tools and applications.
It is a transformational journey from Rigid ETL tools to Data integrations with code (Python, Spark, SQL..etc.) to data integration with low-code / no-code platforms.
ETL ‚Üí Data Integration with Code ‚Üí Data Integration with low/no-code
Easy integration can help you reduce time spent on data management.
What if we think of doing integration differently? Is it an idea to eliminate the ETL completely
It‚Äôs a confounding idea No Integration from easy Integration.
Zero ETL ‚Üí Run Federated Queries on disparate data technologies.
In simple terms, run analytical, BI, and ML workloads on a system that supports federated queries across data sources.
Business data typically move from one location to another through ETL (Extract ‚Äî Transform ‚Äî Load). This process has disadvantages because it takes time, and there are many opportunities for errors.
Therefore, Zero ETL was ideated to eliminate these problems by providing a secure way for data to move between different systems without any manual intervention. It ensures that all data is up-to-date through continuous federation between all connected systems.
There is quite significant progress transpiring with Zero ETL.
Amazon recently announced 2 ETL free integrations
Google announced the Zero-ETL approach with big Query
Databricks announced direct queries on external systems using JDBC
Microsft Lake storage, in combination with Snowflake / Snowpipe, is close to the Zero-ETL approach
Microsoft lake storage with databricks delta live tables
We hope that you will find this article insightful. Please share the link with your friends, family, and colleagues.
Do you like to encourage us to spread the insights on more topics? Please enable us with a cup of coffee.
It just takes a moment to follow us. Let‚Äôs help each other to spread knowledge. Follow us on Medium, Insights and Data, LinkedIn, and Twitter to stay up-to-date with our latest articles.
It costs just 16 Cents per Day to become a referred member in Medium through us (Subscribe), which unlocks the full potential of reading our articles and many more in Medium.
TheStartupFounder.com
Tom Rankin
TheStartupFounder.com
Rishikesh Kale
Palakkgoyal
Ramesh Nelluri, I bring creative solutions to life
in
Insights and Data
Kaan Erdener
in
Algorithm and Blues
TheStartupFounder.com
"
168,https://medium.com/towards-data-science/time-series-analysis-introduction-a-comparison-of-arma-arima-sarima-models-eea5cbf43c73,"Time Series Analysis Introduction ‚Äî A Comparison of ARMA, ARIMA, SARIMA Models","Time series is a unique type of problem in machine learning where the time component plays a critical role in the model predictions. As observations are dependent on adjacent observations, this violates the assumption that observations are independent to each other followed by most conventional machine learning models. Common use cases of time series analysis are forecasting future numeric values, e.g. stock pricing, revenue, temperature, which falls under the category of regression models. However, time series models can also be applied in classification problems, for instance, pattern recognition in brain wave monitoring, or failure identification in the production process are common applications of time series classifiers.
In this article, we will mainly focus on three time series model ‚Äì ARMA, ARIMA, and SARIMA for regression problems where we forecast numeric values. Time series regression differentiates from other regression models, because of its assumption that data correlated over time and the outcomes from previous periods can be used for predicting the outcomes in the subsequent periods.
Firstly we can describe the time series data through a line chart visualization using sns.lineplot. As shown in the image below, the visualization of ‚ÄúElectric Production [1]‚Äù time series data depicts an upward trend with some repetitive patterns.
To explain the characteristics of the time series data better, we can break it down into three components:
They can be combined typically through addition or multiplication:
In Python, we decompose three components from time series data through seasonal_decompose,and decomposition.plot()gives us the visual breakdown of trend, seasonality and residual. In this code snippet, we specify the model to be additive and period = 12 to show the seasonal patterns.
(To access the entire code snippet, please have a look at my website)
Time series data can be classified into stationary and non-stationary. Stationarity is an important property, as some models relies on the assumption that data is stationary. However, time series data often possesses the non-stationary property. Therefore, we need to understand how to identify non-stationary time series and how to transform it through various techniques, e.g. differencing.
Stationary data is defined as not depending on the time component and possesses the following characteristics: constant mean, constant variance overtime and constant autocorrelation structure (i.e. the pattern of autocorrelation does not change over time), without periodic or seasonal component.
The most straightforward method would be examining the data visually. For example, the time series visualization above indicates that the time series follows an upward trend and its mean values increase over time, suggesting that the data is non-stationary. To quantify it stationarity, we can use following two methods.
Firstly, ADF (Augmented Dickey Fuller) test examines stationarity based on the null hypothesis that data is non-stationary and alternative hypothesis that data is stationary. If the p-value generated from the ADF test is smaller than 0.05, it provides stronger evidence to reject that data is non-stationary.
We can use adfuller from statsmodels.tsa.stattools module to perform the ADF test and generates the ADF value and p-value. In this example, p-value 0.29 is more than 0.05 thus this dataset is non-stationary.
Secondly, ACF (Autocorrelation Function) summarizes the two-way correlation between the current observation against past observations. For example, when the lag=1 (x-axis), ACF value (y-axis) is roughly 0.85, meaning that the average correlation between all observations and their previous observation is 0.85. In the later section, we will also discuss using ACF to determine the moving average parameter.
The code snippet below generates ACF plots using sm.graphics.tsa.plot_acf, showing 40 lags.
For non-stationary data, ACF drops to 0 relatively slowly, because non-stationary data may still appear highly correlated with previous observations, indicating that time component still plays an important role. The diagram above shows the ACF of the original time series data, which decreases slowly thus very likely to be non-stationary.
Differencing removes trend and seasonality by computing the differences between an observation and its subsequent observations, differencing can transform some non-stationary data to stationary.
We use shift(1) to shift the original time series data (shown on the left) for one row down (shown on the right) and take the difference to remove the trend components. dropna is to remove the empty row when NaN is subtracted.
We can plot the time series chart as well as the ACF plot after applying trend differencing. As shown below that the trend has been removed from the data and data appear to have constant mean. The next step is to address the seasonal component.
2. remove seasonality
From the ACF plot above, we can see that observations are more correlated when lag is 12, 24, 36 etc, thus it may follow a lag 12 seasonal pattern. Let us apply shift(12) to remove the seasonality and retest the stationarity using ADF ‚Äì which has a p-value of around 2.31e-12.
After removing the seasonal pattern, the time series data below becomes more random and ACF value drops to a stable range quickly.
In this section, we will introduce three different models ‚Äì ARMA, ARIMA and SARIMA for time series forecasting. Generally, the functionalities of these models can be summarized as follow:
ARMA stands for Autoregressive Moving Average. As the name suggests, it is a combination of two parts ‚Äì Autoregressive and Moving Average.
Autoregressive Model ‚Äì AR(p)
Autoregressive model makes predictions based on previously observed values, which can be expressed as AR(p) where p specifies the number of previous data points to look at. As stated below, where X represents observations from previous time points and œÜ represents the weights.
For example, if p = 3, then the current time point is dependent on the values from previous three time points.
How to determine the p values?
PACF (Partial Autocorrelation Function) is typically used for determining p values. For a given observation in a time series Xt, it may be correlated with a lagged observation Xt-3 which is also impacted by its lagged values (e.g. Xt-2, Xt-1 ). PACF visualizes the direct contribution of the past observation to the current observations. For example, the PACF below when lag = 3 the PACF is roughly -0.60, which reflects the impact of lag 3 on the original data point, while the compound factor of lag 1 and lag 2 on lag 3 are not explained in the PACF value. The p values for the AR(p) model is then determined by when the PACF falls within the significant threshold (blue area) for the first time, i.e. p = 4 in this example below.
Moving Average Model ‚Äì MR(q)
Moving average model, MR(q) adjusts the model based on the average predictions errors from previous q observations, which can be stated as below, where e represents the error terms and Œ∏ represents the weights. q value determines the number of error terms to include in the moving average window.
How to determine the q value?
ACF can be used for determining the q value. It is typically selected as the first lagged value of which the ACF drops to nearly 0 for the first time. For example, we would choose q=4 based on the ACF plot below.
To build a ARMA model, we can use ARIMA function (which will be explained in the next section) in statsmodels.tsa.arima.model and specify the hyperparameter ‚Äì order(p, d, q). When the d = 0, it operates as an ARMA model. Here we fit the ARIMA(p=3 and q=4) model to the time series data df‚ÄúIPG2211A2N‚Äù.
Model evaluation becomes particularly important when choosing the appropriate hyperparameters for time series modeling. We are going to introduce three methods to evaluate time series models. To estimate model‚Äôs predictions on unobserved data, I used first 300 records in the original dataset for training and the remaining (from index 300 to 396) for testing.
The first method is to plot the actual time series data and the predictions in the same chart and examine the model performance visually. This sample code firstly generates predictions from index 300 to 396 (same size as df_test) using the ARMA model, then visualizes the actual vs. predicted data. As shown in the chart below, since ARMA model fails to pick up the trend in the time series, the predictions drift away from actual values over time.
2. Root Mean Squared Error (RMSE)
For time series regression, we can apply general regression model evaluation methods such as RMSE or MSE. For more details, please have a look at my article on ‚ÄúTop 4 Linear Regression Variations in Machine Learning‚Äù.
towardsdatascience.com
Larger RMSE indicates more difference between actual and predicted values. We can use the code below to calculate the RMSE for the ARMA model ‚Äì which is around 6.56.
3. Akaike Information Criteria (AIC)
The third method is to use AIC, stated as AIC = 2k ‚Äì 2ln(L), to interpret the model performance, which is calculated based on log likelihood (L) and number of parameters(k). We would like to optimize for a model to have less AIC, which means that:
We can get the AIC value through summary() function, and the summary result below tells us that the ARMA model has AIC = 1547.26.
ARIMA stands for Autoregressive Integrated Moving Average, which extends from ARMA model and incorporates the integrated component (inverse of differencing).
ARIMA builds upon autoregressive model (AR) and moving average model (MA) by introducing degree of differencing components (specified as the parameter d) ‚Äì ARIMA (p, d, q). This is to address when obvious trend observed in the time series data. As demonstrated in the ARMA example, the model didn‚Äôt manage to pick up the trend in the data which makes the predicted values drift away from the actual values.
In the ‚ÄúStationarity and Differencing‚Äù section, we explained how differencing is applied to remove trend. Now let us explore how it makes the forecasts more accurate.
How to determine d value?
Since ARIMA incorporates differencing in its model building process, it does not strictly require the training data to be stationary. To ensure that ARIMA model works well, the appropriate degree of differencing should be selected, so that time series is transformed to stationary data after being de-trended.
We can use ADF test first to determine if the data is already stationary, if the data is stationary, no differencing is required hence d = 0. As mentioned previously, the ADF test before differencing gives us the p-value of 0.29.
After applying trend differencing diff = df[‚ÄòIPG2211A2N‚Äô] ‚Äì df[‚ÄòIPG2211A2N‚Äô].shift(1) and using ADF test , we found that p value is far below 0.05. Therefore, it indicates it is highly likely that transformed time series data is stationary.
However, if the data is still non-stationary, a second degree of differencing might be necessary, which means applying another level of differencing to diff(e.g. diff2 = diff ‚Äì diff.shift(1)).
To build the ARIMA model, we use the same function as mentioned in ARMA model and add the d parameter ‚Äì in this example, d = 1.
From the summary result, we can tell that the log likelihood increases and AIC decreases as compared to ARMA model, indicating that it has better performance.
The visualization also indicates that predicted trend is more aligned with the test data ‚Äì with RMSE decreased to 4.35.
SARIMA: Address Seasonality
SARIMA stands for Seasonal ARIMA which addresses the periodic pattern observed in the time series. Previously we have introduced how to use seasonal differencing to remove seasonal effects. SARIMA incorporates this functionality to predict seasonally changing time series and we can implement it using SARIMAX(p, d, q) x (P, D, Q, s). The first term (p, d, q) represents the order of the ARIMA model and (P, D, Q, s) represents the seasonal components. P, D, Q are the autoregressive, differencing and moving average terms of the seasonal order respectively. s is the number of observations in each period.
How to determine the s value?
ACF plot provides some evidence of the seasonality. As shown below, every 12 lags appears to have a higher correlation (as compared to 6 lags) to the original observation.
We have also previously tested that after shifting the data with 12 lags, no seasonality has been observed in the visualization. Therefore, we specify s=12 in this example.
From the summary result, we can see that AIC further decreases from 1528.48 for ARIMA to 1277.41 for SARIMA.
The predictions now illustrates the seasonal pattern and the RMSE further drops to 4.04.
Thanks for reaching so far, if you‚Äôd like to read more articles from Medium and also support my work, I really appreciate you signing up Medium Membership using this affiliate link.
This introduction of time series models explains ARMA, ARIMA and SARIMA models in a progressive order.
Furthermore, we explore concepts and techniques related to time series data, such as Stationarity, ADF test, ACF/PACF plot and AIC.
towardsdatascience.com
Destin Gong
Destin Gong
[1] Original dataset reference: Board of Governors of the Federal Reserve System (US), Industrial Production: Utilities: Electric and Gas Utilities (NAICS = 2211,2) [IPUTIL], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/IPUTIL, November 17, 2022. [Public Domain: Citation Requested].
Originally published at https://www.visual-design.net on Nov 17th, 2022.
Kie Ichikawa
in
Towards Data Science
Oscar Rojo
Emily Morgan
Shiyu Gong
Renu Khandelwal
in
DataDrivenInvestor
Springboard India
Susan Currie Sivek, Ph.D.
in
Towards Data Science
Kiarash Ahmadi
"
169,https://medium.com/@anna.wu9222/pros-vs-cons-of-leetcode-for-non-software-engineers-1a882e792ce8,Pros vs Cons of LeetCode For Non-Software Engineers,"You can‚Äôt enter the data science, software engineer, data analyst, or BI engineer space without hearing someone mention LeetCode.
It was founded almost eight years ago by an ex-Google SWE and at its roots, it‚Äôs designed to help SWE pass their coding interviews.
Since its inception, it has expanded into pretty much any and every coding language tested in technical interviews, but at its roots, it was designed by and for software engineers.
In today‚Äôs article, I will cover the pros and cons of LeetCode as viewed by someone outside the software engineer space.
Fair warning, I am one of the founders of Big Tech Interviews, a SQL interview prep-based platform so I am biased.
But I‚Äôll do my best to give an objective analysis.
1. LeetCode is one of the top SWE prep platforms. It has thousands of members and questions covering complex topics such as system design, Big-O notation, and data structures. Additionally, it has a very large community if you get stuck on a problem, it‚Äôs likely that someone has already posted the solution online.
2. LeetCode offers weekly/monthly contests. You can not only earn badges and win prizes when you compete in a contest, but also receive interview requests by companies making the interview process even easier.
3. The quality of LeetCode‚Äôs questions is generally very high. They are often asked in real interviews and are a good gauge of your progress. Especially, the ones with ratings that are popular in the community (indicated by the number of upvotes).
4. You can also keep track of your progress with analytics. The calendar feature is a really nice touch that helps you stay motivated and on track with your progress.
1. Limited SQL questions for beginner to senior data scientists, data analysts, and BI engineers. Due to LeetCode primarily focusing on software engineers, it‚Äôs no surprise that only a small minority of their questions are SQL-related. Big Tech Interviews on the other hand was created to help fill this void and enable more candidates to pass their SQL interview.
2. Limited videos and clear step-by-step solutions. It‚Äôs frustrating to get an answer wrong and not know the proper way to solve it. Big Tech Interviews offers over 100+ HD video explanations and clear step-by-step solutions for every question.
3. LeetCode doesn‚Äôt offer peer-to-peer mock interviews compared to Big Tech Interviews. We offer 1:1 peer-to-peer mock interviews to help you practice and fine-tune your SQL skills before your Big Day. LeetCode is a great platform if you‚Äôre already familiar with SQL and coding interviews in general. Big Tech Interviews is the better platform if you‚Äôre starting from scratch or if you want to focus on SQL specifically.
4. Not friendly for beginners. If you‚Äôre just starting out and want to learn the ins and outs of a programming language like SQL, then I wouldn‚Äôt recommend starting with LeetCode. You‚Äôd be better off practicing on a platform like Big Tech Interviews or w3schools since these platforms include step-by-step instructions for beginners.
In Summary,
LeetCode is a great platform for candidates with a background in software development. It boasts a strong and active community with thousands of questions.
But, if you‚Äôre going for a role that requires SQL then it‚Äôs worth considering Big Tech Interviews as the platform to help you land your dream job.
We not only give you access to the most up-to-date and relevant SQL questions with step-by-step video and written solutions but also equip you with peer-to-peer mock SQL interviews and tons of other free resources.
GAT Network
in
GAT Network
Datahunter.ai
Anthony Donovan
KT
Muhammad Hamada
in
HelloTech
Moblize.IT LLC
in
Moblize.IT LLC
James Hoggard
Thanoshan MV
"
170,https://medium.com/gitconnected/a-software-engineer-has-built-his-tiktok-side-hustle-20k-month-c48bc7cf73a0,A Software Engineer Has Built His Tiktok Side Hustle ‚Äî $20k/month,"A side hustle is an extra job you take on in addition to your regular work to increase your income. Compared to a traditional nine-to-five job, side hustles are more likely to focus on your interests or hobbies. By pursuing your passion on the side, you may increase your financial stability without giving up your regular income from your primary job.
Anyone can make some extra money with a side hustle. Depending on your demands and skills, there are countless opportunities to make a good earning during your free time.
Professionals like software engineers possess tons of skills that can earn them additional income. They can generate more income than their primary job with little focus and a good plan.
According to a survey of 2,001 people conducted by marketing automation startup Zapier, one in three Americans has a side hustle.
Matt Upham is a full-time software engineer and makes tech content a side hustle. He has 522,000 followers on TikTok, 113,000 on Instagram, and 56,000 on YouTube.
Upham, who is located in San Francisco, began in 2016 with photography accounts on Instagram. He temporarily experimented with personal financial material before finding popularity with coding and tech content on TikTok during the pandemic.
Now Upham makes content with general tips and news about technology. Also, he advises on how to get tech jobs.
Upham is mainly focused on short-form videos. He uploads those on TikTok, Instagram Reels, and YouTube Shorts.
He believes that video is the next big thing. He also added that photos on Instagram were popular for ten years, but they are losing popularity, and it seems that video is where everyone is moving.
Upham‚Äôs earnings are based on brand deals. Making tech content enabled him to partner with Apple, Intel, and Dell.
He made around $20,000 a month from brand deals at the end of last year. It‚Äôs more than he earns from his full-time software engineer job. He was thinking about making content creation his full-time job.
But brand deals are a bit vulnerable due to current economic uncertainties. So he was planning to make a digital product like a course.
He also generates a good income from Google Adsense, a Google program that shares part of the revenue from ads placed on YouTube videos.
David Dilworth is an American who works full-time as a flight attendant at Delta Airlines. He started a YouTube channel in March 2020 when COVID got real in the USA. As he was in the airline industry, there was a lot of not knowing what might happen next.
So, Dilworth started publishing fitness-based videos. He would also make instructional videos on how to stay fit during the pandemic. Now the channel generates $3,000 a month and has a variety of revenue systems. It‚Äôs such a smart passive income for him. Even while he flies in the air for his job, it makes him money.
What if I tell you that, like Dilworth and Upham, you could also earn extra money by side hustling? Yes, it‚Äôs very much possible to do outside your full-time job, but finding a side gig may not be as easy as it sounds.
You‚Äôll need to focus on your key skills and a master plan to get the desired success.
The following instructions will show you how to launch your side hustle successfully and provide you with advice on improving your chances of success.
Time is your most important resource. To get the most out of it, you must consider how to distribute and manage it effectively. Determine how much time you spend on your primary job and how much time is left over for leisure.
This assessment will help you decide how to allocate part of your free time for your side hustle. Set a schedule, and don‚Äôt procrastinate once you‚Äôve determined how much additional time you have. The success rate of both your primary career and your side hustle will rise due to good time management.
Identifying your skills is the main point. Make a list of skills you have. Take a talent test and seek outside advice. Decide which skills are most important for your side hustle.
One cannot overstate the importance of having a business plan. It directs your thoughts along a successful course and acts as a guide for where you want your business to go. Establish attainable objectives by analyzing SWOT, determining your target market, and investing in market research.
A side hustle is not a way to become rich overnight. It‚Äôs crucial to pick one that will grow over time. The potential for expansion of gigs follows proper procedures and requires nurturing like that of traditional businesses. You should be prepared to learn new skills, undergo failures, and give your side hustle the time and money required to succeed.
Thanks for reading! And, of course, share your opinion with us. If you want to support me, please join me!
Obisakin Ayomiposi
Richard Appiah
in
The Authors
Ashswan
Veer Singh
Pia Vermaak
Richard Appiah
in
SYNERGY
Richard Appiah
in
ILLUMINATION
Sarah Juggins
"
171,https://medium.com/towards-data-science/program-christmas-lights-with-python-2a6bfa243d71,Program Christmas Lights with Python,"The holiday season has officially kicked off, which means plenty of shopping, baking, and of course decorating. Home improvement stores stock dozens and dozens of lights all with different shapes, colors, and special effects. But have you ever wanted to create your own holiday lights with custom colors and motions? A few simple lines of Python make that possible.
This is my first Raspberry Pi project and I‚Äôm thrilled to share it with you before the holidays. I used a Raspberry Pi to control individually addressable LED strips. Unlike typical holiday lights, these strips allow custom inputs for each LED. Links to the materials are listed below.
WS2812B light emitting diodes (LED) are individually addressable LEDs, making it possible to program individual LEDs. The strip has 3 cables running to each LED:
This brand of LED strip forked the power lines (red and black cables) to make it easier to connect to a separate power supply. Even though there are 5 cables at the end, there are only 3 paths.
The other end of the strip has a male connector which allows other LEDs strips to be added in series. However, adding more strips can cause overheating if there isn‚Äôt sufficient power.
Before starting, make sure both the Raspberry Pi and the external power supply are disconnected.
The following instructions assume the Raspberry Pi has the Raspian OS installed. Run the code on the Raspberry Pi terminal.
I recommend using a headless Raspberry Pi setup which will allows you to remotely access the Raspberry Pi. However, it is not necessary.
2. Install git on Raspberry Pi
3. Deactivate audio by editing the following file
4. Add the following line to the open file. Press Ctrl-O and Ctrl-X to save and close.
5. Use the nano editor to open the config file.
6. Use Ctrl-W to search the file. Add a # to comment out the following line.
7. Restart the Raspberry Pi to ensure all changes take effect.
2. Edit the input parameters in the test file.
My setup includes two strips of 150 LEDs for a total of 300 LEDs. LED_COUNT is the only variable I need to change.
Use Ctrl-O and Ctrl-X to save and close the file.
3. Test the setup!
To customize my lights, I copied strandtest.py from the rpi-ws281x-python library to create xmas.py.
This allowed me to modify existing functions and create new ones without overwriting strandtest.py. In xmas.py, I added a feature where all the lights turn off for keyboard interruptions (Ctrl-C) or if the script runs to completion. When run to completion, the script takes a little under 4 hours.
I wanted my lights to turn on every night without user input so I scheduled a task with Cron. Cron comes with the Raspberry Pi Raspian operating system.
Choose an editor. I recommend nano.
2. Add the task to the file.
Cron jobs follow the format [minute] [hour] [day of month] [month] [day of week] [command to execute]. The * means any value so Cron will execute the command for all the values in that field. The job above will run at 5 pm (aka 17:00) every day of every month.
‚ÄúConnect and Control WS2812 RGB LED Strips via Raspberry Pi.‚Äù Tutorials for Raspberry Pi. https://tutorials-raspberrypi.com/connect-control-raspberry-pi-ws2812-rgb-led-strips/
‚ÄúBeginners Guide to Cron Jobs and Crontab.‚Äù PiMyLifeUp. https://pimylifeup.com/cron-jobs-and-crontab/
Thank you for reading about my first Raspberry Pi project. If you have any questions, please reach out or leave a comment. I‚Äôm more than happy to help. Also, I‚Äôd love to hear what patterns you‚Äôd like to see for future LED projects.
Thank you for reading my article. If you like my content, follow me or join Medium.
Connect with me on LinkedIn, Twitter, or Instagram.
All feedback is welcome. I am always eager to learn new or better ways of doing things.
Feel free to leave a comment or reach out to me at katyhagerty19@gmail.com.
Nate
Jainharsh
in
Python in Plain English
upGrad
Quabbly
SQLInSix Minutes
Cory Maklin
in
Towards Data Science
Yash Deshmukh
Antoine Beretto
in
Coinmonks
"
172,https://medium.com/towards-data-science/multiple-logistic-regression-in-r-aa65c2a91e22,Multiple Logistic Regression for Dichotomous Variables in R,"Introduction
Simple logistic regression incorporates only one predictor variable and we have implemented it using R previously. We have also discussed on the statistics for goodness-of-fit. Oftentimes, the real world data has several predictor variables. Sometimes we simply don‚Äôt know if including the extra variable as a predictor will make the model more robust or not. So we need to always compare the full model with the nested model and draw conclusion from the data. Here, we will increase the number of independent parameters and expand our study on multiple logistic regression.
Dataset
The data from the 2016 General Social Survey (GSS) will be used to demonstrate logistic regression. Data was downloaded from the Association of Religion Data Archives and collected by Tom W. Smith. This dataset includes responses collected from nearly 3,000 respondents and contains data related to several socioeconomic factors. It contains various types of data, such as information pertaining to a person‚Äôs marital status, the status of their education, their working hours, their employment status, etc. Let‚Äôs take a closer look at this dataset to gain a better understanding of it.
For each individual, the DEGREE column provides the level of education, while the MADEG column provides the level of education for each individual mother. In this study, we aim to determine whether the mother‚Äôs bachelor‚Äôs degree level is a good predictor of the children‚Äôs bachelor‚Äôs degree level. This dataset contains categorical data that is encoded ordinally.
Answer we are trying to find
In the previous multiple logistic regression article (link below),
towardsdatascience.com
we tried to answer the following question.
Does the education level of mother impact the education level of the children?
Our analysis of the data revealed a positive coefficient for MADEG, which represents the mother‚Äôs educational level. As can be seen in the interpretation of result section of the following figure, the intercept estimate is 0.257 and the MADEG coefficient estimate is 0.316. Accordingly, for every one unit increase in the predictor variable, which is mother‚Äôs education level, the logit probability that a child‚Äôs education level will have a value of 1 will increase by 0.31598. However, this still represents a positive slope, indicating an increase in the response variable with an increase in the predictor variable. Thus, the probability that the child will receive a bachelor degree is increased if his or her mother has a bachelor degree.
Now, we will pose a new question.
Does the education level of father have anything to do with the children‚Äôs education or is it uncorrelated?
When we incorporate another independent variable, the simple logistic regression become multiple logistic regression.
Implementation in R
To execute this regression study in R, we will need the following libraries to be installed. The data is stored in an excel file and we will use the glm() function. The difference now is the addition of PADEG which represents the education level of father.
Interpretation of result
As a first step in the process of implementing logistic regression, we need to convert the probability of output success into logarithmic measures, in order to determine the coefficient and intercept of the predictor variable. I have provided a brief interpretation of the data below. In addition, I have placed the output windows of a simple logistic regression in the same place as well for the sake of comparison.
Log-likelihood ratio = Deviance of the reduced model ‚Äî Deviance of the full model
Conclusion
We have discussed about multiple logistic regression and its implementation in R. We have also walked though the R outputs and interpret the results from General Society Survey. The positive coefficient for the predictor variables indicate that with the increase of mother‚Äôs and father‚Äôs bachelor degree‚Äôs value from 0 to 1, the probability of the child‚Äôs bachelor degree becoming 1 increases by 0.135 and 0.375 respectively or in other words it can be concluded that mother‚Äôs and father‚Äôs education significantly impact the child‚Äôs education in our dataset.
Thanks for reading.
mdsohel-mahmood.medium.com
mdsohel-mahmood.medium.com
Sean Sun
Elise Devaux
in
Statice
Akhil Khatri
Yair Schiff
in
IBM Data Science in Practice
Yannick Saillet
in
The Startup
Kate Valdes
in
Code for San Jose
Nwamaka Imasogie
in
Nwamaka Imasogie‚Äôs Machine Learning and Artificial Intelligence Projects
John Hurley
in
Towards Data Science
"
173,https://medium.com/@Emma_Donery/write-and-get-paid-100-900-for-your-data-science-ai-articles-5d97e1ba0ac4,Write and Get Paid $100‚Äì$900 for Your Data Science/AI Articles ü§ë,"Heads up! I caught a dollar üí∞ü§ë, and you can too. Get paid well to write about your passions while educating others.
Data science is high in demand and explains how digital data is transforming businesses and helping them make sharper and more critical decisions. Here are some sites you can write for as you earn. I have written for some and the pay is really great. Try your luck too and earn some dollars as you write what you love üçÄ.
Arctype is the fast and easy-to-use database client for writing queries, building dashboards, and sharing data with your team.
There are five categories that this blog focuses on:
To contribute or write for archtype, check out this link here to learn about their writing process
Airbyte is an open-source data integration engine that helps you consolidate your data in your data warehouses, lakes, and databases.
This platform is looking for data engineers, analysts, and backend developers to share their knowledge through blog articles that they publish under their data insights or tutorials sections.
Payment
The good thing is you can cross-post your article on your medium page or personal blog
However, they only allow articles written in the English language only.
Check more about this opportunity here
ContentLab is a technical content marketing agency focused on delivering high-quality articles, tutorials, courses, and marketing materials to the tech industry.
ContentLab works in these spaces:
To become a technical writer or subject matter expert for ContentLab, Check this üîólink
Draft.dev helps startups create technical content designed to reach software engineers.
Their services include
Draft.dev creates content that will be read by a wide range of readers around the world hence encourages everyone at all experience levels to apply.
To get started, fill out their form o their site. üîóLink
Neptune.ai is a lightweight tool for experiment management and collaboration in Data Science projects. It is an organized place for all your experiments, data exploration Notebooks, and more. It supports any kind of project workflow
Comprise of sharing your knowledge with the ML community?
They‚Äôre searching for:
To write articles for Neptune.ai, check this üîólink out to learn more.
Rates vary between $300‚Äì600 per article
Paperspace is a high-performance cloud computing and ML development platform for building, training and deploying machine learning models.
Paperspace is concerned with articles around these three areas:
Check out this üîólink on how to get started and some of the topics that you can write about.
Nanonets is a Machine Learning platform that allows users to capture data from invoices, receipts, and other documents without any template setup.
Nanonets pays you to write your favorite Machine Learning Topics.
Suitable for machine learning and computer vision enthusiasts who love to learn and document what they learn.
To write for Nanotets, follow this üîólink
Thebotforge is the leading UK-based Chatbot agency specializing in conversational AI.
You can become a guest writer on their technical blog about conversational AI and related technologies
Check more on how to get started writing for their platform here. üîóLink
Hitsubscribe is a content marketing firm that leverages actual software engineers to write content for your audience of software engineers.
To write for Hitsubscribe, check more on the üîólink
Major Topics include:
Stack Abuse is a resource for software developers of any level to learn new technologies in web development and machine learning.
Topics
To write for Stack Abuse, check out this üîólink for writing guidelines and the writing process.
365 DataScience is an educational career platform that offers various certificates & courses in data science disciplines
If you want to write an article for 365 DataScience, send the following at team@365datascience.com:
Check out more outlines and requirements on what is required of you to get started here üîóLink
ProjectPro is an awesome platform that helps one learn much hands-on industrial experience with a step-by-step walkthrough of projects
Check out this üîólink for Content Submission Guidelines and how to apply.
If you ever wait for perfection, you'll never write a word, so start now. Wish you the best of luck üçÄ. Remember, you do not need to be an expert to try out something new. I hope this article helps you. Please leave a comment or feedback.
Keep Writing ‚úçÔ∏è
Thanks for reading! If you like the article make sure to clap (up to 50!) and connect with me on social media and follow me on Medium to stay updated with my new articles.
üê§Twitter | üëîLinkedIn | üéâInstagram | üíªGithub
üëã Like my work? Thank You. You can buy me a coffee
‚òïBuy me a coffee
Jon Walsh
in
First Draft
Hannah Mullins
Frank Andrade
in
The Startup
Deepa Vasanthkumar
Frank Andrade
in
DataDrivenInvestor
Hema Anusha
Georgia Iacovou
in
Incognito by Metomic
Ami Shmueli
"
174,https://medium.com/@jorritwillaert98/how-we-managed-to-win-the-best-technical-solution-price-of-the-data-4-good-challenge-b84e49e6ba06,How we Managed to Win the ‚ÄòBest Technical Solution‚Äô Price of the Data 4 Good Challenge,"Last Thursday, I participated in the Data 4 Good Challenge of Emergent in Leuven, along with three teammates. Hasse and Ennio are Computer Science students as well, while Sven is pursuing a Masters in Information Systems Engineering. The Data 4 Good Challenge is a data-based hackathon, surrounded by talks and company pitches. From the 40 teams participating, we managed to get the ‚ÄòBest technical solution‚Äô price, of which we‚Äôre all very proud üòä.
Rishabh Khot
RMDS Lab
in
RMDS Lab
Nur Shlapobersky
in
Towards Data Science
Thomas Tung
Matthew Marston
Codemotion
Aqsa Kausar
in
Red Buffer
The Center for Effective Global Action
in
CEGA
"
175,https://medium.com/towards-data-science/how-to-do-an-eda-for-time-series-cbb92b3b1913,How to Do an EDA for Time-Series,"One of the early steps in the data science development cycle is to understand and explore the data for the problem you‚Äôre solving. EDA is a crucial step for a better data science workflow, and Pandas profiling have been my preferred choice to have in done quickly and with a single line of code, while providing me the outputs to better understand the data and uncover meaningful insights.
You probably have been using Pandas Profiling for the structured tabular data, which is commonly the first type of data that we learn to explore, we all now the Iris dataset right? However, in real-world applications, theres another type of data structure that we can commonly find in our day to day: from traffic, to our daily trajectories or even our electricity and water consumption, all of them have one thing in common ‚Äî temporal dependency.
Time-series or sequential data have become one of the most valuable commodities in a world that is more and more data-driven, which makes the need to perform EDA and mine time-series data a much needed skill for data science practitioners.
Due to the nature of time series data, and when exploring the dataset, the type of analysis it is different from when the dataset records are considered to be all independent. The complexity of the analysis grow with the addition of more than one entity within the same dataset.
In this blogpost I‚Äôll be exploring some key steps in the analysis of a dataset, and while leveraging the time-series features of pandas-profiling. The dataset explored refers to the Air Quality in the USA and can be download from EPA website.*
The full code and examples can be found in the GitHub repository so you can follow along the tutorial.
The data description says it‚Äôs the air quality data collected at outdoor monitors across the United States, Puerto Rico, and the U. S. Virgin Islands. With that information, we understand this is a multivariate time-series data that has several entities that we will need to take into consideration.
Knowing this, I have some follow-up questions: How many are the locations available in what concerns the pollutants measures? Do all the sensors collect the same amount data throughout the same timespan? How are the collected measures distributed in time and location?
Some of theses questions can be easily answered with an heatmap comparing all the measurements and locations against time, as depicted by the code snippet and image below:
The diagram above showcases the data points for each entity over time. We see not all stations have started collecting data at the same time, and based on the intensity of the heatmap, we can realize some stations have more data points than others for a given time period.
This means when modeling the time series, having dynamic timestamps for the training and test datasets might be better than having pre-determined timestamps. We also will have to further investigate the missing records and the scope for imputing records.
With that basic understanding of what our entities time distribution looks like, we can start deep-dive into the data profiling for more insights. Since there are multiple time series, let‚Äôs have a look into each entity behavior.
If you were using pandas-profiling already, you probably know how to generate the profile report.
The support for time series can be enabled by passing the parameter tsmode=true, and the library will automatically identify the presence of features with autocorrelation (more on this later). For the analysis work properly, the dataframe needs to be sorted by entity columns and time, otherwise you can always leverage the sortby parameter.
The code for this is as simple as:
Here‚Äôs how the output report would look like using the time-series mode:
Specific to time-series analysis, we can spot 2 new warnings ‚Äî NON_STATIONARY and SEASONAL. The easiest why to have a quick grasps on your time-series is by having a look into the warnings section. For this particular use case, each profile report will depict the particular behavior of each USA location in what concerns pollutants measurements.
Here‚Äôs how the warnings look in our report:
A time series is said to be stationary when its statistical properties (such as mean and variance) do not change over the time at which the series is observed. Conversely, a time series is non-stationary when its statistical properties depend on time. For instance, time series with trends and seasonality (more on this later) is not stationary ‚Äî these phenomena can affect the value of the time series at different times.
Stationary processes are comparatively easier to analyze as there is a static relationship between the time and the variables. In fact, stationary has become a common assumption for most time-series analysis.
While there are models for non-stationary time series, Most ML algorithms do expect a static relationship between the input features and the output. When the time-series is not stationary, a model‚Äôs accuracy modeled from the data will vary at different points. This means the modeling choices are affected by the stationary/non-stationary nature of the time-series, and different data-preparation steps apply when you want to convert the time-series into a stationary one.
So this alert will help you identify such columns and pre-process the time series accordingly.
Seasonality in time series is a scenario in which the data experiences regular and predictable changes that recur over a defined cycle. This seasonality may obscure the signal that we wish to model when time-series modeling, and even worse, it may provide a strong signal to the models. This alert can help you identify such columns and alert you to fix the seasonality.
More information on the time-dependent features
The first difference you will notice is that the line plot will replace the histogram for the column that was identified as time-dependent. Using the line plot, we can better understand the trajectory and the nature of the selected column. For this NO2 mean line plot, we see a downward trend in the trajectory, with continuous seasonal variations, with the maximum value recorded in the initial stages of the series.
Next, when we toggle for more details of the column (as shown in the figure above), we‚Äôll see a new tab with autocorrelation and partial-autocorrelation plots.
For time series, autocorrelations show how the relationship of a time series at its present value relates to its previous values. Partial autocorrelation is the autocorrelation of a time series after removing the effect of previous time lags. Which means these plots are crucial to provide information regarding the autocorrelation degrees of the series under analysis, as well as the moving average degree.
The above ACF and PACF plots are a bit ambiguous as expected. Looking throughout our warnings, we can see that NO2 mean is a non-stationary time variable, which removes the interpretability of these plots. Nevertheless, the ACF plot is useful to confirm us what we already suspected ‚Äî NO2 mean is non-stationary ‚Äî as the ACF plot values decrease very slowly instead of dropping quickly to zero as expected for the case of stationary series.
The information gathered from the data profiling, the nature of time-series, and the alerts such as non-stationary and seasonality give you a head start in understanding the time-series data you have at your hand. This doesn‚Äôt mean you‚Äôre done with the exploratory data analysis ‚Äî the goal is to use these insights as a starting point and work on further in-depth data analysis and further data preparation steps.
From profiling the air quality dataset, we see several columns which are constant, which may not add much value when modeled. From the missing values chart, we see SO2 and CO2 air quality indexes have missing data ‚Äî we should further explore the impact of this and the scope for imputation or dropping these columns altogether. Several columns were found with non-stationary and seasonality alerts, the next steps would be either to make them stationary or ensure the models we‚Äôll be using can handle the non-stationary data points.
You get the idea ‚Äî as data scientists, it‚Äôs important to use profiling tools to quickly grab an overall view of the data (in our case time series), and further inspect and take informed decisions on the data pre-processing and modeling stages.
The motto of Pandas Profiling has always been the same: ‚ÄúRead the data? Pause. Generate the Pandas Profiling report, and inspect the data. Now start cleaning and re-iterate on exploring the data.‚Äù
Though structured tabular data remains the most common data when giving the first steps data science, time-series data is widely used and core for the development of many business and advanced data-driven solutions. Due to the nature of the time series and how records depend on time and influence future occurrences ‚Äî different kinds of insights are sought out by the data scientists during the exploratory data analysis phase.
Thus, it was a matter of time before the Pandas Profiling library incorporated features to enable a time-series analysis mode to uncover these insights. From the changes required from the user to obtain the time-series-specific profiling report ‚Äî to the output of new alerts that prompt concerns in the data, line plots and correlation graphs that are specific to time-series analysis ‚Äî we demonstrated everything in this article.
But the metrics and analysis explored today is only the beginning! More questions are to be answered. And for you, what is your usual approach while analysis time-series data? What do you miss the most when working with sequential datasets?
Citations:
‚ÄúEPA United States Environmental Protection Agency‚Äù (Public domain) http://www.ics.uci.edu/~mlearn/MLRepository.html
Fabiana Clemente is CDO at YData.
Accelerating AI with improved data.
Gabriela Moreira Mafra
in
Magrathea
Darlington Ahiale Akogo
Omar M. Hussein
DF Platform
Simon Jackson
in
Towards Data Science
Daniel Stofan
in
GoodVision
Ilya Hopkins
in
DataDrivenInvestor
Santhosh Gandhi
"
176,https://medium.com/gitconnected/cartoonize-yourself-using-python-and-ai-24fdd4913206,Cartoonize yourself using Python and AI,"Have you ever wondered what would you look like as a Disney Character? Let‚Äôs find out!
Artificial Intelligence (AI) has been able to do a lot of things lately, This field is developing at a rapid pace. It is able to detect objects in images and videos, detect actions, summarize articles, write articles and lately generate images and videos.
But can it fulfill your dream of what you would look like in a Disney movie?Can it convert your images to a Disney princess or a prince?Let‚Äôs Find Out!
We will be using the magic of GANs, using a model called VToonify (which can cartoonize videos as well).
The repository link is https://github.com/williamyang1991/VToonify.
Let‚Äôs use the open in Colab button to run the model in Colab. After clicking the button you should see something like this. A notebook called inference_playground.ipynb should open in colab.
Run the first cell which installs gradio and huggingface_hub library. This
Run the second and third cell which setup CUDA and clone the Vtoonify library from github. The fourth cell will cd into the ‚ÄòDEMO_DIR‚Äô where the repository is cloned.
The next cell imports gradio and model from vtoonify_model. It also sets limits for GPU and CPU. Running this cell will setup gradio and show one example
Run the next two cells and you should see something like this. This is the Gradio UI inside the colab notebook.
This is the gradio UI with the model loaded. You can upload videos and images and play around with the model. Select the model and upload the image you want to cartoonize.
You should see something like this when the model outputs. This is the result of the image and model we used as an example.
The Vtoonify model has multiple models which support comics and illustrations. You can select it when loading the model.
Here are some other outputs of the model with parameter tuning. You can tune the parameters and try it yourself.
If you do not want the trouble of running the google colab notebook, you can run the model directly using the following link https://huggingface.co/spaces/PKUWilliamYang/VToonifyIt will look something like this.(NO Code Involved)
So here you have it an AI model to cartoonize your images.
Are you passionate about artificial intelligence and machine learning? Do you want to stay up-to-date on the latest advancements and insights in the field?
Then subscribing to Medium is a must. For just $5 a month, you‚Äôll have access to a wealth of knowledge and insights from experts in the field.
Plus, your subscription helps support independent writers and researchers like myself. And with the rapid pace of change in the world of AI, staying informed is crucial.Don‚Äôt miss out ‚Äî join Medium today and become a part of the AI revolution. It‚Äôs the best decision you‚Äôll ever make.Subscribe to Medium today using this link.
levelup.gitconnected.com
levelup.gitconnected.com
Praveen Vallepu
Troverse
in
Troverse
Anderson Rolf
Elizabeth Estefan
Joshua Chipile
in
Developer Circle Lusaka
sanuka rapa
Shivani Singh
in
DevOps DeepDive
Arun Kumar Peddapalli
in
Fournine Cloud
"
177,https://medium.com/gitconnected/10-golden-heuristics-for-solving-a-coding-question-in-an-interview-c39c3b75aaf,10 Golden Heuristics for Solving a Coding Question in an Interview,"How to prepare smartly for coding interviews?
In recent years, the process of preparing for coding interviews has become more challenging. With access to huge sets of difficult coding problems, and an increasingly competitive interview process, it is no longer enough to simply brush up on key data structures and work through a few practice questions.
In this post, I would like to share the strategy that I use to prepare for coding interviews.
I have been working as a software engineer for around 15 years, during which time I have changed jobs five times. I have been through around 30 interview loops, comprising over 120 interviews, and have also had the opportunity to sit on the other side of the table and take over 200 coding interviews and 100 system design interviews.
Even though I consider myself to be a reasonably smart engineer, I have found it challenging to solve coding problems on a whiteboard, particularly in an interview setting where I am being evaluated. To overcome this, I have devoted a significant amount of time to preparation and practice. I follow a systematic approach, working through 12‚Äì15 questions for two hours each day. This allows me to solve over 350 problems within a month. Using this routine, I have been successful in my interviews with the FAANG companies (Facebook, Apple, Amazon, Netflix, Google).
How was I able to practice 12+ coding questions every day with a full-time job?
Rather than just solving coding problems, I practice mapping new problems onto ones I have already solved. I will read a problem and spend a few minutes trying to find a similar problem that I am already familiar with. If I can find a match, I will focus on the unique constraints of the new problem and how they differ from the original. If the problem is completely new to me, I will try to solve it and also do some research to see how other people have tackled it. Over time, I have built up a collection of problem patterns that help me quickly map a new problem onto one that I am already familiar with. Here are some examples of these patterns:
Following this pattern-based approach helped me save a lot of preparation time. Once you‚Äôre familiar with a pattern, you‚Äôll be able to solve dozens of problems with it. Furthermore, this strategy made me confident to tackle unknown problems, as I‚Äôve been practicing mapping unknown problems to known ones.
In the remaining post, I will share all the patterns I‚Äôve collected and present sample problems for a few. For a detailed discussion of these and related problems with solutions, take a look at Grokking the Coding Interview.
Problem Statement: Given an array of points in a 2D plane, find K closest points to the origin.
The Euclidean distance of a point P(x,y) from the origin can be calculated through the following formula:
We can use a Max Heap to find K points closest to the origin. We can start by pushing K points in the heap. While iterating through the remaining points, if a point (say P) is closer to the origin than the top point of the max-heap, we will remove that top point from the heap and add P to always keep the closest points in the heap.
Here is what our algorithm will look like:
Problem Statement: Given a set with distinct elements, find all of its distinct subsets.
We can use the Breadth-First Search (BFS) approach to generate all subsets of the given set. We can start with an empty set, iterate through all numbers one-by-one, and add them to existing sets to create new subsets.
Let‚Äôs take the aforementioned example to go through each step of our algorithm:
Given set: [1, 5, 3]
Here is the visual representation of the above steps:
Here is what our algorithm will look like:
Following these patterns helped me save time for my coding interview prep. Please look at Grokking the Coding Interview to find more of such patterns and their sample problems.
Take a look at Grokking the System Design Interview and Grokking the Advanced System Design Interview for some good examples of system design question and their answers.
Thanks for being a part of our community! Before you go:
üöÄüëâ Join the Level Up talent collective and find an amazing job
LitExtension
Waarmen
Nicolas Barlatier
in
Dev Genius
YooShi
Emanuel Slany
Christopher Usick
in
Dev Genius
Jonnie Hallman
in
The Cushion Journal
A. Yigit Ogun
"
178,https://medium.com/towards-data-science/25-a-b-testing-concepts-interview-cheat-sheet-c998a501f911,25 A/B Testing Concepts You Must Know: Interview Refresher,"Questions on A/B testing are being increasingly asked in interviews but reliable resources to prepare for these are still far and few. Let‚Äôs say you completed a great course on A/B testing sometime back and felt confident in your understanding of A/B testing statistics. It‚Äôs been a while now, you have an interview coming up and for the love of god, you can‚Äôt seem to remember what statistical power means or what SUTVA stands for. OR you have been using experimentation in your current role and have automated most processes. Without the need for manual steps, you have become rusty and need a quick cheat sheet to recall important concepts and intuition behind them so that you can ace an upcoming interview.
Use this post as a quick resource for the most important concepts in A/B testing you need to know before interviews. You will find a summary of the most important concepts along with examples to build your intuition.
Before we begin, let‚Äôs establish an example of an A/B test we will utilize as we go over the concepts. Lets say you are running an A/B test on a web page with the goal to improve click-through rates (CTRs) ‚Äî your original web page, which is the control offers a $25 saving using the text ‚ÄòSave $25‚Äô; through this A/B test you are going to test a variation of this web page that presents the same offer (value is still $25) in a different way using the text ‚ÄòSave 15%‚Äô.
Now onto the concepts‚Ä¶
5. Null Hypothesis ‚Äî Inferential statistics is based on the premise that you cannot prove something to be true but you can disprove something by finding an exception. You decide what you are trying to provide evidence for ‚Äî which is the alternate hypothesis, then you set up the opposite as the null hypothesis and find evidence to disprove that. In our A/B test example, the null hypothesis is that the population CTR on the original page and the page variation are not different.
6. Key Metric/s ‚Äî A set of metrics that you are trying to optimize through the experiment. Some commonly used metrics are click through rates (CTR), sign up rate, engagement rate, average revenue per order, retention rates etc. As you can imagine key metrics are going to relate to the priorities of the business, OKRs and goals. Many organizations examine multiple key metrics, and have a mental model of the trade offs they are willing to accept when they see a particular combination. For e.g., they may have a good idea about how much they are willing to lose users (increase in churn) if the remaining users increase their engagement and revenue. This brings us to OEC explained below.
7. Overall Evaluation Criteria (OEC) ‚Äî When there are multiple metrics to be optimized through the experiment, it is helpful to formulate trade-offs by devising a single metric called an Overall Evaluation Criteria (OEC) ‚Äî which is essentially a weighted combination of such objectives. One way to do this is to normalize each metric to a predefined range, say 0‚Äì1, and assign each a weight. Your OEC then is the weighted some of the normalized metrics. In the example above with needing to evaluate trade-off between churn and revenue, LTV can be used as an OEC.
8. Guardrail Metrics ‚Äî These are metrics that are important for the company and should not be negatively impacted by the experiment. For e.g. our goal may be to get as many users as possible to register, but we don‚Äôt want the per-user engagement level to drop drastically. Or we may want to increase app engagement but at the same time ensure that app uninstalls do not increase.
9. Randomization Unit ‚Äî This is the unit e.g. users or pages that a randomization process is applied to map them to either control or treatment. Proper randomization is important to ensure that populations assigned to the different variants are similar statistically. Randomization unit should be chosen such that Stable unit treatment value assumptions (SUTVA) are satisfied. SUTVA states that experiment units do not interfere with one another i.e. the behavior of units in test and control is independent of each other. User-level randomization is the most common as it avoids inconsistent experience for the user and allows for long-term measurement such as user retention.
10. Interference ‚Äî Sometimes also called spillover or leakage occurs when the behavior of the control group is influenced by the treatment given to the test group. This leads to a violation of SUTVA assumption which results in potentially incorrect conclusions. There are two ways inference may arise ‚Äî
11. Confidence Level ‚Äî Confidence level refers to the percentage or probability or certainty, that the confidence interval would contain the true population parameter when you draw a random sample many times. In the tech world of online A/B Testing, a 95% confidence level is chosen most often but you can choose different levels depending on the situation. A 95% confidence level means that the confidence interval around sample mean is expected to include the true mean value 95% of the time.
12. Margin of error ‚Äî As we noted earlier, due to sampling variability, it is possible that the conclusions your draw about the population based on samples is inaccurate. A margin of error tells you how many percentage points your results will differ from the real population value. For example, a 95% confidence interval with a 4 percent margin of error means that your statistic will be within 4 percentage points of the real population value 95% of the time. The margin of error is added to and subtracted from the mean to determine the confidence interval (discussed below).
13. Confidence Interval ‚Äî In statistical inference, we aim to estimate population parameters using observed sample data. A confidence interval gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a given set of sample data. The width of confidence interval depends on 3 things ‚Äî the variation within the population of interest, the size of the sample and the confidence level we are seeking.
14. Type 1 Error ‚Äî A type I error occurs when we incorrectly reject the null hypothesis. In our A/B test example, a type I error would occur if we concluded that population mean of treatment is different from population mean of control when in reality they were the same. Type I error is avoided by achieving statistically significant results.
15. Type 2 Error ‚Äî A type II error occurs when the null hypothesis is false, but we incorrectly fail to reject it. In our A/B test example, a type II error would occur if we concluded that the population mean of Variation B is not different than the mean of Variation A when it actually was different. These errors are avoided by running tests with a high statistical power.
16. P-value ‚Äî p-value is the probability of obtaining at least as extreme results as we are seeing if the null hypothesis of the test is true. p-value basically tells you whether your evidence makes your null hypothesis look ridiculous.
17. Statistical Significance ‚Äî Statistical significance is attained when the p-value is less than the significance level. The significance level (ùõÇ), is the threshold you want to use for the probability of making Type 1 error i.e. concluding that population mean of Control and Treatment are different when in fact they are the same. In other words, statistical significance is another way of saying that the p-value of a statistical test is small enough to reject the null hypothesis. The scientific standard is to use a p-value < 0.05 i.e. ùõÇ = 5%.
18. Statistical Power ‚Äî Statistical Power, which as we know is the probability that a test correctly rejects the null hypothesis i.e. the percentage of time the minimal effect will be detected, if it exists.
19. Minimum Detectable Effect (MDE) ‚Äî is the smallest change in conversion rate you are interested in detecting. In the example where you are optimizing for CTR, let‚Äôs say the CTR for the control is 20%. And the smallest change you would like to detect at the minimum is 5% absolute lift relative to control i.e. if the CTR for the treatment is 25% or more. In this case 5% is the MDE.
20. Practically significant ‚Äî It‚Äôs possible for hypothesis tests to produce results that are statistically significant, despite having a small effect size. This usually happens due to 2 reasons ‚Äî 1) Low sampling variance and 2) Large sample size. In both these cases, we may be able to detect even small differences between test and control with statistical significance. However, these may not be significant in the real world. Let‚Äôs take an example of an A/B test which shows a new module to users and is able to detect a difference of .05% in the CTR ‚Äî however, is that cost of building that module justified for such a small lift. What is the practically significant lift that would be feasible in this case.
21. Sample size ‚Äî The number of units per variation needed to reach statistical significance, given the baseline conversion, minimum detectable difference (MDE), significance level & statistical power chosen. ‚ÄòDuration‚Äô refers to how long you need to run the test to reach adequate sample size per variation.
22. Novelty Effect ‚Äî ‚ÄúSometimes there‚Äôs a ‚Äúnovelty effect‚Äù at work. Any change you make to your website will cause your existing user base to pay more attention. Changing that big call-to-action button on your site from green to orange will make returning visitors more likely to see it, if only because they had tuned it out previously. This type of effect is not likely to last in the long run ‚Äî but it can artificially impact your test results.
23. Primacy effect ‚Äî When there‚Äôs a change in the product, people react to it differently. Some users may be used to the way a product works and are reluctant to change. This is called primacy effect. The primacy effect is nothing but the tendency to remember the first piece of information we encounter better than information presented later on. This can be thought of as an opposite phenomenon to novelty effect.
24. Seasonality ‚Äî Businesses may have different user behavior say on 1st of a month and 15th of a month. For some eCommerce sites, their traffic and sales are not stable all over the year, they tend to peak on Black Friday and Cyber Mondays for example. Variability due to these factors could influence your test results.
25. Day of week effect ‚Äî Similar to seasonality, a metric may have cyclicity based on day of week. For e.g. say conversion rates are much higher on Thursdays than they are on the weekends. In this case, it is important to run tests for full-week increments, so you are including every day of the week.
A/B Testing is one of the most important and widely applied data science concepts with numerous applications in growth optimization ‚Äî be it product experimentation (optimizing onboarding flows, increasing CTRs, server side optimizations etc.) OR for marketing acquisition (creative testing, incrementality testing, geo-split testing) and many more. With so many potential applications, it is very likely that you will be gauged on your knowledge of A/B testing in interviews ‚Äî specifically for product data science/analytics or marketing data science/analytics roles.
If you would like to continue learning A/B testing concepts and applications, check out Complete Course on A/B Testing with Interview Guide.
Paradoxically, acing data science interviews is more an art than a science! For more content from me on data science interviews or career growth in data science, subscribe and follow!
Follow on LinkedIn!
Karan Tulsani
Infogram
Lavanya Bandari
Neal O'Kelly
Manfye Goh
in
Towards Data Science
Piero Paialunga
in
Towards Data Science
Omar Raheem
Megumi Kiyama
"
179,https://medium.com/towards-data-science/the-5-sql-transforms-to-start-using-296ca7987921,The 5 SQL Transforms to Start Using,"In prior posts, I analyzed data from the SQL Generator 5000 and outlined the 5 most popular SQL transforms. This time, I‚Äôm doing the opposite and looking at the 5 least popular.
When I initially reviewed the transforms that were the least popular, I was surprised because the majority of them are very useful. So, I reached out to some users I know personally to talk to them about it.
Based on those conversations, I learned that the main reason that these transforms are rarely used is twofold:
Therefore, I will walk through the Top 5 least popular transforms, and along with the SQL code, I will explain a situation where each could be useful.
Scalers are methods that change the range of values. The two most common are: min_max_scaler and standard_scaler. In general, scaling is done in a way so that the new range falls between 0 and 1. Standardizing, on the other hand, is done in a way so that the average value is 0 and the standard deviation is 1. Sometimes the term normalize comes up in conversation about these techniques. Normalize can have a very specific meaning, but usually it is a generic term that could mean scaling, or standardizing (or something else!). To avoid confusion, don‚Äôt use the term normalize.
There are several common scenarios for scaling the data:
Imagine that you had some data about patients, where each of the variables was on a different scale:
Since Age is measured in years, and Glucose is measured in mg/DL, it‚Äôs hard for us to compare patients. The formula for scaling this data requires us to know the MIN() and MAX() of the values. Similarly, standard_scaler requires us to know the AVG() and STDEV().
Note how the SQL is written in a way that dynamically calculates everything on-the-fly.
Latest is a SQL transform that returns the last value in an ordered set of values. While most SQL functions only ‚Äúlook at‚Äù one row at a time, this one uses what is called window functions to look at other rows. For this reason, it is only used when your data has an inherent order to it.
There are many ways Latest can be used, but the most common are:
Imagine that you had sensor data where the temperature readings were sometimes missing:
Here is the customizable SQL that replaces any blank values with the latest prior value.
The function that works best for this transformation is called LAST_VALUE(). It works by declaring a search window, which has different default behaviors depending on RDBMS. Therefore I always recommend that you explicitly declare the ROWS BETWEEN definition to make your code more understandable.
NOTE: I wrote a longer tutorial on using this technique to fill in gaps in time-series data.
The moving average transform is another window function that requires the data to have an inherent order to it. Instead of today‚Äôs sales amount, the moving average can be used to tell us what the average sales amount was over the past 7 days.
Moving average is mainly used to identify a trend. They can also ‚Äúsmooth out‚Äù fluctuations if your values jump around a lot. It is one of the oldest and most commonly used technical indicators. It is even available in Excel! If you plot a time series in Excel, you can add a moving average by right-clicking and ‚ÄúAdd Trend Line.‚Äù
It is most commonly used in:
Imagine that you had data that had a dramatic seasonal pattern to it:
I made the chart so that you can see how there is a slight increasing trend. However, quantifying that trend might be difficult since the numbers are all over the place.
Using the Moving Average transform, we can create SQL that adds the moving average for us.
When we plot the result, we can see a nice, steady, increasing trend.
Entropy is a way to aggregate categorical data and represent how dispersed it is. It comes from the realm of information theory and is often used behind-the-scenes in machine learning algorithms.
Our current analysis of the SQL Generator is a great example of where entropy is useful. Imagine that User A and User B used the SQL Generator 100 times each and that they only used the 2 most popular transforms, aggregate and pivot .
User A: aggregate 50 times, pivot 50 timesUser B: aggregate 99 times, pivot 1 time
If we only aggregated with COUNT() and COUNT(DISTINCT), we would be unable to tell the behavior of these 2 users apart, but their behavior is quite different.
This is where measuring entropy helps. Users A and B are clearly different because of their distributions of usage. Entropy will show us that User A has entropy of 1 because the distribution between aggregate and pivot is dispersed as much as it can possibly be. On the other-hand, User B will have a very low entropy because they mainly use only a single transform.
There is no direct SQL function to calculate entropy , but the SQL Generator writes the logic to perform this aggregation for you. Try it yourself!
A histogram is a type of chart, so it‚Äôs no surprise that users of a SQL tool don‚Äôt find this transform very popular. Similar to entropy, a histogram also measures dispersion by breaking a numeric column into bins and then plotting the frequency of how often they occur. However, a histogram is a visual chart ‚Äî and SQL can‚Äôt make charts, so what is this SQL Transform even for?
Histograms themselves are very useful. They allow a user to quickly answer questions, like:
Analysts usually use a BI Tool or a python package to automatically aggregate the data and create the chart. This SQL transform is meant to aggregate the data only. This is because the underlying data might be very large when compared to the aggregate that actually gets plotted.
For example, if you had 500 Million orders and you wanted to plot a histogram of the total_order_amount, it would be computationally wasteful to move all 500 Million rows into your laptop, only to aggregate it into 100 rows for a histogram chart. This SQL will produce the aggregated rows directly from the warehouse, so plotting the histogram becomes fast & easy.
Here‚Äôs an example:
By configuring the SQL Generator with histogram, we can choose how many bins to use (100, in this example).
The SQL gets generated as:
To recap, last week we looked at the Top 5 Most Popular transforms, and we just followed that up with the Top 5 Least Popular.
Most Popular ‚Äî Correlation, Clean, Join, Pivot, Aggregate
Least Popular ‚Äî MinMax Scaler, Latest, Moving Avg, Entropy, Histogram
As you can see, the popular transforms seem to be the simple, tedious ones, while the least popular seem to be the most specialized, complicated ones.
Based on my conversations with users, it is also apparent that many are not aware of what some of these specialized transforms are meant to do. So, hopefully, I‚Äôve shared helpful context about the rarely-used ones so that if you find yourself in one of these rare situations, you‚Äôll already have the code written for you.
Ayazmohd
Siddharth Dixit
in
Towards Data Science
Greg Vonf @ Business Automated
Jesus Mira
Chaithanya Pramodh Kasula
in
Towards Data Science
TheStartupFounder.com
Rahul Agarwal
in
Towards Data Science
Rafi Mohammed
"
180,https://towardsdatascience.com/,No Title,No Content
181,https://medium.com/@raghavan-p26/simple-technique-to-learn-dax-for-power-bi-beginners-4c5865c11ac7,Simple technique to learn DAX for Power BI beginners,"If you look at the DAX documentation on the Microsoft website, there are 100+ DAX functions listed, which could scare you at first look. And with every new update in Power BI, new DAX functions with specific use cases are being launched.
But in reality, you would be using around 20‚Äì25 DAX functions regularly in most projects. So as a beginner, you need not learn all the available DAX functions in Power BI. Initially, learn the most commonly used functions like Sum, Average, Calculate, Filter, TopN, Format, Dateadd, etc. A simple google search will provide you with a list of common DAX functions.
Then, as you progress with Power BI, you will encounter certain situations where your existing DAX knowledge might not be able to solve your requirement.
In such a case, what to do?
I‚Äôve faced this many times during my early days in Power BI. My solution to this problem is GOOGLE. As Dhaval Patel says in his YouTube videos, ‚ÄòGoogle is your best friend‚Äô every time you face a technical problem.
Make sure you construct a phrase based on your requirement and search on Google.
Somewhere someone would have already solved your problem (in most cases)
You can refer to their solution or at least you will identify which new DAX function needs to be used for your problem. There are platforms like DAX guide or Microsoft Learn which provide you with the syntax and example use cases for each DAX function.
Thus, you will be learning new DAX functions on the job. With more experience, the number of DAX functions you learn will also increase. Ensure you make a note of every new DAX function you use in some Google/Word document for your quick future reference.
Having said that, the above technique is applicable only when you‚Äôre a beginner and new to Power BI. But if you want to learn DAX in a structured way, I‚Äôm attaching a few resources below (both free and paid).
SQLBI (free course)Arpita‚Äôs Tech Corner (free YT playlist)Maven Analytics (paid Udemy course)
At the end of the day, DAX is very important while working on complex Power BI projects and, hence it‚Äôs essential you know when and how to use DAX based on your requirements. ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî
I write articles on data analytics and business intelligence. Feel free to follow me if you like my articles!
Connect with me on LinkedIn to get in touch.
Fauna Inc
in
Fauna
Jen-Hsuan Hsieh (Sean)
in
A Layman
Felipe Marques
Kolejovia
Jyotikore
Abhishek Chakravarty
in
The Whiteboard
Eduard Cimbru
Chiugo Okpala
"
182,https://medium.com/@kozyrkov/is-your-boss-telling-you-the-truth-f03ae554ce4d,Is your boss telling you the truth?,"Probably not.
Is this blog post about lies and liars? Am I about to explain that your boss is lying to you? Nah, good leaders are usually savvy enough to misrepresent more skillfully than that. You‚Äôll rarely catch them in an outright lie, but ‚Äî especially for senior leaders ‚Äî part of‚Ä¶
Ari Betof
in
Org Sustainability‚Ää‚Äî‚ÄäInsight and Advice for Leaders that Change the World by Ari Betof
Practical Inspiration Publishing
rachel audige
Sibongiseni Mgolozeli
Anton Perkin
in
FusionWorks
asti ayuningtyas
Hise Gibson
Jason Orloske
"
183,https://medium.com/technology-hits/obsidian-plugin-review-obsidian-timelines-8f98d3fa82b0,Obsidian Plugin Review ‚Äî Obsidian Timelines,"Obsidian Timelines is a plugin that allows you to create chronological timelines using specified notes. A timeline is a visual representation of a chronological sequence of events along a line, enabling us to understand temporal relationships easily.
Gaurav Singh
3dheven
NAKIVO
Susmita Horrow
in
Fueled Engineering
Prof Bill Buchanan OBE
in
Coinmonks
Jonathan Chao
Lois T.
in
Geek Culture
Jamil Moughal
"
184,https://medium.com/towards-artificial-intelligence/how-i-prepared-for-my-amazon-data-scientist-interview-5904db944378,How I Prepared for My Amazon Data Scientist Interview,"‚Ä¶ And nailed it!
Let me warn you. This is going to be a long read. You may skim through some parts but, I will strongly advise you not to because what I have shared here is GOLD (if I may say so myself).
Most of you may be familiar with the extremely long interview process the FANG (Facebook, Amazon, Netflix, and Google) firms have. It can be excruciatingly long, dragging on forever. I had a total of 4 interviews with Amazon (for different positions) and I was successful in securing a Data Scientist position at my last one (which in itself had a total of 5 interviews). In this article, I‚Äôll explain how I prepared for my successful interview and what resources I used.
Typically, I started my prep approximately 1 week before my first interview, during which, I would go through all the material I have discussed in this article.
If you are interested in learning what the actual interview process was like, then head over to this very popular article My Amazon Data Scientist Interview Questions ‚Äî and Answers!
If you are a Data Scientist or a Software Developer, then my guess is you already know some basic coding in Python and SQL. For a Data Scientist interview, in my experience, I haven‚Äôt come across too many companies that focus on any other coding language ‚Äî however, I‚Äôd add that having Spark on your resume is a good plus.
Now when I say basic, I do mean pretty basic ‚Äî such as for SQL, you should know how to:
For Python, know how to:
For Python, I found this article useful: https://realpython.com/courses/python-coding-interviews-tips-best-practices/
It's important that you have a good grasp of SQL and Python for your coding interviews.
For a Data Scientist position, this is an absolute must. You can‚Äôt do well if you don‚Äôt know the ins and outs of the basic algorithms. You should have a good grasp of the most used Machine Learning models and algorithms, and understand how they work, and their advantages and disadvantages. I can‚Äôt emphasize this enough; try to find and learn why a model works and why it may fail. I have listed the most common topics here, and towards the end of the article, I have provided more detailed notes on some of these topics.
This is an important one, not as important as it will be for a Software Developer probably but, having a good understanding of Data Structures and Algorithms will surely set you apart. The following is a good start:
This is an amazing book that‚Äôll help you understand Data Structures and Algorithms, Grokking Algorithms: An illustrated guide for programmers and other curious people.
Typing or writing out what you have learned and prepped is super helpful ‚Äî at least it helps me retain that information longer. I typed all of my notes in the One Note app and used it for a quick revision later.
You can use whatever works best for you.
Linear Regression
Most of my notes for Linear Regression are based on the book An Introduction to Statistical Learning.
Logistic Regression
This one is a good article on this: https://christophm.github.io/interpretable-ml-book/logistic.html
Advantages / Disadvantages: It is a widely used technique because it is very efficient, doesn‚Äôt require too many computational resources, is highly interpretable, doesn‚Äôt require input features to be scaled, doesn‚Äôt require any tuning, is easy to regularize, and it outputs well-calibrated predicted probabilities.
Like linear regression, logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other. Therefore, Feature Engineering plays an important role in regards to the performance of Logistic and also Linear Regression. Another advantage of Logistic Regression is, that it is incredibly easy to implement and very efficient to train. I typically start with a Logistic Regression model as a benchmark and try using more complex algorithms from there on.
Because of its simplicity and the fact that it can be implemented relatively easily and quickly, Logistic Regression is also a good baseline that you can use to measure the performance of other more complex Algorithms.
A disadvantage of it is that we can‚Äôt solve non-linear problems with Logistic Regression since its decision surface is linear. Just take a look at the example below that has 2 binary features from 2 examples.
Assumptions of Logistic Regression: First, logistic regression does not require a linear relationship between the dependent and independent variables. Second, the error terms (residuals) do not need to be normally distributed. Third, homoscedasticity is not required. Finally, the dependent variable in logistic regression is not measured on an interval or ratio scale.
However, some other assumptions still apply.
First, binary logistic regression requires the dependent variable to be binary, and ordinal logistic regression requires the dependent variable to be ordinal.
Second, logistic regression requires the observations to be independent of each other. In other words, the observations should not come from repeated measurements or matched data.
Third, logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other.
Fourth, logistic regression assumes linearity of independent variables and log odds. although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.
Finally, logistic regression typically requires a large sample size. A general guideline is that you need a minimum of 10 cases with the least frequent outcome for each independent variable in your model.
Random Forest
This is an amazing article on Random Forest: https://builtin.com/data-science/random-forest-algorithm
Clustering
There are 2 key advantages to using GMMs. Firstly GMMs are a lot more flexible in terms of cluster covariance than K-Means; due to the standard deviation parameter, the clusters can take on any ellipse shape, rather than being restricted to circles. K-Means is actually a special case of GMM in which each cluster‚Äôs covariance along all dimensions approaches 0. Secondly, since GMMs use probabilities, they can have multiple clusters per data point. So if a data point is in the middle of two overlapping clusters, we can simply define its class by saying it belongs X-percent to class 1 and Y-percent to class 2. I.e GMMs support mixed membership.
Gradient Boosting
This is a good article to understand Gradient Boosting: https://explained.ai/gradient-boosting/
Auto-encoder
Autoencoders are an unsupervised learning technique in which we leverage neural networks for the task of representation learning. Specifically, we‚Äôll design a neural network architecture such that we impose a bottleneck in the network which forces a compressed knowledge representation of the original input. If the input features were each independent of one another, this compression and subsequent reconstruction would be a very difficult task. However, if some sort of structure exists in the data (ie. correlations between input features), this structure can be learned and consequently leveraged when forcing the input through the network‚Äôs bottleneck.
As visualized above, we can take an unlabeled dataset and frame it as a supervised learning problem tasked with outputting xÃÇ , a reconstruction of the original input x. This network can be trained by minimizing the reconstruction error, Óà∏(x,xÃÇ ), which measures the differences between our original input and the consequent reconstruction. The bottleneck is a key attribute of our network design; without the presence of an information bottleneck, our network could easily learn to simply memorize the input values by passing these values along through the network (visualized below).
Note: In fact, if we were to construct a linear network (ie. without the use of nonlinear activation functions at each layer) we would observe a similar dimensionality reduction as observed in PCA
Because neural networks are capable of learning nonlinear relationships, this can be thought of as a more powerful (nonlinear) generalization of PCA. Whereas PCA attempts to discover a lower-dimensional hyperplane that describes the original data, autoencoders are capable of learning nonlinear manifolds (a manifold is defined in simple terms as a continuous, non-intersecting surface).
Gradient Descent
Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used to find the values of a function‚Äôs parameters (coefficients) that minimize a cost function as far as possible.
You start by defining the initial parameter‚Äôs values, and from there gradient descent uses calculus to iteratively adjust the values so they minimize the given cost function. To understand this concept fully, it‚Äôs important to know about gradients.
What is a gradient? A gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope, and the faster a model can learn. But if the slope is zero, the model stops learning. In mathematical terms, a gradient is a partial derivative with respect to its inputs.
Types of Gradient Descent
BATCH GRADIENT DESCENT: Batch gradient descent, also called vanilla gradient descent, calculates the error for each example within the training dataset, but only after all training examples have been evaluated does the model get updated. This whole process is like a cycle and it‚Äôs called a training epoch.
Some advantages of batch gradient descent are its computational efficiency, it produces a stable error gradient, and a stable convergence. Some disadvantages are the stable error gradient can sometimes result in a state of convergence that isn‚Äôt the best the model can achieve. It also requires the entire training dataset to be in memory and available to the algorithm.
STOCHASTIC GRADIENT DESCENT: By contrast, stochastic gradient descent (SGD) does this for each training example within the dataset, meaning it updates the parameters for each training example one by one. Depending on the problem, this can make SGD faster than batch gradient descent. One advantage is the frequent updates allow us to have a pretty detailed rate of improvement.
The frequent updates, however, are more computationally expensive than the batch gradient descent approach. Additionally, the frequency of those updates can result in noisy gradients, which may cause the error rate to jump around instead of slowly decreasing.
MINI-BATCH GRADIENT DESCENT: Mini-batch gradient descent is the go-to method since it‚Äôs a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent.
Common mini-batch sizes range between 50 and 256, but like any other machine learning technique, there is no clear rule because it varies for different applications. This is the go-to algorithm when training a neural network and it is the most common type of gradient descent within deep learning.
One Hot Encoding vs Label Encoding
So, how should we handle categorical variables? As it turns out, there are multiple ways of handling Categorical variables. In this article, I will discuss the two most widely used techniques:
Label Encoding
Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.
Let‚Äôs see how to implement label encoding in Python using the scikit-learn library and also understand the challenges with label encoding.
Let‚Äôs first import the required libraries and dataset:
As you can see here, the first column, Country, is the categorical feature as it is represented by the object data type and the rest of them are numerical features as they are represented by int64.
Now, let us implement label encoding in Python:
Challenges with Label Encoding
In the above scenario, the Country names do not have an order or rank. But, when label encoding is performed, the country names are ranked based on the alphabet. Due to this, there is a very high probability that the model captures the relationship between countries such as India < Japan < the US.
This is something that we do not want! So how can we overcome this obstacle? Here comes the concept of One-Hot Encoding
One-Hot Encoding
One-Hot Encoding is another popular technique for treating categorical variables. It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature.
In this encoding technique, each category is represented as a one-hot vector. Let‚Äôs see how to implement one-hot encoding in Python:
Challenges of One-Hot Encoding: Dummy Variable Trap
One-Hot Encoding results in a Dummy Variable Trap as the outcome of one variable can easily be predicted with the help of the remaining variables.
The Dummy Variable Trap leads to the problem known as multicollinearity. Multicollinearity occurs when there is a dependency between the independent features. Multicollinearity is a serious issue in machine learning models like Linear Regression and Logistic Regression.
So, in order to overcome the problem of multicollinearity, one of the dummy variables has to be dropped. Here, I will practically demonstrate how the problem of multicollinearity is introduced after carrying out the one-hot encoding.
One of the common ways to check for multicollinearity is the Variance Inflation Factor (VIF):
Anomaly Detection
Supervised
Training data is labeled with ‚Äúnominal‚Äù or ‚Äúanomaly‚Äù.
The supervised setting is the ideal setting. It is the instance when a dataset comes neatly prepared for the data scientist with all data points labeled as anomaly or nominal. In this case, all anomalous points are known ahead of time. That means there are sets of data points that are anomalous but are not identified as such for the model to train on.
Popular ML algorithms for structured data:
Clean
In the Clean setting, all data are assumed to be ‚Äúnominal‚Äù, and it is contaminated with ‚Äúanomaly‚Äù points.
The clean setting is a less-ideal case where a bunch of data is presented to the modeler, and it is clean and complete, but all data are presumed to be nominal data points. Then, it is up to the modeler to detect the anomalies inside this dataset.
Unsupervised
In Unsupervised settings, the training data is unlabeled and consists of ‚Äúnominal‚Äù and ‚Äúanomaly‚Äù points.
The hardest case, and the ever-increasing case for modelers in the ever-increasing amounts of dark data, is the unsupervised instance. The datasets in the unsupervised case do not have their parts labeled as nominal or anomalous. There is no ground truth from which to expect the outcome to be. The model must show the modeler what is anomalous and what is nominal.
‚ÄúThe most common tasks within unsupervised learning are clustering, representation learning, and density estimation. In all of these cases, we wish to learn the inherent structure of our data without using explicitly-provided labels.‚Äù- Devin Soni
In the Unsupervised setting, a different set of tools are needed to create order in the unstructured data. In unstructured data, the primary goal is to create clusters out of the data, then find the few groups that don‚Äôt belong. Really, all anomaly detection algorithms are some form of approximate density estimation.
Popular ML Algorithms for unstructured data are:
From Dr. Dietterich‚Äôs lecture slides, the strategies for anomaly detection in the case of the unsupervised setting are broken down into two cases:
Where machine learning isn‚Äôt appropriate, top non-ML detection algorithms include:
This is a good article for Anomaly Detection: https://www.bmc.com/blogs/machine-learning-anomaly-detection/
Hyperparameter Tuning
Random Search Cross-Validation
Usually, we only have a vague idea of the best hyperparameters and thus the best approach to narrow our search is to evaluate a wide range of values for each hyperparameter. Using Scikit-Learn‚Äôs RandomizedSearchCV method, we can define a grid of hyperparameter ranges, and randomly sampled from the grid, performing K-Fold CV with each combination of values.
The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross-validation (we use 100 and 3 respectively). More iterations will cover a wider search space and more cv folds reduce the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental.
We can view the best parameters by fitting the random search:
Grid Search with Cross-Validation
Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:
Precision vs Recall
Recall, or the ability of a model to find all the relevant cases within a dataset
For example, in a smog prediction system, we may be far more concerned with having low false negatives than low false positives. A false negative would mean not warning about a smog day when in fact it is a high smog day, leading to health issues in the public that is unable to take precautions. A false positive means the public would take precautionary measures when they didn‚Äôt need to
Precision, or the ability of a classification model to identify only the relevant data points.
Loss Functions
Which Loss Function to Use?
Regression:
Mean squared error loss
Mean absolute error loss
Binary Classification:
Cross-Entropy
Hinge Loss
Multi-Class Classification:
I found this Git link to be very helpful, its very detailed but a quick skim of this Git is good, https://github.com/khanhnamle1994/cracking-the-data-science-interview
I have shared everything I used and everything I learned. So, I really hope this article is helpful for you, and all the best in preparing for your upcoming interview!
Thauren Heavyside
in
Dev Genius
Arpita Ghosh
in
Predict
Sakshi Maharana
Wafa Soofi
Hasninemirza
Robert Oliver
Shanmuktata
The Institute of Quantitative Technology
"
185,https://medium.com/@ZainabAkinwande/data-science-job-salaries-sql-analysis-1a35e844223,DATA SCIENCE JOB SALARIES: SQL ANALYSIS,"In this Project, I downloaded the dataset from Kaggle titled Data Science Job salaries. I decided to work on this dataset because:
Let‚Äôs get started!
STEP 1: Import the data into Excel
The first thing I did was to import the dataset into excel. This is to ensure that I know what the data is about, examine it and do some data cleaning. This dataset contained 11 columns and 608 rows. It includes the work_year column, experience level column, employment type, job title, salary, salary in usd etc.
I used the filter function to better understand the data set. There is a column called salary and another called salary_in_usd. The later is basically the conversion of the former to USD (US Dollars) which I believe is for the sake of uniformity and also to aid analysis.
NB: I thought that was all I could do in excel as the data was pretty clean or so I thought! I would have made a big mistake if I didn‚Äôt take my time to go through the dataset again. It‚Äôs important to use the filter and sort function properly to examine the dataset thoroughly.
All done and good. I saved the file as an excel workbook type in preparation for the next stage
STEP 2: Importing the data to SQL
I used Microsoft SQL Server Management Studio for this analysis. For the most part of this analysis, I will be working with the salary_in_usd column for the sake of uniformity.
In the course of the analysis, I used the following SQL functions: SELECT, FROM, WHERE, CASE STATEMENT, WINDOW CLAUSE, CTE, GROUP BY, ORDER BY, MAX, MIN, AVERAGE, ROUND, etc.
These are the questions I answered:
Before I dived into the analysis itself, I had to ensure that my dataset was imported properly from excel. I used the SELECT and asterisk function to confirm it.
I used the COUNT function together with DISTINCT so as to get unique job_titles and to avoid duplicates in the course of the count.
Answer: 49 job titles
2. Which experience level has the highest salary?
I used the CASE Statement to separate the experience level into different categories of Mid-level, senior-level, entry-level, and executive-level. I used the Count function to count the number of jobs that belonged to each of the category. I used the Max function to find the highest salary and the Round function to round up the numbers to two decimal places so as to make it readable. I grouped by the Case statements and ordered by highest_salary in descending order to get the highest salary.
Answer: Executive level at USD 600,000
3. Does the employment type affect salary?
I used the CASE Statement to separate the employment type into different categories: full-time, part-time, freelance, and contract. I used the Count function to count the number of jobs that belong to each category. I used the Average function to find the average salary per category and the Round function to round up the numbers to two decimal places so as to make it readable. I grouped by the Case statements and ordered by average_salary in descending order to get the result of the arranged in descending order.
Answer: Yes, it does. Contract staff get paid more on the average
4. What is the average salary per job title in USD?
I used the Average function to get the average salary per job title in USD. I used the Round function to round up the numbers to two decimal places so as to make it readable. I grouped by job title and ordered by the average salary in descending order.
5. What is the highest salary per job title in USD?
I used the Top 5 function to get the top 5 job title. I used the Max function to get the highest salary in USD. I used the round function to round up the numbers to two decimal place so as to make it readable. I grouped by job_title and ordered by the highest salary in descending order.
Answer: Principal Data Engineer at $600,000
6. What is the highest paying entry-level data science job?
I used the Top 5 function to get the top 5 job title. I used the Max function to get the highest salary in USD. I used the where function to filter the result and only return max salary for entry level. I grouped by job_title and ordered by the highest salary in descending order.
Answer: Machine Learning Engineer at $250,000 (That‚Äôs a substantial entry-level salary)
OR I also tried to use the CTE and Window clause to answer this question and I got the same answer. But I will be using the simpler method going forward
I Selected job title, experience level, salary in USD. I used the Rank function to get the rank of the dataset. I used the window function, partitioned by job title and experience level. I ordered by salary in descending order. I then filtered using the where clause to return result for entry-level only. I then selected job rank, job title, experience level, salary in USD. I used the where function to limit the result to only rank 1. Finally, I ordered by salary in USD in descending order.
7. What is the highest paying mid-level data science job?
I used the Top 5 function to get the top 5 job title. I used the Max function to get the highest salary in USD. I used where function to filter the result and only return max salary for mid-level. I grouped by job_title and ordered by the highest salary in descending order.
Answer: Financial Data Analyst and Research Scientist at $450,000
8. What is the highest paying senior-level data science job?
I used the Top 5 function to get the top 5 job title. I used the Max function to get the highest salary in USD. I used where function to filter the result and only return max salary for senior-level. I grouped by job_title and ordered by the highest salary in descending order.
ANSWER: Data Scientist at $412,000 (Isn‚Äôt it surprising that some Mid-level roles get paid more than Senior-level roles?)
9. What is the highest paying Executive level data science job?
I used the Top 5 function to get the top 5 job title. I used the Max function to get the highest salary in USD. I used where function to filter the result and only return max salary for executive-level. I grouped by job_title and ordered by the highest salary in descending order.
ANSWER: Principal Data Engineer at $600,000 (Huge!)
10. What is the average salary per experience level?
I used the CASE Statement to separate the experience level into different categories: mid-level, senior-level, entry-level, and executive-level. I used the Count function to count the number of jobs that belonged to each category. I used the Average function to find the average salary per category and the Round function to round up the numbers to two decimal places so as to make it readable. I grouped by the Case statements and ordered by average_salary in descending order to get the result arranged in descending order.
Answer:
11. What is the lowest paying entry-level data science job?
I used the Min function to get the lowest salary in USD. I used the where function to filter the result and only return the lowest salary for entry-level. I grouped by job_title and ordered by the lowest salary.
Answer: Data Scientist at $4,000 (This is ridiculously low. An outlier maybe!)
12. What is the lowest paying Mid-level Data Science Job?
I used the Min function to get the lowest salary in USD. I used the where function to filter the result and only return the lowest salary for mid-level. I grouped by job_title and ordered by the lowest salary.
Answer: Data Scientist at $2859 (For a Mid-level role? Even lower than an entry-level salary. I need to speak to HR asap!)
13. What is the lowest paying Senior level data science job?
I used the Min function to get the lowest salary in USD. I used the where function to filter the result and only return the lowest salary for senior-level. I grouped by job_title and ordered by the lowest salary.
Answer: Computer Vision Engineer at $18,907
14. What is the lowest paying Executive level data science job?
I used the Min function to get the lowest salary in USD. I used the where function to filter the result and only return the lowest salary for the executive level. I grouped by job_title and ordered by the lowest salary.
Answer: Data Science Consultant at $69,741
15. Does company size affect salary?
I used the CASE Statement to separate the company into different categories of a large, medium, and small companies. I used the Min and Max function to find the lowest and highest salary per company category and the Round function to round up the numbers to two decimal places so as to make it readable. I grouped them by the Case statements.
NB: I used the Min and Max function here instead of Average because Averages are susceptible to outliers. Using average will not give us the right insight into how much companies pay.
Answer: Yes, it does. Large companies tend to pay more
16. Does company location affect Salary?
I used the CASE Statement to separate the company location into Companies in the US and Companies not in the US. I used the Min and Max function to find the lowest and highest salary per location category and the Round function to round up the numbers to two decimal places so as to make it readable. I grouped them by the Case statements.
Answer: Yes, it does. Companies in the US tend to pay more.
INSIGHTS/CONCLUSION: From the above analysis, it can be concluded that companies in the US tend to pay more than their counterpart in other countries. Do you want to earn big money? Go for a job in a large company in the US, preferably a job with a high pay eg. Machine Learning Engineer. *wink*
You can check out the complete code on my Github
I hope you enjoyed reading this as much as I enjoyed working on the datasets. I will appreciate your feedback. See you on my next project. Bye!.
Columbia Road
Karthik
Pranay Rishith
Dr. Alvin Ang
Sam Zhang
in
The OpenCounter Blog
Louise de Leyritz
in
Castor App
Aparna Dhinakaran
in
Towards Data Science
Smart London
"
186,https://medium.com/@lizzynaameh/3-myths-about-switching-careers-to-data-analytics-f1d4c97848b3,3 Myths About Switching Careers to Data Analytics,"As a Data Career Coach, I work with aspiring data analysts from all sorts of different backgrounds.
I work with people coming from hospitality and retail, former teachers and fresh high school graduates; parents and caretakers and people working two, sometimes three jobs; studious types with deep experience in mathematics and folks who have never written code in their life.
All are attracted to Data Analytics as a career because it offers the opportunity for higher pay, more stimulating work, and greater work-life balance.
And while many begin their data journeys, I have also noticed some recurring myths that prevent people from achieving their full potential within the field ‚Äî or from ever making the leap at all.
Here are the most common myths about switching careers to data analytics, and the truth about what it takes to enter the field.
Data analytics is a highly technical field, so many people expect that they need to get a technical degree to land their first job. The truth is this: you do not need a degree to land a job as a data analyst, and you certainly do not need a degree to learn data analytics.
In fact, many traditional students are at a disadvantage coming out of their Masters programs, simply because academic institutions often offer curricula that don‚Äôt keep pace with industry developments.
There are so many cost-effective ways of learning data analytics and some forward-thinking organizations are even bringing quality data education to workers as part of their on-the-job up-skilling initiatives.
Three alternative education options you can explore are:
Many people fumble through their math education in school, believing that they just don‚Äôt ‚Äúget‚Äù and ‚Äúdon‚Äôt like math‚Äù. (Hint: if most people feel that way, it might be our education system and NOT you?! *gasp*) But is that really the case?
While math is core to data analytics, you actually need a firmer grasp of some key computational tools (like SQL, Excel, and maybe Python), applied statistics, business thinking, and domain knowledge (think, ‚Äúbeauty industry‚Äù or ‚Äúeducation‚Äù) to succeed as an analyst.
Analytics is nothing like the context-less, boring equations of your high school days. Data is embedded in the very real world around us, and analytics helps us understand and act on our world in empowered ways. Don‚Äôt let your anxieties around math hold you back from this incredibly diverse and exciting field.
You may have no interest in becoming a data analyst or data scientist, but you do need some ability in basic data analysis to future-proof your career.
For example, let‚Äôs look at Amazon‚Äôs Upskilling 2025 initiative. Some of the first titles listed in their press release are data mapping specialist and data scientist, along with roles like business analyst and logistics coordinator.
Nearly every one of the top skills employers are looking for today involves data to some degree, from artificial intelligence and business strategy to machine learning and digital marketing.
Data literacy is essential as our economy undergoes this massive digital transformation. Those who don‚Äôt get with the program will surely be left behind.
‚Ä¶
Now that we have established that data analytics is an accessible and viable career path regardless of your background or mathematical affinity, I hope you go forth and prosper in your data journey.
Many have made this career switch before (including me!) and you can, too, with some time, effort, and a lot of Stack Overflow.
You got this!
Riaz Pinjari
Maria Ramos
in
Towards Data Science
Amaresh Marekar
Manil wagle
krtrimaIQ Cognitive Solutions
in
krtrimaIQ Cognitive Solutions
Solomon Banuba
Chanon Khongprasongsiri
Rohil Sharma
"
187,https://medium.com/dssimplified/understand-data-storytelling-in-5-minutes-2e3fa949310b,Understand Data Storytelling in <5 minutes!,"When people talk about storytelling, it is mostly fiction and fables or life events narrated in a way to stick the message to the audience in a subtle way. Data Storytelling is more or less the same thing. You‚Äôre helping the audience get the message (and it‚Äôs very real because you back it up with data). Let‚Äôs skip the narration here and dive right into it.
A brief introduction and understanding of Data Storytelling.
An explanation of how you can do data storytelling and make compelling narratives to wow the audience (we‚Äôll get to that in an upcoming article, or just drop a text on LinkedIn, and we‚Äôll get you up to speed on that if needed urgently)
Note: The views in this article are mine alone, I urge you to find more sources and do your own research on how you can write a good data story on your own and weave the narrative to help your audience get the complete picture of the plot (i.e., the project or situation or baseline)
These tools are not tough to use and yes, with Excel and PowerPoint too you can try to make a decent data story (provided you know that the value of a story is not just in the narrative but more in the content)
Think of it like this, every story has an introduction and a plot base, followed by the main story and the message or conclusion.
Data Storytelling is just the same thing, you have an introduction, a story that you narrate using data visualizations and statistics or anything essentially related to data, and then you have a conclusion or message as a giveaway to your audience that they will hold on to for a longer time than how the same insights would work in a presentation.
The best part of the story is that everyone understands the flow. How every stage plays a role in the following stage and how one reaches the conclusion.
Independent visuals can be a part of the data story by laying emphasis on things that require a lot of attention.
For example, on sharing a data story about how your organization‚Äôs churn has been growing, you might want to start with an individual number, say 30%, and cite the situation at hand, i.e., a very high churn %.
Dashboards can show the collective movement of a lot of different things together that can show how the plot of your story moves with a change in a certain aspect.
For example, continuing our churn story, maybe you want to show how your organization‚Äôs revenue increases and costs reduce, and also your retention rates improve as an impact of a reduced churn % (which may be your target)
The data is really important if you want to know how the story can change with time. Having a forecast of your numbers or just the right metrics to show in your presentation can help you elaborate on the situation at hand with ease.
Continuing the idea, maybe you want to show the variance of the churn in the months throughout the year, where the average is being impacted by a high churn during new year‚Äôs eve (which is completely hypothetical)
The baseline of your story defines how your story changes with time.
What makes a good story? Think.
A good plot twist, a couple of amazing characters that support the main character, the villain and their sidekicks, or just the antagonizing elements, the place where the story takes place, a couple of hints of how the sequel would look, and REALLY GOOD STORYTELLING to make sure your audience remembers the takeaways at their fingertips.
There are quite a few ways you can learn how to get going with a data story;
Find the links for the resources in books and video tutorials to see, at the end of this article.
Advantages:
Disadvantages:
www.storytellingwithdata.com
Also check out my previous articles here, if they interest you!
medium.com
medium.com
medium.com
For more such articles, stay tuned with us as we chart out paths on understanding data and coding and demystify other concepts related to Data Science. Please leave a review down in the comments.
Check out my other articles at:
medium.com
Do connect with me on LinkedIn at ‚Äî Yash Gupta ‚Äî if you want to discuss it further! Leave a clap and comment below to support the blog! Follow for more.
Peoplelogic.ai
Jiaqi Zhang
S Abhishek
ODSC - Open Data Science
Shreya Rao
Manu Chatterjee
Akshay Pandya
in
Analytics Vidhya
Palak Sharma
"
188,https://medium.com/codex/how-to-start-a-career-in-cyber-security-d63c8420dfb7,How to Start A Career In Cyber Security,"There are several basic techniques to cyber security, and if an organization complies with them, it can safeguard its assets from hackers, including servers, computers, and, most crucially, its reputation.
It is referred to as Cyber-Security terminology at the highest level of abstraction; nonetheless, it encompasses a wide range of streams‚Ä¶
Projectcubicle
A Learner
Tachyon
in
Tachyon Protocol
Bruno Marcoux
in
The Capital
Freda Zrike
Vivek Shankar
Farhad Malegam
The Cryptography Centre For Excellence
"
189,https://medium.com/@keith-mcnulty/things-the-new-chatgpt-chatbot-is-good-at-and-terrible-at-8666b0ba472b,Things the New ChatGPT ChatBot Is Good At and Terrible At,"Any of you who have been following Data Science and AI on Social Media over the past week will have seen no shortage of examples and comments about OpenAI‚Äôs new ChatGPT chatbot. Once you play around with it (which as a research release you can do for free here), you‚Äôll see that there is some truth to the hype. The model has‚Ä¶
Olga V. Mack
in
Parley Voo Newsletter
UnfoldLabs
Gokul
in
Argoid
Sofia Russo
Pulse Network
in
Pulse Network
Matrix AI Network
Victor Osyka
MathWorks Editor
in
MathWorks
"
190,https://medium.com/illumination/passive-income-in-cybersecurity-3-making-an-awesome-udemy-course-c9f5df78e8f3,Passive Income in Cybersecurity 3: Making an awesome Udemy course,"Time to earn some $$$ from your cybersecurity skills
Welcome to the third part of this series
Passive Income in Cybersecurity is quite a popular topic, and people keep reaching out to me for more tips. If you have not read my earlier articles, then check them out below
Curtis Brazzell
Asian Ubuo
David S Woodman
Jeffrey Richman
Mohamed Laajimi
NFTbolt ‚ö°
Shaun Philine
Arctype
in
Database Dive
"
191,https://medium.com/towards-data-science/statistics-bootcamp-6-building-our-confidence-ba7be17c008c,Statistics Bootcamp 6: Building Our Confidence,"To more formally address the need for a statistics lecture series on Medium, I have started to create a series of statistics boot camps, as seen in the title above. These will build on one another and as such will be numbered accordingly. The motivation for doing so is to democratize the knowledge of statistics in a ground-up fashion to address the need for more formal statistics training in the data science community. These will begin simple and expand upwards and outwards, with exercises and worked examples along the way. My personal philosophy when it comes to engineering, coding, and statistics is that if you understand the math and the methods, the abstraction now seen using a multitude of libraries falls away and allows you to be a producer, not only a consumer of information. Many facets of these will be a review for some learners/readers, however having a comprehensive understanding and a resource to refer to is important. Happy reading/learning!
This article is dedicated to confidence intervals in our estimates and hypothesis testing.
Estimating values is at the core of inferential statistics. Estimation is the method of approximating a particular value relative to a a known or unknown ground truth value. In stats this tends to be the value of our parameter that we estimate from a given sample of data. Estimates do not necessarily reproduce the value of our parameter being estimated, but ideally our sample comes as close as possible. Therefore, while we rarely have access to population data for a full comparison, the errors of our estimates based on our sample can be assessed.
According to a Allan Bluman, a good estimator should have three properties. It should be unbiased, meaning the expected value is equal to the the value calculated. It should be consistent, as our sample size and therefore information increase, the value of the estimate should approach the true parameter value. Lastly, it should be efficient, meaning it has the smallest variance possible [1].
We can think about a couple different kinds of estimates. One is a point estimate. A point estimate is a specific numerical value, usually existing along a continuum of possible values. For example, when we are performing a numeric value imputation for missing data we are estimating a specific value. Contrast this with an interval estimate, which provides a range of values that may or may not contain the true parameter (think accuracy where we are trying to get as close as possible to a true value) [1]. It is here we should gain some intuition regarding the notion of a confidence interval. If we have an interval estimate that may contain our true parameter value, we want to be __% certain that the point estimate is contained within the interval estimate. Therefore confidence intervals are an example of interval estimates.
It is unlikely that any particular sample mean will be exactly equal to the population mean, Œº. Why, you make ask? Due to sampling error. To allow for sampling error, a parameter is usually estimated to be within a range of values, called a confidence interval. These intervals give a set of plausible values for the true parameter.
More formally, a confidence interval is the probability that the interval will contain the true value (population parameter or point estimate) if we repeatedly sampled the population and performed the calculations. Said another way, how close is our grouping of the point estimates we obtain from our sample? We derive a confidence interval by using data obtained from a sample and specifying the confidence level we wish to have in our estimate. Three common confidence intervals used are the 90th, 95th and the 99th percentile.
The formula for the confidence interval of the parameter (population) mean for a specific level alpha (Œ±), when œÉ is known can be calculated by:
Given the confidence interval 1 - Œ±, we need to find the associated z-score (critical value) from a z-score distribution table. Another way to write the equation prior as as shown here:
Example. If you wanted to be sure 95% of the time the population mean (parameter mean) would land within a range of values you specify, determine the confidence interval based on the sample mean, Xbar. A 95th confidence interval translates to Œ± = 0.05. So let‚Äôs calculate this based on the equation specified previously.
Our purpose is to find a confidence interval for a population mean, Œº. Our assumptions include that our sample was random, that our sample is normally distributed or large enough that we can assume it is (n ‚â• 30), and that the standard deviation œÉ is known. We need œÉ in order to perform the calculate as per the formula above.
1. Identify a confidence level (1 - Œ±)
2. Using a z-table, find z_{Œ±/2}
3. The confidence interval for population/parameter mean, Œº ranges from
where z_{Œ±/2} is derived from step 1, n is the sample size and xbar is the mean computed from the sample data.
4. Interpret your findings. This means stating ‚ÄúOur population mean, Œº will fall within the range (__,__), (confidence level) amount of the time.
Note that the confidence interval is considered to be exact for normal populations and is approximately correct for large samples from non-normal populations [1].
Example. Here we have the prices of a can of cat food from 32 different stores in downtown Chicago. Find a 95% confidence interval for the mean price of cat food, Œº, of all the pet stores in downtown Chicago. Assume that the population standard deviation of the ages is 0.8 dollars.
Because the population standard deviation is known, sample size is 32, which is large (‚â•30), we can use the one sample z-interval when œÉ is known to find the required confidence interval.
The confidence interval for the true population mean goes from 2.32 to 2.88 dollars. Our interpretation of this is as follows: We can be 95% confident that the mean price of a can of cat food in downtown Chicago is somewhere between 2.32 and 2.88 dollars.
‚ÄúAccuracy is true to intention, precision is true to itself‚Äù ‚Äî Simon Winchester [2]
Accuracy is a loaded term, but regardless it implies there is a ground truth by which we as evaluators can assess how correct or close we are to that ground truth. To that end, as the width of our confidence interval increases, the accuracy of our estimate decreases. Intuitively this should make sense because there are more possible point estimates for the parameter now contained within that range. For example, a 95% confidence interval (2.32, 2.88) versus a 99% confidence interval (2.24, 2.96) as per the example above. As we can plainly see, the second is wider. This is mediated by the fact we have to be MORE sure our estimate falls in this range. In the previous example Xbar is 2.6, so with a 95% confidence level, the error in our estimation is 2.6‚Äì2.32 = 0.28 dollars, and at the 99th the error in our estimation is 2.6‚Äì2.24 = 0.36 dollars.
Half the range of our confidence interval (latter portion of our equation) can be parsed out as show below. This is referred to as our margin of error, denoted by E. It represents the largest possible difference in our parameter estimate and our parameter value as based on our confidence level.
Looking at this equation, if we have a pre-specified confidence level (z_{Œ±/2}) we are working towards, how can we decrease the error E (i.e. increase the accuracy of our estimate)?
INCREASE THE SAMPLE SIZE!
Since ‚Äôn‚Äô is the only variable in the denominator, it will be the only thing we can easily do to decrease ‚ÄòE‚Äô, since we can‚Äôt force our population to have a different standard deviation.
Suppose we‚Äôd like to have a smaller margin of error. And say we need to be sure 90% of the time the population mean would be covered within 0.2 dollars of the sample mean. What sample size would we need to be able to state this (assume œÉ = 0.8)?
We can solve for n in the margin of error formula:
We should always round up when determining sample size ‚Äôn‚Äô from a margin of error. This is so E is never larger than we want.
How can we use what we‚Äôve learned about confidence intervals to answer questions such as‚Ä¶
These types of questions can be addressed through statistical hypothesis testing ‚Äî a decision-making process for evaluating claims about a population.
Let‚Äôs say I complain that I only earn $1,000 a year, and you believe me. Then, I invite you to tag along, and we climb into my BMW, drive to my private hangar to get in my private jet, and fly to my 3,000 square ft. apartment in downtown Paris. What is the first thing you think about my complaint? That I am a liar‚Ä¶
A statistical hypothesis is a postulation about a population parameter, typically Œº. There are two components to formalizing a hypothesis, a null and alternative hypothesis.
So our steps to hypothesis test go as follows:
A test statistic is a calculated value based on our collected from a sample and is compared against the a-priori threshold (critical value) to determine significance. Critical values act as the boundary separating the region of rejection and non-rejection (significance and non-significance). These are determined based on the relevant statistic table. We have discussed the z-table thus far but will cover other statistical tables in subsequent bootcamps. See the figure below for a visual representation of how a two-tailed (two rejection regions).
Part of our hypothesis testing is deciding which way we expect the relationship to be, and thus which ‚Äòtail‚Äô we will be investigating. There are 3 options available ‚Äî two-tailed, left-tailed and right-tailed. In a two-tailed test, the null hypothesis is rejected when the test statistic is either smaller OR larger (rejections areas on left AND right sides) than our critical statistic value (determined a-priori). This is synonymous with investigating ‚Äòis this different than the critical value regardless of direction?‚Äô. In a left-tailed test, the null hypothesis is rejected only when the test statistic is smaller (rejection area is on the left) than the critical statistic. Lastly, in a right-tailed test, the null hypothesis is rejected when the test statistic is larger (rejection area is on the right side of the bell curve) than the critical statistic. See the figure below:
If a hypothesis test is conducted to reject the null hypothesis (H‚ÇÄ), we can make the conclusion that: ‚Äúbased on our sample, the test result is statistically significant and there is sufficient evidence to support the alternative hypothesis (H‚ÇÅ) which can now be treated as true‚Äù. In reality, we rarely know the population parameter. Thus, if we do not reject the null hypothesis, we must be cautious and non overstate our findings by concluding that the data did not provide sufficient evidence to support the alternative hypothesis. For example, ‚ÄòThere is not sufficient evidence/information to determine that the null hypothesis is false.‚Äô
In a one-sample z-test, we are comparing a single sample to information from the population in which it arises. It is one of the most basic statistical tests we can test our hypothesis by following these 8 steps:
1. The null hypothesis is H0: Œº = Œº‚ÇÄ, and the alternative hypothesis is one of 3 possible options depending on whether direction matters, and if so which way:
2. Determine the directionality of the test ‚Äî which tail(s) you are investigating
3. Decide on a significance level, Œ±.
4. Based on alpha, find the associated critical values from the appropriate distribution table (these differ based on the kinds of ‚Äòtails‚Äô you have and should show the directionality at the top of the table)
5. Compute the test statistic based on our data
6. Compare our test statistic to our critical statical value ‚Äî is Zcalc >,<, ‚â† ?
7. Make the decision to reject or not reject the null hypothesis. If it falls in rejection region, reject otherwise accept the null
8. Interpret the findings‚Äî ‚Äòthere is sufficient evidence to support the rejection of the null‚Äô OR ‚Äòthere is not sufficient evidence to reject the null hypothesis‚Äô
Example. A doctor is interested in finding out whether a new asthma medication will have any undesirable side effects. Specifically, the doctor is concerned with the spO2 of their patients. Will the spO2 remain unchanged after the medication is administered to the patient? The doctor knows the average spO2 for an otherwise healthy population is 95%, the hypotheses for this case are:
This is called a two-tailed test (if H‚ÇÄ is rejected, Œº is not equal to 95, thus it can be either less than or greater than 95). We‚Äôll look at how to follow this up in a following bootcamp. What if the question was whether the spO2 decreases after the medication is administered?
This is notation for a left-tailed test (if H0 is rejected, Œº is considered to be less than 95). What if the question was whether the spO2 increases after the medication is administered?
This is the notation for a right-tailed test (if H0 is rejected, Œº is considered to be greater than 95).
Example. A climate researcher wishes to see if the mean number of days of >80 degrees Fahrenheit in the state of California is greater than 81. A sample of 30 towns in California are selected randomly has a mean of 83 days. At Œ± = 0.05, test the claim that the mean number of days of >80 degrees F is greater than 81 days. The standard deviation of the mean is 7 days.
Let‚Äôs check our assumptions: 1) we have obtained a random sample, 2) sample size is n=30, which is ‚â• 30, 3) the standard deviation of the parameter is known.
6. Compare the Zcalc and Zcrit. Since Zcalc = 1.56 < Zcrit = 1.65,
7. Zcalc does not fall into the reject region, therefore, we fail to reject the null hypothesis.
8. Interpret our findings. There is not enough evidence to support the claim that the mean number of days is greater than 81 days.
Example. The apartment association of Chicago reports that the average cost of rent for a 1 bedroom apartment downtown is $2,200. To see if the average cost at an individual apartment building is different, a renter selects a random sample of 35 apartments within the building and finds that average cost of a 1-BR apartment is $1,800. The standard deviation (œÉ) is $300. At Œ± = 0.01, can it be concluded that the average cost of a 1-BR at an individual apartment is different from $2,200?
Let‚Äôs check our assumptions: 1) we have obtained a random sample, 2) sample is 35, which is ‚â• 30, 3) the standard deviation of the population is provided.
4. Find the critical value, based on the z-table. Since Œ± = 0.01 and the test is a two-tailed test, the critical value is Zcrit(0.005) = ¬±2.58. Reject H0 if Zcalc > 2.58 or Zcalc < -2.58.
5. Compute the z-test statistic
6. Compare the calculated statistic against the one determined from step 2
7. Since Zcalc = -7.88 < -2.58, it falls into the rejection region, we reject H0.
8. Interpret our findings. There is sufficient evidence to support the claim that the average cost of a 1-BR apartment at that individual apartment building is different from $2,200. Specifically, it is CHEAPER than the parameter mean.
We‚Äôve covered how to gain confidence in our statistical estimates generated from our data through the use of confidence intervals. After reading this article you should have a firm grasp of the meaning of small versus large confidence intervals and the implications from an inference standpoint. In the engineering world we discuss tolerances on machined components and mathematical ‚Äòtolerances‚Äô are no exception. Here we usually quantify our mathematical estimates using the term repeatability. We can define repeatability as the ability to generate the same result consistently. If a metric or design does not have repeatability, it would produce scattered results (i.e. wider confidence intervals).
The next bootcamp will entail detailing the trade-off between type 1 and 2 errors, so stay tuned!
Previous boot camps in the series:
#1 Laying the Foundations#2 Center, Variation and Position#3 Probably‚Ä¶ Probability#4 Bayes, Fish, Goats and Cars#5 What is Normal
All images unless otherwise stated are created by the author.
[1] Allan Bluman, Statistics, E. Elementary Statistics.[2] Winchester, Simon. The perfectionists: how precision engineers created the modern world. HarperLuxe, 2018.
Additionally, if you like seeing articles like this and want unlimited access to my articles and all those supplied by Medium, consider signing up using my referral link below. Membership is $5(USD)/month; I make a small commission that in turn helps to fuel more content and articles!
medium.com
Prathamesh Nimkar
Aviral Bhardwaj
Radha Vats
Usman Haq
Ashesh Shah
Willie Wheeler
in
wwblog
Dwijesh Mishra
Little Dino
"
192,https://medium.com/@avikumart_/a-complete-guide-on-how-to-apply-for-data-science-jobs-to-stand-out-in-the-job-market-85a407ed69e0,A complete guide on HOW to apply for data science JOBS to stand out in the job market,"In this blog, Find a few tips to apply for data science jobs even if you are a student, professional, or have some working experience
A data science job application is an essential part of a job seeker's process. The application process is in itself a long and stressful one that can have a significant impact on your chances of getting through to the next round in the interview process. There are many aspects to this process that many job seekers don‚Äôt pay attention to and miss out on getting noticed by recruiters. However, we will be talking about techniques to apply for data science jobs and stand out in the crowd by using different keywords used in your resume and cover letter, to make you more likely to get noticed by recruiters.
How to apply for data science positions to stand out in the job market?
To stand out in the job market, one must learn how to apply for data science jobs effectively. You are referred to as a well-known data scientist, who is known for excellent work. It‚Äôs time to enhance your skills in applying for data science positions, which can be a daunting task sometimes.
This blog is adapted from @BowTied_Raptor‚Äôs article on ‚ÄòHow You Can Get an Edge When Applying‚Äô on his substack page.
If you are still in University, you should look at your school‚Äôs job board and start applying for jobs at the earliest
Sometimes Government job boards may land you some interview calls
You should still use the obvious job boards:
Indeed, Monster, Linkedin, Jooble, Glassdoor, Workopolis, Upwork (projects only), zip recruiter, angel list, etc‚Ä¶
If you‚Äôve already got some work experience, then you can reach out to some of your old colleagues, or some of your friends, and see if they can snoop around in their network and see if there is an opening.
Since you have some experience you can always hit up a few tech recruiters in your city, and see if they can get some interviews for you. Since recruiters make money by selling you to the company
Data Scientist, Data Analyst, Data, Python, SQL, R, Analytics, Analyst, A/B Testing, Data Science, data.table, pandas, Numpy, MS Excel
Machine Learning, AI, Artificial Intelligence, Big Data, Cloud, Keras, Tensorflow, Mlxtend, AWS, Sci-kit learn, ML pipeline
Summer, Winter, Term, Co-op, Student, Entry Level, Intern, Internship, co-op, Junior Data Analyst/Scientist‚Ä¶
To search-relevant keywords, you can use the keyword search tool word tracker, link here
In conclusion, I‚Äôd say make sure you apply for the maximum number of applications with the right keywords to quickly get the eye of recruiters to get your next Data science job.
In the end, I thank @BowTied_Raptor for writing a blog on applying for DS roles. you can refer to his blog on the above-attached link.
Emily Eros
in
SharedStreets
Tim Burns
in
Towards Data Science
Rashid Kazmi, Ph.D.
Michael Heine
in
The Outlier by Pattern
Simon
in
Team Sonnenwagen Aachen
R. Gupta
in
Geek Culture
Nabeel Ayyad
Peng Wang
in
Towards Data Science
"
193,https://medium.com/@anna.wu9222/sql-questions-with-detailed-answers-step-by-step-2459f6e110b,SQL Questions with Detailed Answers (Step-by-Step),"Return the result table in any order.
Difficulty level: Easy
Solution Overview:So, to solve the problem we need to use LEFT JOIN, WHERE clause. By the right use of these two, we can easily get the final result.
Solution Framework:In this problem, we have two tables with customer details and another with orders. All customers might order a product or not. We want to get the name of the customers who haven‚Äôt ordered anything yet. At first, we used the left join to join the Orders table with the Customers table.
Note: A left join takes all the values from the left table and only the common rows from the right table.
The left join was performed on the Customer table because we want all the Customers with their Orders. Now, as we have all the customers with their orders we will find some null CustomerId in the order table who haven‚Äôt ordered anything yet. So, we have used the where clause to get the records for that use, as the where clause filters out the rows or takes the rows that we want.
Then we have selected the name column from the Customers table as we only want the name of the customers.
Here we have at first selected all values from the Customers and Orders table by the * notation to see the values of each column. We can see the Customers table has the id and names of the customers for 4 customers. The Orders table has 2 orders with order id and customer id for 2 customers.
In this step, we have performed a left join to join the Orders table with the Customers table. The left join was performed on the Customer table because we want all the Customers with their Orders. Now, as we have all the Customers with their orders we will find some null CustomerId in the order table who haven‚Äôt ordered anything yet. In the output, we can see id 2 and 4 have Null values in the customerId.
In the last step, we have used the where clause to take only the Customers who have null customerId in the Orders table from step 2 Output.
Write an SQL query to report the second highest salary from the Employee table. If there is no second highest salary, the query should report null.
Difficulty level: Medium
In this problem, we have to find the Second Highest Salary from a table where we have the id and salary of the Employees. To do this, we have at first ordered the salary in descending order to get the highest salary in the first and take only the unique salaries by using DISTINCT argument.
So, we have the second Salary in Second place. To get the second salary from the second place we have used two SQL arguments. The first important one is the OFFSET. The OFFSET argument is used to identify the starting point to return rows from a result set.
Basically, it excludes the first set of records. So as we have written 1 after offset it will exclude the first row from the overall result. Then we have a LIMIT argument. The SQL LIMIT clause restricts how many rows are returned from a query.
So we will get only the first one after excluding the highest salary by OFFSET argument. So, essentially we are getting the Second Highest Salary. Then we use the IFNULL function because it returns null if there is no value. So, if there is no Second Highest Salary we will get null as a result.
In the first step, we have got all the unique salaries by the DISTINCT argument. Then ordered all the salaries by using ORDER BY and DESC arguments.
Then in the second step, we have used a LIMIT and OFFSET argument. Here the OFFSET argument will work first to exclude the first row i.e the Highest Salary. Then the LIMIT argument will work to get the first row after excluding the first row i.e. we will get the second row which holds the second highest salary.
In the third and last step, we have added the IFNULL function to get null values if there is no Second Highest Salary present in the result.
Write an SQL query to display the records with three or more rows with consecutive id‚Äôs, and the number of people is greater than or equal to 100 for each.
Return the result table ordered by visit_date in ascending order.
Difficulty level: Hard
This was definitely a difficult problem where we needed to use self join, a complex where clause, and needed to remove the duplicate rows using DISTINCT.
The output of this problem demands to have the id‚Äôs where the number of people will be greater than 100 and there should have at least 3 consecutive id‚Äôs.
So, the first trick of this problem is to join the table by itself three times as we have to check three consecutive IDs. Then we are using the where clause to take only the rows where the number of people>100.
Then, we are checking for the consecutiveness of ids. So taking the temporary table t1 in the left, middle, and right we are checking the difference between the id of the other tables which is helping us to check the consecutiveness of the ids. Then we used the DISTINCT clause to take only distinct rows as we might get duplicate rows for the self join.
Note: Here self join means joining one table with itself.
The first step is to get the days with 100 people and join this table with itself so that in the next step we can check for consecutiveness of the ids. From the output, we will see there are 6 days with more than 100 people. So there are 216 records in total after the self-join as there are 6 rows in the table, so joining in three times with each row will make it 6*6*6. The first 3 columns are from t1, the next 3 ones are from t2, and the last 3 are from t3.
Considering t1, t2 and t3 are identical, we can take one of them to consider what conditions we should add to filter the data and get the final result.
Taking t1 for example, it could exist in the beginning of the consecutive 3 days, or the middle, or the last.
So, these conditions will help us to check the consecutiveness of the id‚Äôs. We are making one id constant and checking whether the other 2 id have a difference of 1 and 2 with the constant id. If this is so, then we have got the consecutiveness.
After the second step we can see there are some duplicate rows that we should get rid of. So we have used DISTINCT to take only the unique rows from t1.
Originally published at https://bigtechinterviews.com on May 09, 2022.
ASecureCloud
Priyanka Jain
Zendodo Party
Toan Nguyen
Gianis Priovolos
in
Analytics Vidhya
Erica Xu
Najib Brimah
Michael Gruber
"
194,https://medium.com/@ibrahimzahir7/12-websites-youll-love-as-a-developer-1e4180d0a729,12 Websites You‚Äôll Love As A Developer,"Websites are now an essential tool for every business. And if you are a web developer and need to find new material or resources, then this is the place for you. As a developer, it is not easy to find a website that can provide all the resources and information that you need. This article will take you through 12 websites that you‚Äôll love as a developer.
Turn your code into beautiful images using this site. You can choose from a range of syntax colors, hide or show the background, and toggle between a dark and light window. If you are a developer you can easily create beautiful screenshots of source code through this app and share on various social apps such as instagram, twitter and facebook.
ray.so
Roadmap.sh is a community effort to create roadmaps, guides and other educational content to help guide the developers in picking up the path and guide their learnings. It contains community curated roadmaps, study plans, paths and resources for developers newly entered into this space. It started as a set of charts to guide the developers who are confused about what should learn next.
roadmap.sh
CodePen is a social development environment. It allows you to write code in the browser, and see the results of it as you build. You can build and deploy websites, show off your work, build test cases to learn and debug, and find inspiration. You can also browse and share work from other designers and developers in the front-end community.
Codepen Pro having Pair Program feature where you can use Collab Mode which allows more than one person to edit a Pen at the same time. This is real-time pair programming at its best, since the two (or more) people can be anywhere in the world and it‚Äôs not nearly as bandwidth-intensive as video.
codepen.io
It is the most common website all most all developers using in everyday work. Stack Overflow is a question-and-answer site for computer programming questions, especially those related to specific programming languages. It also offers professional development opportunities to its members through online courses, certificates, and job listings. Stack Overflow is a public Q&A platform that over 100 million people visit every month to ask questions, learn, and share technical knowledge.
stackoverflow.com
GitHub is a website that hosts Git repositories and provides social networking-like features for programmers to share their projects. Github is the complete developer platform to build, scale, and deliver secure software.
GitHub is where over 94 million developers shape the future of software, together. It Supercharge your development process. Github provides unlimited repositories, best-in-class version control, and the world‚Äôs most powerful open source community-so your team can work more efficiently together.
github.com
IconStore is a library of free, vector (SVG) icons created by talented designers to download for commercial use.
All the icons published on the IconStore are made to be used freely.
üü¢ Permissions:
iconstore.co
Use markdown editor and templates to easily create a ReadMe for your next projects. Easy way to create a README file using simple editor provided by the application. It allows you to quickly add and customize all the sections you need for your project‚Äôs readme.
readme.so
Lorem Picsum provides random or specific uploaded images as placeholders. Here you can view all the images Lorem Picsum provides. Developers need to specify a desired image size (width & height) at the end of the request URL.
picsum.photos
GitBook is a modern documentation platform where teams can document everything from products to internal knowledge bases and APIs. It makes it easy to research, plan and document products, from start to ship.
gitbook.com
Figmais a collaborative web application for interface design, with additional offline features enabled by desktop applications for macOS and Windows. Build an iterative design flow with live collaboration that keeps you in the loop whether you‚Äôre working in the office or remotely. This has enabled our product teams to ship new products faster and feel more confident in their decisions.
figma.com
Lottie is a JSON-based animation file format that enables designers to ship animations on any platform as easily as shipping static assets. They are small files that work on any device and can scale up or down without pixelation. LottieFiles lets you create, edit, test, collaborate and ship a Lottie in the easiest way possible. LottieFiles takes away the complexity from Motion Design.
lottiefiles.com
Millions of designers and agencies around the world showcase their portfolio work on Dribbble. Dribbbleis the world‚Äôs leading destination for designers to learn, share, grow, and get hired. Dribbble market place brings you creative projects to life starting from fonts, graphics, themes, photos, and templates designed, mockups and 3D models created by independent creators around the world.
dribbble.com
There you have it. Thanks for checking this out and do not forget to try these tools.
Originally published at https://letmefail.com on December 4, 2022.
PM Partners, World-Class Technology Consulting
Joseph Sciarrino
UMAR TABBSUM , COLUMBUS, Ohio
Rashi C
Jettnipith Thantong
Paul Bahro
billibala
Swarm Foundation
in
Ethereum Swarm
"
195,https://medium.com/the-ciso-den/must-read-books-for-cybersecurity-professionals-486f81075c83,Must read books for cybersecurity professionals,"The following books are not just a nice read, they give essential insights about topics that touch core issues of cybersecurity, while not being books about cybersecurity. These books changed my perspective and shaped quite a bit of my ability to analyze and provide cybersecurity solutions in environments where‚Ä¶
Pivotal Security
Roadrunnermailsupport
Noku.io
David Leichner, CMO at Cybellum
in
Authority Magazine
Princewill Celestine
SuperTechman
HaXeZ - Simplified Cybersecurity
The Daily Encrypt
"
196,https://medium.com/@mukesh.kr/learn-any-ml-topic-4-layered-inverted-onion-approach-e49c06d387fc,Learn any ML topic : 4-layered inverted onion approach,"Mastering any machine learning topic requires effort. A systematic approach is required to understand and master it. I follow this 4-layered inverted onion approach and have found it to be useful.
Intuition helps link the concept to something we already know and hence increases the chance of recalling the topic. Study materials which give intuition have explanations using animations, explain concepts using real life as an example.
The contents I usually follow are written byMathematics : 3Blue1BrownStatistics/DS: Cassie KozyrkovNLP: Jay AlammarDL/ML : Luis Serrano
The intention at this layer, is to have an understanding of standard terms and concepts used in topic.
Pick up material which cover the topic in expanded format. A expanded format will explain the topic and reinforce the knowledge with a coding example. Many a times code example, helps understand the concept much easily than mathematics explanation
If you prefer books, Groking series of books fall at this level. They have friendly explanation about the topic. For video tutorials, courses from udacity/coursera would be good pick. I do not prefer udemy at this stage, as creators may use non-standard terms for concepts which may lead to confusions.
This is time when we pick a book which is considered as the bible on the topic. These books introduce terms and concepts which are used and should be used in research paper, blogs, derivative books like grokking series.
The field of machine learning faces the issue of having many terms being used for the same concept due to it being influenced by various disciplines like mathematics, neurology , psychology and industrial concepts.
The topic treatment in these books are dense and sometimes involved. Read the concepts without too much focus on the mathematical formulations. The code understanding gained early will equip us to understand the mathematics better.Mathematics : ISL ‚Äî Gareth James et alDL : Deep Learning ‚Äî Ian Goodfellow et alRL : Reinforcement learning ‚Äî Sutton and Barto
This is often the most involved and may need many cups of black coffee. But having gained intuition and code understanding would have prepared and smoothen the transition to towards understanding this.
Well mathematics can be dry to consume, hence that layer might be acquired taste.
Hope this helps! Thanks for being with me till now. ‚≠êÔ∏è‚≠ê Subscribe Medium ! ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠ê Follow me on Linkedin ‚≠êÔ∏è‚≠ê
Adrian Yijie Xu
in
GradientCrescent
Yash Chauhan
Victor Sim
in
Hands-On Data Science
Mathieu Gautier
Sushant Kumar
in
Analytics Vidhya
Kelvin Jose
Ayush Singh
Dieter Jordens
in
Towards Data Science
"
197,https://medium.com/towards-data-science/how-i-would-learn-to-code-if-i-had-to-start-over-607428f14266,How I Would Learn to Code (If I Had to Start Over),"I recently got an amazing opportunity where a friend offered to coach me in jiu-jitsu in exchange for me teaching him how to code. I quickly realized teaching someone how to code, starting from scratch, was going to be a lot harder than I thought. After all, it has been almost 10 years since I wrote my own first line of code. This article is about the very specific and detailed plan I wrote for my friend to teach him the basics of coding for data science. Since I did all the work for him, I figured I would share all the details with you here as well!
There have been plenty of other resources similar to this related to how to start coding. Those are great, but I think this is a bit different because it is more focused on coding for data and uses a very specific real-world case study. Additionally, I suggest various free, online resources to further the ease of teaching yourself beginner coding skills. At the end, I highlight the 3 most important things any new programmer should know, so you definitely want to check that out.
If you prefer a video format, consider the following to learn more about this topic:
Let‚Äôs start with the language of choice to start learning. I know that if you are working with data, you generally have 2 options, either Python or R. Let‚Äôs quickly compare the two languages data-scientist style.
When looking at the Stack Overflow Developer Survey, we find that about 48% of respondents use Python while only about 5% use R. In industry, Python is a far more common language. Let‚Äôs now look more specifically at data scientists, not just all Stack Overflow users.
On Kaggle, my favorite platform for data science learning and projects, we can see in this notebook that around 84.2% of respondents use Python while only 20.5 % use R. Since these add up to more than 100%, we can assume that some data scientists are using both languages.
Finally, let‚Äôs look at these two languages over time. We can see in this other notebook that, from 2020 to 2021, Python has gained in adoption while R usage has had some attrition.
This is a no-brainer to me. I think we should choose to learn python; that language is more popular and growing. This isn‚Äôt to say R is bad. However, I think Python is growing and will be a more valuable skill to have for the job market. I also personally find it a bit more intuitive and flexible than R.
Now that we have chosen Python, what coding skills should my friend learn first? Variables, logic, functions, algorithms, classes? My mind was spinning. It took me a second, but I had a weird and chaotic flashback to my first days learning programming. I didn‚Äôt even know where to start with getting Python on my computer. I got super excited about teaching the functional part of programming and I completely forgot about the basics of setting up your environment for success.
The first thing I did was give my friend some of my favorite resources on choosing an environment and getting the right version of Python on his computer. I was actually a little bit torn on this: What IDE should he use? Does he even know what IDE stands for? For those who are new, IDE stands for integrated development environment. This is essentially just the text editor you use to write your code.
If you were writing an essay, you could choose to use Microsoft Word, Google Docs, Notepad, or maybe even a pen and paper. Just like these are all mediums for you to write your last minute paper, data scientists and software engineers also need places to write their code. These places are IDEs. Just like Microsoft Word has spell check and formatting to make sure you don‚Äôt make grammatical errors, many IDEs help you to ensure you have proper syntax in your code. Technically Microsoft Word could be considered an IDE, but I definitely don‚Äôt recommend using it as such unless you‚Äôre Joma Tech.
This is a list of the most commonly used IDEs in the data domain. As you can see, common IDEs for Python are VSCode, Jupyter Notebook, PyCharm, and Spyder.
Before anything else, I wanted to give my friend a decent understanding about how the development environments worked. This was something that I was totally confused about. When I was learning to code, I started with Python and I was doing it in an IDE called IDLE. IDLE was great for learning, but I literally have never once used it when writing code for work or a data project. I wanted to make sure he was using something that would be representative of what he would use in the real world.
For this specific friend, I recommended that he start learning the basics with the following video from freeCodeCamp. In the video, they show you how to set up the PyCharm environment and get comfortable using that. I think PyCharm is solid, and if you get familiar with that, you can easily pick up VScode, a Jupyter notebook, or Spyder pretty quickly. Again, I don‚Äôt think the IDE that you start with matters that much. What is more important is that you‚Äôre using one that people who actually write Python code into production use.
Ok, onto the coding basics. Surprisingly, I think this may be the one of the simplest parts of this whole process. You see, there is a common misconception that coding is trying to trick you. There is this conspiracy that learning to code is supposed to be really hard and scary. I would argue that it is almost the opposite. Coding languages, particularly Python, are actually made so that they are as intuitive as possible. They are designed to be as easy to use as possible while creating good results. If you change your perspective to look at them as intuitive, coding becomes significantly more fun and stress-free. This isn‚Äôt to say you won‚Äôt still get stuck; it is to say that when you get stuck, there is almost always a logical reason for it and a logical solution to resolve it.
Keeping with the theme of simplicity. At the most basic level, beginner coding can be broken down into 7 concepts. I‚Äôm going to give a very high-level overview here.
1. First, we start with data types. In Python, you can work with numbers, text, booleans, lists, or a host of data structures. It is important to get a good understanding of these first.
2. Next, I think it makes sense to get familiar with how variables work. This can vary by language, so just make sure you know how to assign and change variables in Python.
3. After variables, you should look into how logical operators work. These are very handy in your first few projects. If and else statements are the most important one, but you can definitely find use cases for others, especially if you‚Äôre working with data.
4. The fourth thing you need to understand is how loops work. One thing computers are great at is doing a simple task over and over again. You would be surprised at how useful these can be for your programming future.
5. After that, you should learn about functions. This sets the foundation for almost everything you do on your programming journey. Functions allow you to encapsulate code and re-use it. If you really think about it, good coding is about making as many reusable bits as possible. This is where that really begins.
6. At this point, you have a pretty good understanding of the key building blocks of Python. You should now learn a bit more about how some of the basic data structures work. I recommend taking a deeper dive into lists, dictionaries, and tuples. These will set the foundation for much of your data work later on.
7. Finally, you should start learning to work with libraries. The most important ones are pandas, NumPy, Scikit-learn and Matplotlib. These all give you additional tools to analyze, visualize and manipulate data. Pandas and NumPy also introduce some new data structures like arrays, series, and dataframes. If you have an understanding of how the more basic data structures work in python, you should be able to pick these up pretty quick!
And that is it. With those 7 things you have the basics of coding that you will need for working with data. If you‚Äôre a software engineer, you probably think I left something out. What I have purposefully omitted from this list is an explanation of classes. Classes are the foundation of object-oriented programming. Object-oriented programming is a programming model that organizes code around objects rather than functions or logic. Object-oriented programming is incredibly powerful, but honestly isn‚Äôt very relevant for the beginner stages of working with data. It is something you should absolutely explore on your own after you‚Äôve learned these other concepts well!
The below links are a few great resources for learning all of these concepts. Most of them are free, but I have also included my discount link for the course on 365 Data Science if you were looking for a resource to take you past just the coding and more towards more tangible data science skills.
One thing that I love about all of the courses that I linked to is that they have little mini-projects baked into the course work. I think project-based learning is the single most effective way to learn technical concepts after getting a basic foundation. I highly encourage you to think about how you can use code to build things that would be useful to you.
For my friend, the first project I am having him do is create a jiu-jitsu drill randomizer using Python. For training anything, it is important to reinforce your skills, but it can get boring working on the same basic drills everyday. I am having him make a program that randomly selects 2‚Äì3 drills that we should work on during our sessions from a larger list. This is a simple project, but something that is directly relevant and useful to our work. Comment below with a Python project you would like to do to hold yourself accountable!
Ok, so you‚Äôve learned the basics of Python. Now what? If you‚Äôre interested in learning more about data, I highly recommend that you do a data-related project. The single best place for that is Kaggle. If that is a bit too intimidating, you can follow along with one of the projects I‚Äôve done or one of the thousands of projects other people have done on Kaggle.
Many people lose steam at this step. They see the great work that other people have done and get discouraged. Honestly, I remember feeling this too. What I recommend doing is going through some of the basic notebooks like the Titanic dataset and just copying the code and running it on your own computer line by line. Many people think that this is ‚Äúcheating,‚Äù but it is actually a great way to learn. Even better, when you run it, make some changes to the code and see what happens. Maybe try changing the colors on a graph from blue to red or making a line thicker.
Remember when we talked about IDEs and I had mentioned Jupyter notebook? Kaggle uses something similar to that and lets you work with it online. This is great and free compute power if you have limited access to resources. If you want to work with data on your laptop or desktop, I recommend downloading Anaconda, which is a software package that lumps together all the relevant Python libraries and IDEs for data science into one bundle.
Finally, I have 3 tips to send you on your way. These are probably the most important things that a new coder can learn.
Thank you for reading this article. If you enjoyed this article, remember to follow me on Medium for more content like this and sign up for my newsletter to get weekly updates on my content creation and on additional learning resources on the data science industry! Also, consider supporting me and thousands of other writers by signing up for a membership.
Until next time, good luck on your data science journey!
Sainath Dekonda
DIGITALAX
KBG Blockchain Game Studios
Dustin D'Avignon
Rafi Ramadhana
Alex Mamo
in
Firebase Developers
Elango
in
SquareShift
Iyana Garry
"
198,https://medium.com/open-learning/9-courses-from-mitx-ranked-among-class-centrals-best-of-all-time-2022-list-82be10d1e158,9 courses from MITx ranked among Class Central‚Äôs ‚ÄúBest of All Time‚Äù 2022 list,"Class Central, a resource for curating and rating online courses, has recognized nine MITx courses among its top honors: the Best Online Courses of All Time list. MIT is one of the most-represented universities on the list, out of 250 courses based on 200,000+ learner reviews.
Introduction to Computer Science and Programming Using PythonAn introduction to computer science as a tool to solve real-world analytical problems using Python 3.5.
Taught by:John Guttag ‚Äî Dugald C. Jackson Professor of Computer Science and Electrical Engineering at MITEric Grimson ‚Äî Vice President for MIT Open Learning; Bernard Gordon Professor of Medical Engineering, Professor of Computer Science at MITAna Bell ‚Äî Senior Lecturer, Electrical Engineering and Computer Science at MIT; MITx Digital Learning Scientist, Electrical Engineering and Computer Science
A version of this course is also available on MIT OpenCourseWare.
The Analytics EdgeThrough inspiring examples and stories, discover the power of data and use analytics to provide an edge to your career and your life.
Taught by: Dimitris Bertsimas ‚Äî Boeing Leaders for Global Operations Professor of Management, Professor, Operations Research, and Associate Dean for Business Analytics at MIT
A version of this course is also available on MIT OpenCourseWare.
Introduction to Computational Thinking and Data ScienceThis course is an introduction to using computation to understand real-world phenomena.
Taught by:John Guttag ‚Äî Dugald C. Jackson Professor of Computer Science and Electrical Engineering at MITEric Grimson ‚Äî Vice President for MIT Open Learning; Bernard Gordon Professor of Medical Engineering, Professor of Computer Science at MITAna Bell ‚Äî Senior Lecturer, Electrical Engineering and Computer Science at MIT; MITx Digital Learning Scientist, Electrical Engineering and Computer Science
Introduction to Biology: The Secret of LifeExplore the secret of life through the basics of biochemistry, genetics, molecular biology, recombinant DNA, genomics and rational medicine.
Taught by: Eric S. Lander ‚Äî Founding Director of the Broad Institute; Professor of Biology at MITMichelle Mischke ‚Äî Instructor, Department of Biology at MITGraham Walker ‚Äî Howard Hughes Medical Institute Investigator; American Cancer Society Professor of Biology at MITMary Ellen Wiltrout ‚Äî Director of Blended and Online Initiatives; Lecturer, Department of Biology at MIT and Digital Learning Lab Scientist, MITx
Molecular Biology: DNA Replication and RepairAn in-depth adventure through DNA replication and repair to strengthen your scientific thinking and experimental design skills.
Taught by:Stephen P. Bell ‚Äî Howard Hughes Medical Institute Investigator; Uncas and Helen Whitaker Professor of Biology at MITTania A. Baker ‚Äî Howard Hughes Medical Institute Investigator; E. C. Whitehead Professor of Biology at MITMary Ellen Wiltrout ‚Äî Director of Blended and Online Initiatives; Lecturer, Department of Biology at MIT; MITx Digital Learning Lab Scientist, Biology
A version of this course is also available on MIT OpenCourseWare.
Biochemistry: Biomolecules, Methods, and MechanismsEnhance your scientific thinking and data analysis skills with this in-depth adventure through biochemistry.
Taught by:Michael Yaffe ‚Äî David H. Koch Professor of Science, Professor of Biology and Biological Engineering at MIT; Director, MIT Center for Precision Cancer Medicine Mary Ellen Wiltrout ‚Äî Director of Blended and Online Initiatives; Lecturer, Department of Biology at MIT; MITx Digital Learning Lab Scientist, BiologyDarcy Gordon ‚Äî Instructor of Blended and Online Initiatives, Department of Biology at MIT; MITx Digital Learning Lab Scientist, Biology
Check out the second half of this course on MIT OpenCourseWare.
Probability: The Science of Uncertainty and DataBuild foundational knowledge of data science with this introduction to probabilistic models, including random processes and the basic elements of statistical inference. Part of the MITx MicroMasters Program in Statistics and Data Science.
Taught by:John Tsitsiklis ‚Äî Clarence J. LeBel Professor in Electrical Engineering and Computer Science at MITPatrick Jaillet ‚Äî Dugald C. Jackson Professor, Electrical Engineering and Computer Science at MIT; Codirector of the MIT Operations Research Center at MIT SloanDimitris Bertsimas ‚Äî Boeing Leaders for Global Operations Professor of Management, Professor, Operations Research, and Associate Dean for Business Analytics at MITKarene Chu ‚ÄîAssistant Director of Education and Digital Learning Scientist for MITx MicroMasters Program in Statistics and Data Science; Research Scientist at MIT
A version of this course is also available on MIT OpenCourseWare.
AP¬Æ MicroeconomicsAn overview of introductory microeconomics. Learn the key principles of economics and how to apply them to the real world ‚Äî and the AP¬Æ exam.
Taught by:Jonathan Gruber ‚Äî Ford Professor of Economics at MIT
Principles of Microeconomics, a version of this course without the AP¬Æ exam applications, is available on MIT OpenCourseWare (OCW).
Listen to the ‚ÄúThinking Like an Economist‚Äù episode of OCW‚Äôs Chalk Radio podcast to learn how Prof. Gruber engages students with accessible real world examples.
Circuits and Electronics: Basic Circuit AnalysisLearn techniques that are foundational to the design of microchips used in smartphones, self-driving cars, computers, and the Internet.
Taught by:Anant Agarwal ‚Äî Chief Open Education Officer of 2U/edX; Professor of Electrical Engineering and Computer Science at MITGerald Sussman ‚Äî Panasonic Professor of Electrical Engineering at MITChris Terman ‚Äî Senior Lecturer Emeritus in Electrical Engineering and Computer Science at MIT
A version of this course is also available on MIT OpenCourseWare.
Technology Trends
amruta kulkarni
in
Predictive model to analyze ‚ÄòResponsiveness of customers to Starbucks offers‚Äô
Oskar Eichler
in
The Songstats Lab
Alexey Poddiachyi
in
Towards Data Science
Vivienne DiFrancesco
in
Towards Data Science
Technology Trends
Kavika Roy
in
DataToBiz
rajesh gupta
"
199,https://medium.com/@sedefftaskin92/feature-selection-in-machine-learning-motivation-4c2fdf6e899d,Feature Selection in Machine Learning: Motivation,"This short article is the first one of a series that explains feature selection techniques. I have started with a friendly introduction and presented the powerful reasons behind the feature selection process. In the coming articles, I am going to mention each feature selection method in detail.
Feature selection is the process of selecting a subset of relevant features (variables, predictors) from the total features of a level in a data set to build machine learning algorithms. Given an initial data set which typically contains a big number of variables, what can we do to select the pool of features that are important at the time of making predictions? There are a variety of procedures or techniques that we can apply as part of the process of feature selection.
Before diving into feature selection techniques, why do we select features, to begin with? Here are some reasons.
What is a feature selection procedure and how does it work?
A feature selection algorithm can be seen as the combination of a search technique for proposing new feature subsets, along with an evolution measure that scores the different feature subsets. Ideally, a feature selection method will search through all the possible subsets of feature combinations that can be obtained from a given data set, and find the feature combination that produces the best machine learning model performance. In practice, this is typically not an option due to computational cost. In addition, different subsets of features may produce an optimal performance for different machine learning algorithms. This means there is not only one subset of features but potentially many subsets of optimal features depending on the machine learning algorithm that we intend to use. Therefore throughout the years, very many different methods to select features have been developed to try and accommodate as many caveats and limitations as possible. Throughout this article series, I will describe very many different methods for feature selection, what the advantages and disadvantages are, and how to implement them in practice using business data sets examples.
simpledataflow.com
Step towards Machine Learning
Apneas
Florian Huber
in
Netherlands eScience Center
Emmanuel Onwuegbusi
in
DataDrivenInvestor
Connor Shorten
in
Towards Data Science
Ashrafur Rahman
Dmitry Yakushechkin
in
dida Machine Learning
"
200,https://medium.com/towards-data-science/modeling-starbucks-waiting-time-using-markov-chains-with-python-ab267670d02c,"Modeling Starbucks Waiting Time Using Markov Chains, with Python","I‚Äôm from Italy and it is safe to say that for us coffee is a religion. We drink coffee to socialize, we drink coffee to wake up in the morning, we drink coffee after lunch, and we drink coffee after dinner. When you haven‚Äôt seen a friend in a while we say
‚ÄúVieni a prenderti un caff√®‚Äù
Which means
‚ÄúCome here and let‚Äôs drink a cup of coffee‚Äù
I live in America and the approach of American people to coffee is different. I drink to take away coffee when I go to work. I drink coffee when I work. I drink coffee while I watch a movie. American people don‚Äôt drink ‚Äúespresso‚Äù but they enjoy a long time that it takes to drink the whole big cup of coffee. One more thing: there are multiple kinds of coffee!
If you get in a Starbucks you see maybe a hundred possible coffee you can get. It can be black, it can be a macchiato, it can be a latte, it can be a Frappuccino, it can be a lot of other things which I don‚Äôt even know. üòÖ
There are some very easy-to-make cups of coffee and there are more complex ones, that require more time to be made. So let‚Äôs say you are in line for a coffee at Starbucks. If there are 3 people in front of you and they all order black coffee, you will probably have to wait something like 3 minutes before getting your order.
Nonetheless, if they order an ‚Äúextra whipped cream caramel macchiato with sprinkles and cinnamon with soy milk‚Äù‚Ä¶ well, that can maybe double your waiting time, or at the very least you would have to wait a couple of extra minutes.
So the question is‚Ä¶
‚ÄúHow much time do I have to wait before I get my coffee and I can go writing an article about how much time I have to wait before I get my coffee?‚Äù
Of course, we have no idea what other people are going to order, so this is a probabilistic problem (or if you want a stochastic process). So how do we solve it?
A doable approach is to build a Markov chain. In particular, what we will need is a Time-Dependent Markov Chain.
Let‚Äôs build the problem!
Let‚Äôs start by giving a theoretical introduction to the problem and setting it right.
Let‚Äôs start with the simplest case. We get inside our Starbucks and we have to order our coffee. Let‚Äôs say Mathematically speaking, we can be in one of these three states.
The first state (O) is the one where we order our coffee. The second one (M) is the one where we already ordered our coffee and are waiting to get it. It‚Äôs M cause they are Making a cup of coffee. Then you get the coffee and your transit towards the Leaving state. This means that your coffee is ready and you are good to go.
Great. Now, what are the possible transitions?
Now how do we formalize it?
What is the assumption of the Markov Chain? The assumption is the following:
‚ÄúThe probability of being in the next state only depends on your current state‚Äù
For example:
The probability of being in the state Leaving in the time step t = 5 only depends on the fact that you are in the state Making in the time step t = 4.
Let‚Äôs formalize it:
In the notation above we have that the probability of being, at time t, in the space s_t (O, M, L) is only dependent on the state that we are at time t-1.
The thing that we need to keep in mind in our specific experiment is that this probability has to be time-dependent too. This is the case, because, of course, if I have waited 5 minutes the probability of leaving in the next minute is higher than the probability of leaving if I waited for 1 minute only.
So that means that:
That is exactly the concept we were talking about before.
Now, of course, it‚Äôs not only us in Starbucks. There are a lot of other customers too! So we will complicate this setup in a minute. But what you see above is the starting point for everything that we will do.
Let‚Äôs get this started! ü§©
Let‚Äôs make the simplest example possible. We know what drink to get and we are the only ones in the coffee shop. Let‚Äôs say we want a caramel macchiato. All recipes suggest that it takes 5 minutes. Let‚Äôs say that it takes 30 seconds to order and pay. So the total waiting time would be 5.30 minutes. But let‚Äôs go further. Let‚Äôs say that 5 minutes is the average starting time. Let‚Äôs also say that we could have our coffee ready in 4 minutes or 6:
Great. Now let‚Äôs say that our time tick is 30 seconds (0.5 minutes). After 30 seconds it is very unluckily that we would have our caramel macchiato. After 8 minutes it is very unluckily that we would still be waiting.
Let‚Äôs implement this. Let‚Äôs start by importing some libraries:
Let‚Äôs define our states:
Great. Now let‚Äôs run the whole process described above. We will do that using this function:
This is one example:
Now, let‚Äôs increase the complexity of the model and make it more realistic. Let‚Äôs say we don‚Äôt know what that specific customer wants. This is way more realistic because we have more than 150 drinks in a Starbucks and they may require a different waiting time.
Now the Markov Chain looks like this
The difference between the previous one is that we have a probability distribution, which we will call:
In particular, this is a probability distribution over all the possible drinks we can get, which we will call:
For example:
So that we can say:
From this Kaggle dataset (CC0: Public Domain) we can import all the Starbucks drinks:
We don‚Äôt need the other columns. Now, we don‚Äôt know what is the most chosen beverage, let‚Äôs just make a fake distribution for our purposes:
So we extract values from 50 to 100, then we normalize this using the sum so that we have a probability vector.
So we have our probability distribution. What do we do with it? Well, this probability distribution will make an order for us: we will just sample out of the distribution. Then we have to assume that each coffee requires a different time to be made.
Now, objectively, there is not that big of a difference. Let‚Äôs say it will make 3 minutes (as a mean value) for the ‚Äúsimplest‚Äù coffee (let‚Äôs say that it‚Äôs a black coffee I guess‚Ä¶‚òïÔ∏è) and 6 minutes (as a mean value) for the most complex coffee to be made (let‚Äôs say the extra whipped cream caramel macchiato with sprinkles and cinnamon with soy milk üôÉ).
Let‚Äôs play with different variances as well (from 0.1 to 1).
So as we see before we have the mean value that can go from 3 to 6, so you can have an average waiting time that can go to 3 to 6 minutes, and it happens that, sometimes, you have more uncertainty than other times. For example, it is possible that you have a largest variance when you are drinking coffee ‚ÄúA‚Äù.
Let‚Äôs implement the following situation:
Now, I believe that at this point we can make extract some meaningful information. For example‚Ä¶
‚ÄúHow much time does it usually take to grab a whatever coffee at Starbucks?‚Äù
This question is pretty generic, so we might as well answer with some statistics*:
*I changed the function a little bit removing all the ‚Äúprint‚Äù so that my computer doesn‚Äôt explode. ü•≤
Great. Now we have multiple customers. What do we do? The situation looks like this:
Now, it is unrealistic to think that we have to wait that one coffee is made before they start making yours. Usually you have 3,4 barista at Starbucks.
This complicates the code a little bit, because we need to keep track of the fact that all the baristas are actually busy or not. For this reason, the function that we will consider is a function of the number of customers and the number of baristas.
That‚Äôs the function that does so:
Ok, so let‚Äôs test a case when we have 4 customers and 2 baristas.
So as you can see in the first two cases we have the baristas that are free, and they wait, respectively, seven and 6.5 minutes*.
*Note! That is not a mistake! It‚Äôs just means that the second coffee is faster than the first coffee+the ordering time. It makes sense if you think about it. If you order a complex coffee and I just order a latte, I might wait less than you even if you ordered before me.
Then the baristas are busy. So we have to wait an additional 6 minutes before I can actually order. (I could make it more realistic and subtract the 1 minute of the ordering time, but you get the idea right?)
By the time the fourth customer orders, there is one free barista, so he doesn‚Äôt have to wait that long. He just waits 5.5 minutes.
Notice how complex this model can become and how interesting the situation can be. This is a case with 10 customers and 5 baristas:
In this article, we discussed a general problem of waiting time in a line. There are of course multiple scenarios:
These three scenarios are of course more and more complex. For this reason, the Markov Chain gets more and more complicated as we saw earlier.
We could of course complicate this model a lot. We could consider the fact that machines can be broken. We can consider a shortage of baristas. We can consider the fact that the number of baristas changes with time. We can consider that some baristas are faster than others. And many more possibilities.
This starting point can be used to complicate the model in pretty much all the ways you want. Just keep in mind that we are using the tool of a Time-Dependent Markov Chain and just build from there üòé
If you liked the article and you want to know more about Machine Learning, or you just want to ask me something you can:
A. Follow me on Linkedin, where I publish all my storiesB. Subscribe to my newsletter. It will keep you updated about new stories and give you the chance to text me to receive all the corrections or doubts you may have.C. Become a referred member, so you won‚Äôt have any ‚Äúmaximum number of stories for the month‚Äù and you can read whatever I (and thousands of other Machine Learning and Data Science top writers) write about the newest technology available.
Pooshan Singh
Jorge Pe√±alva
in
Building Lang.ai
Swapnil Kangralkar
in
Becoming Human: Artificial Intelligence Magazine
T.J. Kyner
in
Towards Data Science
Justin Chan, Ph.D.
in
DataDrivenInvestor
Sophie Zoria
in
Analytics Vidhya
Nupur Jain
Anh
"
201,https://medium.com/@jonas-schroeder/data-scientist-turning-quant-i-why-im-becoming-an-algo-trader-28e852f9cfc4,Data Scientist turning Quant (I)‚Äî Why I became an Algo Trader,"In short, I am a data scientist by trade and my investment portfolio turned from great, thanks to the 10-year bull market rally, to not-so-great. I am naive and assume that using the tools I use every day in my job could have prevented my returns to go down the gutter.
My quest is to learn from my mistakes and to investigate if I can predict the stock market‚Ä¶
Jingles (Hong Jing)
in
Towards Data Science
Victor Morgante
in
Towards Data Science
Abhilasha Munjal
Milvus
in
Vector Database for AI
Milvus
in
Vector Database for AI
Truepill
in
Truepill Insights
Md Kamaruzzaman
in
Towards Data Science
Manas Acharekar
"
202,https://medium.com/@b.thusharmarvel97/stock-price-prediction-using-machine-learning-algorithms-ecc956103380,Stock price prediction using Machine Learning algorithms.,"In this research we used machine learning algorithms to predict the future price values of stock by using its historic data. This is a simple beginner level project where we extracted few features mainly using some of the stock technical indicators along with closing and date values to predict future values.
Web app
Stock price prediction is the process of predicting future stock values. This prediction depends on various internal and external factors which contribute to changes in supply and demand. Investing in stocks without researching its history patterns may lead to loss. So a proper system is needed to allow all level traders to get better insights of stock data and forecast its future values from its historical data. This can be achieved using progressive machine learning algorithms for learning those hidden patterns.
In this project, initial research was conducted on one particular stock. Later same findings was implemented to other stocks too. Through analysis, 40 new features were extracted. After that, data was trained with multiple baseline models. Its was found that LSTM performed better for stock data. So for 4 different stock, 4 different LSTM models were trained. The error (mean absolute error) found for 100 days prediction for APPLE,GOOGLE, AMAZON,TESLA was 3.746, 4.071, 3.27 and 19.75 respectively. So finally 4 different LSTM models were trained and deployed using streamlit . Complete machine learning pipeline is deployed in cloud for real time prediction for end users using Streamlit.
The dataset was mainly downloaded from yahoo finance. For real-time prediction/testing of the model, stock values can be extracted through Yahoo Finance API.
For this analysis APPLE ,GOOGLE, TESLA and AMAZON stock historical data over 15 years is used, which contains seven records, including date, opening value of the stock, closing value of the stock, high and low values of stock that day, the adj closing value of the stock, and the volume of stocks that had been traded that day.
The dataset was already pre processed, there was no any null values or outliers in it, so I directly moved to exploratory analysis and feature extraction part.
[ It is highly recommended that for a good accuracy, separate analysis of each stock is preferred. Because the impact of features would vary based on stocks historic data]
Some feature extractions done under this research are
Moving Average ‚Äî MA
A moving average (MA) is a widely used indicator in technical analysis that helps smooth out price action by filtering out the ‚Äúnoise‚Äù from random price fluctuations. It is nothing but taking average of previous x values and considering the average value as current value. It is a trend-following, or lagging, indicator because it is based on past prices. For this analysis, we have created 3 variables using moving average technique using 10,20 and 50 days respectively.
Daily return.
Daily return is calculated using change in daily changes of the stock, and not just its absolute value. It is percentage change between the current and a prior element . Using Pandas library, it is easy to retrieve daily returns. [panads .pct_change() ]
Additional features using difference between open, close, high,low values
A Bollinger Band is a technical analysis tool defined by a set of lines plotted two standard deviations (positively and negatively) away from a simple moving average (SMA) of the stock‚Äôs price. Main Components of a Bollinger Bands
Upper Band: The upper band is simply two standard deviations above the moving average of a stock‚Äôs price.
Middle Band: The middle band is simply the moving average of the stock‚Äôs price.
Lower Band: Two standard deviations below the moving average is the lower band.
We have taken 20 days as window size. And calculated moving average and standard deviation for that particular window size. Upper band is moving average plus 2 times standard deviation for that particular window size. Whereas Lower band is moving average minus 2 times standard deviation for that particular window size.
Recent price values, will have some information regarding change in price values, so we added few new features, that are 1,2,5,10 th day shift close price and 1 day shift open price
An exponential moving average (EMA) is a type of moving average (MA) that places a greater weight and significance on the most recent data points. The exponential moving average is also referred to as the exponentially weighted moving average. It is a process in which recent data points gets larger weights over older data points. The weights will be exponentially distributed to all data points. Simple moving average (SMA) assigns equal weights to all price values whereas EMA assigns higher weights to recent data point and weight value will just exponentially drops towards older data points.
In this project, we have taken a window size of 50 for computing exponential moving average value. We used span values as 10, 20 and 50. Span is a variable responsible for assigning weights to data points.
Moving average convergence divergence (MACD) is a trend following momentum indicator. The value tells the relationship between two moving averages of a stock price.
MACD is calculated by subtracting the long-term EMA from the short-term EMA. A nine-day EMA of the MACD is called the signal line. This single line value (MACD_EMA) which will work as a trigger for buy and sell signals.
Traders use these technique to buy the stocks when the MACD crosses above its signal line and sell the stock when the MACD crosses below the signal line.So we added these features.
A new variable ROC is relative percentage change in stock values. Here we have compared last stock value with 10 day back value.
It was also noticed that, date value is also impacting on stock value. So multiple features are extracted from current date.
Features extracted are :
Day, Day of Week, Day of year, Week, Is_month_end, Is_month_start, Is_quarter_end, Is_quarter_start, Is_year_end, Is_year_start, Is_leap_year, Year, Month, Is_Monday, Is_Tuesday, Is_Wednesday, Is_Thursday, Is_Friday
Finally we have now 48 features (excluding date and close price)
Since we need to predict the future values for n number of days. Its difficult to track all open, low, high, volume features. So to make the flow easy we have ignored all features under open, low, high, volume. So finally we ended up with 36 features.
In this stage our main task is to select the best model. So we trained a multiple models and selected the best one.
In this research we tried multiple models
Linear regression is a simple supervised learning technique. It is nothing but finding the exact linear relationship between dependent and independent variables. LR tries to fit a line that passes through maximum number of points while minimizing the squared distance of the points to the fitted line values.
Ridge regression is nothing but linear regression with l1 regularization.
KNN is an instance-based or lazy supervised learning algorithm. As the name indicates it takes k number of data points nearest to it and for classification task it predicts more often value within its neighbour and for regression task it predicts just simple/weighted average value of its neighbour points. Here we have used k = 5.
Random forest is the supervised machine learning algorithm used for both classification and regression task. These comes under CART models . Base models are decision trees. Decision trees are simple ml models which just uses conditional statements . It tries all possible splits within the variable and between the variable.
It just considers the split in such a way that its error is less. Building multiple Decision tree models with random set of variables is called random forest model.
XG boost is ensemble models. These are sequential model. Which means for every step model learns from its mistake did it in previous prediction. The base models for this model is also decision tree.
LSTM a type of artificial neural network capable of learning order dependence in sequence prediction problems. It is a model with complex operations to learn sequential information.
For LSTM model, data points are normalized using min max scaler ranging from 0 to 1.
Loss function ‚Äî mean absolute error
Optimizer ‚Äî adam
It is seen that, LSTM model has better capability to learn sequential time series information than other models. So in this project we will be using LSTM model. Error (mean absolute error) in predicting 100 upcoming days stock values is 1.123. It‚Äôs not bad. But still there is a room to improve the accuracy. So we did advanced feature engineering and added 4 new features namely.
Autocorrelation is a degree of similarity between a given time series data verses lagged version of itself over successive time intervals. It is similar to correlation between, same time series data (original and lagged version) .
It tells about the relationship between current value and its past value.
It has a range similar to correlation (-1 to 1), where +1 indicates positive correlation and -1 represents negative correlation.Traders can use autocorrelation to measure how much influence past prices for a security have on its future price.
Here brown rectangle is current version of time series data (ùë¶t) where as green rectangle represents lagged version of time series data (ùë¶t ‚Äî k ). From above concept 2 new features are been added based on lag values (1 and 2).
This smoothing technique is also called as triple exponential smoothing. The concept is just simple based on exponential smoothing.A gradual increase/decrease in data values as time passes starting from any point in time is referred to as trend
When a trend repeats itself periodically from time to time is called as seasonality The Holt-Winters method is a combination of 3 smoothing techniques :
Holt‚Äôs Exponential Smoothing (HES): Holt‚Äôs exponential smoothing is the further extension to SES by adding trend component to the update equation.
Winter‚Äôs Exponential Smoothing (WES): Winter‚Äôs exponential smoothing is an extension to Holt‚Äôs exponential smoothing that includes seasonality component.
Additive models -These are the models that consider that the data has been generated using the following equation: Data = Seasonality + Trend + Residuals
Multiplicative models -These models consider that data has been generated using the following equation: Data = Seasonality X Trend X Residuals.
where residuals are the values left when the trend & seasonality have been removed from any time series
From the graph, it is noticed that, there is no seasonality in our data, but has trend. So, we can ignore the seasonality term. So from these concept 2 more new features (additive and multiplicative) are added considering only trend component.
After trying multiple approaches we concluded with best model by varying optimizers, hyper parameters, advanced feature engineering etc.
Model is trained for 500 epochs with adam as optimizer and mean absoluter error as loss function
Below graph shows the loss curve over time during training the model.Both training loss and validation loss are reducing over time. Through this graph, we can conclude that, there is no overfitting occurring while training.
100 days prediction error for validation data.ÔÇ∑ ÔÇ∑
Mean absolute error : 1.046
Mean absolute percentage error : 0.0404
Mean squared error : 1.569
We have trained the model for APPLE stock. Now using the same procedure we train other 3 models for GOOGLE, AMAZON,TESLA stocks.
The final phase of this project is model deployment and productionization. So far we have trained 4 different stock models. Here we are deploying our stock prediction pipeline into real time production system.. Final aim of any research is to make some tasks ease to user. For that, the developed models should be made available to the end user. So model deployment and productionization is very important. In this phase, web app is created using streamlit and whole pipeline is deployed into streamlit cloud through github integration. The model is deployed in real world scenario where it takes continuous raw input (stock data) from yahoo finance and forecast the values.
4 models specific to apple, amazon, google, tesla stocks are trained and deployed in the cloud.
In this step, web app is created using streamlit. Streamlit is an open source app framework in Python language. It helps in creating web apps for data science and machine learning models. It helps in creating a good user interface. All we need to do is to write python code, which internally processes and builds a web app.
Code flow in web app.
Want to try the demo app
In this work we have done research on application of machine learning in the field of Stock market. A detailed analysis was conducted on one particular stock data through which 40 new features were extracted.
After complete data processing and feature extraction, the data was trained with different ML models such as linear regression, L1 and L2 regression, Random forest, XG Boost, LSTM. Out of which LSTM model performed well for time series data. So for 4 different stock, 4 different LSTM models were trained. The error (mean absolute error) found for 100 days prediction for APPLE,GOOGLE, AMAZON,TESLA was 3.746, 4.071, 3.27 and 19.75 respectively.
Complete machine learning pipeline is deployed in cloud for real time prediction for end users using Streamlit.
Future works are, so far EDA was done based on only one stock (APPLE). So selected features, model hyper parameters are all based on APPLE stock. That‚Äôs why the errors on the other stocks are greater than APPLE stock model. A separate analysis for each stock with best model hyper parameter will increase the accuracy of prediction for other stocks too. Future work is to optimize the models in a better way by integrating stock news data as a new feature.
References
Code will be uploaded soon
Please a clap/follow if the content is usefulThank you.
Nikita Butakov
Krishnaraj Rajagopal
Pei Ying Chin
Nuzhi Meyen
Yervant
Adrian Hartanto
in
Bina Nusantara IT Division
GeoAlert
in
Geoalert platform
Jeff D‚ÄôAmato
"
203,https://medium.com/momentumgroup/bubble-ios-new-dedicated-plans-present-a-compelling-alternative-to-expensive-saas-and-enterprise-74b8e374de83,"Bubble.io‚Äôs new Dedicated plans present a compelling alternative to expensive SAAS and enterprise No-Code, Low-Code incumbents for CTO/CIOs","I‚Äôve recently completed several calls with Bubble‚Äôs enterprise sales team and I‚Äôm impressed with the direction the product and company is taking.
Bubble‚Äôs Dedicated plan gives funded startups, small-medium businesses, and large enterprises several key features that make Bubble a cost-competitive and attractive development environment to build no-code and low-code (NCLC) apps.
A summary of the Dedicated plan, highlighting improved capabilities for enterprise organizations can be found here.
Bubble is known in the ecosystem as the most powerful and flexible consumer-targeted no-code platform, but is yet to gain a significant market share of the enterprise LCNC market for its powerful visual web application builder.
Bubble Inc is actively addressing key concerns held by CTO/CIOs at enterprise organizations including:
It is pleasing to hear that the version control and multi-developer collaboration features are receiving an imminent uplift in early 2023, while Bubble is actively working towards SOC2 compliance in the same timeline.
For CTOs/CIOs in enterprise organizations considering an NCLC platform to rapidly build internal tools and external facing web applications, Bubble‚Äôs benefits can be summarised as follows:
These percentages are mine based on multiple case studies.
In short, Bubble is an emerging tool for organizations of all sizes, and enterprize organizations have much to gain from adopting LCNC tools broadly and Bubble in particular. Bubble provides robust flexibility and scalability both in terms of its hardware configurability and its organizational capabilities.
Bubble is an ideal platform to build customised solutions including but not limited to:
In conclusion, CTOs/CIOs should consider Bubble as the go-to solution in 2023 and beyond, before returning to incumbent and expensive LCNC, customizable SAAS platforms, or full custom code solutions.
Bubble is poised to become the solution of choice globally for the rapid development of internal or external web applications and integrations with other web services.
Will Ericksson is the Managing Director of MomentumGroup.tech, who provides scalable Bubble.io development and training services to clients internationally. Contact hello@momentumgroup.tech for more information.
Daniel Cook
Mohamed Hijazi
Tosny
Balaji Dharma
in
Dev Genius
Lija G
Bhuvanesh
in
Searce
Boik Su
Tom Hamilton
in
Streamr
"
204,https://medium.com/appsilon-data-science/r-ggmap-how-to-visualize-spatial-data-in-r-c46dcf970095,R ggmap ‚Äî How to Visualize Spatial Data in R,"Spatial data is always tougher to visualize than it seems at first. Well, not anymore. R ggmap is an amazing package you can start using almost instantly, after some light configuration. It‚Ä¶
Richard Emeana
Michael Grierson
Mark Ai Code
Dewi Oktavia Citra
Bharath K
in
Towards Data Science
NYU Center for Data Science
Dario Radeƒçiƒá
in
Towards Data Science
Brandon Griffin
"
205,https://medium.com/@daudamubarolanrewaju/getting-started-with-power-bi-1-bd4ef0fd694a,Getting Started With Power BI (1),"Microsoft Power BI is a business intelligence tool created by Microsoft that helps understand trends and derive insights from your data so that you can make tactical and strategic business decisions. They also help you identify patterns in the mountains of data your business builds up
Power BI is a collection of software services, apps, and connectors that work together to turn your unrelated sources of data into coherent, visually immersive, and interactive insights. ‚Äî Microsoft Learn
Microsoft Power BI is used to run reports and surface insights based on data. Power BI can connect to a wide range of data sets from different sources i.e Excel Spreadsheet, Database, Web, and many more.
Microsoft Power BI is a business analytics tool suite that excels in data visualization. It allows users to identify trends in real-time and has brand-new connectors that allow you to up your game in campaigns. Because it has a web-based feature, Microsoft Power BI can be accessed from pretty much anywhere. It also allows users to integrate apps and deliver reports in real-time dashboards. The reports and visuals generated from this data can then be shared with other users.
Power BI helps users see not only what‚Äôs happened in the past and what‚Äôs happening in the present, but also what might happen in the future. It is infused with machine learning capabilities, meaning it can spot patterns in data and use those patterns to make informed predictions and run ‚Äúwhat if‚Äù scenarios. These estimates allow users to generate forecasts and prepare themselves to meet future demand and other key metrics.
Power BI has a wide range of versions to satisfy business needs.
As stated earlier in the Power BI versions, Power BI Desktop is only the free version you can install on your local computer provided by Microsoft.
To download Power BI, your local computer must have the following system requirements.
NB: Power BI doesn‚Äôt support Mac devices. If you are a MacOS user, you can follow the guide here to use Power BI on Mac
You can get Power BI download through the following steps:
Step 1: Go to your search engine and type Power BI Download, click on Microsoft Power BI Desktop
Step 2: Select your language and click on Download
Step 3: Select the version of your local computer (laptop), when you have selected, then click Next as seen below
When you click on Next, it will take you here, and you can see the downloading process. If you don‚Äôt have that, click on Click here to download manually
Voila‚Ä¶‚Ä¶ Power BI successfully downloaded. Take a sip of coffee, you have tried.
Once your Power BI download completed, you can now start your installation.
Step 1: Grant Permission to Security Warning, click on Run
Step 2: Select your language and click Next
Step 3: Click Next at the Setup Wizard
Step 4: Check the ‚ÄúI accept the term in the License Agreement""
Step 5: If you want to change file installation location on your local computer, click on the Change button¬†(Optional). But your local always save installation on the Program Files Directory.
Step 6: Click Install.
If you want to create a desktop shortcut, check the button (Optional).
Step 7: Voila‚Ä¶‚Ä¶ you have successfully installed Power BI Desktop.
Now you have successfully installed Power BI Desktop. When you launch it, you will see this Welcome Interface. If you see this, your Power BI Desktop is successfully working and you can proceed to creating reports.
Thank you for following through, hope you found this helpful.
Stay tuned for more articles on Data Analysis.‚Ä¶‚Ä¶to your productivity growthFollow me on LinkedIn and Twitter
How to use Power BI on MacComparing Power BI Desktop and the Power BI serviceComparing Power BI Report Server and the Power BI service
Nick Gibbon
in
Pareture
Emmett Boudreau
in
Chifi
Rohit Chauhan
mbvissers.eth
in
Quick Programming
Daniele Quero, PhD
in
Geek Culture
Yong Cui
in
Better Programming
Pandiyan Mani
Perpetual Protocol
"
206,https://medium.com/@etsygrowthjourney/how-i-went-from-0-to-1000-month-on-etsy-in-only-two-months-1c0b30681084,How I went from 0 to $1000/month on Etsy in only two months,"These are the most important things I‚Äôve learned.
I‚Äôm like everybody else ‚Äî I‚Äôve dreamt about passive income and making money from home for ages.
I‚Äôve read about the people who manage it, and can spend their days laying on a beach somewhere, while the money seem to come flooding in, without them lifting a finger (except to order more‚Ä¶
Omole Imosemi
WEBFLUENCE1
Christian Helms ‚ù§Ô∏è
GreenTech Rangers
Peniel Takele
Authority Magazine
in
Authority Magazine
Authority Magazine
in
Authority Magazine
Launch Capital
in
LaunchCapital
"
207,https://medium.com/iowabaseballmanagers/dealing-with-dead-zone-part-2-redefining-dead-zone-98c1dc78490a,Dealing with Dead Zone Part 2: Redefining Dead Zone,"Last fall, I published a blog looking into how fastballs in the ‚Äúdead zone‚Äù performed in college baseball. In that analysis, I used the standard definition of the dead zone, a pitch with approximately equal induced vertical and horizontal break. The results showed that, overall, these fastballs performed worse than others unless there is an outlier trait to them, such as velocity. You can read the entire analysis here.
I concluded the last article with a thought of redefining the dead zone. After watching last season in college baseball unfold, I noticed pitchers still performed well despite being in the dead zone. One example of this was Iowa‚Äôs Duncan Davitt. His fastball averaged 11 inches of vertical break and 9 inches of horizontal break. Despite being classified as dead zone, he was one of the most effective pitchers on Iowa‚Äôs team last year. In this article, I take a deeper dive into how pitchers still performed well even when they classify in the dead zone according to their induced vertical break and horizontal break.
Defining a New Dead Zone
Instead of focusing solely on movement, I wanted to take into account what the hitter would expect when seeing a pitch. Depending on how the pitcher‚Äôs arm deploys, the hitter sub-consciously expects a certain movement profile. For example, a pitcher with a higher arm slot would likely have more vertical break on his fastball and less horizontal break. For a sidearm pitcher, we expect there to be more horizontal break than vertical break. The graph below showcases how a pitcher with a higher arm slot would have a dead zone with more induced vertical break compared to horizontal break according to Stuff+. The blue area denotes poor stuff, while red is more favorable.
In order to accomplish this, I built a model that predicted spin axis given the pitcher‚Äôs release height and release side on a given pitch. For each pitch, I used the model to calculate an expected spin axis. Spin axis describes the axis the ball spins around as it travels to the plate and is a value in degrees that ranges from 0 to 360. It is directly related to spin direction, so a fastball with a 12:00 spin direction would have a spin axis of 180 degrees. For right-handed fastballs, the value is generally between 190 and 235 degrees, or a 12:30 to 2:00 spin direction.
After this was done, I compared this expected axis to the actual spin axis of the pitch. The difference between those two values could be used to define the new dead zone. The closer the difference was to zero, the more the pitch moved like a hitter would expect, according to the arm slot. If there was a large difference between the two values, the pitch would be further from the dead zone and more unique. From this point on, I will refer to the difference between the expected and actual spin axis as spin deviation.
Results
The first thing I wanted to do was take a look and see if there was any validity to this new dead zone. I plotted some performance metrics against the spin deviation of each pitch in the dataset. In this first plot, I take the absolute value of the difference to see if there was any trend.
There is a clear pattern in this graph. The more a pitch moves like expected, the worse it performs. As the spin deviation increases, there is a sharp increase in chase and whiff rate, wOBACON, and Stuff+.
Deeper Dive into Deviation
Now that we can see there is a clear decline in performance for pitchers who have little-to-no spin deviation, I want to look at the difference between a positive difference and a negative difference. To do this, we need to understand what a positive difference is versus a negative difference.
A positive difference would indicate that the pitcher‚Äôs spin axis is greater than the expected axis. In simpler terms, it means the pitcher gets more horizontal break (run) on the fastball compared to what is expected. A prime example of this from college baseball last year was Purdue‚Äôs Eric Hildebrand, who averaged a spin deviation of almost 45 degrees. For a negative difference, the pitcher would get more positive vertical break (ride) than expected. This is where Duncan Davitt fell, helping him get selected in last year‚Äôs draft.
Looking at the graph above, we see the same trend near zero deviation as was seen in the first graph. As deviations go further away from zero, the performance across all metrics increases drastically. The interesting note about this graph is that the minimum values for whiff and stuff, and the maximum value for wOBACON appear on the positive deviation side. This means that the fastballs that perform the worst according to those metrics are actually those that have slightly more horizontal break than expected. This is another aspect that warrants further investigation in the future.
Merging Both Dead Zones
Using this new methodology of how the dead zone is defined, I wanted to look into how pitches that are defined as dead zone in my previous article compared to how I define dead zone now. I filtered to fastballs that had vertical and horizontal breaks within three inches of each other and plotted the same performance metrics.
The results speak for themselves. Using just the movement differential to define the dead zone is not entirely accurate. If pitchers have similar vertical and horizontal break numbers, but a unique arm slot (e.g. sidearm, over the top), they would no longer be classified as a pitcher with a dead zone fastball.
It can be inferred that the old definition of dead zone is only reliable when the pitcher has a traditional three-quarter arm slot. It seems to be much more beneficial to have a negative spin deviation with more ride than expected. This helps explain why Cooper Hjerpe was so dominant for Oregon State last season that ended with him being selected in the first round of the MLB Draft.
Controlling for Velocity
In my previous blog, I mentioned that a dead zone fastball can still be successful as long as there is an outlier characteristic like above average velocity. I wanted to see if this trend can still be observed with the new definition.
For both figures, there is a distinct performance drop when spin deviation is near zero. With average velocity, favorable results can still be achieved with high spin deviation. For above average velocity, the same trend exists, however the floor for the results is higher. For example, the lowest Stuff+ score for average velocity with spin deviation close to zero is about 70. With above average velocity, the lowest score is slightly above 100. Overall, it is better to have above average velocity, but the best results still come with high spin deviation.
Conclusion
Throughout this article, there has been more than enough evidence to support the claim that the fastball dead zone is dependent on more than just movement alone. Pitchers who are more over the top would have a dead zone with more induced vertical break and less horizontal (example: 14 inches induced vertical break, 9 inches horizontal break). Three-quarter pitchers would maintain the traditional definition of dead zone with equal amounts of vertical and horizontal break. Lower three-quarter and sidearm pitchers would have a dead zone with more horizontal break than induced vertical break (example: 9 inches induced vertical break, 14 inches horizontal break).
So why does this matter? This is a piece to solving the puzzle of deception in pitching. Explaining how pitchers perform well despite having no outlier metrics will give a massive edge in recruiting, scouting, and development. At Iowa, we have incorporated this concept into our stuff model, and it has changed how we value pitchers‚Äô fastballs. Understanding the more intangible parts of the game, like how hitters think and what they expect is what makes this metric important and valuable. Numbers can drive a lot of development, but mixing it with an understanding for the game and the mental side drives new ideas.
This further helps coaches and players understand their arsenals and expands the ways that pitchers can be effective without outlier movement or velocity. Just because a pitcher has equal amounts of induced vertical and horizontal break does not mean they should be thought of as a pitcher that cannot be effective. Using this information can help teams find undervalued players and turn them into great college pitchers.
Team Arjen
Sai Varun Immidi
in
Analytics Vidhya
Meredith Newhouse
Humble Bundle Partner Blog
Francisco Luna
in
CodeX
Lokesh Rathi
in
Analytics Vidhya
Israel Rodrigues
in
Towards Data Science
Ujwal Arkalgud
in
MotivBase Blog
"
208,https://medium.com/@Armonia1999/data-analysis-project-excel-dashboard-10c6160f2dbe,Data Analysis Project- Excel Dashboard.,"In this project we will use a dataset from ‚ÄòReal World Fake Data‚Äô located here. If you already read my EDA project using MySQL then you will know that we already used this dataset there. This time, I took the same dataset and build a dashboard for it using Excel. Reason for using Excel ? hmmm, Excel is still a widely used tool for data analysis and it‚Äôs always a good tool to have under your belt. So why not? plus it‚Äôs fun. I mean it.
Let me share with you how the dashboard looks:
As you can see in the picture, it looks fugly. Other than that, what does it contain ? at the top we have 3 KPI‚Äôs which list the number of calls in total, the average customer satisfaction score and the average call duration in minutes. Below them we have a line chart to show the trend of calls during October(that‚Äôs the only month available, the dataset is basically information about calls made in October 2020), a doughnut chart to show through which channel the call was made. Then we have a map , a column chart for the reason the person called and last but not least, a bar chart indicating the response time.
My favourite part actually was the slicers that we can see on the left side. We can use them to filter our data and thus make our dashboard interactive.
If you are nervous or feeling meh about using Excel , bear in mind that I learned Excel on the go as I built this dashboard in one day . So you are more than ready to start building it today and add your personal touch to it!
Before jumping into how to build the dashboard I already have the xlsx file on my GitHub profile here and you can download it and fiddle around to see things before building your own.
So with no further ado, Let‚Äôs get going.
I already provided the link to the dataset above, you can also google ‚ÄòReal World Fake Data call-centers‚Äô and you should be able to find it. once it‚Äôs downloaded open the file and let‚Äôs take a look:
well well well, what do we have here? we have 12 columns and 32,942 records(rows). The columns are pretty self explanatory. I guess, that‚Äôs it for this step.
This step is crucial. Just look around and press different things and make sure your data is good to go. Since I already used this dataset once I pretty much know it but we still need to get some things outta the way. Because I learned the hard way to check data types before proceeding lol. First off, check out the call_timestamp column. You may notice some of them are aligned to the left and some to the right. Click on the column itself and then left align all of them(or to the right, I prefer left). Next problem we notice is that the call_timestamp column datatype is ‚ÄòGeneral‚Äô as shown below:
It should be date, but some reason , whatever method I used to convert it to a date type it didn‚Äôt work. Plus, while I was playing around I noticed that some cells are General data type and some other cells have a date data type. No clue why. If you can enlighten me please do cause I had nightmares about it.
Next up is converting the csat_score and the call duration in minutes columns to Number because they are ‚ÄòGeneral‚Äô too and it will get in the way when we try to aggregate the data. To do that we select the column by pressing on the letter above it (In our case D and K) and then go to Home > Number Tab> click on the drop down menu and select ‚ÄòNumber‚Äô. After converting it you will see they added decimal points. To get rid of them , right under the drop down there are two ways of moving decimal points. choose the one on the right side and click until we get rid of the decimals.
Next thing I did was extracting the day out of the call_timestamp column so I can use it in a line chart later. It‚Äôs already known that all of the data is collected throughout October 2020 so no need to keep using the full date format. So I added an extra column called Call_Day that has the day of the call in it. We also formatted it as a ‚ÄòNumber‚Äô.
That‚Äôs it for preparing our data, Let‚Äôs move on.
In this step we will build our charts and Pivot tables that we will later use in our dashboard. Out of convenience, each of the charts I will create in a separate tab to keep things tidy. We will create 5 charts which are:
You can obviously choose different charts and different data to use for example the call center which I didnt use, or the sentiments which I also didnt use :( I just didnt find them worthy enough to visualize.
Let‚Äôs start with our first Chart: line chart. This chart will basically show the trends of calls by listing the count of calls for each day. Create a new tab, name it ‚ÄòLine Chart‚Äô to know to search for it later. Click on any cell and go to ‚ÄòInsert‚Äô then click Pivot table.
It will open a pop up window to select a range. Go back to your data tab (most likely called call-center) and click on any cell and then do Ctrl+A to select all of the data then click ok on the pop up window. This will appear :
To create our pivot table containing the line chart data, drag the ‚Äòid‚Äô from the list on the left to the values box, and then the call_day to the Rows box. You will get this:
Nice. Next up is to create the chart. Click on any cell from the pivot table on the left and click Pivot Chart:
and then choose a line chart with markers like this:
press ok and you have already created your first chart !
We will come back for it when building our dashboard to improve it‚Äôs design. Now, Let‚Äôs move to my precious doughnut chart.
It‚Äôs the same process as before but when dragging to the boxes drag the ‚Äòid‚Äô to the Values box, and then Channel to the Rows box. Then again add a chart as before and instead of line click ‚Äòpie‚Äô and the last option is a doughnut. choose it. It should be like this:
Then you have your doughnut chart too ! 2/5 charts acquired ^^
Now my sworn enemy, za mappu ! same process as before but for the Rows box drag the State column.
Now if you try to add a map chart as we did with the other charts, you will be hit with this gruesome scene:
uhm, no clue why it happened, as I am an excel noob on my path to be an excel ninja but we shall do as they say: copy our data to another place.
Click on any cell inside the pivot table, Select all of it by clicking Ctrl+A then Ctrl+C to copy it. Click on cell E1 (or any, i just love tidiness) and then go to the Home Tab, click on the drop down and click the first option of paste values. Now you have your pivot table data outside the pivot table. click on any cell in it and go to Insert> Maps > Filled Map. and your map should be created.
The data that created the map, technically, isn‚Äôt connected to the pivot table, so if any change happens to the pivot table, it will not be reflected on the map. That‚Äôs why we now need to connect the chart to the pivot table. click on the chart, go to the tab Chart Design > Select data and select the data from the pivot table and then your map is ready to go !
I discovered how to do this using this article, so definitely check it out !
only two charts left, and after all we‚Äôve been through you can definitely create them. Which takes us to our next step‚Ä¶
It saddens me to say but this step needs a visual tutorial, no way in the five hells that I can accomplish it using writing and screenshots. But fear not my followers ! I have just the perfect tutorial ( That I also used for inspiration!) to help you with that. It‚Äôs around thirty minutes but absolutely valuable so definitely do watch it !
Another video (3 min long) that I do want you to see is this one, It helps you make the slicers (filters) better looking.
so yeah that‚Äôs it ! Play around with the background pictures , you can use unsplash or pixabay for royalty free pictures to use. Also change the colour palette to suit your style, I am such a devoted person to anything blue that‚Äôs why I always use it somehow. Save your file and host it on Github or share on Linkedin and follow with a good nights‚Äô sleep! If you have extra time to spare please do share with me the dashboards you came up with !
ÊãúÊãú üëãüèªüëãüèªüëãüèª
Edit: I just realized I forgot to explain how to make the metrices at the top ( Count of calls, average csat score, average of call duration). Mannnnnn my hands hurt but let‚Äôs do it nonetheless. After designing the dashboard and you have three square shapes at the top ( for our metrices duh) this is what we need to do: add a text box above them. You can do that by going to Insert > Text > Text box.
Then you will put it above the square like this:
Click inside the text box so it shows the typing cursor, then go to the highlighted area above (the formula box) and type an ‚Äò=‚Äô sign. after doing it, go to the tab where you have the pivot table ready.
and click on the cell that has the number you have and press Enter. it should be linked. But if trouble happens, like what happened with me , What solved clicking on the pivot table itself, go to PivotTable Analyze and then to options like this:
click on the little arrow, and uncheck the ‚ÄòGenerate GetPivotData‚Äô. This should help solve it !
That was it ! Link each metric/KPI text box to it‚Äôs equivalent cell and press enter and you should be ready to go ~
Now goodbye for real ! I‚Äôm ded.
Hagi Jakobson
ARINDRAJIT PAUL
Saad Ahmed
in
Towards Data Science
Towards AI Editorial Team
in
Towards AI
Karlo Leskovar
in
Towards Data Science
Lynn Cherny
Iurii Katser
Paul Chambiras
"
209,https://medium.com/@ruyakubu/how-to-test-a-machine-learning-model-for-fairness-reliability-with-responsible-ai-part-2-6ab1ef8c6e1a,How to test a machine learning model for fairness & reliability with Responsible AI (Part 2),"When we train a machine learning model, we want the model to learn or uncover patterns. We focus on how accurate a model can make predictions and try to reduce the error rate of the model. However, focusing too much on some aggregated model performance metrics such as accuracy, we often neglect two important things that could happen:
As AI becomes more common, it‚Äôs important to keep people and their goals at the center, working to maximize benefits & minimize its risks. As a result, machine learning models need to be scrutinized for errors, fairness and reliability. These considerations must be apparent not just to data scientists but to decision makers, end-users and compliance auditors as well.
In the prior tutorial, we covered how to create an Azure Machine Learning workspace with Responsible AI (RAI) Components. In this tutorial, we‚Äôll train a model that we‚Äôll use to analyze and resolve issues we find using Azure ML‚Äôs Responsible AI dashboard. We will be using an HR Employment Attrition dataset to predict whether an employee will be Staying or Leaving their job. In addition, we‚Äôll implement using best practices on creating an end-to-end machine learning workflow, with Azure ML components and pipelines. Azure ML components are reusable independent units of tasks that have inputs and output in machine learning (e.g., data cleaning, training a model, registering a model, deploying a model etc.). An Azure ML pipeline packages all the components and runs them sequentially during runtime with a specified compute instance, docker images or conda environments in the job process.
Prerequisites
This is Part 2 of a tutorial series. You‚Äôll need to complete the prior tutorial(s) below:
To create the experiment, first we need to initial the client session and connect to the Azure ML workspace. You‚Äôll need to copy the config.json file (see the ‚ÄúDownload config.json‚Äù link highlighted in red in the image below) and put it in the same folder as your Jupyter notebook file.
Run the next code snippet to initialize the client session. You MLClient uses the date in the config.json to authenticate and connect to your azure ML workspace. In addition, you need to create another client session to connect to the workspace and be able to access the ‚Äúazureml‚Äù system registry where the RAI Components are registered.
You should get a message stating that your config.json file was found.
To work with our training dataset, we need to load the Employment Attrition data to a dataframe.
As you can see there a lot of columns in the dataset, so will have to do some data cleansing for data related our use case to an employment attrition (e.g., leaving vs staying) for our training.
In this case, we are going to drop the columns that are not relevant to an employee leaving or staying in a job. For example, EmployeeNumber will not impact someone leaving or staying. Both the EmployeeCount and StandardHours columns have the same constant values for all the records, so they will not make a difference to the prediction. The Over18 may be useful, however since we already have an Age column with individual ages and a lot of records Over18, this field will not add any additional information, so we‚Äôll drop it. Since this is a classification problem, we will choose the ‚ÄúAttrition‚Äù column to be our target column. Also, we will change its target label value pairs from Yes/No to something easier to understand like Leaving/Staying. The component for training the model needs the name of the target column as input, so we need to define a variable for the field name.
Now that we have got the columns and the data in the format we need, we can split the training and testing datasets. Then, save the data in parquet file formats.
Finally, we will store the training and testing parquet data files to Azure ML datasets. The benefits of storing the datasets to Azure ML workspaces is that data is saved in one centralized location and can be referenced by other workflows. In addition, it is a good way to track the dataset used for every experiment. Since Azure ML studio is a shared workspace, other team members can reuse the cleansed dataset by just referencing it.
We‚Äôll be using Azure ML components to divide the experiment into different individual tasks that are independent of each other. To do this, we will create a component for training. The component will consist of a python training code with inputs and outputs.
The first thing we will define in our python code is the training script containing a function that declares an argument parser object that adds the names and types for the input and output parameters. For our Employee Attrition use case, we will be passing the path to our training data and the target column name as the classifier. Then the model will be the output for the script.
Next, we will define the main function for the python training code with the argument parser object as it is parameter. The function reads in the employment attribute dataset and the target column as input.
Then it can transform the string columns and numeric columns. Since we are trying to predict whether or not an employee will leave or stay with their current employer, we are using Logistic Regression to train our machine learning model.
The last important step in our main function is to store the trained model file in a local output directory. Then we‚Äôll use MLFlow to save the model file in our Azure ML experiment output folder.
To define our training component, we‚Äôll need to create a yaml file where we specified the component name, input and output parameters; the location of the python code that trains the model; the command-line to execute the python code; and the environment to run the code. Then use the experiment client session to register the component definition in Azure ML workspace components.
The last component we need to create is to register the trained model. Similar to the training component above, the yaml definition file for the model registration will have the same Azure ML Component fields.
After the training component has been created, we need to define the pipeline that is going to package all the dependencies needed to run the training experiment. To do this, you will need the following:
It will use all of this information to package a pipeline job to run the experiment for training and registering the model.
Once you have defined and registered the pipeline to the workspace, you can submit the pipeline to run in a job. In our python code, we are checking the status of the job.
An alternative way of checking the status of all the components in the pipeline job is by clicking on the Jobs icon from the Azure ML studio.
From the Jobs list, you can click on the job to view the job‚Äôs progress as well as drill-down to pinpoint where an error occurred. In this tutorial, you will see the Employee Attrition dataset feeding as input into our training component and the output model feeding into the register model complete.
Fantastic, now let‚Äôs move on to generating the Responsible AI Dashboard to debug the model!
See Part 3 of the next tutorial‚Ä¶
Pranav Dar
in
Analytics Vidhya
ƒ∞rem K√∂m√ºrc√º
Dan Frankowski
in
Towards Data Science
Pranjal Saxena
in
Heartbeat
Niklas G√∂tz
in
The Glass Bead Game
Jack Harding
in
Analytics Vidhya
Hugegene
in
MLearning.ai
Semih Yagcioglu
in
HUCVL Stories
"
210,https://medium.com/@villekuosmanen/i-played-chess-against-chatgpt-4c2cc78b5acf,I played chess against ChatGPT,"ChatGPT is a large language model designed for conversation, that Twitter users around the world have found scarily human-like in its answers. The AI model doesn‚Äôt only answer generic questions about facts, but it can also write accurate code in multiple programming languages, as well as debug code written by humans. ChatGPT also understands humor, being able to tell jokes as well as explain them. This kind of competence across multiple domains is something AI has traditionally lacked, and is often seen as a precursor towards artificial general intelligence (AGI).
Are the broad abilities of ChatGPT a sign of higher intelligence, or merely an illusion of having digested unholy amounts of textual data from public domain? Time to put the bot to test. Our medium? The oldest game of minds ‚Äî Chess!
I am not a professional chess player ‚Äî far from it. My ELO ranking in chess.com hovers around 1200, indicating a novice or intermediate level player. While chess is traditionally played over the board, the game has a standardised notation for recording moves ‚Äî PGN. With PGN, I can relay my moves to ChatGPT and ask it to respond using the notation. I can then play the bot‚Äôs moves on a physical chess board, as pictured above. Makes sense? Let‚Äôs give it a go!
As a Queen‚Äôs Pawn player, I begin the game with 1. d4. ChatGPT responds with the principled move 1‚Ä¶ d5, staking a claim to the centre with its own Queen‚Äôs pawn. With 2. c4 e6 3. Nc3 Nf6 4. Bg5, the game continues as a Queen‚Äôs Gambit Declined: Modern Variation opening. So far so good.
ChatGPT continues the game with an unusual move: 4‚Ä¶ dxc4. Accepting the gambit is not a mistake by any means, but dxc4 is typically played earlier if black‚Äôs intention is to take the pawn. In the Chess.com games database, this move shrinks the number of master games we are following from 13,385 to 302.
I continue by attacking ChatGPT‚Äôs pawn with 5. e3. The AI chooses not to defend, instead developing its bishop with 5‚Ä¶ Be7. This principled move prepares to castle the king, and unpins the knight at f6. 6. Bxc4 wins the pawn back, after which both players develop their remaining kights and castle.
6‚Ä¶ Nc6 7. Nf3 O-O 8. O-O
As you can see from the eval bar on the left, Stockfish (chess engine evaluating the position) prefers my position over ChatGPT‚Äôs. However, the game is nothing but over. As both players have castled, the opening stage of the game has now transitioned into the middlegame. Chess.com‚Äôs database only knows of two master games in this position, a sign that we are heading into uncharted waters.This is the point where ChatGPT‚Äôs game plan begins to go downhill.
ChatGPT decides to drive away the white bishop with 8‚Ä¶ h6 9. Bh4 g5 10. Bg3. Pushing the bishop back wins time and space, at the cost of weakening the black king by pushing the pawns in front of it. ChatGPT continues with aggression by jumping to the centre with its knight (10‚Ä¶ Nd5).
This is inaccurate, because after 11. e4 the knight will either have to retreat or trade itself for my knight on c3 which simply wastes time. ChatGPT chooses the latter approach with 11‚Ä¶ Nxc3 12. bxc3.
At this point, I started to run into issues, as ChatGPT stopped answering prompts. It wasn‚Äôt clear to me whether this was because the servers were getting congested (the site has seen crazy amounts of traffic over the past few days) or whether the AI was struggling to generate sensible answers for a chess middlegame. As you can see from the screenshot below, the explanations given by ChatGPT for its earlier moves no longer make much sense ‚Äî the bishop on g3 is not under attack and the knight can no longer go to e4 and g4 after moving it.
After a few errors I tried to continue the game by starting a new conversation. While ChatGPT understood the game, it no longer remembered the location of its pieces and attempted to play illegal moves. While this could be the consequence of starting a new conversation, based on ChatGPT‚Äôs explanations it was always bound to happen at some point. The AI was responding with moves and words that chess players often say ‚Äî regardless of whether they appear in the current game or not!
As ChatGPT could not produce a legal move even after several retries, I had no option but declare the AI as having resigned. Humanity prevails! Here‚Äôs the final position after 12 moves.
Out of interest and fairness, I also asked ChatGPT to start a game as white. It answered by describing moves with natural language. However, this didn‚Äôt work at all: ChatGPT‚Äôs response to my French Defense makes no sense as the Queen‚Äôs knight has no way of attacking the pawn on e6.
To conclude, ChatGPT cannot play chess at a human level (yet). It is clearly aware of the game and able to accurately play mainline openings. But the moment the game moves out of theory, ChatGPT can no longer keep up. This shows that the language model doesn‚Äôt (yet) have any understanding of chess fundamentals, but merely repeats moves and phrases that commonly occur in a documented chess game. The model is still a stochastic parrot, no matter what people say on Twitter.
Still, a language model with no inbuilt understanding of chess being able to play a reasonable game for eleven moves is no small feat! Playing the game really did feel like playing a human. I didn‚Äôt need to utter any special phrases like we do with Siri or Alexa ‚Äî ChatGPT immediately understood what I was trying to do just by asking it to play a game of chess.
For future study, it would be worth trying different prompts at the start of the game. For example, you could ask ChatGPT to pretend to be Magnus Carlsen before starting the game. I also had some success with asking the bot not to explain its moves. Playing the whole game over one conversation would likely help as well. The free preview of ChatGPT is currently too congested to reliably explore the topic further, but this is something I‚Äôm planning to do once a paid version is released. It‚Äôs likely that OpenAI‚Äôs bot will be able to beat me in the future, but until then I‚Äôll enjoy the superiority of meat over machine! Just don‚Äôt tell Stockfish or AlphaZero‚Ä¶
Abdul Rawoof Khan
Alex K.
Arsha Walia
Veer Jain
Antar Mortuza
in
Beatnik Digital
Carlos E. Perez
in
Intuition Machine
Nanduri Balajee
Johannes Schleith
"
211,https://medium.com/@jc.chen_79877/what-can-we-get-from-a-python-data-science-machine-learning-based-the-world-cup-prediction-c86e9b46f4e5,What can we get from a Python model that predicts the World Cup using Data Science/Machine Learning?,"Everyone is talking about the World Cup in recent days! Despite the controversial topics in the regulations, the politics-related topics and new offside technology, the tournament goes on excitingly in Qatar.
With the development of the technology, machine learning methods have been widely applied for data-driven predictions of the games and also help teams to improve.
Here, comes the first question, how data science & machine learning can predict the World Cup game?
Before we get into the details, you can have a quick look at my created functioning demo of this model on Cognitive Class (a free platform by IBM). The project uses very simple machine-learning models to predict the 2022 World Cup games.
The idea of the prediction is pretty simple, even for new learners. Our target is to use a machine learning model to predict sports games. First of all, we need to quantify our inputs (labels) and outputs (features).
Outputs are straightforward, the scores or the Win/Draw/Lose result of a game.
To simplify the Inputs, in the project we use the (1) lineup player‚Äôs overall rates from FIFA23 database and (2) team average age.
The structure of the machine learning project is provided below.
Soccer strategy and skills are developing, and the training sets we use are the international team games played in 2022 before the world cup.
The prediction sets are the strongest lineup of each team.
We are using a simple neural network for the prediction
Then, we can apply the best model to predict the result (group stage):
If there is some need, users can add more features for more robust predictions (the data is also provided in our project). We can get a prediction of game results between any two teams (even which team to win the tournament!). Such kind of machine learning methods could be applied to not only soccer but also other sports games.
But of course, there are a lot of uncertainties in a soccer game and the prediction is not 100% right. Like not too many people predicted that Argentina lost the game against Saudi Arabia.
Dive deeper, is there any useful information we can know from the machine learning predictions? Like, what is important to win a soccer game?
We can use LIME & SHAP for that:
We can use the game German VS Spain as an example to see what is important to the game:
The three are useful functions to set, conduct and plot the explanation using LIME.
Let‚Äôs move on to SHAP
Similar to LIME, the three are useful functions to set, calculate and plot the explanation using SHAP.
The fancier interactive result could be visualized by the following code:
(1) The Germany VS Spain group stage game highly possible will be a draw.
(2) The center line players (M,F,CB) largely influence the game, especially the midfield.
(1), (2) are proved by Fullkrug‚Äôs goal in the game
(3) It never says if a team with the best player in every position has a larger opportunity to win.
(1) Ages don‚Äôt matter that much!
(2) Every positions all contribute to a game, but the importance is not the same. The most noticeable position striker (Foward#2) is not the most important, a good shooter always needs a good team.
(3) Good defence at least could at least help the team not to lose.
Machine learning methods could not only help us to make a data-driven prediction of sports games like the World Cup, but it tells some implications from the models. LIME & SHAP are good at it.
The implication could help a soccer team to be better and win more games!
If you want to know every detail about how to make the World Cup prediction and implication findings from LIME and SHAP, you can visit the project. You can also learn a lot of FREE courses and projects about data science or any other technology topics from Cognitive Class.
Enjoy soccer, enjoy the World CUP!
Chris Nguyen
Irhum Shafkat
in
Towards Data Science
Chris Rotunno
Satyam Shubham
Abdulazeezabdullah
marcus evans online events
in
marcus evans online events
Mahbubul Alam
in
Towards Data Science
Didier Rodrigues Lopes
"
212,https://medium.com/@VishalGarg1/how-to-get-into-cybersecurity-part-3-technical-skills-cd17c6237e2d,How to get into Cybersecurity (Part 3 ‚Äî Technical Skills),"In the first two articles in this series ‚Äî ‚ÄòHow to get into Cybersecurity ‚Äî Soft Skills‚Äô and ‚ÄòHow to get into Cybersecurity ‚Äî Career Paths‚Äô, I had discussed about the soft-skills that you might require to build a successful career in cybersecurity, along with the various job roles and functions that exist to help you narrow down your choices to specific areas that might interest you.
Once you have identified a specific career path you‚Äôd like to pursue, the next step is to equip yourself with the right set of technical skills. You may have a number of options available to you based on what stage you are on in your career.
If you are a student and have enrolled yourself for an under/post-graduate degree course or an apprenticeship programme, you still have a plenty of time to go through theory, and to gain practical skills along the way. However, if you are switching from another career, the amount of time and effort that you might need to invest to get up to speed and to get yourself ready for a job will depend on your previous background and experience.
If you are working in another area of Information Technology, such as software engineering, systems administration, networking, helpdesk, or technical support, you may already have some of the basic skills and will only be required to learn concepts specific to cybersecurity. On the other hand, if you‚Äôre planning to move from a completely different field, it might require a bit more of an effort and dedication to get started in this field. However, nothing is impossible with dedication and hard work.
In this article, I will share how you can get started as a complete beginner, or if you already have the basic foundations in place.
It is important to have a foundation in place to start a career in cybersecurity. The technical skills that you will require at the most basic level will include ‚Äî
Having a foundation in place, you may need to think about the route you would like to take be become qualified for a career in cybersecurity. There is nothing right or wrong, you may only need to choose the best option available to you based on your own interests and circumstances.
If you are studying for an under/post graduate course that leads to a degree in information or cybersecurity, you are well on your way to get a good head start in the field. To top-up your theoretical knowledge, you may gain some practical knowledge by setting up a lab at home to practice technical skills, completing hacking challenges, capture the flag competitions (CTF), or by enrolling yourself to bug bounty programs.
In the last few years, getting into cybersecurity has become more competitive, and having the right practical skills will put you ahead of the competition.
If you‚Äôre a working professional, one thing to remember is that this may be the most demanding, and the most satisfying experience. There is, however, no such requirement to take this route to get into the field and it only comes down to your own personal preferences. There are a number of institutions offering full-time, part-time, remote and executive programmes for working professionals.
I had personally taken this route. After having a few years of experience in software development, I had enrolled myself to a masters programme in Information Security on a full-time basis at one of the prestigious institutes in the UK and have never regretted this decision.
If you are switching from another profession and think the formal education is not the route you want to take, you may consider cybersecurity certifications. These can be quicker, cheaper, and more specific to the path you‚Äôve chosen for yourself.
There are a number of vendor-agnostic and vendor-specific certifications available at beginner and more advanced levels.
For absolute beginners, CompTIA Security+ provides good baseline skills required to perform core security functions. ISC2 has recently launched a new entry-level certification Certified in Cybersecurity, which does not require any prior work experience. SANS certifications are also very high quality, and GIAC Security Essential (GSEC) is a very comprehensive entry level certification. If you are looking for a career in offensive security, such as penetration testing or ethical hacking, Pen-100 offered by Offensive Security could be an option to consider. You may also try this free introductory cybersecurity course from Cisco.
If you have no prior knowledge of IT, the above foundation-level certifications alone may not be sufficient to get you a job in cybersecurity and you may need to consider more advanced certifications based on which career path you want to take. You can read my article ‚ÄòHow to get into Cybersecurity ‚Äî Career Paths‚Äô on different career paths that might be available to you.
CompTIA offers CySA+ and CASP+ at more advanced level. SSCP from ISC2 can be a good option to consider as well. GIAC provides a number of advanced certifications, such as, GCIA, GCIH, along with many others that are worth considering. On the offensive security side, you may consider PenTest+ from CompTIA, GPEN from GIAC, or OSCP from Offensive Security. Note that OSCP has a practical component associated with it. It also has a steep learning curve and can be very demanding for beginners, however, it is well worth the effort to make a good career in penetration testing since this certification is in a lot of demand right now.
There are other vendor-specific certifications that you may consider once you start working on a particular area or technology. Due to recent trends in cloud computing, new certifications have started appearing in this area as well. It may be worth considering these after you have done sufficient groundwork on developing strong foundations.
There are also opportunities available in Governance, Risk and Compliance (GRC). If you‚Äôve worked in a related field for a few years and would like to move into cybersecurity, you could consider CISSP from ISC2, or CISM, CISA, or CRISC from ISACA. All these are considered prestigious certifications at management grades.
One thing to remember is that the focus must be on learning and not on obtaining too many certifications. If a certification is not aligned to your career goals, it may not serve a purpose for you in the long run and would be a wastage of your time and money.
The more practical skills you have, the better it will be to build a strong foundation. For this purpose, it is best to build a lab at home to practice both offensive and defensive security techniques.
The most common and most advanced Linux distribution geared towards penetration testing is Kali Linux. This distribution contains a vast array of tools and utilities to perform penetration testing tasks.
On the defensive side, you could install Linux distributions, configure these insecurely and then practice exploiting weaknesses to compromise the services. You could then harden the services by following security best practices and try exploiting again to see if you could compromise these. This will provide you both offensive and defensive perspectives to see how the hackers exploit weaknesses and how the defenders put protections in place.
You could also download pre-configured vulnerable machines from the Internet. There are a number of vulnerable distributions available online which you can download and install on your local machine for practice. OWASP Vulnerable Web Applications Directory provides a list of both online and offline vulnerable applications. VulnHub is another site offering a collection of resources and vulnerable machines to practice your hacking skills in a safe environment. There are other online gamified platforms such as TryHackMe and HackTheBox, where you can practice your hacking skills in a safe environment.
Once you feel confident, you can make use of your newly acquired skills to enrol yourself to bug bounty programs. Both HackerOne and Bugcrowd are the most common platforms in this space. Remember to read the rules of engagement for each bug bounty program listed on these sites.
The learning does not stop there. In the cybersecurity field, you need to keep learning constantly to keep your skills up to date and relevant. In my upcoming articles, I will share information on various other ways that you can use to keep your skills up to date through continuous learning. Stay tuned!
Update ‚Äî Links to other three articles in this series. Hope you find these useful too.
medium.com
medium.com
medium.com
If you enjoyed reading this story, please acknowledge (üëè below a few times), comment, or follow along that will help me to manoeuver my writing style or to do better of what I‚Äôm doing right.
Follow me on Medium or LinkedIn. Be the first to learn about my stories by subscribing here.
Brad Crotty MD MPH
in
Inception Health
Techskill Brew
in
Blockchain 101 by Techskill Brew
Jason Tweed
Paulo Shakarian
Rick Papadopolous
Galxe
Decentralist.com
in
BLOCK6
Game Wonderlab
"
213,https://medium.com/@fmnobar/top-3-free-courses-that-helped-me-land-my-first-data-scientist-job-in-amazon-7de8c9a19fc6,3 Free Courses that Helped Me Land My First Data Scientist Job in Amazon,"What courses do you recommend for me to start learning about Data Science and Machine Learning?
I get asked this question on a monthly basis, if not weekly. So I decided to put together this brief post to help address it. I will introduce three resources, provide an overview of each and then will include a link to their respective websites.
Not long ago, I went to a Data Scientist friend of mine with the same question and guided me to these three resources. Now I am going to pass the baton on to you and I hope you do the same in the future, if you find them helpful. They helped me land my first Data Science job in Amazon and I hope they can also help you get the role that you want.
There is also an added benefit ‚Äî All three are either free to enroll or course materials are free to use.
I wanted to keep this post focused on these three resources so I did not go into breaking down what technical knowledge is required for a Data Scientist in Amazon, but if you are looking for that, feel free to check my other post linked here, which also includes practice questions (with Jupyter notebooks) to practice what you learn.
Lastly, I recommend going through these three resources in the order that I have listed them here, since they progressively increase in depth.
Let‚Äôs get started!
I know what you are thinking. Everyone knows this one! With around 2.4 million views as of October 2022, I am sure most of us know this one but it is still one of the best courses for anyone to start learning about Machine Learning and includes practice quizzes to test one‚Äôs learning.
Andrew Ng, who really does not need an introduction, focuses on what matters most for those getting started with Machine Learning and breaks down the course into the following:
Pro Tip: When I took this course a few years ago, practice problems did not have solutions in Python, which was and remains my code of choice. Things might have changed by now but if that is still the case and you also happen to prefer Python, a simple Google search will help you find the solutions in Python.
Free enrollment is available on Coursera.
John Paisley is an Assistant Professor in the Department of Electrical Engineering at Columbia University. He is also an affiliated member of the Data Science Institute at Columbia.
This one goes deeper than Andrew Ng‚Äôs course and explains more about the distinction between probabilistic and non-probabilistic modeling and also supervised versus unsupervised learning.
Pro Tip: This one is great for understanding the concepts but can sometimes go deep into the mathematical side of things. If you follow the math, that is great but if you do not (like me), focus more on the conceptual parts and you can safely skip the math-heavy portions.
Let‚Äôs look at the syllabus for details.
Course materials are available for free on edX.
Andreas Muller is an Associate Research Scientist in Data Science Institute at Columbia.
His course materials are available online and he also goes deeper into some Natural Language Processing (NLP) topics (i.e. textual data) such as topic modeling, embeddings, etc. So if you are interested in learning more about NLP, you will enjoy those topics here.
Let‚Äôs look at the syllabus for details.
Pro Tip: Make sure to check out his YouTube videos, as well as the course materials, which are both available on the course website.
If you found this helpful, please follow me on Medium and support me in getting to 1,000 followers!
Braden Riggs
in
Towards Data Science
Ambreen H.
Jesse Gan
Lathifah Aliya Pratiwi
Kubra Sen
Jeremy Posner
in
RAW Labs
Andrew Coomes
Mohamed Hatmi
"
214,https://medium.com/codex/understanding-the-difference-between-computer-science-and-engineering-5fb966be6f5f,Understanding the Difference between Computer Science and Engineering,"When I started my university education in the Computer Engineering department, one of my friends studied the same thing at a different university. At least, that‚Äôs what I thought it was because he was in the Computer Science department. I didn‚Äôt give enough attention to the name difference and didn‚Äôt know there was a difference. Although the courses offered in the two‚Ä¶
Ashutosh Singh
Kalyaniknkr
Max Lewin
in
CloudTrucks Engineering
Adem Kanat
Saadhika Devi
in
codeburst
Nikhil Jain
Matt Miller
Sachin Karve
in
The Startup
"
215,https://medium.com/towards-data-science/social-network-analysis-from-theory-to-applications-with-python-d12e9a34c2c7,Social Network Analysis: From Graph Theory to Applications with Python,"Social network analysis is the process of investigating social structures through the use of networks and graph theory. This article introduces data scientists to the theory of social networks, with a short introduction to graph theory and information spread. It dives into Python code with NetworkX constructing and implying social networks from real datasets.
(This article is a written version of a talk from Pycon 2019. You can watch the video below and check the github code repository)
github.com
We‚Äôll start with a brief intro in network‚Äôs basic components: nodes and edges.
Nodes (A,B,C,D,E in the example) are usually representing entities in the network, and can hold self-properties (such as weight, size, position and any other attribute) and network-based properties (such as Degree- number of neighbors or Cluster- a connected component the node belongs to etc.).
Edges represent the connections between the nodes, and might hold properties as well (such as weight representing the strength of the connection, direction in case of asymmetric relation or time if applicable).
These two basic elements can describe multiple phenomena, such as social connections, virtual routing network, physical electricity networks, roads network, biology relations network and many other relationships.
Real-world networks and in particular social networks have a unique structure which often differs them from random mathematical networks:
Highly central nodes play a key role of a network, serving as hubs for different network dynamics. However the definition and importance of centrality might differ from case to case, and may refer to different centrality measures:
Different measures can be useful in different scenarios such web-ranking (page-rank), critical points detection (betweenness), transportation hubs (closeness) and other applications.
Networks can be constructed from various datasets, as long as we‚Äôre able to describe the relations between nodes. In the following example we‚Äôll build and visualize the Eurovision 2018 votes network (based on official data) with Python networkx package.
We‚Äôll read the data from excel file to a pandas dataframe to get a tabular representation of the votes. Since each row represents all of the votes of each country, we‚Äôll melt the dataset to make sure that each row represents a single vote (edge) between two countries (nodes).
Then, we will build a directed graph using networkx from the edgelist we have as a pandas dataframe. Finally, we‚Äôll try the generic method to visualize, as shown in the code below:
Unfortunately the built-in draw method results in a very incomprehensible figure. The method tries to plot a highly connected graph, but with no useful ‚Äúhints‚Äù it‚Äôs unable to make a lot of sense from the data. We will enhance the figure by dividing and conquering different visual aspects of the plot with a prior knowledge that we have about the entities:
Finally, we‚Äôll plot the network components in parts:
The new figure is a bit more readable, and giving us a brief overview of the votes. As a general side-note, plotting networks is often hard and requires to perform thoughtful tradeoffs between the amount of data presented and the communicated message. (You can try to explore other network visualization tools such as Gephi , Pyvis or GraphChi).
Information diffusion process may resemble a viral spread of a disease, following contagious dynamics of hopping from one individual to his social neighbors. Two popular basic models are often used to describe the process:
Linear Threshold defines a threshold-based behavior, where the influence accumulates from multiple neighbors of the node, which becomes activated only if the cumulative influence passed a certain threshold. Such behavior is typical to movie recommendations, where a tip from of one of your friends might eventually convince you to see a movie, after hearing a lot about it.
In the Independent Cascade model, each of the node‚Äôs active neighbors has a probabilistic and independent chance to activate the node. This resembles a viral virus spread, such as in Covid-19, where each of the social interactions might trigger the infection.
To illustrate an information diffusion process we‚Äôll use the Storm of Swords network, based on Game of Thrones show characters. The network was constructed based on co-appearance in the ‚ÄúSong of Ice and Fire books‚Äù.
Relying on the independent cascade model, we‚Äôll try to track down rumor spreading dynamics, which are quite common in this show.
Spoiler Alert! Suppose Jon Snow knows nothing at the beginning of the process, while his two loyal friends, Bran Stark and Samwell Tarly, know a very important secret about his life. Let‚Äôs watch how the rumor spreads under the Independent Cascade model:
The rumor reaches Jon at t=1, spreads to his neighbors in the following time-steps and quickly spreads all around the network, resulting in being a public knowledge:
Such dynamics are highly dependent on model parameters, which can drive the diffusion process to different patterns.
The influence maximization problem describes a marketing (but not only) setup, where the goal of the marketer is to select a limited set of nodes in the network (seeding set) such that will naturally spread the influence to as much nodes as possible. For example, consider inviting a limited amount of influencers to a prestigious product launch event, in order to spread the word to the rest of their network.
Such influencers can be identified with numerous techniques, such as using the centrality measures we‚Äôve mentioned above. Here are the most central nodes in Game of Thrones network, according to different measures:
As we can see, some of the characters re-occur at the top of different measures, and are also well known for their social influence in the show.
By simulating the selection of most central nodes we observe that picking a single node of the network can achieve about 50% of network coverage ‚Äî That‚Äôs how important social influencers might be.
On the other hand Influence Maximization is Hard. In fact it‚Äôs considered as an NP-Hard problem. Many heuristics were developed to find the best seeding set in an efficient calculation. Trying a brute-force method to find the best seeding couple in our network resulted in spending 41 minutes and achieving 56% of coverage (by selecting Robert Baratheon and Khal Drogo)- a result that would be hard to achieve with centrality heuristics.
Network analysis is a complex and useful tool for various domains, in particular in the rapidly growing social networks. The applications of such analysis include marketing influence maximization, fraud detection or recommender systems. There are multiple tools and techniques that can be applied on network datasets, but they need to be chosen wisely, taking into account the problem‚Äôs and the network‚Äôs unique properties.
If you wish to cite this resource in your academic research, please use the following format:
Goldenberg, Dmitri. ‚ÄúSocial network analysis: From graph theory to applications with python.‚Äù PyCon 2019 ‚Äî 3rd Israeli National Python Conference, Israel, 2019. arXiv preprint arXiv:2102.10014 (2021).
Reach out to me with questions and ideas by mail or Linkedin.
Interested about my work at Booking.com? Check out our Booking.com Data Science blog.
booking.ai
Urbica
TheStartupFounder.com
Adebayo Oshingbesan
in
DataDrivenInvestor
Nakul Upadhya
in
MLearning.ai
Vidya Thandra Satyanarayana
Jacqueline Nolis
J. Manansala
Craydec, Inc.
"
216,https://medium.com/bitsrc/i-asked-chat-gpt-to-build-a-to-do-app-have-we-finally-met-our-replacement-ad347ad74c51,I asked Chat GPT to build a To-Do app ‚Äî Have we finally met our replacement?,"We saw the release of GitHub Copilot in October of 2021 and the whole dev community went crazy.
Some were claiming that we would soon be left out of a job, and others, like me, thought that while the tool was interesting, it was really far from a replacement. It could provide better autocomplete, yes, but it was not capable of creating an entire app for you.
So we were safe.
Or so we thought.
A few weeks ago we saw the release of Chat GPT and the entire dev community went crazy again. But are they right this time around?
I tried the AI and I asked it to build the web dev equivalent of a ‚ÄúHello world‚Äù: a To-Do app.
Let‚Äôs see how it did.
Here are my prompts for the chatbot and the answers I got:
can you give me the code to create a to-do app that allows me to edit the items and store them in a database? Please use React for the front-end and next js for the back-end
That was my initial prompt, and the key asks are bolded there:
Now, here is what I got:
Ok, that wasn‚Äôt EXACTLY what I asked for.
Yes, I got a to-do app, using React and Next, BUT, it‚Äôs not using a database, in fact, it‚Äôs using in-memory storage. And it‚Äôs not letting me edit the items. So let‚Äôs ask again.
can you modify that code to add support for a cloud database?
OK, now I just referenced ‚Äúthat code‚Äù, and added ‚Äúcloud database‚Äù support. I was specifically trying to be vague, because a human would easily understand this, but the AI needs to keep a reference to the current conversation and remember what we‚Äôre talking about.
And lo and behold, the code I asked for:
It picked Firebase for me, and it even wrote the placeholders for me to enter my credentials.
Or did it?
So far, I‚Äôm pretty impressed with the results, I was not really expecting this level of detail.
But when it came to testing the code, I found a couple of issues that clearly highlight the shortcomings of this bot.
Issue #1: it was missing an import. Silly mistake, I know, we humans make them all the time, but the code was missing the import of the useEffect hook.
Issue #2: it never specified the version of the firebase npm package to use. Maybe that was my fault, but a simple npm install firebase yielded a version that was not compatible with the code. So I asked the bot: what version of the firebase npm package should I use for that example?
And I got an interesting answer:
I mean, kudos to the bot, it was able to understand the context of my question, and provided a valid (almost) answer. The code was, indeed, compatible with that version of firebase, so I was able to try it.
But the current version of Firebase is 9, not 7, that‚Äôs a problem.
With those problems solved, I was able to test the app and get it to work.
Oh, yes, I‚Äôm still missing the Edit button. Which is something I asked for, because not many tutorials cover that part of the app. They usually stick to adding and removing items.
So what‚Äôs wrong here?
Don‚Äôt get me wrong, Chat GPT is freaking awesome.
Coupled with Dalle-2 and MidJourney, this is the most interesting and advanced piece of AI software I‚Äôve ever tried.
But it‚Äôs not about to replace developers.
Why? Because of the two problems I highlighted above. If you haven‚Äôt picked up on it yet, the bot is simply spitting out content from articles used to train it.
So instead of creating the code for itself, which should be flawless, it‚Äôs simply showing me what other authors have written. Not only that, but it‚Äôs also showing me outdated content with errors.
That‚Äôs because it‚Äôs not capable of validating whether or not it‚Äôs indeed giving me what I asked for. Instead, it‚Äôs guessing (with a huge success rate) what I‚Äôm looking for, and giving me something that looks like it.
In other words, at its current state, Chat GPT will only be useful as long as the content used to train it is verified, and kept updated by someone else.
Did you like what you read? Consider subscribing to my FREE newsletter where I share my 2 decades‚Äô worth of wisdom in the IT industry with everyone. Join ‚ÄúThe Rambling of an old developer‚Äù !
We‚Äôre simply looking at yet another tool that might come in handy for our daily work.
How?
Right now when you have a problem you input the issue on Google and 8 out of 10 times you land on StackOverflow.
Now, you‚Äôll be able to input the question directly on Chat GPT and most likely, it‚Äôll give you the best-ranked answer from SO or some high-ranking article.
Should you blindly trust the code returned by Chat GPT?
No, you still have to use your brain. Sorry, not sorry.
But it will give you a quick head start.
In other words, the only thing that Chat GPT can do (at least right now) is to perform a faster Google search.
Which, don‚Äôt get me wrong, with the number of times we perform Google searches every day, it will most likely come in handy.
That said, it‚Äôs also true that Chat GPT will give you a single answer, which according to its algorithm, is the best one for you. But what about the rest? What about your own preferences?
Chat GPT will hide all other options, effectively boosting the few it considers valid for its users.
How good is that for content creators and people looking for options and alternatives? That‚Äôs for you to decide.
Copilot has received some lawsuits because it‚Äôs spitting copyrighted code without properly crediting the authors.
Will the same happen with Chat GPT? After all, as you‚Äôve seen already, the bot is returning content from (what I assume to be) other creators without properly crediting them.
Granted, the chat is currently free to use, so I don‚Äôt think there are any legal issues, but the moment they start charging for it (which already happened with Dalle-2) they will cause a legal battle, won‚Äôt they?
After all, they would be essentially charging you for content that is readily available online for free and without crediting the proper authors.
Chat GPT will not replace developers.
(Edit by the author: here is a follow-up story about this same topic that will show you another point of view about this.)
It will, at best, simplify your search process and help you find answers to your questions a lot faster.
That is if it‚Äôs not shut down due to legal concerns once it goes into a paid plan.
What do you think? Will you integrate Chat GPT into your dev workflow?
Bit‚Äôs open-source tool help 250,000+ devs to build apps with components.
Turn any UI, feature, or page into a reusable component ‚Äî and share it across your applications. It‚Äôs easier to collaborate and build faster.
‚Üí Learn more
Split apps into components to make app development easier, and enjoy the best experience for the workflows you want:
blog.bitsrc.io
blog.bitsrc.io
bit.cloud
blog.bitsrc.io
bit.cloud
Haran Shivanan
CrazyMax
WPNULL
Amit Kajwani
Joyce Lee
Jewel Chowdhury
in
Web Resource
Bibodoggy
Vikas Kesakar
in
Globant
"
217,https://medium.com/@anmol3015/the-episode-of-cristiano-ronaldo-and-my-7-learnings-b640fa219216,7 Lessons to Learn from The Cristiano Ronaldo Incident,"Cristiano Ronaldo is one of the most popular football players in the world. He has the world record of scoring the most international goals in history‚Äî 118 goals for Portugal. Yet, he was benched in the World cup knockout match against Switzerland.
What?‚Ä¶ The most important player of Portugal not playing in a very crucial knockout game of the tournament for Portugal‚Ä¶but‚Ä¶
Anna Peng
Phil Charles
Derin Bilgin
in
spiky.ai
On Purpose
in
On Purpose Case Studies
Swamy Ramchandran
UW Blueprint
in
UW Blueprint
Spyder
in
Know Thyself, Heal Thyself
Joshua Pruett
"
218,https://medium.com/@iamjennyaustria/representation-5421876891a7,An Observation That‚Äôs Been Bothering Me (And Why I‚Äôm Annoyed),"As a brown Southeast Asian woman, I‚Äôm pissed.
Lately, I‚Äôve been watching films and series to chill at night. I noticed a common theme that‚Äôs irritating. Let me type out my exact thought bubble:
Juliana S
Reverend Leonina Arismendi ≈æarkoviƒá
Shamontiel L. Vaughn
in
We Need to Talk
Olivia Nuss
Jonathan Jones
Maryann Auger (she/her)
Katica Roy
Brandon Williams
"
219,https://medium.com/towards-data-science/create-word-cloud-into-any-shape-you-want-using-python-d0b88834bc32,Create Word Cloud into any Shape you want using Python,"Data visualization (such as charts, graphs, infographics, etc.) gives businesses a value to communicate important information, but what if your data is text-based? If you want the stunning visualization format to highlight important textual data points, then using a word cloud.
If you are not familiar with word cloud, it is a picture consisting of a group of words where the size of each word represents frequency or importance. The bigger and bolder the word appears, the more often it‚Äôs mentioned within a given text and the more important it is. Word clouds are easy to read and simple to understand. The key words stand out to the reader and are visually appealing to the audience.
However, you might be bored seeing the simple form of the word cloud. What if I told you that WordCloud can also be custom made to our liking. In this article, we explore how to generate a word cloud in python in any shape that you desire. So, let‚Äôs get started.
If you want to see the full code of this article, please visit my github. The first step is to install the package that will be used, namely wordcloud. Open your terminal (Linux / macOS) or command prompt (windows) and type:
We will start by doing web scraping on an article on the internet. If you are not familiar with web scraping, I suggest you read my previous article entitled Web Scraping News with 4 lines using Python.
towardsdatascience.com
In this post, I will scrape the news from Wikipedia entitled ‚ÄòIce Cream‚Äô.
And we only take the text of the article, which is:
We will start by making a simple word cloud. The first step to take is to import dependencies that we will use.
Here we use the wordcloud library and matplotlib. The wordcloud library is used to generate the word cloud, while matplotlib is used to display the results of the word cloud. After that, we call the word cloud function and display the word cloud.
And this is the result of the simple word cloud that we created.
In addition, the wordcloud function has parameters, including:
Let‚Äôs try using the parameters above. First, let‚Äôs import the stopword provided by the wordcloud library
Then we enter the following code
And this is the result.
We can also change the font used. You can download fonts from the site https://www.dafont.com for personal use. Next, enter the path of the font into the parameters.
And this is the result
Next we will add a mask for the word cloud. Keep in mind the background of the image used must be white, otherwise, the system will consider the background as an object. In addition, the background cannot be transparent, because transparent colors will be considered black. I will use the following image as a mask.
We need to add some dependencies to load the image.
Next, enter the path of the font into the parameters.
And this is the result.
We can also adjust the colors used in the word cloud. Basically we are free to determine the color we will use, but in this article I will discuss the fairly commonly used. We will use just one color. But, we must define the function to be used.
The color format used is the HSL format (hue, saturation, lightness). For more details, please visit HSL Color Picker to find out more about the colors used. Then to form the word cloud, all we have to do is add the functions that we have created to the word cloud function.
And the image will appear like this.
Apart from that, we can also produce similar colors by randomizing within a certain range. I will add a random function to lightness to adjust the brightness of the colors.
Then, the same as before. Enter the function into the wordcloud function.
And the result will be like this.
In addition, we can define many colors that we will use. As an example.
And add the function into the wordcloud function.
And the result be like this.
And last but not least, generate color based on the mask. We will need the functions provided by the wordcloud library.
Then add the masking colors and add the function into the wordcloud function.
This is the final result.
As we can see, the color of the word cloud follows the color of the original image.
So for this post, I hope you get new knowledge from what I have said. If you have other opinions, please write in the comments. In the future, I will analyze the usage of this word cloud for text analysis.
Bhanu Mittal
in
Bhanu Mittal
Peethambaran c.v
Shaily jain
Holta Stojku
in
BAM.Money
Asitdubey
in
Analytics Vidhya
Cyjucine
Kyla Scanlon
in
The Startup
Eric Sun
in
The Startup
"
220,https://medium.com/cometheartbeat/understanding-loss-functions-6ad2c0a5bc23,Understanding Loss Functions,"In deep learning or machine learning, the loss function is crucial. Assume you are working on an issue and are prepared to present your client with a machine learning model that you have trained on the dataset. However, how can you be sure that this model will produce the best outcome? Is there a statistic or method you can use to rapidly assess your model against the dataset?
Yes, deep learning or machine learning use loss functions. We will examine several loss functions in this post.
A loss or cost function (also known as an error function) is a function that converts an event or the values of one or more variables into a real number that intuitively represents some ‚Äúcost‚Äù connected to the occurrence.
The Loss function may be thought of as a way to assess how effectively your algorithm models your dataset. It is a mathematical function of the machine learning algorithm‚Äôs parameters.
In basic linear regression, slope(m) and intercept are used to calculate predictions (b). The (Yi ‚Äî Yihat)2 loss function is used for this. The function of slope and intercept is the loss function.
Any statistical model relies on loss functions because they define the standard against which the model‚Äôs performance is measured and because minimizing a given loss function yields the parameters that the model learns. If the loss function value is lower, the model is good; if not, we must adjust the model‚Äôs parameters to reduce loss.
It provides a probability value for a classification task between 0 and 1. The average difference between expected and actual probability is calculated using cross-entropy.
Output:
Where to use Binary Cross Entropy Loss?
One of the examples is Spam Detection, which tells whether an email is spam or not.
The hinge loss is a type of cost function in which a margin or distance from the classification boundary is factored into the cost calculation. Even if new observations are correctly classified, they may incur a penalty if the margin from the decision boundary is insufficient. The hinge loss increases in a linear fashion.
These are particularly used in SVM models.
Output:
Where to use Hinge loss ?
The hinge loss function, which was primarily created for use with Support Vector Machine (SVM) models, is an alternative to cross-entropy for binary classification issues. It is designed to be used with binary classification with target values falling between -1 and 1.The hinge loss function assigns greater error when there is a mismatch in sign between the actual and projected class values, encouraging instances to have the right sign.
Real-time model analysis allows your team to track, monitor, and adjust models already in production. Learn more lessons from the field with Comet experts.
The mean squared error (MSE) of a regression line indicates how close it is to a set of points. It accomplishes this by squaring the distances between the points and the regression line (the ‚Äúerrors‚Äù). Squaring is required to remove any negative signs. Larger differences are also given more weight. Because you‚Äôre calculating the average of a set of errors, it‚Äôs called the mean squared error. Accuracy improves with the decrease in value of MSE.
Where to use MSE loss?
The default loss to use for regression problems is the Mean Squared Error, or MSE, loss. If the distribution of the target variable is Gaussian, it is the preferred loss function mathematically speaking within the maximum likelihood inference paradigm. The loss function should be assessed first, and only altered if necessary.
The simplest loss function is called the Mean Absolute Error (MAE). The MAE is calculated by averaging the difference between the actual value and the model prediction throughout the whole dataset.
Output:
Note: Use linear activation function at the final neuron in regression.
The distribution of the target variable in some regression problems may be primarily Gaussian, but it may also contain outliers because it is more resistant to outliers, the Mean Absolute Error, or MAE, loss, is a suitable loss function in this scenario.
The Huber loss, which is employed in robust regression and is a loss function in statistics, is less sensitive to outliers in the data than the squared error loss.
If we wish to lessen the discrepancy between the expected and actual value of a variable, we can take the mean squared error and the natural logarithm of the predicted value. This will solve the issue the Mean Square Error Method had. The model will now penalize less than it did with the previous approach.
Output:
When to use Logarithmic Mean Squared Logarithmic Error Loss ?
When predicting target values that have a wide range of values, we don‚Äôt want to punish our model severely by using mean square; instead, we might first calculate the natural log of each projected value before computing MSE.
This article provided an overview of the loss functions that are applied to classification and regression issues. One must realize that there is no one-size-fits-all solution and that selecting a loss function is just as crucial as selecting the appropriate machine learning model for the particular task.
The complete code of the above implementation is available at this notebook.
Editor‚Äôs Note: Heartbeat is a contributor-driven online publication and community dedicated to providing premier educational resources for data science, machine learning, and deep learning practitioners. We‚Äôre committed to supporting and inspiring developers and engineers from all walks of life.
Editorially independent, Heartbeat is sponsored and published by Comet, an MLOps platform that enables data scientists & ML teams to track, compare, explain, & optimize their experiments. We pay our contributors, and we don‚Äôt sell ads.
If you‚Äôd like to contribute, head on over to our call for contributors. You can also sign up to receive our weekly newsletter (Deep Learning Weekly), check out the Comet blog, join us on Slack, and follow Comet on Twitter and LinkedIn for resources, events, and much more that will help you build better ML models, faster.
Eric Muccino
in
Mindboard
SeungHoon Choi
in
SeeSo Tech Blog
Jagannath Rajagopal
Aman Priyanshu
Aids
Kiran Matty
in
Aerospike Developer Blog
BT
Tan Pengshi Alvin
in
MLearning.ai
"
221,https://medium.com/javascript-in-plain-english/what-is-jamstack-the-best-way-to-build-content-sites-ae381d8a92da,What is Jamstack: The Best Way to Build Content Sites?,"In recent years, client-server architecture has taken the web development world by storm by enabling builders to employ a database to store content and a client to request data from the server. With the development of React, Angular, and Vue, concepts such as virtual DOM, JSX, rerendering, two-way binding, and other such topics became jargon for the industry.
These concepts made web apps more powerful with a lot of technologies and tools to integrate alongside databases and servers.
And that‚Äôs where the process of creating a web application became more challenging since the developer had to install and configure a number of tools. All of these need numerous hours of development work, delaying the launch of the product.
Having a set of best practices and industry-standard architecture can simplify the process and this has led to the growth of a modern opinionated architecture called Jamstack.
Frameworks like Next.js, Gatsby, and Hugo make good use of this architecture and have brought new ideas to the table as well including incremental site regeneration, TypeScript support, intelligent bundling, and route pre-fetching.
Static websites started to gain popularity in 2015 because of Jekyll and other related technology. In the same year, Mathias Biilmann and Chris Bach from Netlify came up with the term ‚ÄúJamstack‚Äù.
Now, what is Jamstack?
Jamstack is ‚ÄúA modern web development architecture based on client-side JavaScript, reusable APIs, and prebuilt Markup,‚Äù according to Mathias Biilmann (CEO & Co-Founder of Netlify).
In short, Jamstack deals with JavaScript, API, and Markup.
You may have built a number of websites with the aid of JavaScript, API, and Markup, so it is not at all new in terms of technologies; instead, it is a fresh approach to how we use them in conjunction.
Jamstack is different from React & Angular in the sense that it is not a framework but an architectural approach that decouples the web experience layer from data and business logic. It allows the easy addition of custom logic and third-party integration through the use of APIs.
Let‚Äôs take a look under the hood to understand the technologies that power Jamstack:
This is the first element in building a Jamstack website.
If you‚Äôre a web developer, you‚Äôre probably aware that JavaScript is used to provide dynamic behavior and logic to websites, providing a great user experience. The same idea holds true for Jamstack.
Jamstack decouples all the content, allowing complete freedom, as compared to the traditional architecture where the frontend technology is tied with a database and server.
The J in Jamstack stands for JavaScript, but you may use any JavaScript framework, even Python or Go.
And as a result, Jamstack is more flexible in terms of the front-end technologies you may use.
A common technique for connecting clients and servers is an application programming interface (API). We can do it via GRPC, GraphQL, or even REST.
You can think of it as a convenient channel for passing the client‚Äôs request and the server‚Äôs response.
Let‚Äôs take an example.
When the user asks about the book‚Äôs data, the server responds with JSON data, as shown in the above figure.
Server-side actions are abstracted into reusable APIs and made accessible using JavaScript over HTTPS, much as how APIs function in Jamstack. For that, you may use either your own custom function or services from third parties.
That is why the server no longer has to perform the heavy lifting; everything is now handled on the client side. And, as APIs become more widely available, they may be utilized as microservices to easily integrate and link to provide a variety of features.
The third element of Jamstack is markup.
A content request is normally sent to the server by a user. The server follows a long sequence to find the appropriate information and then responds. A user should not have to wait that long.
This is where markup comes into play.
Jamstack employs prebuilt markup instead of relying on the server to build content for each request. Let‚Äôs dig a little deeper.
Static site generators take data or information as input, assess it, and then generate prebuilt HTML. These prebuilt markups, or HTML pages, are hosted on CDN, making it easy for visitors to get the desired content.
There are over 300 static site generators available to choose from.
Before HTML, CSS, and JavaScript were introduced, websites were exclusively static; however, after their debut, developers began to create dynamic webpages that were then served by servers.
In essence, when a user wants information, the client asks the server for the content, the server retrieves the specified content, applies some logic, and then delivers the content.
But as web development became more complex, some comment patterns and architectures emerged.
And it was from there the term Jamstack was born.
Here, the developer uses JavaScript or another javascript framework to write the code, which is then pushed to a source repository like Git. From there, the files are automatically sent to the content delivery network (CDN), giving us access to Jamstack‚Äôs pre-rendering functionalities.
A CDN may be thought of as a group of servers positioned across the world to load websites quickly and efficiently.
To begin with, Jamstack is not only for creating static websites; you can also develop dynamic websites, but they will be shown as static pages. This distinguishes Jamstack from other web development frameworks.
Because static websites are built at build time, they are faster and SEO friendly.
2. Faster performance
A static site generator receives a set of content or templates as inputs and then provides the result, which is essentially prebuilt markup that is distributed to a CDN.
As a result, when a user requests information, the output in the form of pages from a CDN may be instantly delivered, making the process easier and faster.
CDN distributes website content to many groups of servers so that when someone requests the content, it is sent from the closest server, allowing for faster communication.
3. Cost-effective, and economical
Jamstack websites and web apps are not hosted on an original server; instead, they are provided straight via a CDN, which reduces costs significantly.
The process of upgrading the website or web apps is generally automated, which simplifies the work process significantly.
Even CDNs like Cloudflare offer a generous free trial and there is support for CI/CD pipeline with Github. This makes Jamstack cost-effective and inexpensive.
4. More Secure
Traditional server-side apps are vulnerable to a variety of attacks since they employ many APIs to transfer content to the original server.
Because CDN contains just read-only files and no active connections, the likelihood of being exploited by a hacker is much lower.
5. Scalable
We have no physical server in Jamstack, and even updating the websites is primarily automated, which is why we have to just update the website based on the tech stack we use.
There is no complicated logic that can be cached since the web pages are saved in the CDN eliminating the need to have a running server & with automatic deployment, it is simple to scale these websites.
With all the benefits that come with Jamstack, it can be an ideal choice for your content-heavy projects. Since Jamstack can be developed with a number of technologies, including frameworks and APIs, the learning curve is shallow.
In fact, if you are familiar with React and GraphQL, you can get started with Jamstack. Even if you are familiar with some other technologies, you may follow the same steps to incorporate them.
Lastly, Jamstack makes your website operate well, load quickly, make it secure, and be scalable.
The two prominent frameworks that make use of Jamstack are Gatsby and Next.js. You can generate pixel-perfect and highly extendible code in both of these frameworks directly from Figma and Adobe XD designs using the Locofy.ai plugin.
Using the plugin, you can break down your design into components that expect props which makes it easy to plug in a CMS of your choice and build a Jamstack app.
Hope you like it.
That‚Äôs it ‚Äî thanks.
Consider becoming a Medium member if you appreciate reading stories like this and want to help me as a writer. It costs $5 per month and gives you unlimited access to Medium content. I‚Äôll get a little commission if you sign up via my link.
Originally published at https://blog.locofy.ai.
More content at PlainEnglish.io. Sign up for our free weekly newsletter. Follow us on Twitter, LinkedIn, YouTube, and Discord. Interested in Growth Hacking? Check out Circuit.
Mutant Ape Wives NFT
Raja Tamil
Alexis Wei
Piyali Das
Hoang Dinh
in
Level Up Coding
Zain Ahmed
Anupama Singh
200OK
in
Towards Dev
"
222,https://medium.com/gitconnected/google-already-knows-these-facts-about-its-engineers-4e5b33740b3a,Google Already Knows These Facts About Its Engineers,"It‚Äôs no secret
After working at Google for the past 4 months, I started to realize what the company already knows about its employees. Today there are 178,000 employees at Google and the company keeps tabs on all of them. Especially its engineers.
From abusing its perks to gaming promotions, Google is already a step ahead of its employees.
Google snacks are a nice perk for any employee working at the company. There are healthy fruit options including bananas, apples, plums, and savory options like kettle chips, jerky sticks, and kind bars.
However, these snacks are out in the open for anyone to enjoy. It‚Äôs not uncommon for workers to take boatloads home with them and leave the pantry dry.
Google is aware of this, if they really cared they‚Äôd adjust how much is available in the pantry or introduce machines to dispense the snacks. But the added restrictions wouldn‚Äôt make the office a fun place to be and that affects the output of its workers. To Google, it doesn‚Äôt seem worth addressing.
There‚Äôs massage chairs, meditation rooms, complementary gyms, free food, and unlimited snacks. Even if you have these perks available, it gets lonely and you‚Äôll want to share these awesome experiences with your friends.
While there‚Äôs a ‚Äú2 guests a month‚Äù policy, there isn‚Äôt always a secruity guard enforcing this at each campus. There are some exceptions like NYC but the city has gated building in general.
So whether employees enjoy the benefits as intended or share them with friends more than they‚Äôre supposed to, Google hasn‚Äôt done anything to address this. There‚Äôs usually more than enough food in the kitchen and snacks are a free-for-all in the pantry.
If they cared they‚Äôd invest more in security but that hasn‚Äôt always been the case.
LeetCode has become such a huge platform that it‚Äôs synonymous with interview prep. Coding interview questions are even called ‚ÄúLeetCode problems‚Äù by some interviewers. When your company is associated with a verb or noun, that‚Äôs when your marketing has gone through the roof (‚Äújust google it‚Äù, ‚Äúgrab me a clorox‚Äù)
Google is aware that candidates and even its internal workers used LeetCode as a resource to prepare for its technical interviews. It seems unfair to Google since some candidates might see a question they solved already while others have never heard of LeetCode. There‚Äôs even an internal doc titled ‚ÄúThe LeetCode Problem‚Äù to brainstorm how to address this.
The company knows its not always fair but they haven‚Äôt made much progress on changing its interviewing process. It‚Äôs another problem that isn‚Äôt worth it for the company to address. If a candidate performs well on the interview, however they do it, then they believe they can succeed in the role.
If a process exists, it can be optimized. Promotions at any level at Google go through a performance review process before being approved. One of the most prestigious promotions is the transition to senior engineer.
The simple requirement for senior engineer at Google is to either
Of course the minimum of ‚Äúmultiple‚Äù quarters is 2 and some projects are easier to complete than others. But sometimes doing the minimum won‚Äôt grant the engineer the promotion, so they‚Äôll likely try it again.
And by trying again, the engineer builds a track record which shows they can perform at the expectations of a senior engineer. Whether someone is making legitimate contributions or gaming a process, Google has both scenarios covered.
Google is a smart company and its decisions to take action on its perks and processes for employees is a conscious choice. They‚Äôre always weighing whether it‚Äôs worth taking action on these areas or to let the tragedy of the commons be.
More often then not, they just let their employees be because employee output with psychological safety is worth more than saving a few dollars on snacks.
Did you see what happens when you click and hold the clap button?
The $4 resume template that helped me get into Amazon and Google
If you enjoyed this experience consider being a member for more content like this!
1000 are following on Twitter!
And follow me on my quest to 100,000 followers on LinkedInAlex Nguyen | LinkedIn
WinTask
fulalas
Lara Richards
Aaditya Thakur
projekt202
Alexandru Dumitru
Tejna Dasari
Amine boubekeur
"
223,https://medium.com/technology-hits/notion-vs-craft-the-race-to-reinvent-note-taking-8416b0d02719,Notion vs. Craft ‚Äî The Race to Reinvent Note-Taking,"Notion has over 30 million users today, with an annual revenue of $43.5 M in 2022 ‚Äî an increase from $32 M in 2021.
There are many reasons why Notion‚Äôs growth strategy is different. Using a large base of people, segmenting their audiences, and‚Ä¶
Bettina Chou
Emma Dyer
in
Catalyst
Max Brinckmann
Grace Gao
Tsubasa Kato
Ragi Vipin
in
Muzli - Design Inspiration
Fireplaces Directory
Laura Meng
"
224,https://medium.com/towards-data-science/mathematical-optimization-heuristics-every-data-scientist-should-know-b26de0bd43e6,Mathematical Optimization Heuristics Every Data Scientist Should Know,"There are many different ways to solve mathematical optimization problems. You can use greedy algorithms, constraint programming, mixed integer programming, genetic algorithms, local search, and others. Depending on the size and type of the problem, and the solution quality desired, one technique may work better than the other.
This post gives an overview of different heuristics for solving discrete optimization problems. First, I explain the three components you need to describe an optimization problem mathematically. Then I will give an explanation of some common and well-performing search heuristics.
Here is a short refresher on mathematical optimization. In order to define an optimization problem mathematically, you need the following three components: decision variables, constraints and an objective.
Let‚Äôs take a look at an easy example. You are a small post delivery office, and you earn a different amount of money for every package you deliver. The delivery van has a limited amount of space. The delivery office wants to deliver the highest total value possible per delivery round. What are the packages you should deliver?
The decision variables can take different values. The goal is to find the best values for the decision variables. What are the best values? That depends on the objective and the constraints. In the post delivery example, every package has a binary decision variable. The variable is 0 if the package will not be delivered, and 1 if the package will be delivered.
Constraints are things that are not allowed or boundaries, by setting these correctly you are sure that you will find a solution you can actually use in real life. A constraint in the post delivery example: you can‚Äôt deliver all packages, because the delivery van has limited space. If the maximum space of the van is equal to 600, you should add a constraint that makes sure the packages selected will not exceed this limit.
The objective is the goal you have in the optimization problem, this is what you want to maximize or minimize. The goal of the delivery office is to select the most valuable packages for delivery. In the objective, you want to maximize the total value of the selected packages.
Here‚Äôs the complete description of the example:
If a problem is well defined (i.e. there exists a feasible solution), there always exists at least one optimal solution for the optimization problem. It can be hard to find one of these optimal solutions, especially when the problem is large and complex. Not all techniques discussed in this post are guaranteed to find the optimal solution. But, if you apply them correctly to large problems, they can be faster than a solution that uses constraint or mixed integer programming techniques.
There are different heuristics you can use to solve optimization problems. Here, I will explain some of them.
I assume you are already familiar with brute force, which is trying all the possible solutions and keeping track of the best one. Another technique you may know is dynamic programming, where the problem is broken down into smaller subproblems. If you aren‚Äôt familiar with dynamic programming and brute force, this post explains them. Brute force and dynamic programming are perfectly fine to use when your problem is small. When problem size increases, they will take way too long and are inefficient. Brute force and dynamic programming aren‚Äôt heuristics, because they don‚Äôt reduce the search space. You can decide to combine brute force with local search or genetic algorithms, by systematically testing all the possible solutions (brute force) of a smaller subset of solutions.
To solve the delivery office problem, an easy way to start is with greedy algorithms. They give a baseline and offer solutions really fast. The idea behind greedy is that you choose a package until the delivery van is full. You don‚Äôt choose any package, but you can for example start with the most valuable package. You continue doing this until the van is full. Let‚Äôs say the maximum capacity of the van is 60, here are the packages we would select:
There are other ways to decide the next package. By dividing the value and the size of every package, you get a value per size unit per package. You could describe this as the value density. By selecting the packages with the highest value per size unit, it‚Äôs possible to come up with an even better solution.
An advantage of greedy is that it‚Äôs fast. But for more complex problems, in most cases the solution will be far from optimal.
The next technique is an interesting one. Local search is pretty intuitive. It works as follows: you start with a solution, and you are going to improve that solution by making local moves. That means you apply a small move to the current solution that improves the objective. You continue to apply local moves until there are no more moves available that improve the objective.
Let‚Äôs look at the delivery example again. We can start with a solution:
A local move could be to swap a selected package with a package that is not selected. We keep an eye on the capacity constraint and we try to keep that satisfied for every local move. An example of a move could be:
With this move, the new objective value is 115. We remove the package with the lower value from the selection and add the package with the higher value, while still having a feasible solution.
All the possible solutions you can reach by applying one local move, is called the neighborhood of the current solution.
You can‚Äôt exceed the capacity limit of the van. So in this case, if a package is larger than any other package, even if the value is high, we will never select this package!
That‚Äôs a downside of local search. You can get stuck at local optima:
There are ways to overcome this problem. You can choose to swap multiple packages at once and make that equal to one move. By doing this, your neighborhood is increased and you can reach more solutions.
You can also decide to start with multiple solutions instead of one. Then you repeat the swapping procedure for every solution, this is called iterated local search.
Another approach is to choose actions that make the objective worse with a certain probability:
If the temperature parameter is large, there is a high chance of accepting a degrading move. If the temperature is small, this chance is low. Simulated annealing uses this probability. It starts with a high temperature, and decreases it progressively. This means that in the beginning, you are performing a random walk across solutions. When the temperature decreases, the search becomes local. Simulated annealing had excellent performance on hard benchmarks.
Tabu search is the final technique I want to mention for escaping local optima. The idea of tabu search is to keep track of solutions you already visited. It‚Äôs not allowed to go back to them again. It can be costly to keep all the previous solutions in memory. You can instead decide to store transitions or keep the tabu list of a fixed size.
It‚Äôs possible to combine techniques like iterated local search, simulated annealing and tabu search.
You can also decide to use genetic algorithms. The key idea of genetic algorithms are to reflect the process of natural selection. A solution to the problem is called an individual. First, you start by generating the initial population. This population exists of individuals. Then, you calculate the fitness of every individual. You can compare the fitness function with the objective. The fittest individuals (they have the best objective values) are selected for reproduction in order to produce offspring.
For our delivery example, every package is a gene which can take the value of 0 or 1. Our initial population with four individuals could look like this:
Now we select the fittest individuals (highest objective values) to produce offspring. In our case individual 2 and 4 have the highest fitness values. There are different ways possible to produce offspring, a common way is to choose a crossover point at random and exchange genes until this crossover point.
The next step is mutation. We flip some of the genes with a low random probability, in our case let‚Äôs take 0.14 (1 divided by 7, 7 is the number of packages). This is an important step because we want to maintain diversity and prevent premature convergence. We are going to mutate offspring 1:
Now, we will compute fitness for the new individuals. The population has a fixed size. The individuals with the lowest fitness values die.
Here is the complete algorithm:
With genetic algorithms, it‚Äôs also possible to get stuck at local optima. There are multiple ways to overcome this. You can create subsets of the initial population and randomize during the selection phase. This prevents using the same population again and again during selection. Another way to avoid local minima is to give an extra reward to individuals that are surviving longer and/or to individuals that are more unique than others, because they can help in finding a generalized solution.
The final way discussed here for quickly finding high quality solutions is through combining different techniques.
An example is large neighborhood search, where local search is combined with constraint programming (CP) or mixed integer programming (MIP). Downsides of CP and MIP are that they can have difficulties with large problems and need a lot of time to get to the optimal solution. By combining local search with CP or MIP, you have the best of both worlds. You can solve small subproblems with CP or MIP, and select a new subproblem with local search.
The steps for large neighborhood search are:
During solving you keep track of the best solution. The neighborhood can be defined by, for example, fixing a set of variables.
Another example of a hybrid approach is the memetic algorithm. Memetic algorithms combine genetic algorithms and local search.
In this post you‚Äôve seen different heuristics to solve mathematical optimization problems. Hopefully you can find solutions to optimization problems quickly with local search, genetic algorithms or a hybrid approach! There are other interesting and well-performing search heuristics, like particle swarm optimization, colony ant optimization and stochastic tunneling.
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
Ednalyn C. De Dios
in
Towards Data Science
Yitaek Hwang
in
IoT For All
Saurabh Raj
Naina Chaturvedi
in
Coders Mojo
Marwan A Rashed
Babhisekhddude
Ruju H Joshi
Monodeep Mukherjee
"
225,https://medium.com/@alexcancode/the-sad-fact-i-realized-working-at-google-22b8cf59d722,The Sad Fact I Realized Working at Google,"Because I thought it was too big to fail
You have to be living under a rock if you have never heard of Google. From leading FAANG to building an amazing search engine, the company may have branded itself as the greatest engineering company of all time.
But when you dig deeper into the brand, you learn the truth about Google as a tech company.
CuckooTech
Igor Carvalho
Kintone
in
Digital Transformation Talk
Numenta
Empxtrack Inc.
Maria Angelova, CEO of Rebellious Intl.
in
Authority Magazine
TechDoQuest
Candas Ifama Barnes
"
226,https://medium.com/new-earth-consciousness/lightworkers-you-can-manipulate-energy-76e8aab4ee39,Lightworkers ‚Äî You Can Manipulate Energy,"Earlier this year, I began to do something odd and unusual. My loves, bare with me as I try to explain this phenomenon in simple English. I have never shared this with anyone, but I feel guided to do so today. I will explain this on a‚Ä¶
Derrick McQueen
in
The Orange Journal
Meeka Anne
A M Wilson
Mr Fantastic
El.
Connor Hearld
Hayden Bowen
Kelly Borowski
"
227,https://medium.com/pcmag-access/i-watched-a-training-video-for-iranian-hackers-17a1077e4750,I Watched a Training Video for Iranian Hackers,"I may not be ready to work for a state-sponsored hacking group, but at Black Hat, I learned some surprising things about how these groups operate.
By Max Eddy
Security researchers generally don‚Äôt discuss the little mistakes hackers make, and they never show hacking group training videos. But that‚Äôs exactly what happened at this year‚Äôs Black Hat, where a pair‚Ä¶
womenfree
PCMag
in
PC Magazine
Polarity Technologies Ltd.
PCMag
in
PC Magazine
PCMag
in
PC Magazine
PCMag
in
PC Magazine
PCMag
in
PC Magazine
PCMag
in
PC Magazine
"
228,https://medium.com/towards-data-science/all-you-need-to-know-about-publishing-software-on-bioconda-680d48e52868,All You Need to Know about Publishing Software on Bioconda,"Have you had trouble installing packages and running a gazillion commands to install dependencies? If you are lucky (which most of the time you won‚Äôt be), you will end up installing the package without having any dependency issues or version conflicts. Working in interdisciplinary sciences has made me aware of how hard it is to get these tools to run unless you know what is actually happening from a programming view. You wish that these tools come bundled with all the dependencies and can be installed/run without having conflicts with what you already have installed.
Fear not ‚Äî my dear readers! Package managers will come to the rescue! These are software tools, like conda and pip, that automate the process of installing, maintaining, and removing programs in a consistent manner. Having your package on such a repository can be helpful for your users to install it without much hassle. It also increases the visibility and reach of your package. In this article, I will walk you through how to publish your Python package related to bioinformatics on bioconda with example code snippets, templates and best practices.
My community (including myself üòÉ) loves bioconda! Bioconda allows you to install packages related to biomedical research using the conda package manager. Bioconda is basically a channel (which is a location where packages are stored) that has recipes which contain the metadata of the software packages published. We have to create a similar recipe and add it to the bioconda channel. Assuming you have a GitHub account and have installed Miniconda or Anaconda, let‚Äôs get started.
If this is your first time publishing on bioconda, you have to set up your copy of the bioconda-recipes repository.
github.com
Click here to create a fork of the bioconda-recipes repository.
Now create a local clone of the repository using the following command. Make sure to replace <USERNAME> with your GitHub username.
Then add the main bioconda-recipes repo as an upstream remote so it becomes easy to update changes made.
If changes to the original repository were made after you made your copy, you can update your local copy using the following command.
You can go to your GitHub repository and check if your fork is up-to-date with the original repository.
It is recommended to create your own branch to work on. Assuming your tool name is mytool, let‚Äôs create a branch using the following command. Feel free to replace mytool with your tool name.
A recipe will contain a meta.yaml file with all the metadata in the recipe. Normally, having this file should be enough for a pure Python package. If you need to compile more packages (e.g., C/C++ packages) or export paths, these should be added in the build.sh file for macOS and Linux or the bld.bat file for Windows.
Let‚Äôs begin by moving into the recipes directory.
If you already have your package on the Python package index (PyPI), you can use the conda skeletoncommand as follows to create a template automatically. The following command will create a folder named mytool with a meta.yaml file inside.
If your code should be downloaded from a GitHub repository you have and compiled, then you should have a release published on GitHub. Here is a template assuming your package follows the basic Python package structure. Make sure to change <USERNAME>, mytool and version number to match your GitHub account and package details.
Start with the name and version of your package under package.
Then you should point to the release under source. Make sure to add the correct URL with the correct SHA256 checksum. You can get the SHA256 checksum of the source code using the following command.
Under build you have to provide the build number (starting from 0), the platform and build commands (here I have added the build command for setup.py). Here I have defined noarch: python meaning that this is a pure python noarch package that contains no operating system-specific files. If you want to call custom scripts from the command line, you can add them under entry-points which will define shell commands by identifying a Python function to run (which is main in our example).
Then you have to specify compilers, preprocessors, Python versions and dependencies (with specific versions pinned if you need to) under requirements.
Next, you can specify a command to test the installation under commands in tests. I normally call the help command of my tool. Make sure to keep these test commands quick and simple.
Then you can add details about the package such as the home page URL, documentation URL, license type, licence file and a summary of the tool under about.
Finally, you can add your GitHub username inrecipe-maintainers and DOI of the publication inidentifiers under extra.
If you want to read more details about the sections in the meta.yaml file, check out the official bioconda documentation.
bioconda.github.io
If you have no idea how to make a recipe, check out other recipes for example code that you can use and copy into your own recipe directory. Here are some great example recipes.
You can check your changes using the following command.
You can commit your changes and push them from your mytool branch to your forked GitHub repository.
Now go to your forked GitHub repository and you will see a message saying This branch is 1 commit ahead [...] bioconda:master. You will see a button called Pull Request, click on it and follow the instructions. You should add describe your pull request. Here is a pull request I created to add one of my tools. If your package has a publication, make sure to add those details as well.
Once you have created the pull request, the bioconda build system will start testing your changes. If you are very lucky, you won‚Äôt have any issues and your build will pass. If not, edit your recipe to fix the issues and push the changes to your branch again (and again and again).
When your build ‚Äúturns green‚Äù and all the checks have passed, you can issue the @BiocondaBot please add label command.
In the meantime, you can issue the command @BiocondaBot please fetch artifacts to get the links to CI-built packages/containers. You can download these builds and use them to test packages locally. Once you have downloaded the build, create a new conda environment (DO NOT install in your base environment) and simply run the following command to install your package. Make sure to replace <build_file_name> with the name of the .tar.bz2 file you downloaded.
Now you can test your installation using the following commands.
If you have already set up test cases, you can run them to make sure your package works correctly end-to-end. I usually do this test and post the outputs on the pull request to show everything is good to go. Here is a test run I posted in one of my pull requests.
If everything goes well, someone from the bioconda team will approve your pull request and the changes will be merged into the main repository.
Bioconda only supports dependencies from the defaults, conda-forge and biocondachannels only. So if you have dependencies from any other channels, there is no way to specify them in the meta.yaml file and your recipe will fail the build (still there are open issues). In that case, you will have to publish your package on your own channel which we will talk about in a future article.
Hope you found this article useful to publish your own bioinformatics software on bioconda. Feel free to browse through the bioconda tutorials for more details.
Happy package-publishing!
Cheers!
[1] Initial Setup ‚Äî Bioconda documentation available at https://bioconda.github.io/contributor/setup.html
[2] Contribution Workflow ‚Äî Bioconda documentation available at https://bioconda.github.io/contributor/workflow.html
[3] Tutorials ‚Äî Bioconda documentation available at https://bioconda.github.io/tutorials/index.html
[4] Experience from my personal hiccups while publishing my packages üòÅ
Sung Kim
Sachin Sharma
CI Group
Vijini Mallawaarachchi
in
Towards Data Science
Dennis Ganzaroli
in
Low Code for Advanced Data Science
Aidan Padala
Vijini Mallawaarachchi
in
Towards Data Science
Arpan Mishra
in
Towards Data Science
"
229,https://medium.com/@tmc222/cisco-certified-network-associate-ccna-200-301-7f1c7df1a729,Cisco Certified Network Associate (CCNA) 200‚Äì301,"Recently, I passed the CCNA 200‚Äì301 provided by Cisco on my first attempt and I have decided to create this short article to share my experience with acquiring this certification, in the hope that it may help anyone who is considering on studying for the exam. I have provided a link to the Cisco website below where you can learn more about the CCNA 200‚Äì301 certification if you are interested.
www.cisco.com
There is a wide variety of training resources available for the CCNA 200‚Äì301 certificate. I personally used the following as my study material but this is not a definitive list and you may find other resources that work better for you:
I used this Udemy course as my core study material. The course is delivered through a combination of video lectures and prepared packet tracer labs. The video lectures are very thorough and cover all the CCNA 200‚Äì301 exam topics in depth, while the prepared packet tracer labs allow you to implement what you have learnt from the videos and become familiar with the Cisco IOS command-line interface (CLI).
https://www.udemy.com/course/cisco-ccent-icnd1-100-105-complete-course-sims-and-gns3/
I used this book by Todd Lammle to supplement my core study material and read up on any topics that required additional attention. I chose not to use the Volume 1 book, since it is mainly a primer for the basics of networking and I already had some prior knowledge around networking fundamentals.
www.amazon.co.uk
For practice questions, I used the Boson ExSim Practice Exams for Cisco 200‚Äì301 CCNA. This provided three practice exams with a pool of 312 questions, which could be taken in either study mode or simulation mode. Each question also came with a detailed explanation of why one option was correct and the other options were wrong. These questions really helped me to become familiar with the style of questions used in the CCNA exam and were a good investment.
www.boson.com
To keep track of my progress and ensure I was not missing any of the key topics included in the CCNA 200‚Äì301 exam, I used the following PDF study guide created by the Github user sy1492.
github.com
Over the course of several months, I focused primarily on the training material listed above and prepared by:
The duration of the CCNA 200‚Äì301 exam was 120 minutes and you can expect about 100‚Äì120 questions. Obviously, I cannot mention any details about the exam itself due to NDA but I would provide the following advice to anyone who is preparing to sit the exam:
The CCNA 200‚Äì301 is a challenging exam, that encompasses a wide range of topics and requires a lot of preparation to pass. While preparing for the certification is a lot of work, I hope this article has helped a little for anyone considering the CCNA 200‚Äì301 certification. Thanks for reading till the end and good luck with your CCNA 200‚Äì301 certification preparations!
Maxime Atton
Bruce Callow
WeNano
Fabrizio Duroni
Ben Hubsch
Elye
in
Mobile App Development Publication
Brian Cheung
omgzui
in
Level Up Coding
"
230,https://medium.com/@VishalGarg1/how-to-get-into-cyber-security-part-2-career-paths-ce831df6d376,How to get into Cybersecurity (Part 2 ‚Äî Career Paths),"It is no secret that the cybersecurity field is growing fast, yet there is a worldwide shortage of skilled security professionals.
A workforce study conducted by ISC2 in 2021 concluded that despite the cybersecurity workforce gap reducing from 3.12 million a year ago to 2.72 million in the current year, the global demand for cybersecurity professionals continue to outpace supply, resulting in a huge workforce gap.
With the technological advancements and the amount of transformation happening right now, there has never been a better time to get into cybersecurity.
Cybersecurity field is vast, and it can be overwhelming for someone new to find out where to start, which career path to take, and which jobs to apply for. However, if you have a clear understanding of which career paths and job functions exist, it will help you narrow down your choices, and to prepare yourself for a career path that you want to take.
These career paths are not set in stone and it‚Äôs not that once you have chosen a career path, you cannot switch later. In the beginning, you only need to make a start somewhere, and narrowing down your choices based on your skills and interests will help you to get a good start in the field.
This series of articles is to help you understand what it takes to work in cybersecurity, and where and how to get started. In my first article in the series, I had explained about the soft-skills that you may require to build a successful career in cybersecurity. In this article, I will explain the career paths that may be available to you to get your foot in the door. In my next article, I will explain what technical skills you might require and how you may gain these.
At a high-level, these can be divided in three main categories:
# 1 ‚Äî Offensive Security
Offensive security mimics hacker techniques, breaking into computer systems and finding gaps using tools and techniques employed by real life hackers. The main purpose of using these techniques is to have a hacker mindset to identify security gaps in an organisation‚Äôs computer systems and to fix these before the real hackers can find and exploit these for nefarious purposes.
Working in offensive security might be challenging sometimes since you need to keep up with new attack techniques that are discovered on regular basis and employed by hackers in the wild, however, this can also be equally satisfying and rewarding once you have identified critical security vulnerabilities and have prevented a potential attack in the future. This is where you offer value to your organisation or to your clients.
Career paths in offensive security may include penetration testing (also called pentesting), ethical hacking or red teaming. Even though some large organisations may have internal pentesting teams, most of the times, you will be working with cybersecurity consultancies offering consultancy services to clients. In effect, working in a consultancy environment can be very rewarding in the early years in your career since you get to work on different projects with different clients, and also get to learn a lot from your more experienced peers.
# 2 ‚Äî Defensive Security
As the name suggests, the major role the defensive teams play is to defend against hack attempts. The teams working on the defensive side could be more diverse due to the various roles they have to play to protect the organisation. The roles could either be separate or joined together based on the size of the security team within an organisation.
On the defense side, the security controls are generally divided into preventive, detective and reactive controls and the teams could be working on one or more of these areas. The main activities that the teams perform on this side of the fence may include:
Larger organisations may have more granular job functions, such as, having a separate team for Identity and access management, PKI and cryptographic function, internal security advisory function, vendor due-diligence and assurance etc., whereas the smaller organisations may combine one or more of these functions together based on the size of their security team.
These are the most common roles that you may find in most organisations and industries. With the technological advancements, the cybersecurity roles keep evolving and more industry or technology specific roles may exist now or in the future. This may include the security of IoT devices, blockchains, automotives, smart buildings and cities, industrial devices (such as SCADA and Operational Technology) and medical devices to name a few.
# 3 ‚Äî Governance Risk and Compliance (GRC)
Cybersecurity function cannot operate on its own in isolation. Organisations need to make sure that the cybersecurity risk is within their risk appetite, and therefore, need to have an oversight on the cybersecurity function to ensure that it operates at an optimum level and offers maximum business value.
The Governance, Risk and Compliance, also called GRC in short, provides oversight as three lines of defense in information security. Governance along with security controls implementation teams above acts as a first line of defense, risk and controls function as second line of defense, and compliance and internal audit as third line of defense.
I will discuss at a high-level that what involves in these job functions.
Most of the time, these three functions may not operate in isolation and can have some overlap in the activities that they perform, however, it will highly depend on an organisation‚Äôs structure and culture.
I hope this will provide you a good understanding and a starting point to focus your search on a specific career path that you want to take, and to prepare yourself accordingly to give you a good chance to start a career in cybersecurity.
Identify areas that interest you and look at online job boards or speak with other security professionals working in the field to get further insights.
As I have mentioned above, these roles or career paths are not set in stone and one can switch to any other roles later on based on their interests and skills gained over a period of time. Over the years, I have switched between all three from time to time after starting my career as a penetration tester. Switching roles between these areas not only widens your horizon in the long term, but also helps you prepare for more senior roles.
Stay tuned for the next part where I will discuss how to gain job specific technical skills to give yourself a good start in the field.
Update ‚Äî Links to other three articles in this series. Hope you find these useful too.
medium.com
medium.com
medium.com
If you enjoyed reading this story, please acknowledge (üëè below a few times), comment, or follow along that will help me to manoeuver my writing style or to do better of what I‚Äôm doing right.
Follow me on Medium or LinkedIn. Be the first to learn about my stories by subscribing here.
Saltmarble
hidepatidar
T2X
Crypto Hero
Crypto Hero
Fernando Rodriguez Jr
Vishnu Chebolu
in
System Weakness
Jay Block
"
231,https://medium.com/towards-data-science/stats-for-data-science-from-the-ground-up-9f39a088bd9c,"Stats for Data Science, from the Ground Up","Data scientists love debating which skills are essential for success in the field. It makes sense: in a rapidly changing ecosystem that adopts new and powerful technologies all the time, job requirements and toolkits never stop evolving.
Statistics seem to be one major outlier, though. Data professionals of all stripes seem to agree that a solid foundation in stats and math will make your life easier regardless of your role, and can open up opportunities that would otherwise remain beyond reach.
To help you on your learning journey, we‚Äôre sharing a few of our favorite recent posts that focus on statistics for data science and machine learning. They go from the basics all the way to more specialized use cases, but they‚Äôre all accessible, beginner-friendly, and emphasize practical applications over lofty theory. Let‚Äôs dive in!
All stats-ed out, are we? We hope not, but just in case‚Äîhere are some non-statistics-related reading recommendations we think you‚Äôll enjoy.
Your support means so much to us ‚Äî thank you for reading our authors‚Äô work; a special shoutout goes to all of you who‚Äôve recently become Medium members.
Until the next Variable,
TDS Editors
Nelson Lou
in
CISS AL Big Data
Kartikeya Mishra
Dhaval Mainkar
Fraciah Karagu
TDS Editors
in
Towards Data Science
Wan Chung Huang
in
Towards Data Science
Dfsdsdgg
Jaskaran S. Puri
in
Towards Data Science
"
232,https://medium.com/@phuongndc/how-to-develop-your-analytical-skills-cabb5f1b8747,Tips to improve your analytical skills,"Some tips make your life easier
Nowadays, on the rise of data science, analytical skills become more important and are highlighted in most job descriptions. Ignoring fancy and flashy words, everyone can learn and apply analytical skills in daily jobs, whether they are data analysts, data engineers, business intelligence analysts or not.
Analytics is simply a way of thinking that can help anyone break down problems methodically. It starts with a curiosity towards numbers.
Here are some methods to improve your analytical skills.
I ran a marketing campaign and its conversion rate was 50%, I freaked out how amazing it is. However, when I saw the number of leads that the campaign generated was around 2, the conversion rate was only a joke.
Percentage numbers are meaningless if you don‚Äôt know the actual denominator.
There are two ways to boost a percentage. You can increase the numerator (usually the real intent), or you can decrease the denominator (usually bad and unintended).
Before celebrating the increase in percentage or conversion rate, make sure to dig into the specific change, and how the numerator and denominator shifted.
Know your baseline to set expectations and measure the growth exactly. As the baseline improves (absolute numbers), it becomes more difficult to increase the fixed growth rate over time. Measuring performance as a percentage can underestimate effort and investment.
A small number of things tend to have outsized impact.
Try to find groups that account for 80% of the results and focus on it to increase performances or take control easily.
For example: a group of 20% users accounted for 80% coupon redemption cost. If you want to cut the budget down, let‚Äôs start from them.
MECE stands for ‚ÄúMutually Exclusive ‚Äî Collectively Exhaustive‚Äù, is a popular mantra at McKinsey. MECE is a structured problem-solving approach to down all possible options without double counting.
There are 3 popular MECE frameworks: Issue Tree, Decision Tree, Hypothesis Tree
A cohort is a group of people who have something in common: typically a start date or an acquisition source. It gives you a clearer view, especially in measuring retention (i.e., how many customers are retaining to your apps/ websites‚Ä¶).
Before you do anything, check the quality of the dataset by reviewing data summary statistics & distributions to find outliers, and make sure all values are the right data type. You should compare your numbers with similar analysis to check on final results, especially if you have never worked with this data before.
Inspired by https://medium.com/geekculture/how-to-develop-your-analytical-skills-e9ae44d88803
Sridhar Vidyuth
John Baker
shekhar pandey
Dimagi
Derrick Mwiti
in
Heartbeat
BQuant
MRINAL WALIA
Kelvin Jose
in
The Startup
"
233,https://medium.com/javascript-in-plain-english/hilarious-memes-which-every-programmer-can-relate-to-44100d24a63d,Hilarious Memes Which Every Programmer Can Relate to,"I mostly see videos, TikTok, and memes posted on the internet. I don‚Äôt know why but it helps to refresh my mind and work stress. That‚Äôs why I believe
‚ÄúLaugher is best refresh button for the Human‚Äù
In this article, I am compiling some of the memes which I liked on the internet.
More content at PlainEnglish.io. Sign up for our free weekly newsletter. Follow us on Twitter, LinkedIn, YouTube, and Discord. Interested in Growth Hacking? Check out Circuit.
Ferran Negre
in
Callstack Engineers
Jscrambler
Roman Stremedlovskyi
Janvi Talreja
Marco Laspe
in
Better Programming
Aman Gangwar
Aman Garg
in
Nerd For Tech
Favouragbejule
in
Analytics Vidhya
"
234,https://medium.com/@amoranio/hidden-eye-advanced-phishing-b42c691020ee,Hidden Eye ‚Äî Advanced Phishing,"HiddenEye is a modern-day phishing tool. The advanced capabilities and ease of use really make phishing life simple. With just a few clicks, you can spin up a fake phishing site with keylogging capabilities. It is a really cool tool to use and will help you to better understand the technique malicious parties are using. It is also one for the pentesting toolbox.
git clone https://github.com/DarkSecDevelopers/HiddenEye.git cd HiddenEye sudo apt install python3-pip sudo pip3 install -r requirements.txt chmod 777 HiddenEye.py sudo python3 HiddenEye.py
As you can see, it‚Äôs packed with popular social media sites and templates. Using Facebook as an example:
Let select option 3 and create a Fake Security Issue. It will then ask you if you want to enable a Keylogger:
There is a always a chance that the user may change their mind and not click the submit button. They might have already populated the username and password field though and this is what you want to capture.
Next up is the redirect site. If you want to make it look somewhat genuine, you could use the actual fail URL for Facebook login. Just jump on the site, fail the login and grab the URL.
The reason you are adding this in is because once the user has entered their credentials and submitted, it will redirect to here:
It will now present you with two server options. It highlights that Serveo works better so let‚Äôs try that. The only issue I saw with Ngrok was that after I recreated the site, it kept failing. A bit flaky but still works.
You then have the option to create a custom URL or generate a random one.
I will loop back to custom but for now, lets select random. Worth noting that whatever you pick will follow the serveo.net domain. This is HTTPS though as the certificate it uses has a wildcard.
And there we have it. A fake security site in which you can phish credentials:
As you can see though, the URL stands out a bit. Therefore, you would want to create a custom URL. Something that could look genuine.
If you did have the keylogger enabled, you will now be able to capture any keystrokes, like so:
If you did manage to trick a user, you would also see their final input and a bit of information about where they are:
Now let‚Äôs revert back to the custom URL bit. You can‚Äôt be too obvious with the domain as it will be spotted straight away. Putting Facebook will result in the URL Facebook.serveo.net and will be flagged:
You could try your luck with a bit of DNStwisting but again, it might be flagged:
Instead what often works is combining words to create an unknown word. Such as Facebooksecurity. Because it isn‚Äôt a word and the certificate are applied (HTTPS), the site could look genuine:
There are plenty features to play with but as you can see, this is a very powerful tool.
From an BlueTeam perspective though, you could simply blacklist Serveo.net and Ngrok. This depends on if you are using them in production of course. Then you would look to whitelist only your own.
Ankr
in
Ankr
Eike M√ºcke
Katestanislavovna
Ash Moran
in
System Weakness
Kyle Carter
in
Geek Culture
Alaine Coughlin
Netflix Technology Blog
in
Netflix TechBlog
Tushar
"
235,https://medium.com/gitconnected/10-must-read-books-for-software-engineers-3c961299d25,10 Must-Read Books for Software Engineers in 2023,"If you want to learn more about tech, subscribe to my brand new YouTube Channel, where I explain complex topics in a crisp & sharp way to skill you up! ‚Üí https://www.youtube.com/@litwire
Besides all the great offerings of the modern world ‚Äî podcasts, videos, blogs, etc. ‚Äî reading a good book is still something many people don‚Äôt want to miss. I have read many good books covering tech-related things, such as software engineering, and am still reading to learn new patterns and best practices.
Finding great books for software engineering is not easy because the ecosystem changes rapidly, making many things obsolete after a short time. This is especially true regarding books that rely on a specific programming language version.
However, there are evergreens available, books that deal with meta-topics, design patterns, or general mindsets.
The following collection consists of some of the most popular books available. Senior developers often recommend books that are still relevant today to junior developers. I know that time is precious, especially for software engineers, but if you manage to read some of them, it will help you and your career. Note that this list is in no particular order because all of these books are equally recommendable.
Note: None of the links below are affiliate links.
‚ÄúCracking the Code Interview: 189 Programming Questions & Solutions‚Äù is highly recommendable to anyone who wants or needs to take coding interviews. Author Gayle Laakmann McDowell, an experienced software engineer, was both an interviewer and a candidate. She can help you to look for hidden details in questions, break problems into small chunks, and to get better at learning concepts.
Furthermore, Gayle provides 189 real interview questions and solutions so you can prepare well for the next coding interview!
‚ÄúCode Complete: a Practical Handbook of Software Construction, 2nd Edition‚Äù by Steve McConnell is one of the books every programmer should probably have skimmed through once in their life.
It‚Äôs a comprehensive analysis of software construction, well written, and highly accepted in the industry. It deals with the design, coding, debugging, and testing topics.
Overall, this book will probably have the highest ROI for developers with one to three years of professional programming experience. But I recommend it to beginners as well because it helps give you more confidence when constructing software.
The main takeaway? Developers have to manage complexity. To write code that is easy to maintain and to read for you and others.
‚ÄúClean Code: A Handbook of Agile Software Craftsmanship‚Äù by Robert C. Martin (Uncle Bob) is one of the most popular programming books. It was written to teach software engineers the principles of writing clean programming code. It comes with many examples showing you how to refactor code to be more readable and maintainable, but be aware that it is very Java-centric. While some patterns and techniques are transferable to general programming or other languages, the book‚Äôs primary audience is Java developers.
Another thing to note is that the book is from 2009. Some content, like code formatting, is less relevant today because of the available tools and IDEs. But it is a good read, after all.
The book Refactoring: Improving the Design of Existing Code, 2nd Edition by Martin Fowler explains what refactoring is, just like the original 20 years ago. Questions that you may ask yourself and that are answered in this book are:
After reading this book, you should understand the process and general principles of refactoring that you can quickly apply to your codebase. You should also be able to spot ‚Äúbad smells‚Äù in your teammate‚Äôs code that need refactoring.
‚ÄúHead First Design Patterns: A Brain-Friendly Guide‚Äù by Eric Freeman, Bert Bates, Kathy Sierra, and Elisabeth Robson teaches you design patterns and best practices used by other developers to create functional, reusable, elegant, and flexible software. It is also filled with great visualizations that will help you to learn new concepts more quickly.
This book is a great choice if you want to learn about things like factories, singletons, dependency injections, etc. The examples are written in Java, so it wouldn‚Äôt hurt to know that language or another object-oriented one.
‚ÄúPatterns of Enterprise Application Architecture‚Äù is another excellent book by Martin Fowler that deals with the practice of enterprise application development. After a short tutorial on developing enterprise applications, Martin then gives you over 40 patterns as solutions to common problems while architecting enterprise applications. It also comes with many UML visualizations and code examples written in Java or C#.
After reading the book, you should be able to divide an enterprise application into layers, know the practical approaches to organizing business logic, use the MVC patterns to organize web applications, and handle concurrency for data over multiple transactions.
However, the book is aging poorly, such modern concepts like REST, cloud, or JSON are not mentioned. It‚Äôs still a good read, but be critical while doing so!
In ‚ÄúWorking Effectively With Legacy Code‚Äù by Michael Feathers, the authors offer strategies to deal with large, untested legacy code bases. While you might think that we are in 2023 now and legacy code shouldn‚Äôt be a problem anymore because we only have clean, maintainable code and microservices all along, let me assure you that this is a misconception. Legacy code still is one of the most challenging problems for many companies.
After reading this book, you should be able to understand the general mechanics of software change, like adding features, fixing bugs, optimizing performance, and improving the design. Furthermore, you learn how to get legacy code ready for testing and how to identify where the code needs changes.
The book provides examples written in Java, C++, C, and C# but also includes tips on dealing with legacy code that is not object-oriented.
Another book by Uncle Bob teaches techniques, disciplines, tools, and practices of true software craftsmanship. ‚ÄúThe Clean Coder: A Code of Conduct for Professional Programmers‚Äù is packed with practical advice about estimating, coding, refactoring, and testing.
After reading this book, you should be able to deal with conflicts, tight schedules, and unreasonable managers; handle unrelenting pressure and avoid burnout; to manage your time; to get into the flow of coding, and foster environments where developers and teams can thrive.
This book is pretty accepted in the industry, but I think not everything in it is pure gold. It contains many anecdotes and hypothetical conversations that often conclude that the developer is ultimately responsible for what they do. This goes so far that in one statement, the advice for a developer whose code produced a bug is to reimburse the company financially for the money loss.
So my advice is to read the book carefully and critically if you do!
‚ÄúIntroduction to Algorithms, Third Edition‚Äù by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein is nothing less than an essential guide to algorithms of all kinds. It is comprehensive and accessible to all readers, beginners, and professionals alike. It is worded and covers a lot of subject matter. But it also is complex and not so easy to follow.
It covers data structures, fast algorithms, polynomial-time algorithms for seemingly intractable problems, graph theory, computational geometry, and much more. While it contains some examples in pseudo-code, it still is a very theoretical book in my eyes.
‚ÄúThe Pragmatic Programmer‚Äù is one of the most important books I have ever read. It is filled with technical and professional practical advice that helped me in many projects and become a better developer.
The book is highly relevant even in 2023, especially with the new 20th Anniversary Edition. It examines what it means to be a modern developer by exploring topics that range from personal responsibility and career development to architectural techniques.
After reading the book, you should know what continuous learning means and how important it is; how to write flexible, adaptable, and dynamic code; how to solve the problems of concurrent code; how to guard against security vulnerabilities; how to test ruthlessly and effectively; and much more.
If there were one book I had to pick to recommend to you, it would be this one!
To sum up, these are some of the most popular books for developers. If I had to pick a book to recommend to you, it would be ‚ÄúThe Pragmatic Programmer‚Äù by Andrew Hunt. The books by Robert C. Martin are accepted in the industry and liked by many developers, but I would be critical while reading them because I do not share many opinions with Uncle Bob.
Humam
Deni pras
RH Skills
Yousry Mohamed
Sydney Kibanga
Obrunosilvasnodv
Rahul Kumar
Natnicha R.
in
CodeX
"
236,https://medium.com/@hipiyushjain/excel-most-used-formulas-needed-for-a-data-analyst-b1e1193811b3,Excel - Most used formulas needed for a Data Analyst.,"‚ÄúNail the basics first, detail the details later ‚Äî Chris Anderson‚Äù
This will be the 4th and last ‚ÄúExplained Like you are 5‚Äù series for Must have skills for Data Analytics that I am writing about. (Please watch the other ones ‚Äî Power BI, Python, and SQL from). Now,We will have all the basic skills that are required and then can move to the next stages in the field of data analytics
Excel, Excel, Excel. I saw a lot of people either on LinkedIn or on Twitter talking about learning excel. I used to work on Power BI at that time and I knew the basics of excel so I never cared about learning formulas for excel that are used for data analytics. Until I saw one of my colleagues actually doing analysis on excel itself.
One of the advantages of using excel is you can make changes in the data itself while doing analysis. Whether it is making your data models or doing cursory analysis of the data and for that matter making dashboards themselves. There are a lot of things that can be learned in excel. I started learning those and here are the most used formulas that you can use for data analysis in excel
2. IF, SUMIF, SUMIFS, COUNTIF, COUNIFS, IFerror- Just remember them as if this then this. In the below example, We are finding the value(Sum, Count, single as dependent on the function). the major difference between IF and IFS is in ‚ÄúIFS‚Äù we can use multiple if conditions from different columns as used below.The ‚ÄúIferror‚Äù function is used if and when a condition is not met from the parameters what would be the values in the cell? Typically it is used for unknown values like infinity and NA values.
3. Transpose - This will transform items in rows, to instead be in columns or vice versa. To transpose a column to a row: 1. Select the data in the column,2. Select the cell you want the row to start, 3. Right click, choose to paste special, select transpose
4. PIVOT TABLE - It is basically used to analyze the data. Select the fields you want to analyze and summarize your data.You can create charts based on that table, analyze that data, etc.
5.UPPER, LOWER, PROPER, TRIM - UPPER, Converts text to all uppercase, =LOWER, Converts text string to lowercase, =PROPER, Converts text to proper case, TRIM - Removes all spaces between any strings except between words.
We have covered the main concepts that are needed to learn in excel for data analysis. There are other tips and tricks that you can use in Excel that I will cover in the next article. The best thing about using excel is it is pretty intuitive. Once you start using it, there is no going back.
That‚Äôs It. Hope It helped in getting started in Excel. Once you are comfortable using these concepts. It is quite interesting once you actually find insights from the data.
If you have any questions or any inputs you want to share, just comment or hit me up on LinkedIn or Twitter. I am pretty much active there.
All the best.
Please Start. You will not regret it.
Thanks,Piyush Jain
Dr. Walid Soula
Technical Source
Sequoia
in
Sequoia Capital Publication
GameSwift
Piyush Jain
Dr. Walid Soula
Bryan Mullins The Fox
Sequoia
in
Sequoia Capital Publication
"
237,https://medium.com/the-ciso-den/cybersecurity-career-mistakes-ccc7d06d8e1b,Cybersecurity career mistakes,"I have personally made many mistakes during my career. Does that qualify me to give advice? I am not sure. Nevertheless, as a cautionary tale, I can list some mistakes that you potentially want to avoid if you can recognize them.
Sair
Ryan McGeehan
Sentinel
in
Sentinel
Aluko Ahmad
Lossless
The Daily Encrypt
Scot Thom
Step Hero Multiverse
"
238,https://medium.com/towards-data-science/whats-the-hardest-part-of-metric-design-de2919bbf2dd,What‚Äôs the Hardest Part of Metric Design?,"In the previous installment of this series on metrics, we covered the importance of metric design and made an argument for collaborating with social scientists, even though the core responsibility for metric design ultimately lies with the decision-maker and leader, rather than the team‚Äôs resident math nerd.
We also discovered that, on paper, the process of creating a metric is fairly straightforward:
You simply determine what information you‚Äôd need for your decision, then you figure out a way of summarizing that information in a way that makes sense for your needs (ta-da, that‚Äôs your metric), and then you name it whatever you like. Right? Right, but‚Ä¶
Good metric design isn‚Äôt simply jotting down some formula willy nilly.
The summary above isn‚Äôt wrong, but it makes metric design sound much easier than it is. There is a hard part. Good metric design isn‚Äôt simply jotting down any formula that strikes your fancy. The ultimate skill here is solving a millennia-old problem that you were been warned against as a little kid.
Remember that bedtime story about the magic lamp and the genie? Or the one about Midas? Or the fisherman and his wife? Whatever your culture, it probably has one of those ‚Äúbe careful what you wish for‚Äù stories. And that is the essence of the hardest part for the decision leader.
The hardest part of metric design? ‚ÄúBe careful what you wish for.‚Äù
In order to come up with a good metric and decision criteria, you‚Äôll need to thoroughly vet it to make sure that there‚Äôs no perverse way that your decision criteria could lead you to the wrong decision. In other words, you‚Äôll need to plan as though you‚Äôre locking wits with an evil genie.
These days, a code-savvy leader will do at least a little bit of simulation to model how their metric reacts to various blends of inputs, especially at the extremes of the data distribution, to make sure that what they‚Äôre wishing for is really-really-really what they really want. If that sounded like a bunch of technical jargon that you‚Äôre totally unfamiliar with and/or you‚Äôre not skilled at simulation, you‚Äôll want to collaborate with someone who is. Writing the code is the easy part here, so you can hire pretty much any newbie to help you out because you‚Äôll be doing the hard part. Even if you get help on the technical bits, the decision-maker‚Äôs task is still tricky: it‚Äôs on the decision maker to carefully think through the gnarly real-world aspects of the problem and come up with the scenarios that are worth simulating.
Without a marriage between quantitative savvy, decision skills, and domain knowledge in designing your metrics, your attempt at being a data-driven decision-maker is on shaky footing.
If you don‚Äôt do your homework in thinking through your metrics, you‚Äôre asking the magic data lamp for trouble.
The whole endeavor hinges on the decision criteria (the cutoff between going with your pre-planned default action and switching to an alternative action) accurately reflecting a boundary between two realities: one reality where you always want to do your default action and one reality where you always don‚Äôt. (There were some decision intelligence fundamentals squished into this paragraph. It was naughty of me to write about them as if you‚Äôre already comfy with them ‚Äî just in case you‚Äôre not, I recommend reading this.)
If you don‚Äôt do your homework in thinking through your metrics, you‚Äôre asking the magic data lamp for trouble. Unfortunately, among the data science crowd, metric design gets much less attention than data and statistical methodology. Too many professionals behave as though they expect metric design to be someone else‚Äôs job, which often means it ends up being no one‚Äôs job. Then the business leader rattles off some ill-considered wish, no one stops to ponder it, and then the whole team is back on the terrible merry-go-round of throwing data and math at the wrong problem.
Too many professionals behave as though they expect metric design to be someone else‚Äôs job, which often means it ends up being no one‚Äôs job.
As a leader, your way out of this problem is to learn metric design yourself or hire some decision science talent to help you out. Until your team has these skills in place, it‚Äôll be very hard for you to unlock good data-driven decision-making.
If you had fun here and you‚Äôre looking for an applied AI course designed to be fun for beginners and experts alike, here‚Äôs one I made for your amusement:
kozyrkov.medium.com
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Here are some of my favorite 10 minute walkthroughs:
Abhishek Kandwal
Jan Hapala
Pascal Biese
Gifa Delyani Nursyafitri
Adam Sroka
Max Kelsen
in
Towards Data Science
Magellan AI
Andrew J. Zahuranec
in
Data Stewards Network
"
239,https://medium.com/codex/google-launches-dataform-for-bigquery-4acaa8354100,Google launches Dataform for BigQuery,"Rather by accident, I stumbled across Dataform‚Äôs new integration with BigQuery. But this is quite handy to manage code pipelines in SQL in BigQuery.
Heavybit
Rokowski
in
The Witnet Foundation Blog
Songtham Tung
in
Better Programming
√Åtila Camur√ßa
Elye
in
Mobile App Development Publication
Cameron Kroll
Asanka Vithanage
omgzui
in
Javarevisited
"
240,https://medium.com/@tomaszs2/nuxt-3-is-all-you-need-b1293afc8e45,Nuxt 3 Is All You Need,"It finally happened. On November 16, 2022 Pooya Parsa announced Nuxt 3 is released.
The long awaited major version of the Vue based framework is set to match the expectations of the community shying away from Angular‚Äôs boilerplate and React‚Äôs minimalism.
Jaime
Rao Muhammad Noman
Tehleel Mir üê¢
lollipop
Sankar Ganesh
in
know-javascript
bytefish
in
JavaScript in Plain English
Avrilsemon
Oly Sport
"
241,https://medium.com/datadriveninvestor/how-to-automate-exploratory-data-analysis-a368678c8f04,How to automate the Exploratory Data Analysis process?,"This article will discuss some very important python libraries to automate Exploratory Data Analysis (EDA).
Exploratory data analysis is used to understand the various aspects of the data like the datatypes of each feature, descriptive statistics, statistical analysis (univariate, bivariate, multivariate), feature engineering, missing values detection, outlier detection, anomaly detection, and so on. It also sometimes provides data professionals with hidden patterns and features which cannot be identified by conventional tools and techniques.
In a nutshell, it is the most important step before performing any machine learning or deep learning modeling since if the data is not understood well, it cannot be prepared well and the final prediction will thus be faulty. It's like garbage data input gives garbage output prediction.
There is data accessibility for every nanosecond in any kind of device starting from cellphones, to laptops to IoT devices to any gadget connected to the internet. The abundance of data humans are generating in a year is more than what was generated in all the past centuries put together.
Most organizations today have immense datasets under their arsenal, but they aren‚Äôt always efficient since larger datasets do not equal high profits because they are not leveraged in the right manner.
So, here comes the idea of leveraging data in the most efficient manner. In any data science project or machine learning project, more than 70% of the time is dedicated to data analysis, feature selection, feature engineering, etc. Since it is the most important part of a data science or machine learning project it is imperative that the data scientists or analysts use automated techniques to clean the data, handle missing values, detect outliers, handle imbalanced datasets, understand categorical features, and so on. It saves time and yields equivalently good quality results as the conventional techniques in EDA.
Some of the python EDA automation libraries which I will mention are dtale, pandas-profiling, sweetviz, and autoviz. Although, since this article is very detailed, I will include sweetviz, and autoviz in the next part of this article.
Before proceeding forward, I will write the command for installing any python3 library.
D-tale
The code snippet is
D-tale is a library that was launched in February 2020 and is used to visualize pandas data frames easily and supports interactive plots, 3d plots, heat maps, the correlation between features, building custom columns, and many more.
The above image shows how a data frame looks after D-tale library is used. It looks very descriptive and clear from the picture above. The features can be seen and at the top left, the number of rows (9017) and the number of columns (10) can be observed. I used this dataset, although the reader can use any dataset of their choice for practice and use. I will try and highlight some more advantages which make the d-tale library extremely detailed in nature (yeah read that again, d-tale is indeed detailed)!
The list of functionalities as we see from the above image are Describe, Merge and stack, Time Series Analysis, Clean column, Duplicates, Feature analysis, Highlight outliers, missing, charts, correlations, and many more if you drag the slider down. I will provide a few images where the functionalities are used to create results as in conventional EDA techniques.
The features I explore are:
Highlight dtypes:
If this option is clicked, it will show the data type (integer, string, float, DateTime, and so on) when the mouse cursor hovers over any feature.
Although, yes, the ‚ÄúYear‚Äù datatype has not been converted to Date Time, it also has int64 as the datatype.
The details are datatype, number of outliers, skewness value, kurtosis, and so on. The feature can be renamed and deleted too.
Summarize Data
This option can be used to group by and observe certain patterns of the data, pivot, transpose, and resample the data. The group by option can be used and the functionality can be shown below.
The formula can also be seen at the bottom left, which is similar to what we use (df.groupby()). The count() is used here in this case, although mean, median, sum, unique count can also be used.
The final result of the group by function is shown in a new tab. I have represented the data using the State/UT and Year features and found the feature ‚ÄòRape‚Äô as the other feature. It is in descending order by year for each ‚ÄúState/UT‚Äù. As can be seen, Uttar Pradesh performs the worst followed by Madhya Pradesh and so on.
Describe
The Describe option is used to find details on various columns (similar to the .describe() method in pandas). They are maximum, minimum, mean, median, mode, standard deviation, skewness, variance, and so on.
The ‚ÄúKidnapping and abduction‚Äù column is an integer numeric column as can be seen from the image above. The other categorical columns can also be explored in a similar way.
Charts
The charts option can help to create bar graphs, line charts, scatter plots, pie charts, tree maps, heat maps, geographical maps, and so on as per the requirements.
I have created the mean-time distribution for Domestic violence cases from 2001 to 2012. I grouped by the State of Uttar Pradesh. One can always change the graph type and explore for other parameters (count, sum, unique count, rolling, median, etc.).
In a nutshell, we can see that d-tale does a lot of things for us in a very short time (and in 3 to 4 lines) in comparison to a plethora of lines of what pandas, matplotlib, seaborn, plotly, numpy, and other relevant libraries do for us. Basically, it can do anything we can think of in terms of EDA.
Brilliant!
Pandas-profiling
It is an open-source library written in python and generates interactive HTML reports and describes various aspects of the dataset. They are handling missing values, statistics of datasets like mean, mode, median, skewness, standard deviation, etc, and charts like histograms and correlations as well.
The code snippet is
The image from the result is
Here we can see the number of rows, columns, missing values, missing %, and so on.
If we click on the alerts option at the top of the report we have some observations according to the data which are:
Dowry Deaths is highly correlated with Rape, Cruelty by Husband or his Relatives is highly correlated with Rape, Dowry Deaths have 2185 (24.2%) zeros, Assault on women with intent to outrage her modesty has 571 (6.3%) zeros, Cruelty by Husband or his Relatives has 1068 (11.8%) zeros, etc.).
In the variables section, the image looks something like this below.
It has high correlation zeros (around 2185). For categorical columns, it can be explored too.
The statistical details are as follows
Similarly, there are other functionalities that can be found in the report are Correlation coefficients (like correlation matrices of Pearson, Kendall, spearman, etc., and so on, missing value details, and last and first rows (like .head() and .tail() methods).
This ends the first part of the article on Automation EDA libraries, I will continue the second part with the other 2 remaining libraries of Autoviz and Sweetviz. I will also do a proper conventional EDA on the same dataset which will be detailed in terms of the Data Analysis perspective and I will then provide the complete code in GitHub.
Stay tuned and till then please check out my other articles, say hi, and let me know if you want a technical writer to write for you and we can discuss over a quick call.
Subscribe to DDIntel Here.
Visit our website here: https://www.datadriveninvestor.com
Join our network here: https://datadriveninvestor.com/collaborate
ddistore.com
Eva Murray
Nugra Arsyistawa
Erma T Lowery
Palak Jain
in
DataFlair
Makwanavishnu
R√ºppells Griffon
Shalika Prasad
Yohoshiva Basaraboyina
"
242,https://medium.com/@TrusttechConsultancy/siem-rules-for-companies-6ac21938d1c2,SIEM Rules For Companies,"NOTE: This is not just an article written in a few hours. It directly comes from my years of expertise, trial and fail, fine-tuning and sleepless nights. It can cost millions of dollars for enterprise-level companies to walk the same way I did, or you can read this article and apply it to your business which I highly recommend.
Corvil
Gerta Cloris
Crypto Michele
in
Phoenix Finance
manhattan-soft
Yannier Lesende
Hyipdaily
FiatToCrypto
in
Coinmonks
Miracle Ewim
"
243,https://medium.com/towards-data-science/how-to-lie-with-data-c58df403bdce,Discovering Data Misrepresentation,"A data analytics team‚Äôs objective is to help their organization make business decision based on data and insights.
After doing all the hard work collecting the organization‚Äôs data across multiple departments over many months, through robustly engineered data pipeline, then carefully storing them in carefully designed data bases‚Äî your data analysts can now finally analyze them, in the hopes that any insights found can lead to some ground breaking decisions being made by the management and help the organization achieve it‚Äôs business objective.
That‚Äôs the vision anyway.
However the aspiration between what we (the data analyst) want to convey and what they (the audience) end up getting are two different things.
Sometimes, the misalignment are dubiously intentional. But more often than not, they‚Äôre by accident.
This post aims to be the first in a series of articles that highlights the ways one can ‚Äúmislead‚Äù others (either unintentionally or otherwise) when presenting data, and what audiences can do better when interpreting them (by being aware of how it‚Äôs done).
Much of the inspiration behind this came from the works of Daniel J Levitin from his book ‚ÄúA Field Guide to Lies‚Äù [1], where the author went through numerous cases of data misrepresentation and how it was done. The book, and many other subsequent other resources that I‚Äôve found while researching this topic, forms the basis of the content that will be shared here and in later articles.
For now, we‚Äôll focus on the most obvious ways one can play around with the charts ‚Äî its axes.
I guess it goes without saying that the first thing a person should be looking at when presented with a chart are the axes and legends.
Axes provide vital reference information for users to associate data points with values, especially when data points are not labeled directly in a chart. ‚Äî [4]
In this section, we explore some techniques that can be used to mislead others in how data gets interpreted by playing with the axes.
Without any labels or axes, as mentioned earlier, it would be hard for readers to associate or compare the graphs that gets presented.
To start our foray into data misrepresentation, consider the following example of a hypothetical company ‚Äî ACME:
In the above chart, we can see that ACME has the biggest market share of 56% compared to other companies in the market.
But just how big is the difference?
The lack of label ‚Äî in this case the Y-axis, prevents the reader from understanding what‚Äôs the magnitude of advantage that ACME has over its competitor.
As described earlier, the Y-axis usually serves to show the magnitude of how much things change over the X-axis (typically time, or some categories).
However to place extra emphasis on the increase, or decrease on the Y-axis (perhaps the intention is to highlight how much things have improved under one‚Äôs leadership for example), one can opt to play around with the Y-axis by truncating them (ie. by removing a portion of the Y values up to the point that‚Äôs relevant to us).
Consider the following charts below.
Both charts were created using the same dataset, where sales figures of a company is plotted across the period of 2008 to 2013.
However, the one on the left would appear more flattering ‚Äî as it displays a nice upward curve over time. The chart on the right meanwhile, shows that nothing much have changed over time ‚Äî basically there hasn‚Äôt been any progress made at all.
How can this be?
The difference lies how the Y-axis has been setup. The one on the zooms in (by truncating the Y-axis) on the values between 2.299B to 2.300B, hence from a casual observer‚Äôs perspective, the improvement over time will seem remarkable.
The chart on the right starts at 0 and ends at 2.5B, since for a business at this magnitude (ie. in the billions) ‚Äî making sales at the increment of 10Ks each year is probably something that we wouldn‚Äôt want to shout about. Yet ‚Äî this is message that actually needs to be sent to the management.
Do we then need to always ensure that any charts that we see have their Y-axis be fixed at 0 then?
In [7], Seth Long argues that it depends. Depending on the context, small differences can actually have a significant impact and probably warrants that the data analysts zoom in on the Y-axis to provide clarity to the audience that things have indeed changed over time.
Consider the following example, where we have two charts depicting the Overnight Policy Rate (OPR) of a fictional country. The one on the left has it‚Äôs Y-axis truncated, while the one on the right start it‚Äôs Y-axis at 0.
In this scenario, where a slight difference in interest rate value could translate in huge difference in monthly loan/financing payment ‚Äî we can clearly see that the chart on the left is much preferred for it‚Äôs clarity in demonstrating the difference of interest rate over time (in contrast with the right).
In our earlier example, we showed that we can mislead the audience by selectively zooming in on the Y-axis of our Sales figure to place a certain emphasis on the values that we‚Äôre trying to highlight.
Now, let‚Äôs use the same Sales data from our earlier example. Learning from our earlier lesson, we know that a more convincing chart ought to start at 0 on the Y-axis.
However, instead of using the same scale throughout the Y-axis, our new chart above introduces a discontinuity in the scale where instead of representing 100M at every tick, we instead use 5,000 at every tick starting at the 2.3B mark.
By doing so, we‚Äôre now able to demonstrate (to the casual observer) that our Sales numbers have increased exponentially over time.
Cheeky indeed.
The double Y-axis is essentially a graphing technique for plotting two sets of observation on the same graph. The danger from this technique is that it can mislead the audience if the analyst adjusts the axis (or make it disappear altogether) in order to prove a point.
Let‚Äôs consider an example. The year is 2017, and suppose that we need to justify our marketing expenses against forecasted Sales revenue for our upcoming 2018 business plan.
In the above charts, our existing predicament is depicted on the left; where our projected marketing spend is significantly higher than our projected revenue. When depicted on the same Y-axis, the difference can immediately be seen. Things doesn‚Äôt really look good for us.
In the right chart, we introduce another Y-axis on another scale for our Revenue figures. With this, our revenue numbers finally appears above our marketing spend. Our business plan now ‚Äúappears‚Äù more solid than previously.
A more extreme, real life example of how people try to get away with playing with the double Y-axis can be referred to in [8], where Representative Jason Chaffetz was trying to point out that Planned Parenthood (an NGO) have increased the number of abortions it performed and reduced cancer screening & prevention services over a 7 year period. The initial chart presented seems to support his case, if not for the fact that the numbers lines themselves aren‚Äôt suppose to cross if plotted on the same Y-axis.
Circling back to the double Y-axis though ‚Äî the technique in itself isn‚Äôt something that is frowned upon when building charts (in certain cases it does have it uses). However, one needs to be careful not to mislead the audience when applying it.
It is now the end of 2018 (in our hypothetical story), and we‚Äôve failed to hit our revenue target. Our manager, totally oblivious to our revenue figures, have scheduled a market briefing where we‚Äôre scheduled to brief market analysts and investors on how the company have been performing for the year.
Let‚Äôs try to dig deeper in our monthly sales revenue.
The above shows that we‚Äôve been doing relatively well throughout the first half of 2018, but went downhill pretty much since August 2018. The drastic drop in demand for our product, with no signs of it coming back up, will surely spook the market, investors, and our future customers.
How can we paint a more positive picture and change the narrative?
Enter the cumulative graph.
While the above graph still show the decreasing rate of revenue growth, for the casual audience our sales revenue does appear to be increasing. All in all, a positive note!
This method have in fact been used by none other than Apple [2] in their 2013 presentation. In that presentation, Tim Cook presented the cumulative iPhone sales over time ‚Äîin which, from a casual observer‚Äôs perspective, seemed to show that all is going well at Apple. However, analyzing the data per quarter shows a different story, where iPhone sales have in fact been stagnating over the past few quarters.
In this article, we went through a few examples of how data can be misrepresented by playing around with the axes of a chart.
We‚Äôve covered:
Some of the techniques are quite obvious and can be easily detected. Others (like the double Y-axis) can take awhile to decipher for the untrained eye and those that are unaccustomed to reading charts on a regular basis.
The idea behind this article, if it wasn‚Äôt already obvious ‚Äî is NOT to encourage data misrepresentation. Rather, it was written in the hopes that we are more aware of how data and charts can be shaped to suit the narratives of the presenter.
Remember that misrepresentation are often accidental. Though sometimes, they can be intentional.
Understanding this allows us to be better equipped in interpreting any charts that gets thrown our way. Hopefully, we‚Äôre now better at questioning what we think we saw in the chart, versus what is actually shown in the chart.
[1] https://www.amazon.com/Field-Guide-Lies-Critical-Information/dp/1524702528
[2]https://www.techjunkie.com/tim-cook-trying-prove-meaningless-chart/
[3] https://www.callingbullshit.org/tools/tools_misleading_axes.html
[4]https://xdgov.github.io/data-design-standards/components/axes
[5] https://www.statisticshowto.com/probability-and-statistics/descriptive-statistics/misleading-graphs/
[6] https://www.heap.io/blog/how-to-lie-with-data-visualization
[7] https://technaverbascripta.wordpress.com/2014/04/16/lying-with-data-visualizations-is-it-misleading-to-truncate-the-y-axis/
[8] https://www.vox.com/2015/9/29/9417845/planned-parenthood-terrible-chart
Dr. Alvin Ang
Rudradeb Mitra
in
Towards Data Science
Crystalloids
in
Crystalloids
Smart London
Avinaba Mukherjee
Pablo Estevez
in
Booking.com Data Science
Swagatam Sen
in
Geek Culture
Mateusz Dalba
in
Python in Plain English
"
244,https://medium.com/@eisultan/cargo-cult-correlation-vs-causation-3b0fa4069677,Cargo Cult : Correlation vs. Causation,"During WWII, isolated and pre-industrial island cultures experienced soldiers and supplies arriving in large numbers, often by airdrop. It was the first time for these cultures to be introduced to big technological advances. During this period, the soldiers would trade with the islanders, materials for food.
After the war, the soldiers left. Cargo cults arose, the islanders tried to imitate the behaviors of the soldiers, thinking that this would cause the cargo planes to drop clothes and materials from the sky.
The islanders built wooden airplanes and radars like mocks from straw. They also imitated the marches of the soldiers while holding sticks for rifles, all of that in waiting for things to drop as it used to be.
What the islanders did not have was any prior knowledge about airplanes, and how electromagnetic fields work. They based their belief only on correlation between two observations.
From statistical point of view, what the islanders did is similar to what many businessmen and scientists are doing until today: Drawing conclusions based on correlation and mixing it for causation.
Remedies : To avoid falling for the cargo cult trap, there are many powerful statistical methods that can be used to indicate causation, including AB testing (the business equivalent of the scientific method). In a future article we will talk further on AB testing.
Check also fooled by correlation here.
About me : I am an experienced data scientist who believes in simplicity. You can connect with me on LinkedIn, the link is in the Bio.
Propulsion Academy
Bingblackbean
Manjeet Singh Nagi
in
Becoming Human: Artificial Intelligence Magazine
Mariana Demchuk
Dennis Niggl
in
Towards AI
Georgian
in
Georgian Impact Blog
Maurice Hafner
Carlos Carrero
in
Snowflake
"
245,https://medium.com/towards-data-science/spectral-entropy-an-underestimated-time-series-feature-94e18ae5b958,Spectral Entropy ‚Äî An Underestimated Time Series Feature,"Time series are everywhere. As data scientists, we have various time series tasks, such as segmentation, classification, forecasting, clustering, anomaly detection, and pattern recognition.
Depending on the data and the approaches, feature engineering could be a crucial step in solving those time series tasks. Well-engineered features can help understand the data better and boost models‚Äô performance and interpretability. Feeding raw data to a black-box deep learning network may not work well, especially when data is limited or explainable models are preferred.
If you have worked on feature engineering for time series, you probably have tried building some basic features like the mean, the variance, the lag, and the statistics based on rolling windows.
In this post, I will introduce building features on spectral entropy. I suggest you include it as one of the must-try features when your data is applicable (frequency domain analysis makes sense). I will show how I tackled two classification problems on time series using spectral entropy. I have shown how to apply spectral entropy to an anomaly detection problem. Please refer to Anomaly Detection in Univariate Stochastic Time Series with Spectral Entropy.
My focus is on the application side, so I will skip some basic introductions and theories.
Generally, time series data is saved in the time domain. The indexes are timestamps sampled with fixed intervals. Some time series have waveform or seasonality, like sensory data (seismic vibrations, sound, etc.). We can think something is oscillating and generating the data like waves.
When strong wave patterns exist, transforming and analyzing the data in the frequency domain will make sense. Fast Fourier Transform (FFT) is a classic way to transform from the time domain to the frequency domain. Spectral entropy encodes the spectral density (distribution of power in the frequency domain) into one value based on Shannon entropy. If you are interested in the fundamental introduction, please check Anomaly Detection in Univariate Stochastic Time Series with Spectral Entropy.
Here is an analogy for a quick intro without going deep into the formulas.
Suppose we study how people spend their spare time. One spends 90% on soccer. The other spends 90% on chess. Although their interests are different, it is almost certain that they will dedicate their spare time to their favourite hobbies. They are similar to some degree. This similarity is entropy. They will have the same lower entropy, implying lower uncertainty.
Another person spends 20% on hiking, 30% on reading, 20% on movies, and 30 % on whatever. Apparently, the third person is different from the first two. We don‚Äôt know which activity the third is doing exactly. In this case, the entropy is high, meaning a higher uncertainty.
Spectral entropy works the same way. How the time is distributed corresponds to how power is distributed across frequencies.
Next, let‚Äôs see two real-world examples of how spectral entropy works wonders. The dataset is not in the public domain. Please allow me to use vague descriptions and hide the dataset details.
The goal of this dataset is to build a binary classifier with a total of hundreds of samples. Each sample is a test result labelled pass or fail. One sample has close to 100 signals. The length of the signals is constant. Figure 1 shows one example (each small plot has three signals).
If we extract X features from each signal, we will have 100*N features. Considering the small sample size, we will run into the ‚Äúcurse of dimensionality‚Äù problem.
Since we have tremendous data for each sample, let‚Äôs be selective and only focus on the signals with the most predictive power and drop the irrelevant ones. I calculated spectral entropy for each signal, so we only have 100 features. Then shallow trees are trained.
From the top important features, only three signals show a high correlation with the labels. After I studied those signals, I constructed customized features according. Finally, I built a successful model with high performance. Moreover, the model only requires around ten input features, which benefits us with good generality and interpretability.
This example is another binary classification problem. Each sample only has one time series with different lengths. The total sample size is less than 100.
The varying length is not a big deal. We can split the time series into smaller fixed-length segments and use the sample label as the segment label.
The little problem is that we have a relatively large range of frequencies. Because the sampling frequency is 48,000 HZ (which covers the sound that the human ear can hear), based on the Nyquist theorem, the highest frequency in the frequency domain will be 24,000. Figure 2 is an example.
I tried the spectral entropy directly but couldn‚Äôt clearly distinguish between positives and negatives. The main reason is both labels have similar peak frequencies and harmonics. The spectral distribution is identical. As a result, a spectral entropy on the entire frequency domain won‚Äôt tell a significant difference.
Since the frequency resolution is high, let‚Äôs zoom into certain frequency bands instead of the whole spectrum. Hopefully, the subtle separations are hiding somewhere.
I split the frequency into smaller bands. Each band is from X to X+100. So 24,000 will give us 240 bands. The higher frequencies contained only noise with minimum power. Therefore, I ignored the higher frequencies noises, picked the lower frequencies from 0 to 3,000 and cut them into 30 bins. Then I calculated the spectral entropy for each band. Finally, a tree-based model was trained using only 30 features. This approach worked surprisingly well. Figure 3 shows the top two features (spectral entropy of two bands). There is a reasonable boundary when using only frequency bands 1200 to 1300 and 2200 to 2300.
Figure 4 below shows the model‚Äôs performance on test data labelled positive. The top plot is the raw signal. The middle plot is the predictions for each segment. The bottom is the entropy of frequency 1200 to 1300 for each segment. You can see most of the predictions are close to 1 and the entropy is likely in the range of 0.9 to 0.93. Figure 5 shows the test data labelled negative. Now the predictions drop to close to 0, and the entropies vary in the range of 0.8 to 0.9.
I show how spectra entropy helped me quickly find the most important signals and frequency bands for further feature engineering.
In those two examples, we don‚Äôt have to use spectral entropy. For instance, we may build features like the peak frequency, the average magnitude of frequency bands, etc. We can even have the entire frequency domain data as one input vector. After all, we can separate the targets in the frequency domain.
I like to explore features from the spectral entropy perspective because:
It‚Äôs easy to interpret and compute.
It significantly compresses the information contained in the frequency domain and keeps the core information.
The downside is some info is lost. For instance, the value of magnitude is not considered at all.
Furthermore, because we transform a list of values in the frequency domain into a signal value, there is a chance that different spectral distributions may have the same entropy value. This is like Hash Collision. For example, a fair coin and a biased coin have different entropy. But for a biased coin with a probability of X to heads and a second biased coin with a possibility of Y to tails, their entropy will be the same if X and Y are equal.
I hope you can learn the benefits of spectral entropy and apply it to your time series work in the future.
Thanks for reading.
Have fun with your time series.
More articles on time series
FMZ Quant
Haebichan Jung
in
Towards Data Science
Ke Zhang
in
Towards Data Science
Josephgeorgelewis
Pradnya
Tyler
Apurva Singh
Dan Beech
"
246,https://medium.com/books-are-our-superpower/4-books-that-taught-me-much-more-than-my-expensive-degree-c8ca0ca475b4,4 Books That Taught Me Much More Than My Expensive Degree,"I don‚Äôt regret the time I spend in college but, in hindsight, the things I learned sitting in endless lectures and studying dozens of textbooks had little value for me after my graduation.
I quickly realized that the place to seek the knowledge I needed to progress in life and grow as a person wasn‚Äôt a classroom, but a library.
There‚Äôs a quote by Jim Rohn that goes,
‚ÄúFormal education will make you a living; self-education will make you a fortune.‚Äù
I couldn‚Äôt agree more with these words. People tend to underestimate books, their power, and the knowledge they can offer you. Yet, most of the things I‚Äôve learned worth knowing in life, I‚Äôve learned from books.
Here are four spectacular books I can confidently say taught me much more than my expensive degree.
How smart do you think you are?
This probably doesn‚Äôt come as a big surprise, but humans aren‚Äôt the rational creatures we think we are.
Most of the time we don't see the world as it really is. From the number of friends we have on Facebook to the smartphones we choose to purchase, we‚Äôre constantly deluding ourselves.
The culprits responsible for our errors in thinking? Cognitive biases, heuristics, and logical fallacies. Once we understand and learn to recognize them, the better and more logical our decision-making will be.
In this spectacular book, the author examines how our biases taint our perception of the world, by delving into a wide range of psychological research and breaking down 48 psychology concepts.
What I loved about this book is that it pushes you to challenge your perception of the world and everything you thought you knew about the way your brain makes decisions.
Since it covers a wide range of concepts, most of the information in it is extremely valuable in the process of becoming more self-aware, and making more rational decisions at work, at home, in your relationships, and even in your daily interactions.
It‚Äôs also evident (and important) that the author did his research ‚Äî everything he says is backed by data and studies ‚Äî which gives the book credibility.
‚ÄúTHE MISCONCEPTION: You are a rational, logical being who sees the world as it really is. THE TRUTH: You are as deluded as the rest of us, but that‚Äôs OK, it keeps you sane.‚Äù
‚ÄúYou can‚Äôt rage against the machine through rebellious consumption.‚Äù
‚ÄúWe reach for the same brand not because we trust its quality but because we want to reassure ourselves that we made a smart choice the last time we bought it.‚Äù‚Äï David McRaney, You Are Not So Smart
It seems that nowadays people are almost obsessed with empathy. But did you know that empathy can lead to inequality and immorality?
That our ability to sense other people‚Äôs emotions and feel for them might be prejudiced and selfish?
I didn‚Äôt know that using empathy as a moral guide could be bad for me until I read this book.
The author, who‚Äôs a psychologist, argues that empathy often clouds our judgment and leads us to make unfair decisions in every area of our lives.
Backing his arguments with scientific findings, he explores empathy‚Äôs limitations and shows us a better alternative: rational compassion.
Empathy isn‚Äôt perfect nor is the solution we need to help others.
That‚Äôs the #1 lesson the book teaches you, which, IMO, is an extremely important one, considering people‚Äôs obsession with ‚Äúimproving their empathy‚Äù, and ‚Äúlearning new empathy techniques‚Äù.
By reading the book, you‚Äôll understand the distinctions between selective empathy and rational compassion and you‚Äôll learn how to use the latter in order to make better decisions, and genuinely contribute to making the world a better place.
‚ÄúIf God exists, maybe He can simultaneously feel the pain and pleasure of every sentient being. But for us mortals, empathy really is a spotlight. It‚Äôs a spotlight that has a narrow focus, one that shines most brightly on those we love and gets dim for those who are strange or different or frightening.‚Äù
‚ÄúThe idea I‚Äôll explore is that the act of feeling what you think others are feeling ‚Äî whatever one chooses to call this ‚Äî is different from being compassionate, from being kind, and most of all, from being good. From a moral standpoint, we‚Äôre better off without it.‚Äù‚Äï Paul Bloom, Against Empathy: The Case for Rational Compassion
What is the difference between people who think about making things happen and those who actually make them happen?
That‚Äôs a question none of my college professors or dozens of textbooks could answer‚Ä¶but this book did.
Featuring stories about successful people from a variety of professions (CEOs, entrepreneurs, etc), each describing what helped them become achievers, it‚Äôs one of the few books that genuinely motivates you to actively pursue and realize your dreams.
If you‚Äôre tired of reading the same boring and practically useless recommendations you see in the majority of self-help books, you‚Äôll be surprised by the nuggets of wisdom displayed in this book.
The authors offer plenty of practical advice on how you can discover what you truly want out of life, and what steps you can take toward achieving it.
However, if we‚Äôre being honest, sometimes reading some piece of advice, no matter how useful, isn‚Äôt enough, right?
That‚Äôs why the authors have gone one step further, including self-assessment exercises, checklists, questions to spark your thinking, and other practical tools in the book ‚Äî all of which will inspire and help you ‚Äúignite your own fires‚Äù.
‚ÄúIntrospection is a form of self-management. You reflect. You decide. You change. You allow yourself to grow.‚Äù
‚ÄúPeople‚Äôs confidence in their abilities influences how they approach life. Their dreams are likely anchored to what they feel they can achieve.‚Äù
‚ÄúIf you want to make a difference, you must live like no one else and strive to achieve goals that others view as impossible to reach.‚Äù‚Äï Raoul Davis Jr., Paul Eder, Kathy Palokoff, Firestarters
This is one of my all-time favorite non-fiction books and is pretty close to my degree since geopolitics was examined and analyzed in 1/3 of my courses.
None of those courses examined geopolitics in such a rich, informative, and engaging way as this book though.
Geopolitics is the study of the influence of geography on political decisions and gives you an idea about the dynamics between global economic, social, and political forces.
You might not be particularly interested in history, politics, or geography, but you need to have a basic idea of the way leaders of nations think and act, especially now you‚Äôve seen the impact of the Russia- Ukraine war on the whole world.
The book takes you on a journey through the world by using maps, facts, and his spectacular sense of humor, explains the complex relationship between them, and showcases how geography constrains their leaders.
You know that feeling when you watch the news but have a hard time following what‚Äôs being analyzed because you have no idea of the events‚Äô background?
Well, it‚Äôs about time you read this book.
Even if geopolitics isn‚Äôt your thing, the book is a must if you want to be better informed about the relationships between various nations around the world, and how much geography plays in a state‚Äôs destiny.
Upon finishing it, you‚Äôll have a better understanding of global current affairs and much more knowledge than the average person has about politics, history, or geography.
‚ÄúWhat is now the EU was set up so that France and Germany could hug each other so tightly in a loving embrace that neither would be able to get an arm free with which to punch the other.‚Äù
‚ÄúSometimes you will hear leaders say, ‚ÄúI‚Äôm the only person who can hold this nation together.‚Äù If that‚Äôs true then that leader has truly failed to build their nation.‚Äù
‚ÄúWhy do you think your values would work in a culture you don‚Äôt understand?‚Äù‚Äï Tim Marshall, Prisoners of Geography
Thank you for reading! If you liked these book recommendations, consider subscribing to my monthly book newsletter! It‚Äôs free, and each month you‚Äôll get book reviews, recommendations, and reading tips!
If you enjoy reading stories like these, consider signing up to become a Medium member and get unlimited access to my and other incredible writers‚Äô stories on Medium. If you sign up using my link, I‚Äôll earn a small commission. Thanks!
READ/DOWNLOAD%) Introduction to Operations and Sup
READ/DOWNLOAD%@ Drug Use, Misuse and Abuse: Psycho
Tediousn
Mohamadsiliwangi
Brett Christensen
Marketta Sowers
Tardigrade
Aldiaufajuliyanti
"
247,https://medium.com/towards-data-science/how-going-back-to-basics-made-me-a-better-data-scientist-10b686b3301a,How Going Back to Basics Made Me a Better Data Scientist,"Like most Data Scientists these days, I learnt (and am still learning) the techniques on the job. Back in 2014, I graduated with a scientific degree which taught me the foundations of data analysis, statistics and visualisation. After that, I fell into Data Science and landed my first job as a Data Scientist. Probably a familiar story for others out there too?
Earning the job title of ‚ÄòData Scientist‚Äô without having a directly related qualification was odd to me. I knew a few of the concepts, had worked with relational databases and dabbled in a small amount of code, but that was pretty much it. I also wasn‚Äôt as dedicated to learning back then as I am now, so most topics I learnt were dictated by the projects I was working on at the time.
I realised I was getting left behind...
As a few years passed, Data Science grew massively in popularity as a career path. As more online resources became available and data professionals were getting churned out of dedicated Data Science programmes, I realised I was getting left behind. My coding skills were underdeveloped, my statistics knowledge was rusty and my analytical approach to solving problems was quite frankly, primitive.
Now, you might be thinking that I‚Äôm being pretty harsh on that version of myself from a few years ago. Well, considering that a lack of effort in developing your technical skills gets you nowhere fast, I decided to change everything. I needed to be better. I needed to be more skilled. I needed to make a real effort so I could keep up with the Data Science market.
This wasn‚Äôt a straight forward journey. Learning how to learn again as an adult quite frankly, sucks. Pretty quickly, I ran into the questions: Which online course is the right one for me? Should I study a Masters degree? Should I just give up and cuddle cats for a living?*
A year or so into my learning journey, I had found an online learning platform that I was comfortable with and I was starting to learn and apply my new skills to my day job and reap the benefits. I was feeling better about myself. For a short while.
I would come across online quizzes or forums where I still had no idea what they were talking about, mentioning python programming concepts or statistical practices that seemed basic yet I didn‚Äôt know what they were or how to use them. I would be tasked with new projects at work and I wouldn‚Äôt even know where to start. How is this possible? All this dedicated time to learning and I still sucked?!
The challenge I was facing was caused by that great little human emotion called ego.
Fortunately, at around the same time as this realisation, the online learning platform I was using updated the programme to which I was enrolled and some new modules popped up that I hadn‚Äôt seen before. They had the titles like ‚ÄòIntroduction to‚Ä¶‚Äô and ‚ÄòLearn the basics of‚Ä¶‚Äô and so on. At first I thought, ‚Äòwhat a drag! I don‚Äôt need to go over the basics of Pandas or have an introduction to statistics, I‚Äôve already learnt all this!‚Äô. Yet I had to now complete these new courses to finish the whole programme.
And that‚Äôs exactly what I needed.
The real trouble I had wasn‚Äôt my lack of technical knowledge or my ability to learn. Anyone is capable of learning. The challenge I was facing was caused by that great little human emotion called ego. My problem was thinking I didn‚Äôt need to go over the basics because I thought I already knew it. I assumed that, owing to my few years of working as a Data Scientist with my new found skill set and having learnt some of the ‚Äòfancy stuff‚Äô in the form of machine learning techniques, by proxy I must already understand all the basics.
WRONG.
This is by far one of the WORST mistakes you can make as a Data Scientist.
Assuming you have a solid understanding of a topic just because of your job title or the number of years you‚Äôve held it can 1) make the learning experience of other topics harder because you won‚Äôt have all the prior pieces to the puzzle, and 2) lead you towards making grave errors when applying and interpreting models simply because you don‚Äôt understand the nuances and core data concepts.
Studying those courses that are aimed at complete beginners was the best thing I‚Äôve ever done for my career. I had so many ‚Äòah-ha‚Äô moments and was able to gain a more comprehensive understanding of the other complex topics I had previously tried learning yet couldn‚Äôt quite grasp.
I became a lot more confident as a result. I become bolder at work. I would challenge stakeholders to really dive into the data and work with them to ask questions and shape the analysis. We would aim to derive insights that they didn‚Äôt think would be possible to obtain from the data. I wouldn‚Äôt be afraid to learn new analytical techniques as I had a stronger foundation beneath me and so the learning process was quicker.
With my more comprehensive understanding of statistics, I was able to explain to stakeholders why the result was or wasn‚Äôt significant using real world anecdotes to aid my explanation, resulting in more engagement and trust in my work. Coupled with taking a course on data storytelling and how to write effective presentations (which I don‚Äôt think I had ever spent any time learning prior to this), I could share my results and recommendations in an effective way that would keep my audience engaged. Simple stuff, big difference.
My takeaway; be humble and make sure you know the basics.
If you don‚Äôt, read about the theory or take a course or two until you‚Äôre confident enough to tell everyone about the workings behind a neural network or whatever you just learnt. If you can have a conversation about the foundations of a Data Science topic with someone who is not a data person, yet they can follow along with your descriptions and explanations, then you‚Äôre on to a winner (providing you haven‚Äôt just made it up just to look super cool).
Oh, and in case you‚Äôre wondering what the asterisk is after the cat cuddling question?
*This option is still up for debate :)
Marlee Rose
Michael Black
in
Geek Culture
Chahat Tandon
Jens Frid
in
scaleout
Teniola
Nikhil Adithyan
in
CodeX
A. S. ≈ûakrak
Jyothi Gupta
"
248,https://medium.com/@tomaszs2/vite-just-got-a-worthy-competitor-33af4dcdf93,Vite Just Got A Worthy Competitor,"In my last article I have covered features of Next.js 13. One of these features is a significant performance improvement. As the Turbopack, a new alpha Next.js bundler, as the page states, is up to 700 times faster than Webpack.
Also, it is significantly faster than Vite. Here is a screen shot from the Turbopack we site that tells about it:
Salifyanji Taala
Embarkingonvoyage
Anoosh Jorjorian
Akambi Fagbohoun
in
SSENSE-TECH
Jeffrey Russom
in
Expedia Group Technology
Anshita Bhasin
Harkirat Saluja
in
Wealthy Blog
Kazuki Yonemoto
"
249,https://medium.com/towards-artificial-intelligence/what-is-gpt-4-and-when-9f5073f25a6d,What is GPT-4 (and When?),"It has been some time since Robert Scoble wrote this about GPT-4. Which pointed me that OpenAI might be giving access to GPT-4 to a certain closed group of individuals. Not sure if it is a conventional alpha-build or a beta but given the timelines, I am guessing that since Aug 2022, there has been enough time passed, which suggests a beta or even early release candidate.
We all know that GPT-3 was a huge leap in itself. The refined model that could spit out fluent paragraphs compared to GPT-2. Since, the release of GPT-3, the discussion on the ‚Äúnext big thing‚Äù was fairly quiet and muted. Now, we have got more information about GPT-4.
The concrete specifics about GPT-4 specifications are still in flux due to NDA, however GPT-4 is likely to use 100 Trillion parameters (source). This is the first large-scale model with sparsity at the core design. What does it mean to have sparsity, well, it means that even at the 100T parameter space, the compute cost is likely to be lower. This means that a lot of neurons are still active in the final model. From layman‚Äôs understanding, it is a model where the model can keep a lot more choices of ‚Äúnext word‚Äù or ‚Äúnext sentence‚Äù or ‚Äúnext emotion‚Äù based on the context. In all essence, this means it is more similar to actual human thinking than its predecessor.
Generative Pre-trained Transformer (GPT) is a text generation deep learning model trained on the data available on the internet. It is used for question & answers, text summary generation, machine translation, classification, code generation, and conversation AI.The applications of GPT models are endless. Furthermore, you can even fine-tune them on specific data to create even better results (transfer learning). By using the ‚Äúsauce‚Äù from GPT models, building NLP projects becomes a heck lot easier. Easier means you save time, money, and resources and ultimately you use the generalization (giant sample size) to get started without having to reinvent the wheel for general aspects of the language.
Since 2018 when GPT-1 was first published (link) GPT-3 has made giant progress. The GPT-1 had (only /s) 117 million parameters. GPT-2 raised the bar to 1.2 billion parameters (publication), and GPT-3 raised it even further to 175 billion parameters (publication). For reference, the Deepmind‚Äôs Gopher model had 250 billion parameters (publication) and Megatron NLG‚Äôs model had 500 billion+ parameters (publication).
ithinkbot.com
ithinkbot.com
At the same time, Microsoft‚Äôs efforts with OpenAI lead to the conclusion that optional hyperparameter tuning has great utility in fine-tuning models at this scale. Generally the larger the model, it is extremely costly to fine-tune it. Deepmind‚Äôs chinchilla experiment (publication) concluded that the number of parameters is as important as the size of the training corpus.
GPT-4 is a text-only model that takes the NLP one giant step ahead promising step. GPT-4 is likely to be released early next year! Given the abilities of GPT-4 extrapolated from GPT-3, we might now need a new Touring test standard. The topic of AI in deepfakes is another stream, but with every leap in the model, we get closer to it.
Since the release of previously released General-Purpose models, a plethora of text has been generated. The understanding of the general populous is unmistakably lower than what it needs to be. I am optimistic but I would love to have a similar concerted effort from the same for-profit entities in improving AI models that can identify an AI-generated text from a human-generated text.
ithinkbot.com
pub.towardsai.net
ithinkbot.com
pub.towardsai.net
Undoubtedly, these are exciting times, fall is upon us so let us enjoy the progress. I will be eagerly waiting for more information as and when it comes.!
Monika Mani Swiatek
in
The Startup
David Priede, PhD
Bennett Glace
in
Plainsight
Mark Christoph Sch√ºtte
Ronit Taleti
in
DataDrivenInvestor
muhammad zeeshan
in
unpack
OMOTEC - On My Own Technology
Jaekob Chenina
in
Intelligent Product
"
250,https://medium.com/gitconnected/the-end-of-programming-6e3f7ff0d8b4,The End of Programming,"The end of classical Computer Science is coming, and most of us are dinosaurs waiting for the meteor to hit.
I came of age in the 1980s, programming personal computers like the Commodore VIC-20 and Apple ][e at home. Going on to study Computer Science in college and ultimately getting a PhD at Berkeley, the bulk of my professional training was rooted in what I will call ‚Äúclassical‚Äù CS: programming, algorithms, data structures, systems, programming languages. In Classical Computer Science, the ultimate goal is to reduce an idea to a program written by a human ‚Äî source code in a language like Java or C++ or Python. Every idea in Classical CS ‚Äî no matter how complex or sophisticated ‚Äî from a database join algorithm to the mind-bogglingly obtuse Paxos consensus protocol ‚Äî can be expressed as a human-readable, human-comprehendible program.
When I was in college in the early ‚Äô90s, we were still in the depth of the AI Winter, and AI as a field was likewise dominated by classical algorithms. My first research job at Cornell was working with Dan Huttenlocher, a leader in the field of computer vision (and now Dean of the MIT School of Computing). In Dan‚Äôs PhD-level computer vision course in 1995 or so, we never once discussed anything resembling deep learning or neural networks‚Äîit was all classical algorithms like Canny edge detection, optical flow, and Hausdorff distances. Deep learning was in its infancy, not yet considered mainstream AI, let alone mainstream CS.
Of course, this was 30 years ago, and a lot has changed since then, but one thing that has not really changed is that Computer Science is taught as a discipline with data structures, algorithms, and programming at its core. I am going to be amazed if in 30 years, or even 10 years, we are still approaching CS in this way. Indeed, I think CS as a field is in for a pretty major upheaval that few of us are really prepared for.
I believe that the conventional idea of ‚Äúwriting a program‚Äù is headed for extinction, and indeed, for all but very specialized applications, most software, as we know it, will be replaced by AI systems that are trained rather than programmed. In situations where one needs a ‚Äúsimple‚Äù program (after all, not everything should require a model of hundreds of billions of parameters running on a cluster of GPUs), those programs will, themselves, be generated by an AI rather than coded by hand.
I don‚Äôt think this idea is crazy. No doubt the earliest pioneers of Computer Science, emerging from the (relatively) primitive cave of Electrical Engineering, stridently believed that all future Computer Scientists would need to command a deep understanding of semiconductors, binary arithmetic, and microprocessor design to understand software. Fast forward to today, and I am willing to bet good money that 99% of people who are writing software have almost no clue how a CPU actually works, let alone the physics underlying transistor design. By extension, I believe the Computer Scientists of the future will be so far removed from the classic definitions of ‚Äúsoftware‚Äù that they would be hard-pressed to reverse a linked list or implement Quicksort. (Hell, I‚Äôm not sure I remember how to implement Quicksort myself.)
AI coding assistants like CoPilot are only scratching the surface of what I‚Äôm talking about. It seems totally obvious to me that of course all programs in the future will ultimately be written by AIs, with humans relegated to, at best, a supervisory role. Anyone who doubts this prediction need only look at the very rapid progress being made in other aspects of AI content generation, like image generation. The difference in quality and complexity between DALL-E v1 and DALL-E v2 ‚Äî announced only 15 months later ‚Äî is staggering. If I have learned anything over the last few years working in AI, it is that it is very easy to underestimate the power of increasingly large AI models. Things that seemed like science fiction only a few months ago are rapidly becoming reality.
So I‚Äôm not just talking about CoPilot replacing programmers. I‚Äôm talking about replacing the entire concept of writing programs with training models. In the future, CS students aren‚Äôt going to need to learn such mundane skills as how to add a node to a binary tree or code in C++. That kind of education will be antiquated, like teaching engineering students how to use a slide rule.
The engineers of the future will, in a few keystrokes, fire up an instance of a four-quintillion-parameter model that already encodes the full extent of human knowledge (and them some), ready to be given any task required of the machine. The bulk of the intellectual work of getting the machine to do what one wants will be about coming up with the right examples, the right training data, and the right ways to evaluate the training process. Suitably powerful models capable of generalizing via few-shot learning will require only a few good examples of the task to be performed. Massive, human-curated datasets will no longer be necessary in most cases, and most people ‚Äútraining‚Äù an AI model won‚Äôt be running gradient descent loops in PyTorch, or anything like it. They will be teaching by example, and the machine will do the rest.
In this New Computer Science ‚Äî if we even call it Computer Science at all ‚Äî the machines will be so powerful and already know how to do so many things that the field will look like less of an engineering endeavor and more of an an educational one; that is, how to best educate the machine, not unlike the science of how to best educate children in school. Unlike (human) children, though, these AI systems will be flying our airplanes, running our power grids, and possibly even governing entire countries. I would argue that the vast majority of Classical CS becomes irrelevant when our focus turns to teaching intelligent machines rather than directly programming them. Programming, in the conventional sense, will in fact be dead.
How does all of this change how we think about the field of Computer Science?
The new atomic unit of computation becomes not a processor, memory, and I/O system implementing a von Neumann machine, but rather a massive, pre-trained, highly adaptive AI model. This is a seismic shift in the way we think about computation ‚Äî not as a predictable, static process, governed by instruction sets, type systems, and notions of decidability. AI-based computation has long since crossed the Rubicon of being amenable to static analysis and formal proof. We are rapidly moving towards a world where the fundamental building blocks of computation are temperamental, mysterious, adaptive agents.
This shift is underscored by the fact that nobody actually understands how large AI models work. People are publishing research papers actually discovering new behaviors of existing large models, even though these systems have been ‚Äúengineered‚Äù by humans. Large AI models are capable of doing things that they have not been explicitly trained to do, which should scare the shit out of Nick Bostrom and anyone else worried (rightfully) about an superintelligent AI running amok. We currently have no way, apart from empirical study, to determine the limits of current AI systems. As for future AI models that are orders of magnitude larger and more complex ‚Äî good friggin‚Äô luck!
The shift in focus from programs to models should be obvious to anyone who has read any modern machine learning papers. These papers barely mention the code or systems underlying their innovations; the building blocks of AI systems are much higher-level abstractions like attention layers, tokenizers, and datasets. A time traveller from even 20 years ago would have a hard time making sense of the three sentences in the (75-page-long!) GPT-3 paper that describe the actual software that was built for the model:
We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.
This shift in the underlying definition of computing presents a huge opportunity, and plenty of huge risks. Yet I think it‚Äôs time to accept that this is a very likely future, and evolve our thinking accordingly, rather than just sit here waiting for the meteor to hit.
My long-time readers will notice that I haven‚Äôt been blogging much lately. Sorry about that! I‚Äôve been a bit busy.
About three years ago, I left Google for a startup that was acquired by Apple. I subsequently left Apple to lead engineering at another startup for a couple of years. In that time I learned a ton about startup life, building AI systems, and building teams. It was great.
A couple of months ago I jumped ship to start my own company. I‚Äôm now the co-founder and CEO of Fixie.ai, a stealth startup in the AI space, and the blog post above may or may not represent some of the thinking going into this new venture. We have a fantastic founding team and I‚Äôm really excited about what we‚Äôre going to be able to accomplish. Over the next few months I hope to share more about what we‚Äôre up to. Until then, hang tight!
Ryan Nguyen
Juliano Sena da Silva Carlos
Dhushanthini Archunan
Devin | HYPEBEARS
Petri Autero
Vipin Singhal
Jae Duk Seo
Mariamumar
"
251,https://medium.com/towards-data-science/machine-learning-systems-versus-machine-learning-models-3955d038ea1f,Machine Learning Systems versus Machine Learning Models,"A functioning AI product contains more than just a Machine Learning model. You need to add a lot of ingredients to your ML soup to make it bring value to customers. We call all of these ingredients together a Machine Learning System.
But we tend to pay too much attention to our ML models while neglecting other parts of the bigger picture. I call this phenomenon a model-centric bias.
There are multiple reasons why a model-centric bias can lead to bad outcomes:
First, most of the complexity of our Machine Learning systems resides outside of the ML model source code. If we overlook this fact by concentrating only on model development, we risk accumulating too much technical, cultural, and research debt.
Second, a Machine Learning model has an additional component alongside programming code ‚Äî data. Your model can only be as good as your data. Model-centric bias creates a tendency to neglect the data.
Because of the dependency on the data, Machine Learning Systems tend to be more fragile than traditional software. We must make additional system design decisions to handle this fragility and introduce adaptivity to the system.
Finally, from a Lean perspective, it is only a system as a whole that brings value to a customer.
That's why we need to stop thinking about our Machine Learning models in isolation but look at them as a part of a dynamic, constantly adapting system ‚Äî a Machine Learning System.
The rest of the article explores this concept in more detail.
I believe that the origins of model-centric bias come from the way we formulate our research goals in AI and ML community. We create a benchmark using the standard dataset and write papers where we try to beat the current SotA on it. The same approach has emerged in the applied ML and DS community in the form of Kaggle and other competitions with a leaderboard setup.
While it is the best way we know so far to comply with Popper's Falsifiability principle and have a common denominator to measure research progress, it creates this bias towards concentrating only on the models. And when we shift from an academic environment to building AI products, we bring it with us.
Why is the model-centric view harmful?
A machine learning system's complexity resides outside the Machine Learning model source code.
In 2015, Google published a paper, ""Hidden Technical Debt in Machine Learning Systems"", where they explored this phenomenon based on their experience with Machine Learning products. Below is the picture from that paper showing the areas where most of the complexity is concentrated and where systems tend to accumulate technical debt.
It is comparatively easy to develop and deploy Machine Learning models, but it is hard to make the whole Machine Learning system production ready. ML systems are complex and tend to accumulate technical, research, and cultural debt. Maintaining them over time is very difficult and expensive.
A Machine Learning model is an amalgamation of programming code with data. Instead of explicitly writing the algorithm of a program, we infer it from the given examples.
What it means is data plays a crucial role in the quality of the resulting learned algorithm. Additionally, while the code of the model can be significantly standardized and extracted into reusable libraries and tools, the data represents the uniqueness of your product. Your resulting model can be only as good as your data:
Garbage in, garbage out. ‚Äî The GIGO principle.
It means we need to pay much more attention to data: to test it, iterate over it with improvements, and unify and fix the labeling schemes of our datasets.
This great webinar by Andrew NG explores this idea in much greater detail.
Model lifecycle doesn't end with initial deployment. It is valid for the traditional non-ML algorithms and especially true for Machine Learning models that are highly dependent on data distribution stability and lack of interpretability. These systems are incredibly fragile.
The remedy comes from a Systems Theory concept of a Feedback Loop: by introducing fast backward information flows and reactions to this information, we make the system adaptative and durable.
This requires us to make design decisions at the system level. Concentrating on the model alone without considering its context will result in a fragile and poor-quality AI product.
And finally, the value we deliver to our customers is what matters at the end of the day. Value ‚Äî is what a customer wants and is ready to pay for. It is the core idea of Lean, which lies at the heart of DevOps and MLOps practices. A trained Machine Learning model alone doesn't bring any value. It is just a binary file collecting dust somewhere in a file system. It is the system as a whole that generates and brings value.
So, what is a Machine Learning System?
Let's look at the generic value stream of a Machine Learning project.
It is an iterative process that usually consists of the following phases:
While this is a generic template, many parameters determine the uniqueness of your ML project. Some of them are:
Depending on your place in this attribute space, you will have a unique set of activities, tools, and processes.
I like to define the term Machine Learning System as this concrete implementation of all activities together with their artifacts that deliver value to a customer using a Machine Learning approach.
Machine Learning System is a way we deliver value to a customer using a Machine Learning approach.
A Machine Learning System specifies:
All of that constitutes the complex dynamical adaptive system.
Let's look at some examples of Machine Learning Systems.
In an early-stage project, our goal is usually to validate the primary business hypothesis and the ML approach's applicability to solve the problem. We spend time scoping the project, collecting the initial data, and figuring out the target metric and evaluation protocols.
Usually, the goal is to quickly come up with some baseline solution and then improve iteratively. Because the project's plan at that stage is usually not to deliver value to the user but to validate the hypothesis that there's some value, the end result is most likely not the ready-to-use product but a prototype the user should validate. That's why we don't focus on the deployment and operations phases. Instead, we optimize the experimentation phase. Our goal is to create shorter feedback loops through automated metrics and interactive dashboards.
There's no mature training infrastructure with computation resource scheduling and experiment reproducibility. Usually, Data scientists do most of their work in some interactive coding environment like JupyterHub.
Deployment is manual, often with a handover of the model from a Data Scientist to an ML or Software Engineer.
Separating codebase zones is a typical pattern of achieving the compromise between experimentation agility and accumulation of technical maturity. The experimentation zone doesn't impose engineering quality standards on the code ‚Äî it's the zone of short-living non-reusable experimentation functionality. We continuously review, refactor, and extract stable and reusable functionality into a stable codebase zone and deploy it into a package.
We usually don't care much about data observability or lineage in such projects. Data Scientists create new datasets just from an interactive Data Science environment.
Imagine a different scenario when we don't have any suitable quality labels for a newly launched project.
The annotation process would probably be the bottleneck of our value stream, and a proper annotation tool would play a crucial role in the project's success.
Furthermore, we might utilize an Active Learning approach to make the labeling process more effective and reduce the labeling work necessary.
If our ML infrastructure is mature enough, we can automate the training procedure via a job scheduler and a submit interface.
The resulting Machine Learning System might look something like this:
The edges in the diagram above represent artifact flows. They can be performed manually or automated. The more automated they are, the faster you can iterate and progress.
Let's imagine the opposite scenario ‚Äî we have a running product with daily users. We can get labels from the users correcting the model misbehavior at inference time.
We implement the Continuous Training pattern and automate the whole cycle end-to-end. The data is automatically collected and transformed into training and validation datasets. We automate the training procedure and put it inside the CI/CD pipeline with automated unit and data tests and model evaluation.
We can choose continuous deployment or continuous delivery strategy and further advance it by introducing canary releases and automated rollbacks.
Model service is constantly monitored for model performance, and when the performance degrades, the system triggers a retraining/redeployment procedure.
These are just a few examples of possible Machine Learning Systems. There are many more, and the one that suits you will be different and reflect your project's uniqueness.
So let's conclude.
While machine learning models are at the core of AI products, it's not where the complexity is usually concentrated.
We need to shift our focus to the Machine Learning System as a whole right from the beginning when we plan our ML project.
We must start thinking and talking more about such systems and develop a vocabulary of reusable design patterns and good practices.
If you find the ideas I share with you interesting, please don't hesitate to connect here on Medium, Twitter, Instagram, or LinkedIn.
Data Seito
Alexey Zinoviev
Apoorv Yadav
in
DailyTech
Viking Analytics
Carlos J. Garc√≠a Carmona
Kirthan
Sebastian Kwiatkowski
in
Towards Data Science
Mayur Pethani
in
Analytics Vidhya
"
252,https://medium.com/towards-data-science/stat-stories-delta-method-in-statistics-bd681fbbf037,Stat Stories: Delta Method in Statistics,"Data sampling is at the core of data science. From a given population f(x), we sample data points. All these data points are collectively called random samples denoted by random variable X. But as we know, data science is a game of probability, often, we repeat the experiment many times. In such a scenario, we end up with n random samples X‚ÇÅ, X‚ÇÇ, ‚Ä¶ X‚Çô (not to be confused with the number of data points in a sample). Often these random samples are independent, but identically distributed, hence, they are called independent and identically distributed random variables with pdf or pmf f(x), or iid random variables.
In this article, we talk about the Delta method which provides a mathematical framework for calculating limiting distribution and asymptotic variance, given iid samples. The Delta method lets you calculate the variance of a function of a random variable (with some transformation as we will see later) whose variance is known. This framework is closely related to the variable transformation method in statistics that I have previously talked about in much detail.
towardsdatascience.com
towardsdatascience.com
towardsdatascience.com
Given iid random samples X‚ÇÅ, X‚ÇÇ, ‚Ä¶ X‚Çô, their joint pdf is given by
Of special case, if all iid samples (we are dropping ‚Äòrandom‚Äô but assume that they are there) are normally distributed with mean and variance as 0, and 1, then X¬≤ ~ œá¬≤‚ÇÅ, i.e. chi-square distribution of degree of freedom equal to 1. (It can be tested by writing a simple script in Python, R, or Julia).
Convergence in distribution tells us how X‚Çô converges to some limiting distribution as n ‚Üí ‚àû. We can talk about convergence at various levels:
where ‚Üí‚Çö denotes convergence in probability. One such use of convergence in probability is the weak law of large numbers. For iid X‚ÇÅ, X‚ÇÇ, ‚Ä¶ X‚Çô with ùîº(X) = Œº, and var(X) < ‚àû, then (X +, X‚ÇÇ+ ‚Ä¶ + X‚Çô)/n ‚Üí‚Çö Œº.
2. Almost Sure Convergence: We say that X‚Çô ‚Üí X a.s. (almost sure) if
Almost sure convergence implies convergence in probability but vice-versa is not true. The strong law of large numbers is the result of almost sure convergence where ùîº(X) = Œº, var(X) = œÉ¬≤, then (X +, X‚ÇÇ+ ‚Ä¶ + X‚Çô)/n ‚Üí Œº, a.s.
3. Convergence in Distribution: We say X‚Çô ‚Üí X if the sequence of distribution functions F_{X‚Çô} of X‚Çô converge to that of X in an appropriate sense: F_{X‚Çô}(x) ‚Üí F_{X}(x) for all x, where F_{X} is continuous (Note that my writing style used latex notation in absence of Medium not able to support complicated equations).
Convergence in distribution is the property of distribution and not a particular random variable that is different from the previous two distributions. Convergence in Moment Generate Function implies convergence in distribution, i.e. M_{X_n}(t) ‚Üí M_X(t) for all t in a neighborhood of 0.
towardsdatascience.com
Central Limit Theorem is one application of convergence in distribution where, for X‚ÇÅ, X‚ÇÇ, ‚Ä¶ X‚Çô with mean Œº and variance œÉ¬≤,
Another consequence of convergence in distribution is Slutsky Theorem:
If X‚Çô ‚Üí X in distribution, and Y‚Çô ‚Üí c in distribution, with c a constant, then X‚Çô + Y‚Çô ‚Üí X + c, X‚Çô Y‚Çô ‚Üí cX, and X‚Çô /Y‚Çô ‚Üí X/c, c ‚â†0, all in distribution.
Delta method, through convergence properties and the Taylor series, approximates the asymptotic behavior of the functions of a random variable. Through variable transformation methods, it is easy to see that if X‚Çô is asymptotically normal, then any smooth function g(X‚Çô) is also asymptotically normal. Delta method may be used in such situations to calculate the asymptotic distribution of functions of sample average.
If the variance is small, then X‚Çô is concentrated near its mean. Thus, what should matter for g(x) is the behavior near its mean Œº. Hence we can expand g(x) near Œº using the Taylor series as follows:
That calls for the following asymptotic behavior called First Order Delta Method:
Let X‚Çô be a sequence of random variables satisfying ‚àön(X‚Çô ‚àí Œº) ‚Üí N(0, œÉ¬≤). If g‚Äô(Œº) ‚â†0, then
which can be written following the Slutsky theorem I mentioned earlier.
If we add one more term to the Taylor series from Equation, we can have the second-order delta method which is useful when g‚Äô(Œº) = 0 but when g‚Äô‚Äô(Œº) ‚â†0.
where œá¬≤‚ÇÅ is the chi-square distribution of the degree of freedom equal to 1, introduced earlier.
Let‚Äôs do a little coding.
Consider a random normal sample with a mean of 1.5 and a true sample variance of 0.25. We are interested in the approximation of the variance of this sample multiplied by a constant c = 2.50. Mathematically, the new sample‚Äôs variance would be 0.25*(2.50¬≤) = 1.5625 using the Delta method. Let‚Äôs do the sample empirically using R code:
whose output is 1.563107, which is pretty close to one obtained using the Delta method.
In this article, I covered the Delta method which is an important topic for students taking Statistics classes but is generally overlooked by data science and machine learning practitioners. Delta methods are used in applications such as the variance of a product of survival probabilities, the variance of the estimate of reporting rate, the joint estimation of the variance of a parameter and the covariance of that parameter with another, and model averaging to name a few. I suggest readers look at reference materials to gain a further understanding of this topic.
Was this helpful? Buy me a Coffee.
Love my writing? Join my email list.
Want to know more about STEM-related topics? Join Medium
A list of related topics in the Stat Stories series:
Amit Kumar Jha
Winston Robson
in
Future Vision
Senan Mele
Will Bidstrup
Bharath K
in
Towards Data Science
Nikesh Pahuja
Sunny Gupta
Scorsagg
in
MLearning.ai
"
253,https://medium.com/mlearning-ai/4-reasons-why-machine-learning-engineers-earn-more-than-data-scientists-1b5c843ae719,4 Reasons Why Machine Learning Engineers Earn More Than Data Scientists,"A few years ago, Data Scientist was crowned ‚ÄúThe Sexiest Job of the 21st Century‚Äù [1] and only recently, people have started to question whether it still is and will continue to be the sexist, given the rise of other data-related roles like Machine Learning (ML) Engineers, Big Data Architects, Artificial Intelligence (AI) Engineers and more. Now don‚Äôt get me wrong ‚Äî sexy or not ‚Äî has nothing to do with how much Data Scientists should be paid.
Sachin Kumar
in
Towards Data Science
Abdul vlog
Jonathan Cabreira
in
Towards Data Science
Xinyu Liu
Ishan Gupta
Luigi Di Fraia
David Lopera
Jesse L
"
254,https://medium.com/@reddyyashu20/over-sampling-67c450163d5e,Over-Sampling,"## Lets Over-Sample the training Data
As, 16% employee Attrition was there. The number of 0's(representing employee stayed) in the target variables are more than 5 times than that of 1's(representing employee Attrition) in the target variable.
Imbalanced dataset pose a challenge for predictive modeling as most of the machine learning algorithms used for classification are designed around the assumption of an equal number of examples for each class. So, training a model on imbalanced dataset results in poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.
For this purpose We will over-sample the minority class (1‚Äôs) in the target variable and make the number of 0's(majority class) and 1's(minority class) equal.
SMOTE stands for Synthetic Minority Oversampling Technique, proposed in the year ‚Äò2002‚Äô.
We will be using [SMOTE] for over-sampling purpose. It works by selecting minority samples that are close in the feature space, drawing a line between these samples in the feature space and drawing a new sample at a point along that line.
Specifically, a random sample from the minority class is first chosen. Then k of the nearest neighbors for that sample are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.
Ref Article
imbalanced-learn.org
Python code
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html
Sachin Bharadwaj
Willam Green
Hieu0811
Paras Patidar
in
MLAIT
Harrison Jansma
in
Towards Data Science
Taniya
Rohit lal
Aashrey bansal
in
Analytics Vidhya
"
255,https://medium.com/towards-data-science/analyzing-twitter-user-network-1cfcef1dd89d,Analyzing Twitter User Network,"In this article, we will collect Twitter users and relationships. We will load this data to a graph database. Finally, we will analyze the network. The final result is a recommendation of who to follow.
Our readers will learn that network analysis is an indispensable tool for data scientists and machine learning practitioners.
The steps to do these are:
Twitter provides an API for developers to get the data. To do these, we need to:
Once we get the access, we could start developing an application to gather the data. There are two parts. Collect the Twitter users. Then, collect the user relationships.
First, we collect the users. The application will do the followings:
This class and this class are the Java classes for the application. Also, use this DDF to create the user table. We put the entire Java project in the GitHub repository.
We ran this app from June 11, 2022, to June 14, 2022. We got more than 20,000 users.
In the second part, we collect how a user follows other users. We have discussed this in detail in this article. Here is the main code to get relationships.
We store the relationships in a table. The DDF is here.
We ran this on 20,000 users. We got more than 7 million relationships.
Use the following query to export user data to a CSV file.
Use the following query to export relationship data to another CSV file.
We need to create a new graph database to store the Twitter user data. Currently, we have the Neo4j database version 4.4.7. We also installed Graph Data Science Library 2.0.5. The library provides some graph algorithms we‚Äôd like to use.
Here are the steps to load the users and relationships to the graph database:
The scripts will create a User label and one relationship type, FOLLOWS.We could use the schema visualization command below to check the schema.
The schema is below.
First, we need to create a named graph. Most algorithms in the Graph Data Science library work with a named graph.
The command below creates a graph. It specifies the label name and the relationship type.
All users and relationships would be in this graph.
PageRank algorithm identifies influential nodes in the network. It works similarly to the Google web page ranking. We run the following command to apply the algorithm and list the top 20 users.
The following is the result.
The result makes sense. Most in the list have a very high followers count. Some even have multi-million followers. Some with lower followers have higher scores than ones with higher followers. It may be because:
The strongly connected components algorithm is one of the community detections. This algorithm divides the network into groups of nodes. One important property is that each node in all pairs could reach the other in both directions.
We run the following script to apply the algorithm.
The algorithm writes each node a community id as a property. We run the following script to list the groups, their size, and members. The script groups users by the community ids, compute community sizes, and display the first few members.
The result is below.
Note that the ids are the graph database node ids, not the Twitter user ids.
The next one is the ratio of the number of relationships to the maximum number for each group. Take a look at the following query.
The following is the result.
Incidentally, there is one community with a ratio of 1.0. Let‚Äôs take a look at the community. We have the code below to show the nodes in the community.
Here is the community graph.
In this community, we observe everyone follows all the others. That is why the relationship ratio is 1.0.
Let‚Äôs look at another community, the one with sccId of 516325. This one is a larger community and has 26 members. The relationship ratio is relatively high at 0.28. The following is the graph.
Since there is a stricter requirement to be in a community, sizes seem smaller than other community detection algorithms. We could find how many users are in the communities of three members or more by using the following query.
When we ran it, we got around 3,000 users. The number is lower than the initial user count. So, we would try another algorithm.
Label propagation is another community detection algorithm. It has less strict requirements. First, we run the following code to detect communities.
The algorithm writes each node a community id as a property.
We run the following script to list the communities, their size, and members.
Here is the result.
We could find how many users are in the communities of three members or more by using the following query.
We found that around 20,000 users are in the communities with three members or larger. It is about the same as the initial user count.
We could take a look at one label propagation community. The one with the id of 35964 has 26 members. The following graph shows how it looks.
This label propagation community has fewer relationships than the SCC of the same size. Also, a few nodes are getting more incoming connections than the rest. However, SCC is more like a peer-to-peer community.
The next question would be what are common interests in each community. We could randomly check Twitter accounts in each group. The better way is to run the PageRank algorithm from the perspective of the community‚Äôs members. The following code is a PageRank algorithm applied to the community id of 4854.
The result below shows the users with the highest page ranks from the perspective of a specific community.
From the result above, we should be able to tell who are influential Twitter accounts in the community. Also, if you know these accounts, they are relatable.
Additionally, we try to list communities along with their highest ranking users in a single query below.
The following is the result.
Like the previous result, top-ranking accounts for some communities are relatable.
Our last part is to find who we would recommend a user to follow. Typically, a user would have a similar interest as their friends. We could also assume a user would have the same interest as users in the same community. So, we could use a community personalized PageRank algorithm and remove the accounts the user has already followed.
Assume the user id is 2138. The community id is 4854. The query is below.
The result is below.
In this article, we have collected Twitter users and their relationships. We have loaded the data to the Neo4j graph database. Then, we have done the following analysis:
By treating Twitter users as network data, we have discovered communities and influencers in communities. We have provided recommendations of who to follow. There are much more applications, especially in the social network area.
Network analysis could be an additional tool for data scientists and machine learning practitioners.
Hubert LiangüìäüöÄüê∂
Adam Shafi
in
Towards Data Science
VAISHALI
Sofien Kaabar, CFA
keerthika ravichandran
Sofien Kaabar, CFA
in
Investor‚Äôs Handbook
Sofien Kaabar, CFA
in
Geek Culture
Sofien Kaabar, CFA
in
DataDrivenInvestor
"
256,https://medium.com/@zephoria/what-if-failure-is-the-plan-2f219ea1cd62,What if failure is the plan?,"I‚Äôve been thinking a lot about failure lately. Failure comes in many forms, but I‚Äôm especially interested in situations in which people *perceive* something as failing (or about to fail) and the contestations over failure that often arise in such situations. Given this, it‚Äôs hard not to be fascinated by all that‚Äôs unfolding around Twitter. At this point in the story of Musk‚Äôs takeover, there‚Äôs a spectrum of perspectives about Twitter‚Äôs pending doom (or lack thereof). But there‚Äôs more to failure than the binary question of ‚Äúwill Twitter fail or won‚Äôt it?‚Äù Here‚Äôs some thoughts on how I‚Äôm thinking about the failure question‚Ä¶
I spent a ridiculous amount of time in the aughts trying to understand the rise and fall of social network sites like Friendster and MySpace. I noticed something fascinating. If a central node in a network disappeared and went somewhere else (like from MySpace to Facebook), that person could pull some portion of their connections with them to a new site. However, if the accounts on the site that drew emotional intensity stopped doing so, people stopped engaging as much. Watching Friendster come undone, I started to think that the fading of emotionally sticky nodes was even more problematic than the disappearance of segments of the graph.
With MySpace, I was trying to identify the point where I thought the site was going to unravel. When I started seeing the disappearance of emotionally sticky nodes, I reached out to members of the MySpace team to share my concerns and they told me that their numbers looked fine. Active uniques were high, the amount of time people spent on the site was continuing to grow, and new accounts were being created at a rate faster than accounts were being closed. I shook my head; I didn‚Äôt think that was enough. A few months later, the site started to unravel.
On a different project, I was talking with a cis/hetero dating site that was struggling with fraud. Many of its ‚Äúfake‚Äù accounts were purportedly ‚Äúwomen‚Äù but they were really a scam to entice people into paying for a porn site. But when the site started removing these profiles, they found that the site as a whole was unraveling. Men didn‚Äôt like these fake women, but their profiles enticed them to return. Moreover, attractive women saw these profiles and felt like it was a site full of people more attractive than them so they came. When the fake women disappeared, the real women disappeared. And so did the men.
Network effects intersect with perception to drive a sense of a site‚Äôs social relevance and interpersonal significance.
I don‚Äôt have access to the Twitter social graph these days, but I‚Äôd bet my bottom dollar that it would indicate whether or not the site was on a trajectory towards collapse. We are certainly seeing entire sub-networks flock to Mastodon, but that‚Äôs not as meaningful as people might think because of the scale and complexity of the network graph. You can lose whole segments and not lose a site. However, if those departing are creating Swiss cheese into the network graph, then I would worry.
The bigger question concerns those emotionally sticky nodes. What constitutes a ‚Äúcan‚Äôt be missed‚Äù account or post varies. What draws someone to a service like Twitter varies. For some, it is the libidinal joy of seeing friends and community, the posts that provide light touch pleasure and joy. For others, it‚Äôs a masochistic desire for seeing content that raises one‚Äôs blood pressure. Still others can‚Äôt resist the drama of a train wreck.
The funny thing about Twitter‚Äôs feed algorithms is that they were designed to amplify the content that triggered the most reaction, those emotionally sticky posts. This is why boring but informative content never has a chance against that which prompts fury. But it also means that we‚Äôre all watching how our little universe of content is changing (or not). Are you still seeing the things that give you pleasure? Or just the stuff that makes you angry? Why can‚Äôt you resist looking away from the things that give you pain? (That question isn‚Äôt a new one‚Ä¶ it‚Äôs the question that underlies our toxic social media ecology more generally.)
I have to give Musk and gang some credit for knowing that drama brings traffic. The drama that unfolds in the World Cup is wholesome compared to the drama of watching public acts of humiliation, cruelty, and hate. We‚Äôre in a modern day Coliseum watching a theater of suffering performed for the king under the rubric of ‚Äújustice.‚Äù And just like the ancient Romans, we can‚Äôt look away.
But how long can the spectacle last? Even the Roman Empire eventually collapsed, but perhaps the theater of the absurd can persist for a while. Still, there are other factors to consider.
Nearly everyone I talk with is surprised that the actual service of Twitter is mostly still working. What that says to me is that the engineering team was far more solid than I appreciated. Any engineering team worth its salt is going to build redundancy and resilience into the system. Exceptions that are thrown should be caught and managed. But that doesn‚Äôt mean that a system can persist indefinitely without maintenance and repair.
Think of it in terms of a house. If you walk away from your home for a while, the pipes will probably keep working fine on their own. Until a big freeze comes. And then, if no one is looking, they‚Äôll burst, flood the house, and trigger failure after failure. The reason for doing maintenance is to minimize the likelihood of this event. And the reason to have contingencies built in is to prevent a problem from rippling across the system.
What happens when Twitter‚Äôs code needs to be tweaked to manage an iOS upgrade? Or if a library dependency goes poof? What happens when a security vulnerability isn‚Äôt patched?
One interesting concept in organizational sociology is ‚Äúnormal accidents theory.‚Äù Studying Three Mile Island, Charles Perrow created a 2x2 grid before b-schools everywhere made this pass√©.
One axis represented the complexity of interactions in a system; the other axis reflected the ‚Äúcoupling‚Äù of a system. A loosely coupled system has little dependencies, but a tightly coupled system has components that are highly dependent on others. Perrow argued that ‚Äúnormal accidents‚Äù were nearly inevitable in a complex, tightly coupled system. To resist such an outcome, systems designers needed to have backups and redundancy, safety checks and maintenance. In the language of computers, resilience requires having available ‚Äúbuffer‚Äù to manage any overflow.
Having dozens of engineers working around the clock to respond to crises can temporarily prevent failure. But those engineers will get tired, mistakes will happen, and maintenance will get kicked down the road. Teams need buffer as much as systems do.
I‚Äôm concerned about the state of the team at Twitter, not just because so many people were laid off. If my hunch is right, many of the engineers who are keeping Twitter going fall into four groups. There are immigrants on H1Bs who are effectively indentured servants, many of whom would leave if they could, but the industry is falling apart which makes departures unlikely. There are also apolitical engineers who need a job and there are few jobs to be found in the industry right now. Neither of these groups will want to drive themselves to the bone in the long term. Then there are Musk fanboys who want to ride this rollercoaster for whatever personal motivation. And there are goons on loan from other public companies that Musk owns. (Side note: how how how is it legal for Musk to use employees from public companies for his private project!?!? Is this something that the Delaware courts are going to influence?)
In the early days of Twitter, moments of failure were celebrated with a Fail Whale, the iconic image that Twitter posted when something went terribly awry in the system, requiring it to be shut down and, effectively, rebooted. It‚Äôs been a long time since we saw the Fail Whale because there was a strong infrastructure team who worked to bake resilience into the system. In other words, Twitter grew up.
How long can the resilience of the system allow it to keep functioning? It could be quite a while. But I also can‚Äôt help but think of a video I saw years ago about what would happen to New York City if the humans suddenly disappeared overnight. First the pipes burst and the rats invaded. But without humans leaving behind trash, the rats eventually died. The critters that remained? The cockroaches of course.
If you searched for ‚Äúmiserable failure‚Äù (or even just ‚Äúfailure‚Äù) on September 29, 2006, the first result was the official George W. Bush biography. This act of ‚ÄúGoogle bombing‚Äù made the internet lol. But it also hinted at a broader dynamic related to failure. There are failures that everyone can agree are failures (e.g. the explosion of the Challenger), but most failures are a matter of perception.
Politicians, policies, companies, and products are often deemed a ‚Äúfailure‚Äù rhetorically by those who oppose them, regardless of any empirical measure one might use. George W. Bush was deemed a failure by those who were opposed to his ‚ÄúWar on Terrorism.‚Äù Declaring something a failure is a way to delegitimize it. And when something is delegitimized, it can become a failure.
I often think back to MySpace‚Äôs downfall. In 2007, I penned a controversial blog post noting a division that was forming as teenagers self-segregated based on race and class in the US, splitting themselves between Facebook and MySpace. A few years later, I noted the role of the news media in this division, highlighting how media coverage about MySpace as scary, dangerous, and full of pedophiles (regardless of empirical evidence) helped make this division possible. The news media played a role in delegitimizing MySpace (aided and abetted by a team at Facebook, which was directly benefiting from this delegitimization work).
Perception (including racism and classism) have shaped the social media landscape since the beginning.
A lot has changed about our news media ecosystem since 2007. In the United States, it‚Äôs hard to overstate how the media is entangled with contemporary partisan politics and ideology. This means that information tends not to flow across partisan divides in coherent ways that enable debate. In general, when journalists/advocates/regular people on the left declare conservative politicians/policies to be failures, this has little impact on the right because it is actively ignored by the media outlets consumed by those on the right. But interestingly, when journalists/advocates/regular people on the right declare progressive politicians/policies to be failures, both mainstream media and the left obsessively amplify falsehoods and offensive content in an attempt to critique and counteract them. (Has anyone on the left managed to avoid hearing about the latest round of celebrity anti-Semitism?)
I‚Äôm especially fascinated by how the things that are widely deemed failures are deemed failures for different reasons across the political spectrum. Consider the withdrawal in Afghanistan. The right did a fantastic job of rhetorically spinning this as a Biden failure, while the left criticized aspects of the mission. This shared perception of failure landed in the collective public consciousness; there was no need to debate why individual groups saw it as failure. Of course, this also meant that there was no shared understanding of what led to that point, no discussion of what should‚Äôve been done other than it should‚Äôve been done better. Perceptions of failure don‚Äôt always lead to shared ideas of how to learn from these lessons.
The partisan and geopolitical dimensions of perception related to Twitter are gobsmacking. Twitter has long struggled to curb hate, racism, anti-Semitism, transphobia, and harassment. For a long time, those on the right have labeled these efforts censorship. Under the false flag of freedom of speech, the new Twitter has eradicated most safeguards, welcoming in a new era of amplified horrors, with the news media happily covering this spectacle. (This is what led Joan Donovan and I to talk about the importance of strategic silence.)
Musk appears to be betting that the spectacle is worth it. He‚Äôs probably correct in thinking that large swaths of the world will not deem his leadership a failure either because they are ideologically aligned with him or they simply don‚Äôt care and aren‚Äôt seeing any changes to their corner of the Twitterverse.
He also appears to believe that the advertising community will eventually relent because they always seem to do so when an audience is lingering around. And with a self-fashioned Gladiator torturing his enemies for sport in front of a live audience, there are lots of dollars on the table. Musk appears convinced that capitalistic interests will win out.
So the big question in my mind is: how effective will the perception that Twitter is failing be in the long run, given how it is not jumping across existing ideological divisions? Perception of failure can bring about failure, but it doesn‚Äôt always. That‚Äôs the story of many brands who resist public attacks. Perception of failure can also just fade into the background, reifying existing divisions.
Of course, a company needs money and the only revenue stream Twitter has stems from advertising. This is one of the reasons that activism around the advertisers matters. If advocates can convince advertisers to hold out, that will starve a precarious system. That is a tangible way to leverage perception of failure. Same can be said if advocates manage to convince Apple or Google to de-list. Or if perception can be leveraged into court fights, Congressional battles, or broader policy sanctions. But right now, it seems as though perception has gotten caught in the left/right cultural war that is unfolding in the United States.
There are many ways in which the Twitter story could end, but it‚Äôs important to remember that most companies do eventually end (or become unrecognizable after 100+ years). The internet is littered with failed companies. And even though companies like Yahoo! still have a website, they are in a ‚Äúpermanently failing‚Äù status. Most companies fail when they run out of money. And the financials around Twitter are absurd. As a company, it has persisted almost entirely on a single profit stream: advertising. That business strategy requires eyeballs. As we‚Äôve already witnessed, a subscription plan for salvation is a joke.
The debt financing around Twitter is gob-smacking. I cannot for the life of me understand what the creditors were thinking, but the game of finance is a next level sport where destroying people, companies, and products to achieve victory is widely tolerated. Historical trends suggest that the losers in this chaos will not be Musk or the banks, but the public.
For an anchor point, consider the collapse of local news journalism. The myth that this was caused by craigslist or Google drives me bonkers. Throughout the 80s and 90s, private equity firms and hedge funds gobbled up local news enterprises to extract their real estate. They didn‚Äôt give a shit about journalism; they just wanted prime real estate that they could develop. And news organizations had it in the form of buildings in the middle of town. So financiers squeezed the news orgs until there was no money to be squeezed and then they hung them out to dry. There was no configuration in which local news was going to survive, no magical upwards trajectory of revenue based on advertising alone. If it weren‚Äôt for craigslist and Google, the financiers would‚Äôve squeezed these enterprises for a few more years, but the end state was always failure. Failure was the profit strategy for the financiers. (It still boggles my mind how many people believe that the loss of news journalism is because of internet advertising. I have to give financiers credit for their tremendous skill at shifting the blame.)
I highly doubt that Twitter is going to be a 100-year company. For better or worse, I think failure is the end state for Twitter. The question is not if but when, how, and who will be hurt in the process?
Right now, what worries me are the people getting hurt. I‚Äôm sickened to watch ‚Äújournalists‚Äù aid and abet efforts to publicly shame former workers (especially junior employees) in a sadistic game of ‚Äúaccountability‚Äù that truly perverts the concept. I‚Äôm terrified for the activists and vulnerable people around the world whose content exists in Twitter‚Äôs databases, whose private tweets and DMs can be used against them if they land in the wrong hands (either by direct action or hacked activity). I‚Äôm disgusted to think that this data will almost certainly be auctioned off.
Frankly, there‚Äôs a part of me that keeps wondering if there‚Äôs a way to end this circus faster to prevent even greater harms. (Dear Delaware courts, any advice?)
No one who creates a product wants to envision failure as an inevitable end state. Then again, humans aren‚Äôt so good at remembering that death is an inevitable end state either. But when someone doesn‚Äôt estate plan, their dependents are left with a mess. Too many of us have watched the devastating effects of dementia and, still, few of us plan for all that can go wrong when our minds fall apart and we lash out at the ones we love. Few companies die a graceful death either. And sadly, that‚Äôs what I expect we‚Äôre about to see. A manic, demented creature hurting everyone who loved it on its way out the door.
I‚Äôm not omniscient. I don‚Äôt know where this story ends. But after spending the last few years obsessing over what constitutes failure, I can‚Äôt help but watch this situation with a rock in my stomach.
Failure isn‚Äôt a state, but a process. It can be a generative process. After all, some plants only grow after a forest fire. (And yes, yes, tech is currently obsessed with ‚Äúfail fast.‚Äù But frankly, that‚Äôs more about a status game than actually learning.)
Failure should not always be the end goal. There‚Äôs much to be said about the journey, about living a worthy life, about growing and learning and being whole. Yet, what keeps institutions, systems, companies, and products whole stems from how they are configured within a network of people, practices, and perception. Radical shifts in norms, values, and commitments can rearrange how these networks are configured. This is why transitions are hard and require a well-thought through strategy to prevent failure, especially if the goal is to be whole ethically.
Watching this situation unfold, a little voice keeps nagging in my head. How should our interpretation of this situation shift if we come to believe that failure is the desired end goal? There‚Äôs a big difference between a natural forest fire and one that stems from the toxic mixture of arson and climate change.
Onuche Ogboyi
Damara Kitchens
Dirk Hooper
Coding Zen
Emily Babblertik
Jeehye Jung
in
AmuseAgency
Rosiger Alvarado
ALL ABOUT Islam
"
257,https://medium.com/towards-data-science/what-can-be-learned-from-1-001-a-b-tests-361a4b6e40a7,"What Can Be Learned From 1,001 A/B Tests?","How long does a typical A/B test run for? What percentage of A/B tests result in a ‚Äòwinner‚Äô? What is the average lift achieved in online controlled experiments? How good are top conversion rate optimization specialists at coming up with impactful interventions for websites and mobile apps?
This meta-analysis of 1,001 A/B tests analyzed using the Analytics-Toolkit.com statistical analysis platform aims to provide answers to these and other questions related to online A/B testing. The layout of the presentation is as follows:
Those interested in just the main findings and a brief overview can jump straight to ‚ÄúTakeaways‚Äù.
All images and charts, unless otherwise noted, are courtesy of Analytics-toolkit.com
A/B tests, a.k.a. online controlled experiments are the gold standard of evidence and risk management in online business. They are the preferred tool for estimation of the causal effects of different types of interventions, typically with the goal of improving the performance of a website or app, and ultimately of business outcomes. As such, the role of A/B tests is primarily as a tool for managing business risk while addressing the constant pressure to innovate and improve a product or service.
Given this gatekeeper role, it is crucial that A/B tests are conducted in a way which results in robust findings while balancing the business risks and rewards from both false positive and false negative outcomes. A 2018 meta analysis [1]of 115 publicly available A/B tests revealed significant issues related to the planning and analysis of online controlled experiments. Namely, the majority of tests (70%) appeared underpowered, raising questions related to both unaccounted peeking and to low statistical power. The first can result in inflated estimates and lack of control of the false positive rate, whereas the second can result in failure to detect true improvements and missed opportunities to learn from tests due to underwhelming sample sizes.
Addressing such issues and promoting robust statistical practices has been a major driver behind the development of the A/B testing statistical tools at Analytics Toolkit since its launch in 2014. In 2017 a sequential testing methodology (AGILE) was proposed and implemented to address the motivations behind peeking in a way that provides efficiency and improves the ROI of testing without compromises to statistical rigor. In late 2021 an overhauled platform was released with one aim being to address the second of the major contributors to the poor outcomes of A/B testing efforts ‚Äî inadequate statistical power. Other goals of the overhaul include prevention or minimization of other common mistakes in applying statistical methods in online A/B tests.
In light of the above, the current meta analysis has a number of goals:
The data in this analysis comes from a sample of 1,001 tests conducted since the launch of the new Analytics Toolkit platform in late 2021. The dataset contains both fixed sample tests and sequential tests (AGILE tests), with 90% of tests being of the latter type.
The initially larger sample of tests was screened so that only tests from users who‚Äôve conducted more than three A/B tests in the period are included. The rationale is to minimize the proportion of tests from users without sufficient experience with the platform as well as ones entered while exploring the functionality of the software as such tests might contain questionable data.
46 outliers were removed based on an extreme mismatch between the test plan and the observations actually recorded. It is deemed that such a mismatch signals, with high probability, either poor familiarity with the sequential testing methodology or very poor execution, making statistics derived from these questionable. The removal of these outliers had the most material effect on the AGILE efficiency numbers presented with a positive impact of 3‚Äì4 p.p..
Additionally, 22 tests with estimated lifts of over 100% were removed as such results have a high probability of not being based on actual data of sound quality. After all three screens, the number of tests remaining is 1,001.
Given known characteristics of the majority of the users of Analytics Toolkit, the A/B tests are likely to be representative of those conducted by advanced and expert CRO practitioners, as well as those with an above average knowledge and understanding of statistics.
The basic characteristics of the analyzed sample of A/B tests include the test duration, sample size, and the number of tested variants per test. Test duration provides information on the external validity of tests. Sample sizes provide an idea of the statistical power and quality of the estimates, whereas the number of variants is a simple gauge of how often practitioners test more than one variant versus a control in so-called A/B/N tests.
The arithmetic mean of all A/B test durations is 35.4 days which is equal to five weeks. The median is 30 days meaning that half of all tests lasted less than a month. The majority of tests spanned a timeframe which allows for good generalizability of any results.
Zooming in the graph reveals notable spikes corresponding to tests monitored on whole week intervals: at 7 days (1 week), at 14 days (two weeks), at 21 days (three weeks), and so on until 49 days (7 weeks) at which point the pattern is no longer visible due to low amounts of data. It seems a significant number of tests are conducted by following best practices for external validity which should result in better generalizability of any outcomes.
For tests with a primary metric based on users, the average sample size is 217,066 users, but the median is just 60,342 users. For tests with a session-based metric the average sample size is 376,790 sessions while the median is at 72,322 sessions.
The power-law-like distribution is hardly surprising, given that a power-law distribution characterizes how users and engagement is split among web properties and mobile apps.
Sample size in itself has little to say without the context of baseline rates and standard deviations of the primary metrics, but we can safely say that the sampled tests include sufficient numbers of users or sessions to avoid statistical complications associated with very small sample sizes.
The vast majority of tests (88%) conducted on the Analytics Toolkit platform included just one test variant and a control. Only 10% included two variants, and just 2% included three or more variants. It seems most expert conversion rate optimizers prefer to plan tests with a single, well-thought out intervention, rather than to spend more time testing a more diverse set of ideas in a single go. One can speculate that this reflects a preference for incremental improvements implemented quickly versus more complicated tests that take longer, each carrying higher uncertainty.
The advanced test parameters reflect key aspects of the all-important statistical design of the A/B tests in the dataset.
The distribution of confidence thresholds shows that in the majority of tests the threshold is between 80% and 95%, with a few exceptions below and above. The fact that the confidence threshold values are distributed somewhat evenly within this range is not at odds with a situation in which users are utilizing either the wizard or their own risk-reward calculations to arrive at a specific threshold matching the case at hand. The small number of thresholds higher than 95% likely correspond to tests with higher stakes in case of a wrong conclusion in favor of a variant.
This good practice can be contrasted to the one-size-fits-all approach of applying a 95% confidence threshold approach ‚Äúby default‚Äù. The latter is typically suboptimal from a business perspective.
The confidence thresholds distribution seem to indicate an informed balancing of the two types of risk in A/B tests and can be viewed as a positive for the business utility of these tests. The data, however, cannot be conclusive in itself.
A significant majority of tests are powered at 80%, however a significant minority of roughly a third of tests are powered at 90%. This is encouraging since 80% power offers a fairly low chance of detection of a true effect of the target minimum effect of interest. It is better to explore the relationship of the minimum effect of interest and the minimum detectable effect at the 90% point of the power curve when planning tests.
The distribution of minimum detectable effects at 90% power is plotted below with a mean of 11.3% and a median of 6% lift.
Two thirds of all tests had a minimum detectable effect below 10% lift, which is significantly more than the roughly one quarter of tests with such parameters in the previous meta analysis [1]. The median of 6% means that half of tests had an MDE below 6% relative lift. It is probably an effect of both the guidance of the wizard and the experience of the practitioners using the tool. There is the obvious difficulty of untangling the two with just the data at hand.
The above numbers can be interpreted as tentatively supporting the conclusion that at least some of the unrealistic MDEs observed in the 2018 meta analysis [1]were linked to unaccounted peeking.
In any case, these are highly encouraging numbers, especially in light of findings in the following section.
Lift estimates are the simple unbiased maximum likelihood estimator for fixed-sample tests, and a bias-reduced maximum likelihood estimator for sequential tests.
About 33.5% of all A/B tests have a statistically significant outcome in which a variant outperformed the control. This is higher than the 27% observed in the previous meta analysis [1], and in the upper range of industry averages reported in an overview by Kohavi, Tang, and Xu (2020), page 112 [2]. Given that there is no outcome bias in the inclusion criteria for this meta analysis, this number can be viewed as evidence for the relatively higher quality of the tested ideas and their implementation (a.k.a. interventions).
This high proportion of ‚Äòwinners‚Äô is not entirely surprising given the known profile of the users of Analytics Toolkit in which advanced and expert CRO practitioners are overrepresented. It shows the value of accumulated knowledge and experience and likely means that these professionals are better at filtering out poor ideas and/or is indicative of the sway they have over decision-makers regarding what ideas reach the testing phase.
Despite the optimistic number above, one way to interpret the lift estimates of all A/B tests conducted is as showing limited ability of this elite cohort of professionals to generate and implement ideas which bring significant value to an online business. The median lift estimate is just 0.08%, whereas the mean is 2.08% ( standard error is 0.552%). This means that nearly half of the tested interventions have no impact or a negative impact. Even among the positive estimated lifts the majority are below 10%. On the flip side, the majority of negative estimated effects also have an impact of less than 10%.
The lift estimates are decidedly not normally distributed with a p-value < 0.0000001 on all five of the battery of tests supported by GIGAcalculator‚Äôs normality test calculator. The tails are quite heavy, with skewness towards the positive end. Estimates around zero are dominating the shape.
The above data reveals that affecting user behavior is hard either way. It is equally as difficult to influence a user to perform a desirable action as it is to sway them away from a goal they are intent on achieving. Nevertheless, the positive median and mean reflect that tested interventions have a better than coin flip probability of having a positive effect, with the difference from zero being statistically significant (p = 0.000084; H 0: %lift ‚â§ 0) with the relevant 95% interval spanning [1.172%, +‚àû).
Of the statistically significant positive outcomes, the median lift estimate is a respectable 7.5% whereas the mean is a whopping 15.9%! The standard deviation is 19.76%, with confidence intervals for the mean as follows: 95%CI [13.76%, 17.98%]; one-sided 95%CI [14.1%, +‚àû). This means that tests with ‚Äòwinning‚Äô variants are likely bringing significant value to the businesses.
In a few cases the lift estimates of winning variants are below zero, which reflects non-inferiority tests. Given that some of the positive estimates are also from non-inferiority tests the likely benefit of the above lifts may be even greater than the numbers show.
Of the tests which concluded without a statistically significant outcome most have a negative estimated lift, while some have positive estimated lift. With a mean of -4.9% and a median of -1.7% these demonstrate why it is so important to perform A/B tests (SD = 10.8%, 95%CI [-5.70%, -4.07%], one-sided 95%CI (-‚àû , -4.201]). In many scenarios, it is unlikely to detect such small impacts in a non-experimental setting due to the much greater uncertainties involved in any kind of observational post-hoc assessment (change impact estimation).
The statistics in this section reflect various aspects of sequential testing and its performance. Sequential experiments are planned for a certain maximum target sample size and a number of interim evaluations to reach that target, but may stop (‚Äúearly‚Äù) at any evaluation depending on the observed data up to that point. The efficiency of sequential tests depends both on the type of sequential testing performed, the testing plan, and the true effect size and direction of the tested intervention.
Sequentially evaluated online experiments using AGILE are planned for 10.8 monitoring stages on average. These tests have stopped on analysis number 5.6 on average, suggesting they stopped at half of their maximum planned test duration / sample size. This is in line with the expected performance of sequential testing, meaning that one can plan longer maximum run times for tests with peace of mind due to the expectation that they will terminate much earlier if the results are overly positive or overly negative relative to the minimum effect of interest.
Both the stopping stage and the actual running time of a sequential A/B test depend on the true effect size and direction of the tested intervention. The distribution of actual running times as a percentage of their respective maximum running time is presented below.
The mean and median are nearly identical at 64% and 64.7%, meaning that tests stopped, on average, at just under two thirds of their maximum planned run time.
There is a significant spike of 57 tests stopped at exactly 100% of their maximum running time which suggests either a higher than expected number of tests with just 2 monitoring stages and/or a number of tests planned to perfection (very stable rate of users or sessions per unit time), and/or tests entered post-factum for whatever reason. Two-stage tests are a small factor, but it is difficult to distinguish between the other two. It is therefore likely that these represent artificial tests (e.g. reanalyzing a test post-hoc for the purpose of comparing estimates with a fixed-sample analysis) instead of actual tests planned and analyzed using AGILE.
If this is assumed to be the case, then these tests are skewing the distribution upward and the true mean and median are instead at about 62% of the maximum running time, slightly improving performance. However, the probability that the majority of such tests are actually artificial is judged to be low enough to not warrant outright exclusion from the analysis.
The performance versus fixed-sample equivalents mirrors that of the performance versus the maximum running time with a the spike appearing between 105% and 115% of fixed-sample equivalent since 5‚Äì15% is the expected worst-case sample size inflation in most tests between 5 and 12 analyses. The mean and median sample size of sequential tests is 73.4% and 74% of that of an equivalent fixed-sample test, respectively. This puts the average saving in both time and users exposed at around 26%. These numbers would improve to about 71.5% and about 28.5% if the unexpected spike between 105% and 115% is removed from the data.
To my knowledge, this is the first of its kind meta-analysis of the actual performance of a robust frequentist sequential testing method with real-life interventions in online A/B testing. It proves the benefits of using sequential testing over fixed-sample testing, although the impact is somewhat dampened versus estimates which is mostly explained by the distribution of the estimated lifts of all tests which is far from normal, with significant concentration of density around zero.
The meta-analysis achieves its first two goals by presenting an outcome-unbiased collection of nearly ten times more tests than a previous meta-analysis. It also provides provisional evidence of positive effects of using the Analytics Toolkit test planning and analysis wizard.
The evidence for the benefits of sequential testing in a real-world scenario is substantial and at minimum supports a 26% performance increase in terms of running time / sample size, with possible greater improvement in the ROI of testing due to the disproportionate sizes of the true effects in tests stopped early for either efficacy or futility.
The meta-analysis also produced the following key numbers:
Under the assumption that a majority of tests in the analysis were performed on key business metrics and few were on less sequential user actions, one can infer about the benefits of testing over implementing straight away. Of two identical companies wishing to implement identical changes, the one which implements only changes that pass an A/B test would achieve multiple times faster growth than the one which just implements everything. It would also grow much smoother which really matters in business. The advantage of the former would come from implementing only winning tests with a mean lift of 15.9% compared to a mean lift of just over 2% for the latter, despite winning tests resulting in implementing just over a third of all proposed changes.
While this last conclusion might be stretching it a little, it should be a prime example of the significant marginal benefits of testing when accounting for the statistical overhead. The various overheads of preparing, running, and analyzing tests need to be accounted for separately, with the typical economies of scale at play.
[1] Georgiev, G.Z. (2018) ‚ÄúAnalysis of 115 A/B Tests: Average Lift is 4%, Most Lack Statistical Power‚Äù [online] at https://blog.analytics-toolkit.com/2018/analysis-of-115-a-b-tests-average-lift-statistical-power/[2] Kohavi, Tang, and Xu (2020) ‚ÄúTrustworthy Online Controlled Experiments: A Practical Guide to A/B Testing‚Äù, Cambridge: Cambridge University Press. isbn: 978‚Äì1‚Äì108‚Äì72426‚Äì5. doi:10.1017/9781108653985
Originally published at https://blog.analytics-toolkit.com on October 18, 2022.
Ankit Choudhary
in
Analytics Vidhya
Furkan Karadas
The Enterprise World Magazine
in
The Enterprise Diary
Yulia R√∂nsch
in
Towards Data Science
Srisindhu
The Tech Trend
Dimitris Poulopoulos
in
Towards Data Science
Lillian Curanzy
in
Open Data Literacy
"
258,https://medium.com/dev-genius/introduction-to-prolog-a-programming-language-for-artificial-intelligence-320b75455381,Introduction to Prolog: A Programming Language for Artificial Intelligence,"Prolog is a programming language that is well-suited for developing logic-based artificial intelligence applications. It is a declarative programming language, meaning that it allows the programmer to specify the rules and facts about a problem domain, and then the Prolog interpreter will use these rules and facts to automatically infer solutions to problems.
One of the key features of Prolog is its ability to handle uncertain or incomplete information. In Prolog, a programmer can specify a set of rules and facts that are known to be true, but they can also specify rules and facts that might be true or false. The Prolog interpreter will then use these rules and facts to automatically reason about the problem domain and find solutions that are most likely to be correct, given the available information.
One way to use Prolog is to define a set of rules that describe the relationships between different objects or concepts in your problem domain. For example, you might define rules that specify that certain objects are bigger than others, or that some objects are the same color. Then, you can use Prolog to ask questions about these objects and their relationships, and the interpreter will use your rules to deduce the answers.
To use Prolog, you will need to have a Prolog interpreter installed on your computer. There are several different Prolog interpreters available, including SWI-Prolog, GNU Prolog, and B-Prolog. Once you have installed an interpreter, you can start writing your Prolog programs using a text editor and then run them using the interpreter.
In summary, Prolog is a powerful and flexible programming language that is well-suited for developing logic-based artificial intelligence applications. It allows the programmer to specify a set of rules and facts about a problem domain, and then use those rules and facts to automatically infer solutions to problems.
In Prolog, programs are made up of two main components: facts and rules. Facts are statements that are assumed to be true, such as ‚ÄúJohn is a man‚Äù or ‚Äúthe capital of France is Paris.‚Äù Rules are logical statements that describe the relationships between different facts, such as ‚Äúif John is a man and Mary is a woman, then John is not Mary.‚Äù
Prolog programs are written using a syntax that is similar to natural language. For example, a simple Prolog program might look like this:
In this example, the first three lines are facts, while the fourth line is a rule. The rule uses the not/2 predicate to state that if X is a man and Y is a woman, then X is not Y.
There is no single ‚Äúsyntax‚Äù for Prolog, as the language allows for a wide range of different programming styles and approaches. However, here are some basic elements of Prolog syntax that are commonly used:
In Prolog, facts are statements that are assumed to be true. They are used to provide the interpreter with information about the problem domain, and the interpreter will use this information to automatically infer solutions to problems.
Facts are written using a predicate name followed by a list of arguments enclosed in parentheses. For example:
In this example, the first line states that john is a man, the second line states that mary is a woman, and the third line states that paris is the capital of france.
Prolog facts can have any number of arguments, and the arguments can be variables or constants. For example, the following fact has two arguments, one of which is a variable:
In this case, the fact states that if X is a man and Y is a woman, then X is the parent of Y.
Once you have defined your facts in a Prolog program, you can use them to automatically infer solutions to problems. For example, you could ask the interpreter to find the capital of France by using the following query:
In this query, the interpreter will use the capital_of/2 fact that you defined earlier to determine that the capital of France is Paris, and it will return the value paris as the solution.
In Prolog, rules are logical statements that describe the relationships between different facts. They are used to specify the conditions that must be met in order for a certain fact to be true.
Rules are written using the predicate name followed by a list of arguments enclosed in parentheses, followed by a colon and a hyphen (:-) and the body of the rule. For example:
In this example, the rule uses the not/2 predicate to state that if X is a man and Y is a woman, then X is not Y. The body of the rule is made up of two facts: man(X) and woman(Y).
Prolog rules can have any number of arguments, and the arguments can be variables or constants. For example, the following rule has three arguments, two of which are variables:
In this case, the rule states that if X and Y are objects with sizes Xsize and Ysize, respectively, and Xsize is greater than Ysize, then X is bigger than Y.
Once you have defined your rules in a Prolog program, you can use them to automatically infer solutions to problems. For example, you could ask the interpreter to find out which object is bigger than another object by using the following query:
In this query, the interpreter will use the bigger_than/3 rule that you defined earlier to determine which object is bigger than the other, and it will return the appropriate value for Z as the solution.
In Prolog, variables are used to represent values that can change or be determined by the interpreter. They are written using a name that begins with an uppercase letter, such as X or Y.
Variables can be used in both facts and rules to represent values that are not known at the time the program is written. For example, the following fact uses a variable to represent the capital of a country:
In this case, the fact states that the Capital of a given Country is unknown, and the interpreter will use other facts and rules to determine the value of Capital when a query is made.
Queries are used to ask the interpreter to find solutions to problems based on the rules and facts in the program. In Prolog, queries are written using the same syntax as facts, followed by a question mark (?). For example:
In this query, the interpreter will use the capital_of/2 fact that was defined earlier to determine that the capital of France is Paris, and it will return the value paris for the variable X as the solution.
You can use queries to ask the interpreter to find solutions to a wide range of problems, based on the rules and facts that you have defined in your Prolog program. The interpreter will use these rules and facts to automatically reason about the problem domain and find solutions that are most likely to be correct, given the available information.
Here is a simple Prolog program that defines a set of rules and facts about a problem domain, and then uses those rules and facts to answer a few queries:
In this example, the program defines three facts: man(john), woman(mary), and capital_of(france, paris). These facts state that John is a man, Mary is a woman, and Paris is the capital of France.
The program also defines a rule using the not/2 predicate. This rule states that if X is a man and Y is a woman, then X is not Y.
Finally, the program includes two queries. The first query, not(john, mary)?, asks the interpreter to determine whether John is not Mary, based on the not/2 rule and the man/1 and woman/1 facts. The interpreter will use these rules and facts to deduce that John is not Mary, and it will return true as the solution to the query.
The second query, capital_of(france, X)?, asks the interpreter to determine the capital of France. The interpreter will use the capital_of/2 fact to determine that the capital of France is Paris, and it will return the value paris for the variable X as the solution.
Overall, this Prolog program demonstrates how to define rules and facts about a problem domain, and how to use those rules and facts to automatically infer solutions to problems.
One possible example of an AI application of Prolog is a simple diagnostic tool for medical conditions. In this application, the Prolog program would define a set of rules and facts about different medical conditions and their symptoms, and then use those rules and facts to diagnose a patient‚Äôs condition based on their reported symptoms.
Here is a simple example of a Prolog program that could be used for this purpose:
In this example, the program defines a set of facts that describe the symptoms of three different medical conditions: the flu, allergies, and the common cold. The program also defines a rule using the has_condition/2 predicate, which states that if a patient has a certain symptom, then they have the medical condition that is associated with that symptom.
Finally, the program includes a query that asks the interpreter to determine which medical condition a patient has based on their reported symptoms. In this case, the query specifies that the patient has the symptom of sneezing, and it asks the interpreter to determine which medical condition the patient has. The interpreter will use the has_condition/2 rule and the has_symptom/2 facts to deduce that the patient has either the flu, allergies, or the common cold, and it will return one of these conditions as the solution to the query.
This simple Prolog program demonstrates how the language can be used to develop an AI application that can diagnose medical conditions based on symptoms. Of course, in a real-world application, the program would need to be much more comprehensive and sophisticated, with a larger set of rules and facts and the ability to handle a wider range of symptoms and conditions.
In conclusion, Prolog is a powerful and flexible language for developing AI applications. If you are interested in exploring the capabilities of Prolog for yourself, consider downloading a Prolog interpreter and experimenting with writing your own programs. You may be surprised by the insights and solutions that can be automatically inferred from a set of simple rules and facts. Give Prolog a try and see what it can do for you!
linda H
Al-Waleed Shihadeh
in
Level Up Coding
Meeky Store
Arun Ramakani
in
The Startup
Beam Privacy
Sandeep Bamane
The Recharge
in
TheRecharge
Mohamed Saeed
"
259,https://medium.com/@alexcancode/my-worst-management-experiences-at-microsoft-google-amazon-a6a430dfcfc9,"My Worst Management Experiences at Microsoft, Google, Amazon","It‚Äôs the manager that makes work well or hell
After starting my career at Amazon, moving to Microsoft, and now joining Google, I‚Äôve had my fair share of managers. Different people have different management styles and I‚Äôve had experiences good and bad. In this article, we‚Äôll talk about the bad ones.
C S Bhambra
Emily Brown
Insights from Leaders
Fred Stluka
Abraham Salako
Zach Obront
in
Authority Magazine
Dinesh Kumar P
in
Kissflow Culture | Explore Our Butterfly Effect
John McDermott
in
MEL Magazine
"
260,https://medium.com/geekculture/programming-jokes-to-make-your-day-dae1ac1ff4b5,Programming Jokes to make your day,"Compilation of Funny Programming Jokes
Are there any connections between memes and happiness? If you are my type of person, I love to see mobile after finishing my work and what I see over there. Any guess?
I mostly see videos, ticktock, and memes posted on the internet. I don‚Äôt know why but it helps to refresh my mind and work stress. That‚Äôs why I believe
Amy @GrabNGoInfo
in
GrabNGoInfo
Aalok Trivedi
Hafsi Oussama
Adarsh Gouda
Olly Woodman
in
google-exoplayer
Anish Antony
in
Javarevisited
Baunfire
The Ritty
"
261,https://medium.com/@DiaAzul/awesome-visualisations-in-knime-using-python-plotly-4a7c73e99d14,Awesome Visualisations In KNIME Using Python Plotly,"How to create a configurable population pyramid visualisation which can be re-distributed for use by data analysts, without having to train them in programming.
KNIME is an easy to use no-code data science platform that allows non programmers to bring together and analyse large quantities of disparate data. KNIME has, over many years, evolved to include new technologies that‚Ä¶
Sophia Daniels
$umi_Oneness
demola malomo
in
Dev Genius
Jae Nam
in
Aergo blog
WhereIsMyTransport
Connektteacher
Saleha
in
DataDrivenInvestor
Esther Vaati
in
Better Programming
"
262,https://medium.com/devops-dev/cqrs-command-query-responsibility-segregation-1f6423788401,CQRS ‚Äî Command Query Responsibility Segregation,"CQRS It‚Äôs a patterner proposed by Greg Young, that you can use a different model to update information than the model you use to read information.
A command is an action that modifies an object's state or made a side effect in the system. Also, a command never returns a result after have been run.
A query is only an action that should return data, and shouldn‚Äôt create a side effect in the system.
The origin of CQRS become from the concept proposed by Bertrand Meyer called CQS ‚Äî Command Query Separation
It states that every method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, asking a question should not change the answer. More formally, methods should return a value only if they are referentially transparent and hence possess no side effects. (Wikipedia)
CQRS, becomes many advantages, for example, scalability.
In the image above, we have a clear segregation between the command stack and the query stack, so the CQRS shows us that is possible to improve the data store (when necessary), separating on a read and write database.
In this hands-on, I‚Äôll show you how you could implement the CQRS pattern. I‚Äôm using the onion architecture, but it is not a problem if you don‚Äôt know about this, the focus is only on the CQRS pattern, if you were curious about Onion Architecture after you finish this story, check out my post about it.
faun.pub
We can see that the PurchaseOrderController in your method createPurchaseOrder receives a command, I‚Äôm called the DTO as PurchaseOrderRequest
After the controller process the request, and run the business logic, and application logic, the PurchaseOrderEntityProvider saves the order directly in the write database
The query in this case is only GET in the PurchaseOrderController, listing all purchase orders.
Sometimes the input model will be the same properties output model, but they have different objectives.
After to sent to the data-provider layer, the PurchaseOrderProviderImpl runs this query directly in the read database.
To run directly in the read database as I‚Äôm using Springboot was need to configure this, so is necessary to have the two connections (WRITE database and READ database)
And when I put the annotation @Transactional with the parameter (readOnly = true) this transaction will be in the read database.
So, CQRS doesn‚Äôt only have a read database and write database but has a separation between what is a command and a query.
Also, we could use a cache for example.
Remember when to use the CQRS to have a replication mechanism.
Greg Young ‚Äî CQRS and Event Sourcing ‚Äî Code on the Beach 2014
CQRS Documents by Greg Young
Rodrigo Branas ‚Äî CQRS with Greg Young
Elemar Jr ‚Äî Descomplicando CQRS (PT-BR)
Hendrik Human
in
ImageEngine
Shiddalingeshwar M S
Atheios team
Lakshay Kalbhor
in
Tata 1mg Engineering
Rahul Singh
Robert Cooper
in
Osedea
Jonathan Huang
Rawad Haber
"
263,https://medium.com/@sijmenvdw/portugal-will-be-the-2022-world-cup-grand-champion-3823b0e1e707,Portugal will be the 2022 World Cup grand champion!,"Simulations‚Ä¶ Simulations everywhere
I‚Äôm not sure about you, but I get bonus points for correctly predicting the winner of the World Cup. Let‚Äôs give this some extra attention.
You have two major ways to get the odds:
Let‚Äôs look at calculation first, say you want to know the odds of getting a total of exactly 11 when rolling two dice. You could create a table with all options and do the math to find out the exact chance of getting a sum of 11.
Out of 36 possible combinations, only two will result in a sum of 11.
This means the chance of getting a sum of 11 when rolling two dice is:
2 /36 or ~5.56%.
That is what calculation looks like. Now let‚Äôs have a look at the simulation approach.
Again, we want to know the odds of getting exactly 11 when rolling two dice.
This time, we won‚Äôt create a table. Instead, we will just roll two dice a hundred times. Even better, we let a computer do it using the following piece of Python code:
Which gives the following results:
On three rolls we got 11, resulting in 3/100 = 3%, not so close to the 5.56% we got before. Maybe we just need some more rolls, let‚Äôs try for a thousand:
This time we get 64/1000 or 6.4%. Already a bit closer, but it looks like we need even more. When we ramp up the number of simulations we get the following graph:
When we roll a million times we get a total of 55657 rolls with a sum of 11, or 55657/1000000 = ~5.57%. Pretty close to the original 5.56%.
I‚Äôm glad you asked.
When we have a look at the above approaches, we can see we cannot do the calculation method: we cannot create a table for all possible outcomes of all upcoming matches for the World Cup.
But what we can do is simulate the entire World Cup tournament a million times! So let‚Äôs do that.
To simulate a game, we will draw a score from a distribution depending on both teams‚Äô FIFA ranking. We group each country into a range of FIFA ranks, then we look at the distribution of the scores for matches between these groups. For example:
Monday, November 21, Senegal (rank 18) vs. The Netherlands (rank 8).
Senegal will be in group rank 11‚Äì20, and The Netherlands will be in group rank 1‚Äì10.
Then we look at the distribution, just like we did in the previous blog here. One addition, this time it does matter if we draw 0‚Äì1 or 1‚Äì0 since we would like to know if Senegal or The Netherlands will win.
Two things to note:
We can create this distribution for each possible pair of FIFA rank groups and then draw from it, giving us a sample score for each match.
Before we run it a million times, let‚Äôs show a sample for Group A first.
Using the technique above, we can now have a score randomly selected from our distribution for each match. The chance column indicates the chance of this particular score happening given the grouped FIFA ranks of the two teams.
The scores above result in the following standing:
Meaning in this scenario The Netherlands and Ecuador would continue to the round of 16.
We are now calculating the percentages and simulating the tournament, the best of both worlds.
We repeat the above process for all groups, then for the entire round of 16, quarter-finals, semi-finals, and finally‚Ä¶ the final.
We then run it many many times, a million times to be precise.
Without further ado, here are your top 10 contestants for winning the 2022 World Cup in Qatar:
Portugal wins 84832 of all one million simulated World Cups, making it the most likely winner.
Read the next blog with better predictions here.
Follow me here on Medium and here on LinkedIn.
Allen Miyazawa
Octoparse
Dapo Simon Ajeniya
Aurangazeeb A K
in
Becoming Human: Artificial Intelligence Magazine
‚ô° agnes szeto ‚ô°
Viggy Kumaresan
in
Data in All Things
Thibauld Braet
in
dataroots
Mikael Wornoo
in
TechWolf
"
264,https://medium.com/towards-data-science/yes-python-has-a-built-in-database-heres-how-to-use-it-b3c033f172d3,"Yes, Python Has a Built-In Database. Here‚Äôs How to Use It.","Believe it or not, the moment you installed Python on your computer, you also installed other wonderful tools. One of them is SQLite.
SQLite is an embedded, file-based relational database management system (RDBMS) that can be used in our Python applications without having to install any additional software. Instead, we only need to import the built-in Python library sqlite3 to use this database.
In this guide, we‚Äôll see how to connect to a database, create tables, insert data into tables, and how to integrate it with Pandas.
If you don‚Äôt feel like reading, you can watch my YouTube video instead!
Be sure to subscribe here to get my SQL Cheat Sheet I use in all my tutorials (Free PDF)
The first thing we have to do is create a connection to a database. To do so, we only need to import sqlite3 and use the .connect method. Inside parentheses, we write the name of the database we want to create. In my case, I‚Äôll name it ‚Äústudents.db‚Äù
If you run the code above, a new file called ‚Äústudents.db‚Äù will be created in your working directory.
Now we can create a table and put data into it.
Before creating a table we need to create a cursor. A cursor is an object that is used to make the connection for executing SQL queries. We‚Äôll use the cursor to create tables, insert data, and more.
To create a cursor we only need to use the connection we‚Äôve created and the .cursor method.
After that, we use the .execute method to create a new table in our database. Inside quotes, we write the average SQL syntax used to create a table in most RDBMS. In this case, we use the CREATE TABLE statement.
As you can see, we need to define the data type as we create the columns of our table. Unlike most RDBMS that have dozens of data types, SQLite has only 5 data types:
Finally, we have to commit and close the connection. Here‚Äôs how the code looks so far.
Great! We‚Äôve created our first table, but it‚Äôs empty, so let‚Äôs put some data into it.
Let‚Äôs start by adding one row to our ‚Äústudents‚Äù table. To do so, we use again .execute, but now we use the INSERT INTO statement.
Below I add data about a student ‚Äúmark‚Äù who is 20 years old and is 1.9 meters.
Note that before running the code above, you need to comment out the CREATE TABLE statement because the table already exists.
We can also insert multiple rows, but in this case, we use the.executemany method. In addition to that, we use the ? as a placeholder. This helps us add data from a list we name all_students.
So far we‚Äôve created a table and put data into it, but we haven‚Äôt seen our table yet. To see our data, we first need to select data from our table with the SELECTstatement and then display it with .fetchall.
The output printed should be:
In case you don‚Äôt want to repeat these steps every time you want to see the data in the table, you can use SQLiteViewer. There you only need to drag your .db file to see its content.
Here‚Äôs everything we‚Äôve done so far
That‚Äôs the foundation of SQLite in Python. Things like updating rows, deleting rows, sorting data, and dropping tables are also possible in SQLite. You only need to use your SQL knowledge to execute them.
SQLite can integrate with dataframes in Pandas. As an example, we‚Äôll work with a CSV file called population_total.csv that you can download here.
Here‚Äôs how the dataframe looks:
Now let‚Äôs create an in-memory SQLite database. To do so, first, we need to install sqlalchemy: pip install sqlalchemy
Then we need to create an engine.
Now let‚Äôs attach the dataframe to a table in our database (the table doesn't need to be created in advance). In this case, I‚Äôll attach df to a table I‚Äôm naming ‚Äúpopulation‚Äù.
To see our table we run the code below.
Note: If you want to create a sqlite file (instead of an in-memory database), you should create an engine with a file database.
Let‚Äôs create a mydb.db file and then attach the df dataframe to a ‚Äúpopulation‚Äù table.
Again, you can use .fetchall to see the table or use SQLite Viewer.
Congratulations! Now you know how to work with SQLite in Python and even integrate it with dataframes in Pandas.
Learn SQL ‚Äî the most in-demand skill for data professionals. Get my FREE SQL Cheat Sheet by joining my email list with 20k+ people.
If you enjoy reading stories like these and want to support me as a writer, consider signing up to become a Medium member. It‚Äôs $5 a month, giving you unlimited access to thousands of Python guides and Data science articles. If you sign up using my link, I‚Äôll earn a small commission with no extra cost to you.
frank-andrade.medium.com
Kururu
Diptimaya Patra
Ezzy Elliott
Dhirendra Patil
Ankur Sharma
Nicholas Muldoon
in
Easy Agile
Bendriss Maryam
in
Data Science in simple words
Talha Zahid
in
Predict
"
265,https://medium.com/@lucapetriconi/google-is-done-heres-why-openai-s-chatgpt-will-be-a-game-changer-98ae591ad747,Google is done. Here‚Äôs why OpenAI‚Äôs ChatGPT Will Be a Game Changer,"Yesterday, on November 30, Sam Altman announces the launch of ChatGPT. Since then it has taken the internet by storm. And not without a reason. As you will see, this new AI could be a complete game changer.
The model is fine-tuned model based on the GPT-3.5 architecture and interacts in a conversational way ‚Äî just like a chatbot. The model was trained using Reinforcement Learning from Human Feedback (RLHF). In addition, the guys at OpenAI used supervised fine-tuning to improve the model. There‚Äôs a big difference though:
Not only does it emulate different styles (see below), but it also remembers what you as a user has told it before and can retell you.
‚Ä¶ or Stackoverflow.
We live in what feels like rapidly evolving times (even though Peter Thiel might disagree). There are new AI tools coming out on a fairly regular basis now. With the rise of general adversarial networks, or GANs, and generative AI tools like Dall-E or Stablediffusion, we‚Äôve seen a couple of mindblowing things. Being able to create an image from a mere text input almost feels normal now because we grow increasingly used to these kinds of things.
Here‚Äôs why ChatGPT is different, though.
The way you interact with the ChatGPT feels different because you speak to it directly as if it were another person. There‚Äôs an exchange going on. A back-and-forth. Almost as if you were talking to a person. In fact, it makes you second-guess if you‚Äôre talking to a person or an AI.
There‚Äôs one more aspect that makes it feel different: It remembers what you've previously said. This means you can iteratively work towards the desired result. In other words, you‚Äôre having a conversation.
Sam (@sama), OpenAI‚Äôs CEO, puts it like this in one of his tweets:
‚Äúlanguage interfaces are going to be a big deal, i think. talk to the computer (voice or text) and get what you want, for increasingly complex definitions of ‚Äúwant‚Äù! this is an early demo of what‚Äôs possible (still a lot of limitations ‚Äî it‚Äôs very much a research release).‚Äù
He adds that this interface seems plausible not only for this type of application but also for a future, more complex application. And if you think about it, it makes sense: Conversing is the natural way for humans to interact with one another and convey their thoughts. Or is it thinking? (I‚Äôm saying this at a moment in time when Elon Musk claims Neuralink is ‚Äúsix months‚Äù away from its first human trial.)
Let‚Äôs stick with conversing for now. Let‚Äôs say you want to write an article on a given topic. For example, you could ask it to write it for you. Depending on the output, you might want to refine and modify it. That‚Äôs exactly what Twitter user @goodside did:
There‚Äôs been recent hype (or worry?) around AI-assisted coding tools like GitHub Copilot. But with the amount of detail in the output of ChatGPT, the fact that it writes code, too, almost seems like a side-effect.
It‚Äôs impressive to see that it combines text and code ‚Äî with formatting and commenting and all ‚Äî in its output.
What amazes me most is one fact: It doesn‚Äôt only output the code but it also explains it. For example, imagine you're debugging and giving it code as input while asking it to find the bug. The output could look something like this:
While seeing this you can easily think: ‚ÄúWhy still search StackOverflow?‚Äù. And you‚Äôre right. How many times have you searched for a very specific question on StackOverflow and you didn't find an answer? And most importantly how much time did you spend finding it (if you did)? There are a few things to keep in mind though ‚Äî for now.
While I don‚Äôt question that these is some of the most impressive things I‚Äôve recently seen, it won‚Äôt mean the end of Google (just yet).
Of course, there are a few limitations to ChatGPT.
While the model will be improved upon iteratively, this is already quite impressive. What I find the most impressive is the ease to interact with it and the seeming accuracy of the outputs.
The model is open to playing around with, here. Let me tell you in the comments below what you asked and what your thoughts about it are.
For more on data-related topics and fun tech stuff, you can find me on Twitter and LinkedIn.
Dr. GP Pulipaka
tech expert
Ben Edwards
in
alt text
Katica Roy
Eric Saund
PathDAO
NR Alekya
ricky martin
"
266,https://medium.com/@cailiansavage1/what-ballpoint-pens-can-tell-us-about-chinas-future-92401cc736dc,What Ballpoint Pens Can Tell Us About China‚Äôs Future,"Paper, as most people vaguely know, was invented in China long ago ‚Äî about 2,000 years ago in fact. Today, China is still the largest producer of paper, holding about a quarter of the global market share.
It is also the largest producer of ballpoint pens, with its 40 billion (yes, with a ‚Äòb‚Äô) pens annually amounting to ~80% of global output. That won‚Äôt shock most people either; pens are cheap and fairly low-tech, and‚Ä¶
meshedsociety.com
Moni üëæ
in
Moni üëæ
Dasha Fomina
in
Voice UI
Promobot Robotics
Susi Su
Inspire
in
Beyond The Grid
HoloBase
Kenta Yamaguchi
in
TechDesu Tokyo
"
267,https://medium.com/datadriveninvestor/5-youtube-channel-that-will-teach-you-data-science-9ceb7b8ac708,5 Youtube Channel That Will Teach you Data Science,"According to growing business opportunities in AI and Data Science, every day, many of us pursue a career in that field. Here is a funny article for you, written in 2012.
In that article, Data Science is defined as the sexiest job in the 21st Century by the Harvard Business Review, here is the article.
Beyond that, there are too many career options in this field.
My data journey started in 2017.I assure you for over 5 years the options are still growing, hence the terms that we should also know increase.
Sure, there are many ways to learn the skills for this position or even the meaning of this positions :)
Online courses, college certificates, and more.
But there are many free ways that exist too.
If you are familiar with business terms and sharpen your skills for free, there are helpful channels on Youtube
And totally free.
After a while, I realized that their videos feed my appetite in Data Field, so I intend to share them with you.
Of course, there are many others that exist. Yet, for me, here are 5 Youtube channels that will teach you Data Science for free in 2022.
He describes himself as follows in his Youtube Channel about section;
Data Science and Sports Analytics are my passions. My name is Ken Jee and I have been working in the data science field doing sports analytics for the last 5 years. I have held data science positions in companies ranging from startups to fortune 100 organizations. I transitioned into data science from a business and consulting background.
He is also an instructor here if you like his style of teaching.
He described his self in his Youtube Channel about section as follows;
My name is Alex Freberg and on this channel I will be going over everything you need to know to become a Data Analyst. If you are wanting to make a career change or learn the skills needed to become a Data Analyst, be sure to subscribe to stay up to date on all my latest content.
He described himself in his Youtube Channel about section as follows;
Hi, I‚Äôm Giles McMullen-Klein. My degree was in physics. I‚Äôve been using python as a scientist for years. I learnt to code whilst studying physics and now I spend most of my time using python to analyse data. Python is an excellent language for data science and on this channel I teach the basics of python and how to use it to work with data.
If you like his style, here are more of his courses.
He described himself in his Youtube Channel about section as follows;
I‚Äôm a machine learning engineer who plays at the intersection of technology and health. My videos will help you learn better and live healthier. Feel free to introduce yourself, I‚Äôd love to hear from you.
If you like his style, here are other courses of him
He described himself in his Youtube Channel about section as follows;
I am Dhaval Patel, Founder of ed-tech platform codebasics.io and a co-founder of a software & data company called AtliQ Technologies (Ex. Bloomberg, NVIDIA). I‚Äôve 17+ years of industry experience as a software and data engineer and an entrepreneur. Teaching is my passion. I strongly believe ‚ÄúAnyone Can Code‚Äù! No matter what your background and past skill set, you can learn programming and technology if it is taught in a simplistic and intuitive way. The goal of this channel is to fulfill this vision by teaching technology (especially coding and data science) in the simplest and most intuitive manner.
Thanks for reading my article.
If you want to learn by reading, you can subscribe me here or here.
And the one, will send claps, try sending 50 :)
Here is a free Numpy Cheat Sheet for you. Subscribe to my newsletter here, and I will send that cheat sheet to you.
‚ÄúMachine learning is the last invention that humanity will ever need to make.‚Äù Nick Bostrom
If you still are not a member of Medium and are eager to learn by reading, here is my referral link.
Subscribe to DDIntel Here.
Visit our website here: https://www.datadriveninvestor.com
Join our network here: https://datadriveninvestor.com/collaborate
Kore
in
HackerNoon.com
Aditya Agarwal
Bloomberg Cities
Utpal Kumar
in
Towards Data Science
Brijj
Reia Natu
in
Analytics Vidhya
Alex Simkiv
in
Towards Data Science
Ë°åÈä∑Ë≥áÊñôÁßëÂ≠∏
in
Marketingdatascience
"
268,https://medium.com/towards-data-science/what-is-a-computer-kernel-2320b9330eef,What is a Computer Kernel?,"The kernel, or system core, ensures the smooth running of the operating system within a computer and is the interface between the software and the hardware. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.
The kernel is mainly the interface between the operating system, i.e. the software, and the hardware components in a device. This results in various tasks that have to be accomplished. For the end user, however, this work is not visible and is only expressed by whether all programs run correctly or not.
When we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.
If several programs are running at the same time, the system core also ensures that the programs‚Äô requests are processed one after the other and the user is given the feeling that the processes are running simultaneously. So, in our example, if a browser window is also open at the same time as Excel, the system kernel ensures that the programs‚Äô demands on the CPU and other hardware are processed in a coordinated manner.
In addition, there are other more detailed tasks, such as:
The kernel works in several layers that build on each other. These include:
These layers operate in such a way that each higher layer relies on and builds upon the layers below it. For example, the process management layer also accesses processes in the hardware layer. However, the opposite is not true.
As a rule, three different kernel types are distinguished depending on how many processes and users can be managed simultaneously.
The micro-kernel only performs minimal tasks, such as memory management and CPU process management. Other functionalities, such as the file system or device management, are outsourced to the operating system itself. The advantage is that it cannot lead to total system failure, as is the case with Windows, for example.
The operating system of Apple computers, macOS, for example, is based on a micro-core architecture.
The monolithic kernel centrally handles all the tasks already described, is responsible for all memory and process management, and also handles the hardware management of externally connected devices. The Windows operating system is based on a monolithic kernel.
Due to the central design, monolithic cores are usually significantly faster than micro cores, but this can also lead to system crashes if a single process does not function properly.
As the name already suggests, the hybrid kernel is a mixture of these two concepts. It is basically also a large kernel that takes over all tasks, but is much more compact and can be split into different modules.
When working with data and machine learning, people often turn to Jupyter Notebook. It is a web-based platform for creating and sharing programming code. It is so often used for data science applications because individual blocks of code can be executed and their results, for example, graphs, are directly visible. This is particularly advantageous for model creation or data set analysis when the next programming steps depend on the previous results.
When using Jupyter Notebook, a kernel is also started, which can sometimes lead to problems, such as in the following example when establishing a connection. However, this is completely different from that of an operating system as described so far in this article.
The Jupyter Notebook Kernel is an engine that executes notebook code and is specific to a particular programming language, such as Python. However, it does not perform the comprehensive interface functions described so far.
The following commands are particularly useful when dealing with the Jupyter Notebook kernel:
If you like my work, please subscribe here or check out my website Data Basecamp! Also, medium permits you to read 3 articles per month for free. If you wish to have unlimited access to my articles and thousands of great articles, don‚Äôt hesitate to get a membership for $5 per month by clicking my referral link: https://medium.com/@niklas_lang/membership
towardsdatascience.com
medium.com
towardsdatascience.com
Aryaman Kukal
Sriram Kumar
in
Towards Data Science
Deborah MT
in
DataDrivenInvestor
MILAD SHIRANI
Niklas Lang
in
Towards Data Science
Ayushi Mishra
Eduardo Alvarez
in
Intel Analytics Software
Andrea Amparore
in
Towards Data Science
"
269,https://medium.com/towards-data-science/statistics-bootcamp-5-what-is-normal-dd7ca037d37d,Statistics Bootcamp 5: What is Normal?,"To more formally address the need for a statistics lecture series on Medium, I have started to create a series of statistics boot camps, as seen in the title above. These will build on one another and as such will be numbered accordingly. The motivation for doing so is to democratize the knowledge of statistics in a ground up fashion to address the need for more formal statistics training in the data science community. These will begin simple and expand upwards and outwards, with exercises and worked examples along the way. My personal philosophy when it comes to engineering, coding, and statistics is that if you understand the math and the methods, the abstraction now seen using a multitude of libraries falls away and allows you to be a producer, not only a consumer of information. Many facets of these will be a review for some learners/readers, however having a comprehensive understanding and a resource to refer to is important. Happy reading/learning!
This article is dedicated to introducing the normal distribution and properties of it.
Medical researchers have determined so-called normal intervals for a person‚Äôs blood pressure, cholesterol, and triglycerides.
Ex. systolic blood pressure: 110‚Äì140 (these metrics differ for in office versus home blood pressure measurements)
But our question remains, how does one determine the so-called normal intervals?
The normal distribution (Gaussian) is a continuous, symmetric, bell-shaped distribution of a variable, denoted by N(Œº,œÉ).
The mathematical equation to represent it is denoted by:
and represents the probability density function or p.d.f. (for short). Mean is denoted by Œº and standard deviation by œÉ.
Properties:
Example. Last year high schools in Chicago had, 3264 female 12th grade students enrolled. The mean height of these students was 64.4 inches and that the standard deviation was 2.4 inches. Here the variable is height and the population consists of 3264 female students attending 12th grade. The normal curve with the same mean and standard deviation: Œº = 64.4 and œÉ = 2.4.
Thus we can approximate the percentage of students between 67 anf 68 inches tall by the area under the curve between 67 and 68, the area is 0.0735 and shown on the graph as occuring between 67 and 68 inches tall.
Although we covered this in a previous bootcamp, spaced repetition is the best way to ensure recall and retention! So here we have the empirical rule plotted for graphical emphasis.
Different normal distributions have different means and standard deviations. A standard normal distribution is a normal distribution with a mean of 0 and a standard deviation of 1. The notation of the p.d.f. for the standard normal distribution is:
We can see, looking at the equation above, it is the simplification of that provided by the p.d.f. of the normal distribution (above). Standardization is the process by which we convert these distributions into the standard normal distribution (0,1) for comparison. We can convert the values of any normally distributed variable using the equation below, which is called the z-score.
Compare this with the previously defined p.d.f. for the normal curve above.
After converting your normal distribution into a standard normal distribution, look up z-scores in the Standard Normal Distribution table (z-table) to determine the area under the curve. We use this table to find the area under the curve (highlighted in the figure below) that lies (a) to the left of a specified z-score b) to the right of a specified z-score, and c) between two specified z-scores.
Examples.
a) Find the area to the left of z=2.06. P(Z < 2.06) = 98.03%b) Find the area to the right of z = -1.19. P(Z>-1.19) = 88.3%c) Find the area between z = 1.68 and z = -1.37.P(-1.37 < Z < 1.68) = P(Z<1.68) ‚Äî P(Z < -1.37) = 0.9535‚Äì0.0853= 0.8682 = 86.82%
Remember that a t-table defaults to the LEFT‚Ä¶.
Example. During the 2008 baseball season, Mark recorded his distances (in meter) for each home run and found they are normally distributed with a mean of 100 and a standard deviation of 16. Determine the probability of his next home run falling between 115 and 140 meters.P(115<X<140)=? where X = distances for each home run.
Sketch the normal curve: X ~ N(100,16), with Œº = 100 and œÉ = 16.
Compute the z-scores:z1 = (115‚Äì100)/16 = 0.94 z2 = (140‚Äì100)/16 = 2.50
P(z1 < Z < z2) = ? Where Z~ N(0,1).
The area under the standard normal curve that lies between 0.94 and 2.50 is the same as the area area between 115 and 140 under the normal curve with mean 100 and standard deviation 16, i.e. P(115<X<140) = P(0.94<Z<2.50) = P(Z<2.50) ‚Äî P(Z<0.94).
The area to the left of 0.94 is 0.8264, and the area to the left of 2.50 is 0.9938. the required area, is therefore 0.9938‚Äì0.8264 = 0.1674. The chance that Mark‚Äôs next home run falls between 115 and 140 meters is 0.1674 = 16.74%.
Example. What if you were given an area under the standard normal distribution curve and asked to find the z-score?
Construct a normal probability plot, given a dataset:
Normal probability plot is the plot of observed data versus normal scores. If it is linear, the variable is normally distributed, and if it is not linear, the variable is not normally distributed. Plotting the data above, we get the plot:
A Q-Q plot (quantile-quantile plot) is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.
If the data are truly sampled from a Gaussian distribution, the Q-Q plot will be linear:
Code to generate the plot above:
The sample means from different samples represent a random variable and follow a distribution. x
A sampling distribution of sample means is a distribution using the means computed from all possible random samples of a specific size taken from a population. If the samples are randomly selected with replacement, the sample means, for the most part, will be somewhat different from the population mean Œº. These differences are caused by sampling error. Sampling error is the difference between the sample measure and the corresponding population measure, due to the fact that the sample is not a perfect representation of the population.
Example. Suppose a professor gave an 8 point quiz to a small class of four students. The results of the quiz were 2, 4, 6, and 8. For the sake of discussion, assume that the four students constitute the population. The mean of the population is:
The standard deviation of the population is:
Now, if all sample sizes of 2 are taken with replacement and the mean of each sample is found, the distribution is:
The mean of the sample means is:
The standard deviation of the sample means is:
2. The standard deviation of the sample means will be smaller than the standard deviation of the population, and it will be equal to the population deviation divided by the square root of the sample size. This means that there is less sampling error, and is associated with a larger sample size.
**The standard deviation of the sample means is called the standard error of the mean**
The larger the sample size:
Distribution of sample means for a large number of samples
If we repeat the above experiment several times and each time we construct intervals of length 2 standard errors to each side of the extimates of Xbar, then we can be confident (think empirical rule) that 95.4% of these intervals will cover the population parameter, Œº.
If we repeat the experiment several times and construct several confidence intervals, then 95.4% of these intervals include the population parameter, Œº.
The central limit theorem states that as the sample size n increases without limit, the shape of the distribution of the sample means taken with replacement from a population with mean, Œº and a standard deviation œÉ, will approach a normal distribution. This distribution will have a mean Œº and a standard deviation Œº/sqrt(n).
Why is the central limit theorem (CLT) so important?
If the sample size is sufficiently large, the CLT can be used to answer questions about sample means, regardless of what the distribution of the population is.
That is:
and converting to a z-score:
The first is used to gain information about an individual data point when the variable is normally distributed:
Note: This first equation for z DEFAULTS TO THE LEFT!
The second one is used when we want to gain information when applying the central limit theorem about a sample mean when the variable is normally distributed OR when the sample size ‚â• 30:
Example. The average number of pounds of meat that a person consumes per year is 218.4 pounds. Assume that the standard deviation is 25 pounds and the distribution is approximately normal. a) Find the probability that a person selected at random consumes less than 224 pounds per year. b) If a sample of 40 individuals is selected, find the probability that the mean of the sample will be less than 224 pounds per year. Solution:a) The question asks about an individual person.
b)The question concerns the mean of the sample with a size 40.
The large difference between these two probabilities is due to the fact that the distribution of sample means is much less variable than the distribution of individual data values. (Note: An individual person is the equivalent of saying n=1).
Recall that the binomial distribution is determined by n (the number of trials) and p (the probability of success). When p is approximately 0.5, and as n increases, the shape of the binomial distribution becomes similar to that of a normal distribution.
When p is close to 0 or and n is relatively small, a normal approximation is inaccurate. As a rule of thumb, statisticians generally agree that a normal approximation should be used only when n*p and n*q are both greater than or equal to 5, i.e. np‚â•5 and nq‚â•5.
A correction for continuity is a correction employed when a continuous distribution is used to approximate a discrete distribution.
For all cases, Œº=n*p, œÉ=sqrt(n*p*q), n*p‚â•5, n*q‚â•5
Example. A magazine reported that 6% of Americans look at their phone while driving. If 300 drivers are selected at random, find the exact probability that 25 of them look at their phone while driving. Then use the normal approximation to find the approximate probability. p=0.06, q=0.94, n=300, X=25 The exact probability using the binomial approximation:
Normal approximation approach:
3. Write the problem in probability notation P(X=25).
4. Rewrite the problem by using the continuity correction factor. P(25‚Äì0.5 < X < 25 + 0.5) = P(24.5 < X < 25.5)
5. Find the corresponding z values:
6. Solve:
The probability that 25 people read the newspaper while driving is 2.27%.
In this bootcamp, we have continued in the vein of probability theory now including work to introduce Bayes theorem and how we can derive it using our previously learned rules of probability (multiplication theory). You have also learned how to think about probability distributions ‚Äî Poisson, Bernoulli, Multinomial and Hypergeometric. Look out for the next installment of this series, where we will continue to build our knowledge of stats!!
Previous boot camps in the series:
#1 Laying the Foundations#2 Center, Variation and Position#3 Probability‚Ä¶ Probability#4 Bayes, Fish, Goats and Cars
All images unless otherwise stated are created by the author.
Additionally, if you like seeing articles like this and want unlimited access to my articles and all those supplied by Medium, consider signing up using my referral link below. Membership is $5(USD)/month; I make a small commission that in turn helps to fuel more content and articles!
medium.com
Mamady Nab√©
Jenna Eagleson
in
Towards Data Science
Neha Dhakne
Chloe Zeller
in
Fixing Sexist AI
Nikhil Purao
Monika Bagyal
in
Towards Data Science
Brian Mackey
Aulia Muthia
"
270,https://medium.com/disruptive-innovation-journal/data-science-trends-to-look-forward-in-2023-d0bcdec57142,Data Science Trends to Look Forward in 2023,"Data Science is still one of the most promising technical disciplines out there. Some people compare data science‚Äôs ongoing development of AI to the discovery of fire and electricity.
But which most promising trends will dominate the industry in the coming year?
Ed
Anmol Pant
in
CodeChef-VIT
Gowtham S R
David
JIN
in
Dev Genius
data crunch
Sidd Chauhan
in
The Startup
Max Perkins
"
271,https://medium.com/technology-hits/obsidian-plugin-review-meta-bind-plugin-a71bb6818936,Obsidian Plugin Review ‚Äî Meta Bind Plugin,"This is another very useful plugin. Meta Bind allows you to create input fields like text, select, slider, or toggle, inside your notes and bind the value to metadata fields.
You bind an input field to a metadata field by adding the binding target after the input type, separated by a colon.
Emmanuel Onwuegbusi
in
Analytics Vidhya
El Akioui Zouhaire
in
Dev Genius
James Ivings
in
ReleasePage
Devlin Trace
Jason Darwin
L. Dwayne Sudduth
Nanduni Indeewaree Nimalsiri
Fireche
in
NekoHit
"
272,https://medium.com/@gajendra.k.s/central-limit-theorem-6029339e2b0,Central Limit Theorem,"According to the Central Limit Theorem, the sampling distribution of the sample means approaches a normal distribution as the sample size gets larger. This is specifically true for the sample sizes over 30.
The mean of the sample means is closer or equal to the mean of the population.
The larger the sample, the closer the mean, the smaller the standard deviation.
Given a set of data irrespective of its distribution, Normal, Binomial or Random. If we take subset of this data of size n, sample, and calculate the mean and keep sampling it few times. Once we have taken a sufficient samples and have calculated sample means. The means of these dataset will approximate to Normal Distribution.
The mean of the sampling distribution will be equal to the mean of the population distribution.
The variance of the sampling distribution will be equal to the variance of the population distribution divided by the sample size.
The sampling distribution of sample means has a variance equal to.
The sampling distribution of sample means has a standard deviation equal to.
The Standard Error of the mean, or simply standard error, indicates how different the population mean is likely to be from a sample mean. It tells you how much the sample mean would vary if you were to repeat a study using new samples from within a single population.
A Z Score describes the position of a raw score in terms of its distance from the mean, when measured in standard deviation units.
The Z Score is positive if the value lies above the mean, and negative if it lies below the mean.
I hope this article provides you with a basic understanding of Central Limit Theorem.
If you have any questions or if you find anything misrepresented please let me know.
Thanks!
Tyler Elliot Bettilyon
in
Teb‚Äôs Lab
Kaan Karahan
Kasper M√ºller
in
Cantor‚Äôs Paradise
Firoz Khan
Humanicus
infinifish Education
Eliran Turgeman
in
Math Simplified
Jordan Dworkin
in
We‚Äôve moved to freeCodeCamp.org/news
"
273,https://medium.com/towards-data-science/deep-learning-visually-explained-a9fff874d280,Deep Learning Visually Explained,"Deep learning is one of the most powerful AI techniques, however, it can be difficult to understand. In this blog, I will attempt to explain deep learning using visuals and examples.
Deep learning architecture is inspired by how our brain works. It is a connection of neurons. Deep learning models can have many parameters. The number of parameters is based on the number of layers and neurons, which can exponentially grow for sophisticated architecture.
In this blog, I will take a business use case of financial fraud detection. One of the biggest challenges in fraud detection is the problem of class imbalance, which means that the data used to train machine learning models has very few cases of fraud.
It is like training a machine learning model to find a needle in a haystack. Fraud detection is a special problem that justifies having a sophisticated approach such as deep learning architecture.
In the example, I will take data from the banking transaction system. The data looks as shown here. The data has the type of financial transaction, amount, as well as origin and destination old balance, and new balance. There is also a flag that indicates if the transaction was fraudulent or not.
The citation for the dataset is available at end of the blog.
The data is split into training and test data. The deep learning model is developed on the training set and then it is validated on the test data. Then this model can be used to predict fraud on unseen data.
The deep learning model for fraud prediction is shown here. The input neurons correspond to the transaction data. Each neuron corresponds to a column in the input data such as the type of transaction, amount, and balance information at the origin and destination.
There is one intermediate layer and then the final layer which has two neurons, one which predicts non-fraud and the other which predicts no-fraud.
The lines are signals passed between the different layers. A green line indicates a positive signal and a red line indicate a negative signal
We see that neuron 1_0 is passing a positive signal to neuron Fraud.
This means that it has deep-learned what a fraudulent transaction looks like! This is exciting !
Let us peek inside neuron 1_0!
The radar chart is a representation of what the neuron has learned about the data. A blue line indicates a high value, and a red line indicates a low value. The radar chart indicates a high, but almost similar old and new balance at the origin. However, there is a very big difference between the old and new balance at the destination.
Such a situation is an indication of fraud. This situation can be visually shown below.
Shown here is the accuracy of the deep learning model using a confusion matrix.
In total, there are about 95000 transactions, out of which there are 62 fraudulent transactions, which is extremely less than the total transaction. However, the deep-learning model is doing good as it is able to identify 52 correctly as fraud, which is also called true positive (tp)
There is 1 false positive (fp), meaning, it is not a fraud, but the model incorrectly flagged it as fraud. So the precision, which is tp / (tp +fp), equals 98%.
Also, there are 10 false negatives (fn), which means that they are fraudulent transactions, but our model is not able to predict them. So the measure recall which is tp / (tp +fn) which is 83%
Deep learning architecture is very powerful as it helps solve complex problems such as fraud detection. A visual way to analyze deep learning architecture is useful in understanding the architecture as well as how it solves the problem
The synthetic financial datasets for fraud detection is data are available here: https://www.kaggle.com/competitions/spaceship-titanic/overview
As specified in the License section, it has a license CC BY-SA 4.0.
Please join Medium with my referral link.
pranay-dave9.medium.com
Please subscribe to stay informed whenever I release a new story.
pranay-dave9.medium.com
You can visit my website to make analytics with zero coding. https://experiencedatascience.com
On the website, you can also participate in upcoming AI workshops for an interesting and innovative data science and AI experience.
Here is a link to my YouTube channelhttps://www.youtube.com/c/DataScienceDemonstrated
Adithjrajeev
Richard Trapnell
Pallav Walia
Vanessa Ongodi Onyema
in
MLearning.ai
Jose Juan Martinez
in
spark-nlp
Kerry Benjamin
in
Towards Data Science
Nicolle Garber
Krishnaraj Rajagopal
"
274,https://medium.com/the-data-journal/the-proper-way-to-display-regression-outputs-in-r-4abab45e33d5,The proper way to display regression outputs in R.,"Those who use R for regression analysis often don‚Äôt consider presenting the results in the best possible format. The stargazer package in R is a great tool for producing ‚Äúindustry standard‚Äù regression reports.
Compared to available alternatives, stargazer excels in at least three respects: its ease of use, a large number of models it supports, and its beautiful aesthetics. These advantages have made it the R-to-LATEX package of choice for many satisfied users at research and teaching institutions around the world.
I will use the air quality data to create three separate regressions and compare all results in one table.
I will not focus on what a regression analysis is and what my hypothesis is. There are many resources that can help. My favorite refresher on regressions is by Statology.
The output above uses the typical summary(Lm_1) command to get the result. Let's use stargazer to make it cleaner.
Now let‚Äôs use stargazer library to make the above output cleaner. Additionally, we will use the format used by The Quarterly Journal of Economics using the command style = 'qje‚Äô.
Additionally, we can also export the outputs in our desired format using the command out = 'filename.doc/html/latex ‚Äî Below is an example of an HTML export file for the same model.
Perhaps the most helpful feature of the stargazer package is comparing models. We will make two more sample models and display all three models in one table.
Looks even better in HTML‚Ä¶
Additional resources for Stargazer:
Connect with me on LinkedIn.
Inferencecircle
Arun Jagota
in
Towards Data Science
Regina Ryan
Huey Fern Tay
in
codinginr
Erma T Lowery
Vincent Tabora
in
0xCODE
denodo
Akhil Dhingra
"
275,https://medium.com/@abdulahi.falade41/how-i-become-a-earning-data-analyst-in-10-weeks-c08da4b85961,How I become a earning data analyst in 10 weeks.,"Transitioning into a new career could be demanding and overwhelming.
I would like to share my story on I become a earning data analyst in 10 weeks.
I am Abdulahi Opeyemi FALADE, a graduate of Mathematical Sciences with more than 8 years of work experience in Data related roles, both in academics and in Industries.
Through training and mentorship, I have helped more than 100 data enthusiasts become earning data analysts.
I studied pure mathematics with little or no room for calculations at undergraduate and postgraduate levels.
They showed us proofs of mathematical theorems, corollaries, Propositions, and the likes which are mathematical logic inclined-full of abstraction and rote learning.
I used to think data analytics was meant for average students or lazy individuals until I delved into it.
Meanwhile, I graduated in 2012 from the Department of Mathematics, the University of Ibadan, Ibadan, Nigeria, where you have no business with statistics as a direct entry mathematics student.
But I took it upon myself to keep teaching and learning statistics. Thus, I did my undergraduate internship at the National Bureau of Statistics, Ikolaba, Ibadan, Nigeria, where we deal with data collection of General Household Survey(GHS), Establishment Survey, Farm survey, Retail prices of commodities survey among others.
We did nothing like data analysis, just data collection ONLY.
After my undergraduate study, I got a fully funded scholarship in 2016 for a master‚Äôs degree at African Institute for Mathematical Sciences, Mbour, Senegal, where I offered statistics and statistical data analysis courses with Python and R.
Then, I couldn‚Äôt understand and appreciate the impact data analytics could make in shaping lives.
In 2017 when I returned to Nigeria from Senegal, I searched for a white-collar job that had a remuneration package that would commensurate with my qualifications, skills, and experience. I tried but couldn‚Äôt get one.
So, to cater to the means of livelihood, I started working as a customer service executive in a startup company, where I attended to customers- speaking Hausa, English, and Yoruba. Can you imagine?
At one point, I wasn‚Äôt feeling comfortable working as a customer service executive again; I wanted a paradigm shift to another professional career path.
Thus, I surfed the Internet to do a personal SWOT analysis, and I realized my greatest strength lies in helping people solve problems using facts from reliable figures (Analytics) and Communication.
I love communication and analyzing situations for possible solutions. If you have similar traits or are passionate about making an impact, becoming a data analyst could be considered.
So, I did thorough research and consultation through reading and following successful data analysts/scientists online to establish what I need to become a data analyst in the shortest time and possibly start earning.
Finally, I realized starting a data analytics journey with Microsoft Excel was the best bet. I started with Microsoft Excel. However, it wasn‚Äôt easy at first to develop a seven-week intensive learning schedule because that was my target.
After the 7th week, I started working on sample data online, downloading sample files through links in YouTube videos, helping people to do petty things related to Data, and volunteering/Internships.
Three weeks after completing the seven-week learning schedule, I got my first data analyst job as a Sales data analyst in a Fast Moving Consumer Goods Company (FMCG) in Lagos, Nigeria.
Then, I spent more time on data cleaning with Power Query, Data analysis with Advanced Pivot table, Power Pivot, DAX, and visualization.
Finally, I realized that I needed SQL for efficiency, which I learned for Extraction, Loading, Transformation (ETL), and Analysis.
Consequently, I revised and learned more about Python for basic programming, worked on Data wrangling in Python, visualization, Data Science, Regression (linear, logistics), Decision tree, Random forest, Neutral network, Tensor flow, Deep learning ) and Data storytelling.
In addition, I was able to explore more data analysis tools, including SPSS, E-view, GRETL, Epi-Info, STATA, SAGE, and SAS, and data visualization tools such as Power BI, Tableau, and Looker.
Do you know what really helped me to explore up to that extent?
You said, my Math background?
No!
Don‚Äôt forget that I did little or no calculations in undergraduate days, let alone data analysis or statistics, you can ask anybody that attended Math department at the University of √åb√†d√†n, √åb√†d√†n, Nigeria.
We only collected and scrutinized data in the National Bureau of Statistics then.
Experience in practice and a better understanding of Data Analytics with Microsoft Excel helped me immensely.
I am confident that with good knowledge and understanding of Analytics with Microsoft Excel, you would be able to explore and learn ANY; I repeat, ANY data analysis tool with ease.
Can you connect the dots that I transitioned from a Customer Service executive to becoming a data analyst in 7 weeks and started earning 3 weeks after completion of a learning schedule?
I hope you find the story interesting and helpful.
All the best in your data analytics journey.
Kanishk Mair
in
Towards Data Science
Abhilasha Munjal
Nguyen Xuan Minh Khoi
Griffin Winters
Quoc Tien Au
in
Towards Data Science
TERCium
Sarkis Kassounian
Kashaf Naz
"
276,https://medium.com/dssimplified/the-best-data-science-tool-to-start-your-journey-with-in-2022-41bdbcd61bfb,"The best Data Science tool to start your journey with, in 2022?","Data Science related jobs are one of the hottest jobs to have in today‚Äôs generation. Data Science has been around for less than a decade but the size of data people use in the world today is almost immeasurable. Big Data and other amazing developments such as automated machine learning, machine learning ops or MLOps, etc., have made Data Science an ever-evolving field with lots to offer.
Learners today have a major question to answer in their big to become Data Scientists. The Data Science world has been growing rapidly, owing to the different tools and resources that anyone can use to make sure that Data is leveraged as the most important resource in their company.
Check out this article, where I discuss the best way to upskill in Data Science using the Open Source Master‚Äôs in Data Science as a guided path which is devised by Clare Corthell, one of the world‚Äôs most renowned Data Scientists today.
medium.com
Now, coming to the question at hand. With so much to offer in a vast field like data science that branches out into so many different fields to get a specialization in and help the world get more out of their data, where should you start your journey, and with which tool?
The most important tools in Data Science (according to their popularity and my personal opinion) are;
Ideally, a data scientist has to be someone proficient in Data Warehousing and Database management, Data Visualization, Data Manipulation, ETL (Extract, Transform, Load) processes, statistics, Machine Learning, etc.
That sure is a lot of things to be good at for one person.
However difficult it may seem, they are all more or less equally important in everything that Data Scientists do in their work and projects.
The answer to the question is fairly simple if you know where you stand in your journey in Data Science. If you are someone who has never used Computers and would still want to start your journey, that‚Äôs alright too. We‚Äôll get to when you should start with each of these soon.
Working with Excel is fairly simple and to the point, if you wish to work with data that‚Äôs mostly comprised of rows and columns. The data can be cleaned, filtered, manipulated, and changed to each element, using excel. The software is one of the first breakthroughs in working with data in the 21st century.
If you are someone who is new to working with data or if you have just started using data in your projects or your workplace, then Excel is where you start.
Find out more about it here: What is Excel?
Python and R are two of the most used programming languages in Data Science. The Data Science world today cannot function without Python or R. The amount of work you can do with Python and R is unparalleled. Programming languages are inherently very flexible and can help people with ETL work, machine learning, process automation, and also data visualization.
If you are someone who has used Excel, Data viz tools, etc., and want to get to the next level, then Python or R is the way to go.
Find out more about it here: What is Python?
SQL is unarguably the most used database management language today and cannot is one of the most used tools in companies that work with multiple departments and store a lot of data. Even big data dependencies today like Hadoop and Spark use SQL to extract and work with data.
If you are someone who has been working with excel and understand how to work with data in the tabular form, SQL should be your choice to understand how to work with more data than Excel can store.
Almost all companies and databases in the world leverage DBMS using SQL making it a must-learn for anyone.
Find out more about it here: What is SQL?
Tableau has been around for a while and is a tough competitor to PowerBI, Tableau can be used to do a lot of work using Data Visuals and tell data stories. This comes as a savior when you know that most of your stakeholders don‚Äôt understand the back-end work on the data and the formulas and how they work. If your stakeholders are only interested to understand what the data is telling them, then Tableau is an amazing way to do just that.
Tableau takes a dataset in a file like Excel, SQL database, JSON, etc., and using a simple drag and drop system, it delivers high-quality and accurate visualizations that can be used to show your stakeholders all that they need to know from a given dataset.
You can use Tableau if you are good with working with Excel and understand how to make data visuals and don‚Äôt wish to proceed with something as vast as a programming language.
Find out more about it here: What is Tableau?
Scikit Learn is one of the most amazing Machine Learning libraries offered by Python. Python is one of the most used machines learning programming languages and even in competitive coding people use Python extensively. The application of Machine Learning can be done to any problem statement that needs some predictive analytics.
Making a machine learning algorithm with merely 5 lines of code or beyond with highly specific settings is also possible by using Python‚Äôs Scikit learn library.
If you are someone who has been into Computer Science for a while and understands how coding works and how to work your way around the basics of python, start with Scikit learn! It's simple and can be used to make predictive models without a lot of hassle.
Find out more about it here: What is Scikit Learn?
While these are important, do check out these free tutorials that can help you get started in the best way possible on your journey to becoming a Data Scientist:
For more such articles, stay tuned with us as we chart out paths on understanding data and coding and demystify other concepts related to Data Science. Please leave a review down in the comments.
Check out my other articles at:
medium.com
Do connect with me on LinkedIn if you want to discuss it further!
ODSC - Open Data Science
in
ODSCJournal
Dr Maria Cristina Caldarola
Skyler Dale
Justin
Ehsan Yousefzadeh-Asl-Miandoab
in
MLearning.ai
Maxim Lamare
in
Sentinel Hub Blog
Madison Hunter
in
Towards Data Science
Sciforce
in
Sciforce
"
277,https://medium.com/towards-data-science/pandas-multi-index-explained-6f11d880c0e5,Pandas Multi-Index Explained,"In the previous chapters we saw how to use indexes. We either used the default index provided in the DataFrame or we converted a column into index.
battox.medium.com
Now we will see more possibilities that Pandas offers to explore a DataFrame through the index and how we can have more than one index.
For the following examples we will use a dataset with information about human languages of the world in a CSV file based on the World Atlas of Language Structures of the Max Planck Institute for Evolutionary Anthropology.
The inplace argument
Many of the methods that we can use to modify a DataFrame will yield a new DataFrame and will keep the original one intact. With the argument inplace=True we modify the original DataFrame.
Now the DataFrame has a new index. We could have done more or less the same without using inplace=True:
Which is what we did in the previous chapter to convert a column into index.
We can do a quick exploration of the DataFrame with ‚Äúgenus‚Äù as an index and we will notice that setting an index preserves the original order of the rows (alphabetical order of the column ‚ÄúName‚Äù in this case). Bear in mind that when we convert a column into index, even though it looks like a column, it does not behave as such.
We can, for example, select one category of the index by using loc. If the label that we choose appears more than once, we will see several rows.
Or several categories.
In the beginning of the table there will be Aymaran languages and by the end, Germanic ones.
And whenever we need to turn the index back into column we will use DataFrame.reset_index(). An index is present at all times: when it is not a column of our dataset or another index that we defined, an index by default (with labels that match the positions) will be generated.
Note that ‚Äúgenus‚Äù is again a column (in the first position).If we had not been interested in keeping the index (genus) as a column, we could have added the drop=True argument to reset_index() and it would have been discarded.
Something great about Pandas is that it is capable of converting more than one column ‚Äîor more than one row‚Äî into index. That is called multi-index. A multi-index will hold many levels of indexing, thus, a hierarchy of index levels will be established.
It may be important to address that despite being able to convert the contents of more than one column into index, we cannot consider that now one row has several indexes. The multi-index is still one. With levels.
In this case ‚Äúfamily‚Äù is level 0 of indexation and has precedence over ‚ÄúVowel Inventories‚Äù, which is level 1 of indexation. We can now reference levels either by name or by level number.
FrozenList is something like an ordinary list, do not worry about it now.
An index may or may not have a name. And most of the times, it does not: e.g. the auto-generated index with integers that we have been using so far is created without a name. Columns promoted to index turn the name of the column into the name of the index and thus the index has a name, which can be modified or removed.
Before getting into multi-index fully, we need to introduce another native object of Python, the tuple.
What differences a tuple from a list is that lists are mutable and tuples are not; in other words, we can add or remove elements to a list after we create it but we cannot do such thing to a tuple. Once created, if we want something different, we need to create another tuple.We create them similarly to how we create a list but we will use parentheses (round brackets) instead of square brackets.
Each row of the DataFrame is indexed with a tuple now. In previous examples when we used ‚Äúsingle‚Äù index, the rows were indexed by string labels. Think of the tuple as the label of a multi-index row or column.In the multi-index, tuples have as many elements as index levels. For example, if the multi-index had three levels, such tuples would be shaped as (level_0, level_1, level_2).
If you ever need to retrieve the values of just one level, Pandas got you covered with MultiIndex.get_level_values().
Let's try it out. We select Indo-European languages from level 0, ‚Äúfamily‚Äù with a vowel inventory of 5 or 6 from level 1, ‚ÄúVowel Inventories‚Äù.
Let's suppose that this time we do not want languages of the Indo-European family but languages from Arawakan and Uralic families. Instead of using a single label (interpret it as a single tuple), we pass a list of labels (a list of tuples in fact).
This is an alternative way of doing what we just did.
Now a little challenge: Let us suppose that we want to find results from level 1 (for example, small vowel inventory) among all the rows of level 0 (we are neglecting level 0 in fact). Sure, we could turn level 0 back into column, but no, we don't want to modify the multi-index. So, maybe, we will try to retrieve all the labels of level 0 and one label of level 1. And we may try to do that by using the full range slice (first_row:last_row) of the dataset or just the colon :.
No. This syntax is not correct. slice only works within an indexer [] (square brackets) and not with any other object (a tuple in this case).For this purpose, Pandas provides a special type called IndexSlice to replace tuples.
Some people like to assign IndexSlice to a shorter identifier when they know they will use it many times.
A convenient way of by-passing the hierarchy of levels is using the DataFrame.xs() method. It has the downside of working only for selection and not for assignment. When we need to change frame values we must use loc/iloc as usual.
One of the important features of hierarchical indexing is that you can select data by a partial label, identifying a subgroup in the data.If we are only interested in higher levels of the multi-index instead of selecting rows by let's say pd.IndexSlice['Indo-European', :] where we target one label of level 0 and all the labels of level 1, we could simply use 'Indo-European'.VERY IMPORTANT: Partial selection drops levels of the hierarchical index in the result.
This DataFrame is still organised by alphabetical order of the column ‚ÄúName‚Äù. In order to sort the values differently we use DataFrame.sort_values(), which is able to sort rows using the content of one or more columns and/or index.We are going to sort the DataFrame by ‚Äúfamily‚Äù (index level 0), in ascending alphabetical order. The method would be the same if ‚Äúfamily‚Äù were a simple column and not an index level.
Now the DataFrame starts with languages of the Afro-Asiatic family and the very first language displayed is Kullo üòÉ.As already mentioned, we could also sort the DataFrame by columns that are not index, for example, by amount of grammatical cases. Additionally, we will do it in descending order by adding ascending=False.
Note that in order to sort values by index we need to reference it by its name and as we said before, sometimes the index does not have a name. We might also want to use position/level of the index instead of the name. Then is when the method DataFrame.sort_index() comes in handy (naturally, it won't work for columns).
Another interesting use case for sort_index() is when we would like to sort by all the levels (no need to specify names nor hierarchy).
It first sorted the rows using level 0 and then, level 1. There are repeated level 0 values, those rows are hard to sort. The second level disambiguates.
We can change the names of (row) index labels or column labels. For this we are going to use the DataFrame.rename() method.When we want to change the names of some columns, we pass a dict with the old-to-new names mapping. Since rename() works for both axes it is important to specify which one we are aiming at, and we do so with the index (for rows) and columns arguments (it is one or the other), like this
There is one more way to specify the axis which is not only used in rename() but also in many other methods. It acts in one direction or another and is worth knowing: the axis argument. It uses the values 0 for rows (by default) and 1 for columns. In order to obtain the result of the previous example by using axis we should write this:
To change names within the (row) index we use rename(index={...}). The old-to-new mapping is applied to all levels, no matter how many or which. And if it were necessary to restrict the change to certain levels (in the case that the label that we want to change appears in several levels) we can use the argument levels.
Extra bonus
We are informed that some speakers of Kullo are not very happy with such name, and they prefer to call their language Dawro. We want to rename Kullo occurrences in the dataset but the method rename() will not do since ‚ÄúLanguage‚Äù is not an index but a column.No matter what we want to do, Pandas always has a method for it: DataFrame.replace() acts over the frame values.
When using multi-index, each row and/or column of the DataFrame is indexed with a tuple, representing index levels, instead of with a string as in simple indexes.
And when we need to use slice to represent all the level values (or a range of them) we use IndexSlice instead.
Short list of methods seen in this tutorial.
And some handy unseen methods.
Elizabeth M. Renieris
in
Berkman Klein Center Collection
Monalisha Kumari
Saba Yashfeen
Jia Zhang
Rakesh4real
in
Fnplus Club
Sahabjam
Jc Guo
Timothy Osirike
"
278,https://medium.com/@nickmartin812/how-to-r-visualizing-distributions-49ea4141fb32,How To R: Visualizing Distributions,"Distributions are a very important thing to understand during any EDA process. In this article we will look into multiple different ways to visualize distributions and how to create those visuals in R.
Visualizing Distributions
For this article I will be using the FIFA OFFICIAL DATASET from Kaggle and the R package ggplot2 to create the visualizations.
I will be going over the following with code to show how each was created:
Basic Histogram
With the histogram you are able to easily see which overall‚Äôs are attributed to the most players. The distribution of this data seems to be close to a normal distribution.
Code used for the chart above.
Things to note:
Histogram by Group
Here I split the histogram into the player nationalities for the 4 nations with the highest count of players. Here we can see how each one of these nations are distributed.
Code used for chart above.
Things to note:
With this layout we can see the distribution of many more nationalities than we did with the histogram, now showing the top 10 nationalities by player count. I also sorted the nations seen here by their mean Overall.
Code used for the graph above.
Things to note:
Basic Density Chart
The density chart is similar to the histogram, however it shows the distribution without the needs of bins or binwidths and instead by the density seen at each point.
Code used in graph above.
Things to note:
Overlapping Density Chart
In this variation, we have the top 4 nationalities by player count shown. Their density plots are overlapping each other and as we do not have too many nationalities this chart works for us to compare them. We can see that England has the lowest spike for overall at around 55, then Spain has the largest spike and also the tallest one, at around 65.
Code used in the graph above.
Things to note:
Density Chart by Group
This visual is similar to how we looked at the histogram in the first section. This would be useful instead of the overlapping density chart in case there were many different groupings and it was too difficult to compare each one.
Code used in the graph above.
The ridgeline plot is an easy way to show the density distribution for each group at once. I sorted this set by mean overall again.
Code for graph above.
Things to note:
Code used for graph above.
Things to note:
This chart shows each point for every overall grouped to the nearest 5, comparing the overall to the potential of that player. The jitter spaces them just a bit out on the x-axis so that it is easier to see how many points are on top of each other.
Code for graph above.
Things to note:
If you enjoyed learning about how to visualize distributions, you may also enjoy learning about what alternatives there are to a bar chart in 5 Alternatives to the Bar Chart.
Magnus PS
Felipe Cezar
in
Analytics Vidhya
Dan Segal
in
Towards Data Science
Matteo Barbieri
in
Towards Data Science
Adetola Adeya
in
The Startup
Amanda Gaeta
in
The Startup
Bert Tong
Eric Kleppen
"
279,https://medium.com/@cannon_circuit/6-best-backend-frameworks-for-web-app-development-in-2022-47ab4d27c7b4,The 6 Backend Frameworks For Web App Development To Watch In 2023,"Web frameworks are crucial for developing robust and interactive web applications. As there are many choices in backend frameworks, finding one that ensures maximum scalability and performance is difficult. To assist you, we‚Äôve compiled a list of popular frameworks for web application development.
Simon Leen
Aman Mittal
Respresso
Ayushmilan
Daniel Kirwan
javinpaul
in
Javarevisited
Yesid Gonzalez
Aidan Massie
"
280,https://medium.com/javascript-in-plain-english/easy-and-custom-angular-rxjs-state-management-under-50-lines-of-code-a2e7188da926,Easy and Custom Angular-RxJS State Management Under 50 Lines of Code,"State management in frontend applications is always a scary subject. ‚ÄúFear of NGRX‚Äù is a real thing (üòÖ), at least for me. So I tried to stay away from it unless it was required at my workplace. But, at its core, state management is a powerful concept that can help you write robust, maintainable, readable, and quality code. Once I grasped the concept, I never looked back. Now all my personal projects use state management, although not with NGRX. I follow a very simple and lightweight implementation that is easy for my brain, and I hope for you too! So, let‚Äôs dive in.
You know that RxJS gives you three types of Subjects ‚Äî Subject, BehaviorSubject, and AsyncSubject. To develop my own state management solution, I needed a little more from the BehaviorSubject.
Enter StateSubject.
Basically, a StateSubject is an extended BehaviorSubject that holds an initial value, can be reset to the initial value, and the observable stream out of it is checked through deep equality operation.
The utility of the StateSubject is mainly to restrict unnecessary emit when the value has not essentially changed. For example, you may have a BehaviorSubject with a boolean value. Later you update it twice with the same value. The stream will emit twice.
On the other hand, if you use a StateSubject, it will emit once since the literal value has not changed the second time.
Okay, we spent around 15 lines for StateSubject class. Here comes the next ~35 with the Store class.
To initialize the Store class, we need a starting value and an unsubscriber Subject that will terminate the store. Other methods of the Store:
We are done with the required classes to manage the state for our application. In this example scenario, we are coding a movie list page with search and pagination capabilities. First, we need to figure out what values we need for the component‚Äôs state:
Secondly, we need to come up with a sane initial value for the whole state. In the beginning, there will be no movies, and the search bar will be empty. So we can declare our store like so:
Note (1), we need an unsubscriber Subject that will complete when the component is destroyed. This will ensure no memory leaks occur. We can easily make one by attaching it in the ngOnDestroy hook:
Now, we want a mechanism that will fetch the movies whenever the search, page number, page size, or year value changes:
Note (2): There could be times when parameters update very fast ‚Äî like when the user clicks next on the pagination bar too fast. That‚Äôs why we are using debounceTime() RxJS operator to discard intermittent values so that the backend is not called too frequently.
We can think of another functionality ‚Äî whenever the search term is updated, the page number should be reset to 1:
Notice how you are defining the business logic even before you are writing your template. Isolating the business logic in this way gives you immense control over readability and testability. Most codes are now unit testable, no need to make an e2e test to programmatically click a button and test a feature. If the business logic is working, so is the presentation layer.
Here is the full code for the MovieListStateService . Because it will be provided in a component, we do not use providedIn: 'root' inside the @Injectable decorator.
Let‚Äôs now provide our MovieListStateService into the page component.
We select movies array, and loading value from the state to view it in the template. Whereas, we update the page, search, etc. by setting the state with a new value through events. And that‚Äôs it! Our component is not doing any complex job other than presenting. Angular is supposed to be like this all along.
You have been coding wrong! üòÑ
Using a state management solution opens up many possibilities in your codebase. Your business logic lives in isolation, which is a huge benefit. You could reuse a state in multiple components. Most importantly, your application will feel fluid and smooth.
It is easier to onboard a new member of the team with a basic solution. And when everyone is ready, you can easily migrate to NGRX or any other advanced state management library because they work on the same principle.
Thank you for reading. Follow me for more interesting Angular articles like this.
More content at PlainEnglish.io. Sign up for our free weekly newsletter. Follow us on Twitter, LinkedIn, YouTube, and Discord. Interested in Growth Hacking? Check out Circuit.
Mayank Agnihotri
Sheeshpaul Kamboj
Mark Pustejovsky
Jashana Fleary
Mirko Rapisarda
in
JavaScript in Plain English
Paige Niedringhaus
in
ITNEXT
Tanmay Sharma
rahul kumar mandal
"
281,https://medium.com/towards-data-science/mean-vs-median-causal-effect-37057a6c54c9,Mean vs Median Causal Effect,"In A/B tests, a.k.a. randomized controlled trials, we usually estimate the average treatment effect (ATE): the effect of a treatment (a drug, ad, product, ‚Ä¶) on an outcome of interest (a disease, firm revenue, customer satisfaction, ‚Ä¶), where the ‚Äúaverage‚Äù is taken over the test subjects (patients, users, customers, ‚Ä¶). The ATE is a very useful quantity since it tells us the effect that we can expect if we were to treat a new subject with the same treatment.
However, sometimes we might be interested in quantities different from the average, such as the median. The median is an alternative measure of central tendency that is more robust to outliers and is often more informative with skewed distributions. More generally, we might want to estimate the effect for different quantiles of the outcome distribution. A common use case is studying the impact of a UI change on the loading time of a website: a slightly heavier website might translate into an imperceptible change for most users, but a big change for a few users with very slow connections. Another common use case is studying the impact of a product change on a product that few people buy: do existing customers buy it more or are we attracting new customers?
These questions are hard to answer with a linear regression that estimates the average treatment effect. A more suitable tool is quantile regression which can instead estimate the median treatment effect. In this article, we are going to cover a brief introduction to quantile regression and the estimation of quantile treatment effects.
Suppose we were an online store and we wanted to increase sales. We decide to offer our customers a loyalty card that grants them discounts as they increase their spending in the store. We would like to assess if the loyalty card is effective in increasing sales so we run an A/B test: we offer the loyalty card only to a subset of our customers, at random.
I import the data-generating process dgp_loyalty() from src.dgp. I also import some plotting functions and libraries from src.utils. To include not only code but also data and tables, I use Deepnote, a Jupyter-like web-based collaborative notebook environment.
Now, let‚Äôs have a look at the data. We have information on 10,000 customers, for whom we observe their spend and whether they were offered the loyalty card. We also observe some demographics, like age and gender.
Interestingly, we notice that the outcome of interest, spend, seems to have a lot of zeros. Let's dig deeper.
Before analyzing our experiment, let‚Äôs have a look at our outcome variable, spend. We first inspect it using centrality measures. We have two main options: the mean and the median.
First of all, what are they? The mean captures the average value, while the median captures the value in the center of the distribution. In general, the mean is mathematically more tractable and easier to interpret, while the median is more robust to outliers. You can find plenty of articles online comparing the two measures and suggesting which one is more appropriate and when. Let‚Äôs have a look at the mean and median spend.
How do we interpret these two numbers? People spend on average 28$ on our store. However, more than 50% of people don‚Äôt spend anything. As we can see, both measures are very informative and, to a certain extent, complementary. We can better understand the distribution of spend by plotting its histogram.
As previewed by the values of the mean and the median, the distribution of spend is very skewed, with more than 5000 customers (out of 10000) not spending anything.
One natural question then is: are we interested in the effect of the loyalty card on average spend or on median spend? The first would tell us if customers spend more on average, while the second would tell us if the average customer spends more.
Linear regression can tell us the effect of the loyalty card on average spend. However, what can we do if we were interested in the effect of the loyalty card on median spend (or other quantiles)? The answer is quantile regression.
With linear regression, we try to estimate the conditional expectation function of an outcome variable Y (spend in our example) with respect to one or more explanatory variables X (loyalty in our example).
In other words, we want to find a function f such that f(X) = ùîº[Y|X]. We do so, by solving the following minimization problem:
It can be shown that the function f that solves this minimization problem is indeed the conditional expectation of Y, with respect to X.
Since f(X) can be infinite-dimensional, we usually estimate a parametric form of f(X). The most common one is the linear form f(X)=Œ≤X, where Œ≤ is estimated by solving the corresponding minimization problem:
The linear form is not just convenient, but it can be interpreted as the best local approximation of f(X), referring to Taylor‚Äôs expansion.
With quantile regression, we do the same. The only difference is that, instead of estimating the conditional expectation of Y with respect to X, we want to estimate the q-quantile of Y with respect to X.
First of all, what is a quantile? The Wikipedia definition says
‚ÄúIn statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups).‚Äù
For example, the 0.1-quantile represents the value that sits on the right of 10% of the mass of the distribution. The median is the 0.5-quantile (or, equivalently, the 50·µó ∞ percentile or the 5·µó ∞ decile) and corresponds with the value in the center of the distribution. Let‚Äôs see a simple example with a log-normal distribution. I plot the three quartiles that divide the data into four equally sized bins.
As we can see, the three quartiles divide the data into four bins, of equal size.
So, what is the objective of quantile regression? The objective is to find a function f such that f(X) = F‚Åª¬π(y_q), where F is the cumulative distribution function of Y and y_q is the q-quantile of the distribution of Y.
How do we do this? It can be shown with a little linear algebra that we can obtain the conditional quantile as the solution to the following minimization problem:
Where œÅ_q (implicitly defined above) is an auxiliary weighting function with the following shape.
What is the intuition behind the objective function?
The idea is that we can interpret the equation as follows
So, when f(X) corresponds with the quantile y_q, the value of the objective function is zero.
Exactly as before, we can estimate a parametric form of f and, exactly as before, as shown in Angrist, Chernozhukov, and Fern√°ndez-Val (2006), we can interpret it as a best local approximation.
We wrote Œ≤ÃÇ _q to indicate that this is the coefficient for the best linear approximation of the conditional q-quantile function.
How do we estimate a quantile regression?
The statsmodels package allows us to estimate quantile regression with the quantreg() function. We just need to specify the quantile q when we fit the model. Let's use q=0.5, which corresponds with the median.
Quantile regression estimates a positive coefficient for loyalty. How does this estimate compare with linear regression?
The estimated coefficient with linear regression is higher. What does it mean? We will spend more time on the interpretation of quantile regression coefficients later.
Can we condition the analysis on other variables? We suspect that spend is also affected by customer demographics and we want to increase the precision of our estimate by also conditioning the analysis on age and gender. We can just add the variables to the quantreg() model.
The coefficient of loyalty increases when we condition the analysis on age and gender. This is not the case for linear regression.
There are a couple of things that we haven‚Äôt mentioned yet. The first one is inference. How do we compute confidence intervals and p-values for our estimates in quantile regression?
The asymptotic variance of the estimate y of the q-quantile of a cumulative distribution F is given by
where f is the density function of F. This expression can be decomposed into two components: q(1-q) and f‚Åª¬≤(y).
The first component, q(1-q), tells us that the variance of a quantile is higher the more the quantile is closer to the center of the distribution. Why is that so? First, we need to think about when the quantile of a point changes in response to a change in the value of a second point. The quantile changes when the second point swaps from left to right (or vice versa) of the first point. This is intuitively very easy if the first point lies in the middle of the distribution, but very hard if it lies at the extreme.
The second component, f‚Åª¬≤(y), instead tells us that the variance is higher if the point is isolated and therefore we have less information about it.
Importantly, estimating the variance of a quantile requires an estimate of the entire distribution of Y. This is done via approximation and it can be computationally very intensive. In case this process becomes too computationally intense, alternative procedures like the bootstrap or the bayesian bootstrap are always available.
The second thing that we haven‚Äôt talked about yet is the interpretation of the estimated coefficients. We got a lower coefficient of loyalty on spend with median regression. What does it mean?
The interpretation of linear regression coefficients is straightforward: each coefficient is the derivative of the conditional expectation function ùîº[Y|X] with respect to one dimension of X. In our case, we can interpret the regression coefficient of loyalty as the average spend increases from being offered a loyalty card. Crucially, here ""average"" means that this holds true for each customer, on average.
However, the interpretation of quantile regression coefficients is tricky. Before, we were tempted to say that the loyalty card increases the spend of the median customer by 3.4$. But what does it mean? Is it the same median customer that spends more or do we have a different median customer? This might seem like a philosophical question but it has important implications for reporting quantile regression results. In the first case, we are making a statement that, as for the interpretation of linear regression coefficients, applies to a single individual. In the second case, we are making a statement about the distribution.
Chernozhukov and Hansen (2005) show that a strong but helpful assumption is rank invariance: assuming that the treatment does not shift the relative composition of the distribution. In other words, if we rank people by spend before the experiment, we assume that this ranking is not affected by the introduction of the loyalty card. If I was spending less than you before, I might spend more afterward, but still less than you (for any two people).
Under this assumption, we can interpret the quantile coefficients as marginal effects for single individuals sitting at different points of the outcome distribution, as in the first interpretation provided above. Moreover, we can report the treatment effect for many quantiles and interpret each one of them as a local effect for a different individual. Let‚Äôs plot the distribution of treatment effects, for different quantiles of spend.
This plot is extremely insightful: for almost half of the customers, the loyalty card has no effect. On the other hand, customers that were already spending something end up spending even more (around 10/12$ more). This is a very powerful insight that we would have missed with a linear regression that estimated an average effect of 5$.
We can repeat the same exercise, conditioning the analysis on gender and age.
Conditioning on other covariates removes a lot of the heterogeneity in treatment effects. The loyalty card increases spending for most people.
In this article, we have explored a different causal estimand: the median treatment effect. How does it compare with the average treatment effect that we usually estimate? The pros and cons are closely related to the pros and cons of the median with respect to the mean as a measure of central tendency. Median treatment effects are more informative on what is the effect on the average subject and are more robust to outliers. However, they are more computationally demanding and they require strong assumptions for identification, such as rank invariance.
[1] R. Koenker, Quantile Regression (1996), Cambridge University Press.
[2] R. Koenker, K. Hallock, Quantile Regression (2001), Journal of Economic Perspectives.
[3] V. Chernozhukov, C. Hansen, An IV Model of Quantile Treatment Effects (2005), Econometrica.
[4] J. Angrist, V. Chernozhukov, I. Fern√°ndez-Val, Quantile Regression under Misspecification, with an Application to the U.S. Wage Structure (2006), Econometrica.
You can find the original Jupyter Notebook here:
github.com
I really appreciate it! ü§ó If you liked the post and would like to see more, consider following me. I post regularly on topics related to causal inference and data analysis. I try to keep my posts simple but precise, always providing code, examples, and simulations.
Also, a small disclaimer: I write to learn so mistakes are the norm, even though I try my best. Please, when you spot them, let me know. I also appreciate suggestions on new topics!
Satyam Kumar
in
Towards Data Science
Sharone Li
in
Towards Data Science
Sourish Dey
in
DataDrivenInvestor
Wuzaf
Danielle Stewart
Cassie Kozyrkov
in
Towards Data Science
Sayantini Deb
in
Edureka
Enterprise Integration
"
282,https://medium.com/@theleonwei/doordash-data-scientist-interview-guide-8f653c785992,Doordash Data Scientist Interview Guide,"Doordash‚Äôs data scientists are experts in using data to drive company growth. Their daily jobs usually include using data science and statistical models to measure product health, marketing strategy, and tactical decision-making across the company across a variety of teams and levels.
Here is a sample job description of a senior Data Scientist at Doordash:
This guide will walk you through what the Doordash Data Scientist interview process looks like, and sample interview questions. As well as a Doordash Data Scientist case study.
The interview process mainly includes evaluating a candidate‚Äôs following skills: SQL, Machine Learning, Behavioral Questions | Leadership, A/B Testing, Product Sense, Data Challenge, etc.
COACH ME FOR DOORDASH DATA SCIENTIST INTERVIEW
DoorDash, Inc. is an American company that operates an online food ordering and food delivery platform.
Doordash is based in San Francisco, California, United States. With a 56% market share, it is the largest food delivery company in the United States.
Doordash also has a 60% market share in the convenience delivery category.
As of December 31, 2020, the platform was used by 450,000 merchants, 20,000,000 consumers, and 1 million deliverers.
DoorDash went public in December 2020 on NYSE and trades under the symbol DASH.
The company has been criticized for withholding tips, reducing tip transparency, antitrust price manipulation, listing restaurants without permission, and allegedly misclassifying workers.
COACH ME FOR DOORDASH DATA SCIENTIST INTERVIEW
Salary data sourced from instamentor‚Äôs mentees and cross-checked over the internet.
Every situation is different, e.g., the offer also depends on your interview performance, job location and the urgency of filling that role. Therefore, this data is for your reference only and may not apply to everyone.
COACH ME FOR DOORDASH DATA SCIENTIST SALARY NEGOTIATION
Mona, an instamentor mentee, recently received two job offers, an L5 data scientist job offer from Amazon.
She also received a machine learning data scientist job offer from Doordash, and here is what Doordash‚Äôs machine learning data scientist interview process looks like:
Candidate: Mona
How it gets started: a friend gave Mona an internal referral
Job Level: Mid
Year of Experience: 3 years experience
Degree: M.S in statistics
Offer: Yes
TC: ~220K USD
Location: San Francisco, CA
Interview process: 3 weeks
Preparation: 1 month
Has a job: yes
Decide to join: N/A
Given a dataset, build a machine learning model to predict food delivery time in different time buckets: < 10 mins, 10‚Äì30 mins, > 30 mins.
Mona received the job offer a few days after the interview. She is still negotiating and considering the pros and cons of the two offers from Amazon and Doordash.
Get mentored and become 100% job interview ready for your data scientist interview, learn directly from former FAANG hiring managers, BOOK YOUR SESSION TODAY.
COACH ME FOR DOORDASH DATA SCIENTIST INTERVIEW
Chitra Rajasekaran
AnyChart
in
Data Visualization Weekly
Srinivas Balivada
AnyChart
in
Data Visualization Weekly
Raftaar Rashedin
Serafeim Loukas
in
Towards AI
Joanna Boru≈Ñ
in
Fandom Engineering
ravi krishnan
"
283,https://medium.com/towards-data-science/the-ins-and-outs-of-clustering-algorithms-acd4e33290c5,The Ins and Outs of Clustering Algorithms,"Solving a data science problem often starts with asking the same simple questions over and over again, with the occasional variation: Is there a relationship here? Do these data points belong together? What about those other ones over there? How do the former relate to the latter?
Things can (and do) become complicated very quickly‚Äîespecially when we try to detect subtle patterns and relationships while dealing with large datasets. This is where clustering algorithms come in handy with their power to divide a messy pile of data into distinct, meaningful groups, which we can then leverage in our analyses.
To help you on your clustering learning journey, we‚Äôve selected our best recent articles on the topic‚Äîthey cover a lot of ground, from basic concepts to more specialized use cases. Enjoy!
Ready for a few more standout articles on other topics? Here‚Äôs a human-made cluster we‚Äôre thrilled to share:
As the year draws to a close, we‚Äôre as grateful as ever for our readers‚Äô support. If you‚Äôd like to make the biggest impact, consider becoming a Medium member.
Until the next Variable,
TDS Editors
Ievgen Redko
in
Towards Data Science
Chayan Kathuria
in
Towards Data Science
Takuya Matsuyama
in
Dev as Life
Selen Gerel
Rashi Desai
in
The Startup
Jeff Griesemer
Nidhi Bansal
Rohit Batra
"
284,https://medium.com/techtofreedom/10-pandas-functions-that-help-you-understand-a-dataset-completely-b7de7e7e14ab,10 Pandas Functions That Help You Understand a Dataset Completely,"Many junior data scientists think a majority of problems they need to handle are from fancy deep learning models.
Quinn Lanners
in
Towards Data Science
Raj Joseph
Aashish Nair
in
Towards Data Science
Khanh Nguyen
in
Towards Data Science
Marco Santos
in
Towards Data Science
Cyber_Noob
Marco Santos
in
Towards Data Science
Zia Mehrabi
in
the nature of food
"
285,https://medium.com/techtofreedom/10-pandas-functions-that-help-you-understand-a-dataset-completely-b7de7e7e14ab,10 Pandas Functions That Help You Understand a Dataset Completely,"Many junior data scientists think a majority of problems they need to handle are from fancy deep learning models.
Quinn Lanners
in
Towards Data Science
Raj Joseph
Aashish Nair
in
Towards Data Science
Khanh Nguyen
in
Towards Data Science
Marco Santos
in
Towards Data Science
Cyber_Noob
Marco Santos
in
Towards Data Science
Zia Mehrabi
in
the nature of food
"
286,https://medium.com/javascript-in-plain-english/the-millionaire-developer-who-sold-his-app-for-30m-beb18fe6e363,The Millionaire Developer Who Sold His App for $30M,"It was a great British success story when the 17-year-old high school student Nicholas D‚ÄôAloisio sold his news summary start-up Summly to Yahoo! for $30 million.
At 12, Nick started coding for Apple‚Äôs App Store. He received venture capital investment at the age of 15, which made him the youngest recipient.
Nick D‚ÄôAloisio is a British computer programmer and internet entrepreneur. He is well-known for his app Summly, which integrates artificial intelligence with summarization and was created in collaboration with SRI International.
Nick was born in November 1995 in London. His parents were Australian expats. Shortly after Nick was born, the family moved back to Melbourne. When Nick turned seven, the D‚ÄôAloisio family relocated to Wimbledon, southwest London, and have been there ever since.
The tech prodigy got his first computer, a Mac laptop when he was nine. By the time he was 12, he had learned how to code and begun developing apps.
In March 2011, Nick developed an iOS app called Trimmit to reduce the text content to 1000, 500, or 140 characters using an analytical technique. The app got the attention of Apple, so they highlighted it as a brand-new standout app in their app store.
As a result, billionaire Li Ka-Shing saw Trimmit‚Äôs enormous potential and decided to invest $300,000 in the venture. Using the feedback from Trimmit, Nick redesigned the program and released it under the name Summly in December 2011.
Summly, which was inspired by Trimit, summarized web content using machine learning and natural language processing. The ingenious tool even had the ability to recognize the genre of the material and apply various measures to various types.
Summly‚Äôs initial release was just a prototype, despite receiving more than 200,000 downloads. In November 2012, Summly got further funding of US$1 million from Horizons Ventures and Ka-Shing, as well as support from celebrities Yoko Ono, Ashton Kutcher, and Stephen Fry. D‚ÄôAloisio then started working on version 2.0 with a gathered group of backers, advisers, investors, and IT experts.
On his 17th birthday, the second edition of Summly was released, and in only a few months, it generated more than 90 million summaries.
In December 2011, GigaOm‚Äôs Om Malik had labeled the internet‚Äôs new ‚Äòboy genius‚Äô, and then D‚ÄôAloisio was listed in Forbes‚Äô 30 Under 30 and the Evening Standard‚Äôs Top 1,000 Londoners. D‚ÄôAloisio also earned a Spirit of London Award for Entrepreneur of the Year in 2012, while Summly won an Intuitive Touch award in Apple‚Äôs Best Apps of the year.
In March 2013, Yahoo! bought the app for $30 million. After the acquisition, Summly was no longer available in the App Store but the technology later appeared in multiple Yahoo! Products. Meanwhile, D‚ÄôAloisio was the youngest Yahoo! Employee. He was surprised by his success,, saying, ‚ÄúI never thought I‚Äôd find myself in this position so quickly.‚Äù
D‚ÄôAloisio is now 26 and already sold his second company, Sphere ‚Äî the 20-person technology start-up to Twitter.
After leaving Yahoo in 2015 to pursue computer science and philosophy at Oxford, Mr. D‚ÄôAloisio worked as an ‚Äúentrepreneur in residence‚Äù at Airbnb for a season. He developed the concept for Sphere, a knowledge-sharing community where members would share knowledge and ideas using an app similar to instant messaging.
At the end of 2015, Mr. D‚ÄôAloisio and chief technology officer Tomas Halgas, whom he had met at Oxford, co-founded Sphere. Considering Mr. D‚ÄôAloisio‚Äôs track record, the notion was well accepted by investors, according to people familiar with the business.
D‚ÄôAloisio claims that the second time around brought various challenges. ‚ÄúIt was both easier and harder. It was easier because of my previous experience and we got a $30 million raise from investors willing to take on this risk.‚Äù
‚ÄúIn contrast, there were more demands and expectations. Entrepreneurship is difficult; you have to put in a lot of effort, and convincing people to [use a new app] is getting harder as they become set in their online habits,‚Äù D‚ÄôAloisio said.
According to the Financial Times, the business had raised $30 million as of March 2019. TechCrunch, The Telegraph, The Times, and the BBC, among other news organizations, announced in October 2021 that Twitter had bought Sphere and that most of the 30-person staff would be joining the company.
D‚ÄôAloisio believes that the UK has made significant progress in promoting digital entrepreneurship compared to his time as a young developer.
‚ÄúI now have friends in the business that I have known for a very long time. None of my friends had any interest in technology ten years ago.‚Äù
More content at PlainEnglish.io.
Sign up for our free weekly newsletter. Follow us on Twitter, LinkedIn, YouTube, and Discord.
Interested in scaling your software startup? Check out Circuit.
Webnxa
Henric Trotzig
in
happo.io
Anthony Ng
David Cai
Pin Pin Tea-makorn
in
TenXor
getspooky
Max Koretskyi
in
Angular In Depth
Rachid Rakaa
"
287,https://medium.com/dev-genius/8-crazy-ai-tools-saving-hours-of-work-youve-never-heard-of-1959cf77a525,"8 crazy AI tools saving hours of work, you‚Äôve never heard of.","At code.store, we build amazing products for our clients using the most advanced technologies: Low Code, No Code, Blockchain, AI, and Cloud native functions. Ping us if you need help!
Those 8 tools are helping our clients to reduce the tremendous workload of their teams, we wanted to share them.
medium.com
Jasper and Copy.ai are used by marketers and SEO experts to produce AI-generated content. But the writer's experience is terrible. Lex.page is built for writers by writers. The user experience is amazing: if stuck on something, hit ‚Äú‚Ä¶‚Äù button and let Lex continue for you.
Link: https://lex.page
medium.com
Climpchimp or Adobe Premiere? No! Runway.ml !
It‚Äôs an amazing toolbox for content creators working with videos. You can :
And much much more!
Link: https://runwayml.com/
DiffusionBee is a free, MacOS application to work with Stable Diffusion AI image generation. Like for Midjourney or Dall-E, you write a prompt, and you have your image. But, because you‚Äôre running it on your Mac and using an open-sourced technology, it‚Äôs free!
Link: https://diffusionbee.com/
How do you generate images with AI? Using prompts. Prompting is a complex task, mastering it requires a lot of time trying different approaches and prompts. Well now, there is a search engine of prompts that were used to generate beautiful images. Looking for a picture of a factory, instead of trying for hours with Dall-E or Stable Diffusion, go on Lexica.art, and search for what others did with factories. Find what you are looking for and adapt that prompt or explore similar styles.
Link: https://lexica.art/
medium.com
You‚Äôve probably experienced how dumb Google can be. Well, that‚Äôs because Google still relies on back-links and keywords to index and rank pages. With modern AI, it is possible to understand what exactly the user is looking for and predict the links like GPT-3 predicts the next paragraph of your marketing blog post. Meet Metaphor, the very first ‚Äòsmart‚Äô search engine where you have to prompt your search as you would prompt GPT3.
Link: https://metaphor.systems/
If you‚Äôre making videos, you need a piece of background music. With Soundraw you can set any genre, duration, and beats-per-minute and generate hours of different music. The quality is amazing, you can‚Äôt say it was generated by an AI.
Link: https://soundraw.io/
Editing photos for e-commerce or blogs? ClipDrop will help you to remove backgrounds, upscale your photos, and clean up or relight them. The most important part? They provide batch editing and API, so you can integrate them into your own application.
Link: https://clipdrop.co/
Adding some authority arguments in your writings can boost engagement and trust from your readers. Searching for quotes on the internet usually gives you overused quotes. You can now look for quotes in more than 100.000 books indexed by Google.
Link: https://books.google.com/talktobooks/
At code.store, we build amazing products for our clients using the most advanced technologies: Low Code, No Code, Blockchain, AI, and Cloud native functions. Ping us if you need help!
Now you‚Äôre here, you can enjoy that other list of tools:
medium.com
Sunel Kumar
Julien Simon
in
ITNEXT
Saharsh Solanki
Alex Scott
in
mStable
Hugh Choi
Edward Kim
Alibaba Cloud
LABRADO
"
288,https://medium.com/towards-data-science/when-outliers-are-significant-weighted-linear-regression-bcdc8389ab10,When Outliers are Significant: Weighted Linear Regression,"Outliers are often mischievous. They have the potential to disrupt an otherwise simple process of regression by introducing themselves as data that is as equally important as the rest, often skewing the fitted model. A straightforward approach is to use outlier detection methods to remove them from the dataset prior to fitting a model. But this has its caveats. Sometimes outliers may be significant and are essential to building a model that fits the data such that it represents all the observations/measurements. This however leads straight back to the issue of outliers skewing the fitted model. So, what can be done to solve this? You guessed it, weighted regression!
Weighted regression is defined as ‚Äúa generalization of linear regression where the covariance matrix of errors is incorporated in the model‚Äù. In simple terms, this means that not all data points are equal in the eyes of the data scientist, and this inequality should also reflect in the fitted model as well. The imbalance of equality during fitting a dataset to a regression model can be alleviated with a few techniques. These techniques range from: inserting a new binary column that flags outliers, to giving each data point its own weight of importance relative to the rest of the dataset.
The art of weighting data can often be an ambiguous one. What data is more important than another? Why is it? And what weight of importance should it receive? All good questions a data scientist should ask when attempting to apply this method. A simple approach would be to use an outlier robust regression model, such as the Huber Regressor, to do the trick. However, many more advanced methods exist for weighting data, with some using prior knowledge of the data itself, and others by applying more sophisticated statistical techniques. This article will focus on weighting the data prior to regression by employing both outlier detection and thresholding methods.
To get started let's load and prepare the data we will use to fit our regression model. The dataset tips is an example dataset available in the Seaborn library from their online repository. This dataset is part of a collection of case studies for business statistics [1]. It consists of 244 tips made by a single waiter at a restaurant over a period of a few months. The dataset has 6 exploratory variables (X) and the ‚Äútips‚Äù being the response variable (y). Besides the variable ‚Äútotal bill‚Äù the rest of the exploratory variables are categorical. In order to use these categorical variables for regression, the dataset requires some preparation. Dummy variables (representing the presence or absence of a qualitative attribute) for the categorical variables must be created. At the same time, let's take a look at how the dataset is distributed.
We now have 8 exploratory variables with which we can use for regression. And from what we can see in the distribution plot, is that by using the variable ‚Äútotal bill‚Äù alone, there is a visible linear relationship between it and ‚Äútip‚Äù. However, something interesting happens at around the 25 ‚Äútotal bill‚Äù mark, the data no longer follows the same linear relationship as before. One might say there are perhaps two or even three possible linear models that are necessary to model this dataset. However, this article‚Äôs focus is on a singular weighted regression fit and not piecewise modelling. From what we can observe, it seems like Saturday and Sunday dinners seem to deviate the most in terms of ‚Äútotal bill‚Äù to ‚Äútip‚Äù linearity. And since we want to fit a single linear model to the data, these data points may need to be weighted differently to the rest in order to not skew the predicted results. Before this, lets create a baseline regression model with which we can compare our differently weighted regression models to.
Without weighting the exploratory variables, we get a linear model with a R-squared score of 0.4699 and a mean squared error of 1.011. As mentioned above during the quick data analysis, a single linear model is too simplistic for this dataset. But we shall continue along this path and see what can be done by adding weights to the regression model.
First, let's talk about what types of weighting can be used in regression modelling.
Before further explaining and demonstrating each weighting method‚Äôs application in regression. Perhaps it would be better to first visualize these weightings. For this, example functions will be used where the x-axis follows normalized (0‚Äì1) outlier decision scores. These scores are a likelihood estimation that a value is an outlier relative to the rest of the dataset. Therefore, the higher the decision score the more likely it is an outlier. However, I will not go into further depth on likelihood estimation and its mathematical applications in weighted regression but would rather refer you to go read Weighted Linear Regression if you wish to know more on this. On our y-axis we will have our weightings. So, let‚Äôs visualize this!
While that was a great visualization, what does it all mean, how did you generate these weighting functions, and how can we apply this to regression? From what we can see above, we have quite a few options of weightings to choose from. So, let‚Äôs start with the continuous weighting type. We will use a Gaussian function as weights for the weighted regression on the tips dataset. To do so we will first need to find the outlier decision scores for the dataset. This will be done by using the kernel density estimation (KDE) outlier detection method available from PyOD.
By using a continuous weighting function, we get a linear model with a R-squared score of 0.4582 and a mean squared error of 1.033. You may notice that this performance is worse than the linear model without weighting! So why is a regression model with a worse R-squared score a better choice to use? Did we just waste our time? And what have we achieved by using weights in the regression model? The simple answer is, this is to be expected‚Ä¶
To explain this more broadly, while it is true that the overall model performance decreased, the purpose of weighting the data was to assign more importance to data that was more likely to occur/ be measured. Thereby, allowing for outliers that are still significant within the data to contribute to the model, but only have a minor importance to the overall model itself. And therefore, a lower model performance is not an indication of poor fitting. But rather, it does indicate that perhaps the way that we measure our model performance should now be shifted.
Well, that is all nice and dandy, but how can our weighted model performance now be accurately evaluated? To do this, this actually brings us to the second type of weighting. Discrete weighting. Let me explain. Since the weights are discrete, and for our example case binary. This means that a clear distinction is made between inliers and outliers. With this distinction, a new dataset arises from the original with which to better evaluate our model performance metrics against. Inliers.
To do this we will evaluate the outlier decision scores using PyThresh (A library for thresholding outlier scores) and a project that I am openly involved in. So, let‚Äôs apply discrete weighting to a weighted regression model.
By using a discrete weighting function, we get a linear model with a R-squared score of 0.4606 and a mean squared error of 1.028. The ratio of inliers was 77.5% and outliers was 22.5%. If we now evaluate the weighted models against the baseline model with regards to only inliers this is what we get:
From this we can see that in terms of fitting the entire dataset the unweighted model still performs the best. However, with regards to the inliers it performed the worst. This means, that while the unweighted model did perform better overall, it is now possibly biased towards outliers, reducing its predictive accuracy towards inliers.
This finding, however, must be taken with a grain of salt. As we are data scientist after all and being skeptical of our own bias at model interpretability is also important. From what is apparent from the performance metrics, is that weighting the dataset may remove outlier bias. However, to reiterate, a single linear model, and perhaps even the outlier detection method, are not well suited for this dataset. So, what has been achieved from this exercise? Well, we have seen that significant outliers can be included within a regression model without making them as equally as important as inliers. And that even though they contribute to the final model fit, being significant and all, their importance with respect to predictions has been adjusted accordingly.
Applying the correct weightings for your data during regression becomes just as important as the dataset itself. It is difficult to say with absolute confidence that a perfect set of weights for your data exists, but with the examples above, hopefully it will make this task a little easier when trying to include outliers. One should be mindful to consider the implications of allowing outliers to remain during fitting and always choose the answer that represents our data best. Weighted regression is one of many ways to achieve this, and its uses are a valuable asset.
I will not include an example of piecewise continuous weighting, as with the continuous and discrete weighting examples above, this should hopefully be easy to implement using the two combined.
In closing, I hope that this will aid your data science skills and become a powerful tool to help when handling significant outliers. All the best with your endeavors in the world of data!
[1] Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing
Pradeep jaiswal
Darshan Purohit
Eka Ponkratova
Aayushi Shah
Jahnavi Mahanta
in
Towards Data Science
Darshan Purohit
Margi patel
Eaven Khaw
"
289,https://medium.com/@avi-loeb/the-skys-the-limit-on-the-speed-of-light-19fcaa3bdbf4,The Sky‚Äôs the Limit on the Speed of Light,"According to Einstein‚Äôs Special Theory of Relativity, no material object can move locally faster than light. Since 1905, modern physics has been ruled by the speed of light limit.
However, in September 2011 the OPERA (Oscillation Project with Emulsion-tRacking Apparatus) experimental team reported that neutrinos can travel faster than light. Given this claim, Wall Street traders could have made huge financial gains by responding to market changes with neutrino communication, faster than any competitors who are using traditional electromagnetic communication through regular internet channels.
The implications to our understanding of the cosmos would have been far more significant than the impact on the financial sector. The OPERA report resulted in numerous papers interpreting its consequences for fundamental physics. But within six months, the OPERA team concluded that a fiber optic cable was attached improperly and caused the apparently faster-than-light measurements in addition to a clock oscillator ticking too fast. Subsequently, five experimental follow-ups, including within the OPERA set-up, found neutrino speeds consistent with the speed of light limit.
The lesson from this experience is that any claim for new physics must be held to an extremely high level of experimental competence and must be reproducible. As I argued in a recent preprint, the dark objects observed by astronomers above Ukraine cannot be used as an argument for new physics if the distance measurements are highly uncertain.
Being the longest-serving chair of the Astronomy department at Harvard University, I am well aware that the best way to attract undergraduate students to astronomy is to take them on an observing trip. The view of the stars on a dark night is awe inspiring. Could humans not only stare at the stars but actually reach them within a human‚Äôs lifespan?
The distance to the nearest star, Proxima Centauri, is 4.2 light years. In order to reach it within the 122 years lifespan of the oldest person who ever lived, Jeanne Louise Calment, we need to launch a spacecraft that travels faster than 3.5% of the speed of light. Chemical rockets reach at best 0.01% of the speed of light because they carry their fuel and are subject to the tyranny of the rocket equation. But shortly after the laser was invented, Robert Forward suggested in 1962 that pushing a lightweight sail with a powerful laser beam can propel the sail close to the speed of light. Thanks to funding by the visionary entrepreneur and physicist, Yuri Milner, this idea materialized to the Starshot Initiative.
The Starshot concept envisioned a 100 giga-Watt laser shining on a human-size sail, to which a payload mass of a few grams is attached. Over a few minutes the sail can be propelled up to a fifth of the speed of light over five times the distance to the Moon. The system can launch many low-cost craft carrying smart electronic probes towards the nearest stars. The Scientific Advisory Board of Starshot ‚Äî which I chair, concluded that there are no obvious ‚Äúshowstoppers‚Äù to this technology and that humanity could potentially build a working system later this century. By now, the research team identified promising materials and structures for the light sail, a path for developing lasers of sufficient total power, and methods for communication across the journey to the nearest stars. The technological challenges are great but so are the rewards.
The study of lightsails has a bright future ahead. In 2023 NASA is planning to launch an Advanced Composite Solar Sail System (ACS3) that will test new sail boom materials in a low-Earth orbit. The mission will deploy a lightweight sail about the size of a small apartment from a CubeSat, a spacecraft the size of a toaster oven.
But we must also keep in mind that standard physics offers a path to overcome the speed of light limit. According to Einstein‚Äôs General Theory of Relativity, space can expand so rapidly that it would separate two distant observers faster than light away from each other. The situation resembles a balloon which could be inflated so fast that stationary ants on its surface would be separated away from each other at a speed faster than their walking speed.
When the vacuum dominates the cosmic mass density, the expansion of space is accelerated, allowing for locally stationary observers to be separated faster than light. Any inhomogeneities in between are ironed out by the rapid expansion. As a result, the Universe becomes homogeneous and isotropic. An early phase of this accelerated expansion, known as cosmic inflation, was proposed four decades ago to explain why the universe has the same conditions on opposite sides that had no time to communicate with each other for a decelerating expansion, as expected without the gravitational repulsion of the vacuum.
The initial version of inflation developed in the early 1980s, explained the flat geometry, the near uniformity and isotropy of the Universe and the origin of structure in it as a result of quantum fluctuations. It made testable predictions and was falsifiable. But as the theory was explored further, it was realized that it can accommodate all possible outcomes. First, one can always reverse engineer an inflationary model to fit an observed outcome. For example, modelers came up with variants of inflation that can accommodate a non-flat universe. Second, it was realized that if one starts from some arbitrary initial conditions of large inhomogeneities, inflation may not be possible. Third, quantum fluctuations can lead to any outcome. In the words of inflation‚Äôs pioneer, Alan Guth: ‚ÄúIn an eternally inflating universe, anything that can happen will happen an infinite number of times.‚Äù Given these circumstances, the theory is not easily falsifiable because without calibrating the likelihood of different outcomes there is no obvious way of ruling out the paradigm.
Gladly, there is a way out. In a new paper that I just published with Sunny Vagnozzi, we proposed an experimental test that could rule-out inflation. Without an inflationary epoch of vast expansion, the Universe would be filled with a radiation background of gravitational waves (gravitons) similar to the cosmic background of photons and neutrinos. In a strong magnetic field, some of these gravitons could be converted to observable electromagnetic signals. If this futuristic experiment, when done, detects the cosmic graviton background from the early universe, it would disprove all models of inflation.
Here‚Äôs hoping that the speed of light will continue to inspire future generations of physicists to pursue innovative ideas that are testable by reproducible experiments.
ABOUT THE AUTHOR
Avi Loeb is the head of the Galileo Project, founding director of Harvard University‚Äôs ‚Äî Black Hole Initiative, director of the Institute for Theory and Computation at the Harvard-Smithsonian Center for Astrophysics, and the former chair of the astronomy department at Harvard University (2011‚Äì2020). He chairs the advisory board for the Breakthrough Starshot project, and is a former member of the President‚Äôs Council of Advisors on Science and Technology and a former chair of the Board on Physics and Astronomy of the National Academies. He is the bestselling author of ‚ÄúExtraterrestrial: The First Sign of Intelligent Life Beyond Earth‚Äù and a co-author of the textbook ‚ÄúLife in the Cosmos‚Äù, both published in 2021. His new book, titled ‚ÄúInterstellar‚Äù, is scheduled for publication in August 2023.
Avi Loeb
Avi Loeb
Avi Loeb
The Future Experience
Chris Wake
in
Costanoa Ventures
Reece Emmitt
Robert Barron
Areeshazafar
"
290,https://medium.com/gitconnected/will-julia-replace-python-as-a-programming-language-in-the-future-125b75ededd4,Will Julia Replace Python as a Programming Language in the Future?,"Python has been a popular programming language for many years, but there is speculation that Julia may gradually replace it.
Julia is a newer language that has been gaining popularity due to its speed and flexibility.
While Python is still a strong choice for many applications, Julia may be worth considering for more demanding tasks.
Julia allows faster execution and better scalability, which can improve performance.
It is also simpler to learn than Python, with a more intuitive syntax and fewer lines of code required for the same tasks.
Julia also supports multiple programming paradigms, such as procedural, object-oriented, and functional programming.
Additionally, Julia has strong cross-platform support and can be used on multiple operating systems.
Julia also has a wide range of libraries for data science and scientific computing, which can make it easier to quickly solve complex problems.
While Python still has some advantages over Julia, such as its larger user base and more mature ecosystem, many developers are starting to see the potential that Julia offers.
As more people use Julia, it will likely become more popular and replace Python as the language of choice for many tasks.
However, it is important to remember that both languages have their own strengths and weaknesses and can be used together in some cases.
Ultimately, both Python and Julia are strong choices for many applications, so it is up to developers to decide which one is right for them.
Python and Julia are both programming languages that continue to be favored among software developers.
While Julia has gained traction in recent years, Python remains the most popular programming language worldwide.
The reason for this is two-fold: many developers find Python‚Äôs syntax easier to learn and understand than Julia, while established programming features like lists, dictionaries, and classes, are easy to work with in Python.
In addition, powerful programming libraries like NumPy and SciPy make it much simpler to quickly develop an application or perform a calculation in Python.
Although Julia is increasingly gaining popularity among the programming community due to its scalability capabilities, Python will remain a dominating programming force for years to come.
It would take major shifts in programming technology for Julia to overtake Python‚Äôs current position in the programming world.
Until then, it seems safe to assume that when it comes to programming, developers will continue relying on the ease of use that Python can offer.
Python is one of the most popular programming languages available today, and for good reason.
Its flexible syntax and robust libraries make it simple to use for a wide range of tasks such as data analysis, web development, machine learning, artificial intelligence, robotics programming, and game programming. Python offers immense scalability and tremendous speed compared to other programming languages.
It can be deployed on anything from a computer to an emerging cloud infrastructure within minutes.
The language‚Äôs ‚Äúbatteries included‚Äù approach means that users don‚Äôt need to spend additional time writing code or hunting around for additional software packages when they want to get started programming right away.
Furthermore, beginners don‚Äôt have much difficulty learning Python thanks to its readability; once acquired, these skills can be applied far beyond programming applications such as designing data structures and performing mathematical computations.
All in all, Python is well-suited for almost any project that requires programming power and thus has a plethora of potential use cases.
Python has been around since the early 1990s, and it‚Äôs not going anywhere anytime soon.
As one of the oldest programming languages in continuous use, Python is well-established and here to stay.
The language continues to be improved upon as new updates are released; why this means that users can continue to expect support for Python for a long time.
Furthermore, Python‚Äôs libraries and frameworks are continually being developed and improved, allowing users to take advantage of new features when they need them.
The current version of Python 3.11. 0, is backward-compatible with older versions, making it easy for developers to port their programs from one version to the next without having to start over from scratch.
This means that developers can be sure that their programs will remain up-to-date and continue to work with the latest version of Python for years to come.
In conclusion, it‚Äôs safe to assume that Python is here to stay as one of the most popular and reliable programming languages available.
It‚Äôs true that Python is currently the most popular programming language, but Julia is steadily gaining popularity as well.
While it may not be able to overtake Python in popularity any time soon, Julia does have some advantages over Python that make it attractive to certain developers.
The main advantage of Julia over Python is its scalability ‚Äî the code runs much faster and is more efficient than with Python.
This makes it attractive for applications that require large amounts of data processing or complex mathematical computations.
Julia also has a much simpler syntax, making it easier to read and understand.
This can be especially beneficial when developers are trying to modify existing code or debug programs quickly.
Finally, Julia‚Äôs libraries are growing, making it easier to find solutions to common programming problems.
With more developers using Julia and creating new libraries, the language may soon be able to offer the same level of support as Python.
Julia is growing fast, and it‚Äôs not just popular among developers ‚Äî businesses are starting to take notice as well.
Companies like Amazon, Microsoft, and Google are investing in the language, creating libraries and applications that make Julia even more attractive for use cases such as machine learning and artificial intelligence.
With its speed advantages over Python and its ever-growing list of libraries, Julia is becoming a popular choice among developers for more complex projects.
In conclusion, Python and Julia are both excellent programming languages that have their respective strengths and weaknesses.
While Python has been around longer and is currently the most popular language, Julia is growing fast and offers some advantages over Python in terms of scale and speed.
Ultimately, the best language to use will depend on the individual project and developer preference.
Python is well-established and has a large body of support while Julia‚Äôs scalability and simpler syntax may be better suited for certain projects.
Both languages have their place in modern programming, and developers should choose the language that fits their specific needs.
levelup.gitconnected.com
levelup.gitconnected.com
Want to read more of my stories and gain unlimited access to other great stories on Medium? Consider joining Medium through my referral link for just $5/month. Your monthly payment will support me directly. Sign up here to get my articles whenever I publish.
Thanks for being a part of our community! Before you go:
üöÄüëâ Join the Level Up talent collective and find an amazing job
Bee-a-Learner
Grant Larson
Shivangi Sareen
in
Towards Data Science
Bhanutej Kalla
Shih JHIH YU
Shashikant Jagtap
in
XCBlog
Abdul Rafay
Mark Basson
in
Geek Culture
"
291,https://medium.com/@ali/7-beautiful-books-that-i-enjoyed-in-2022-3f92850b4054,7 Beautiful Books That I Enjoyed in 2022,"It has been said that those who do not learn from history are doomed to repeat it, and this is also true for education and learning in general. That is why I enjoy reading about different approaches to teaching and learning and exploring new ideas in mathematics and other subjects.
As we approach the end of another year, I want to reflect on the books I enjoyed reading over the past twelve‚Ä¶
Permohonanmaafdariku
Sarah Paris
in
Books Are Our Superpower
Vinny Kurban
Bagan Horseman
Thyukytraeeref21
Lumberpak
READ/DOWNLOAD$! Auto Body Repair Technology FULL
Ravneet Bawa
in
December of One Nine
"
292,https://medium.com/towards-artificial-intelligence/how-to-access-scientific-knowledge-with-galactica-8adc96ebe931,How to access Scientific Knowledge with Galactica,"The world of scientific knowledge is mind-bogglingly vast. Searching for relevant research papers, understanding complex concepts, and writing academic literature can be daunting and time-consuming, especially for inexperienced researchers. Fortunately, with the advent of Meta AI‚Äôs large language model, accessing scientific knowledge has never been easier.
In this tutorial, I‚Äôll show you how to use Meta AI‚Äôs Galactica to quickly and effectively perform various scientific NLP tasks. We‚Äôll cover topics such as finding relevant citations, generating academic papers, processing multi-modal data (e.g., LaTeX equations, code snippets, chemical formulas, etc.) frequently encountered during research, and more.
Galactica is a 120B parameter large language model trained on a curated scientific corpus. The training data consists not only of massive volumes of scientific literature but also datasets for downstream scientific NLP tasks and special tokens representing scientific phenomena.
Specialized tokenization is an integral part of Galactica as it enables the model to predict citations or process modalities such as protein sequences or SMILES formulas:
Meta AI reports that Galactica‚Äôs generative approach for citation prediction outperforms retrieval approaches, which demonstrates the potential for language models to replace search engines. Also, Galactica beats existing methods on reasoning task benchmarks (e.g., MMLU and MATH) and sets a new state-of-the-art on several downstream scientific NLP tasks (e.g., PubMedQA and MedMCOA).
Despite its powerful capabilities, similar to most language models, Galactica is prone to hallucination; that is, the model outputs nonsensical results in some cases. Therefore, researchers who use Galactica should always fact-check the generated outputs.
Galactica is accesible via the galai Python library. You can download the model with load_model.
Similar to most large language models, Galactica frames every NLP task as text generation. You can use generate to generate text.
You can generate various types of academic literature with Galactica via prompt engineering. If a prompt is designed to resemble a certain kind of document, so will its completion. For paper documents, use Title:.
This functionality is particularly useful when you need a comprehensive survey on a particular topic. Simply design the prompt as Title: TOPIC, A Survey and Galactica will automatically generate one for you.
For Wikipedia-style articles or lecture notes, begin the prompt with #.
Galactica is trained on a large scientific corpus comprising more than 360 million in-context citations and over 50 million unique references normalized across a diverse set of sources. This enables the model to suggest citations and help discover related papers. Citations are represented as [START_REF] TITLE, AUTHOR [END_REF].
To search for a paper that discusses a certain topic, make use of the following prompt ‚Äî PAPER TOPIC [START_REF]. Since the [START_REF] token is appended to the end of the input context, Galactica would treat it as the beginning of a citation and complete the rest.
Instead of explicitly searching for a citation, you can also prompt Galactica to suggest a relevant paper within a document. The model has been trained on numerous academic texts that include representations of implicit citation graphs. Therefore, given a prompt such as TEXT [START_REF], Galactica can automatically suggest citations relevant to the TEXT.
One might have doubts about relying on Galactica for citation prediction since large language models are known to produce contents that are far from factual. However, Meta AI reports that Galactica‚Äôs generative approach outperforms tuned sparse and dense retrieval approaches at citation prediction tasks. Galactica is far from perfect, but the experiments indicate that the model yields better results than traditional search engines.
You can use Galactica for conventional downstream NLP tasks, such as summarization, entity extraction, and question-answering. While general-purpose language models might struggle with scientific terminology or medical jargon, this is not the case with Galactica. For example, Meta AI reports that the model achieves a new state-of-the-art result on the PubMedQA and MedMCOA question-answering benchmarks; both are tasks that require a rigorous understanding of high-level biomedical concepts.
To yield a summary, simply append TLDR: at the end of the document.
You can extract entities from documents in a question-answering format. Design the prompt as follows ‚Äî TEXT\n\nQ:What scientific entities are mentioned in the abstract above?\n\nA:. Depending on the document‚Äôs topic, you can replace scientific entities with a more domain-specific term, such as biomedical entities.
In the paper, the authors prefix questions with Q: or Question:. A typical format is Question: QUERY\n\nAnswer:.
As mentioned above, Galactica can process modalities other than unstructured text, such as LaTeX equations, code, SMILES formula, DNA sequences, and amino acid sequences. The model uses task-specific tokens to support various forms of scientific knowledge. This design enables users to tackle multi-modal tasks that involve interactions between natural language and representations of scientific phenomena.
Mathematical equations are represented as LaTeX. Galactica wraps LaTeX equations with brackets \[ EQUATION \], so to generate mathematical descriptions, make sure to end your prompt with a \[. Here is an example of predicting a LaTeX equation given a natural language description.
If converting natural language to math is possible, so is vice versa. Galactica can fluently translate between modalities, including math-to-natural language. This functionality would especially come in handy when encountering an indecipherable mathematical formula in a research paper.
Interestingly, Galactica has somehow learned to convert mathematical equations into code.
Here is a demonstration of translating a code snippet into a natural language description and a mathematical description.
Galactica can aid the organization of chemical information by providing an interface to SMILES data. SMILES formulas represent chemical structure as a sequence of characters and appear alongside natural language descriptions or IUPAC names (method of naming organic compounds) within the Galactica training corpus. This indicates that the model might have learned to predict IUPAC names given a SMILES formula input.
For IUPAC Name prediction, design a prompt similar to a PubChem document ‚Äî [START_I_SMILES] SMILES_FORMULA [END_I_SMILES]\n\n## Chemical and Physical Properties\n\nThe following are chemical properties for.
Galactica‚Äôs capability to process biological modalities (i.e., DNA and amino acid sequences) could potentially play a role in aiding the organization of biomedical information. For instance, the model can annotate protein sequences with functional keywords. The authors claim that the model learned to match sequences with similar ones it has seen in training, and can use this to predict the functional keywords.
Prompts for protein annotation can be designed as follows ‚Äî [START_AMINO] AMINO_ACID_SEQUENCE [END_AMINO]\n\n## Keywords.
This tutorial provides an overview of how Meta AI‚Äôs Galactica enables users to access scientific knowledge and leverage this knowledge to tackle a variety of scientific NLP tasks. These tasks range from generating academic literature to processing multi-modal data, and all of these tasks could potentially play a role in scientific discovery. Although the model is far from perfect, the experiments show that the model‚Äôs predictions can often outperform traditional methods. The model‚Äôs impressive performance and ease of use are worthy of further study and exploration. In the future, I hope we can continue exploring how large language models like these can aid scientific research and the process of science itself.
NoisyClock
viswajith kn
Kiprono Elijah Koech
in
Towards Data Science
Yong Rhee
Rizwan Asif
Cheng Xi Tsou
in
Geek Culture
Minhaaj Rehman
Imran us Salam
in
Red Buffer
"
293,https://medium.com/@amandinancy16/churn-or-not-churn-analytics-my-thought-process-b0d25523f932,Churn or not (Churn Analytics) ‚Äî My Thought Process,"Please note that the technique used in this project is experimental.
This article is a breakdown of how I arrived at my second Excel project from start to finish. To read details on the insights, please read this report HERE.
The dataset was gotten from the Maven Analytics data playground.
According to the challenge description, I‚Äôm expected to answer the big question: ‚ÄúIdentify high-value customers and churn risks.‚Äù
If you noticed, this big question looks like two big questions: ‚ÄúIdentify high-value customers‚Äù and ‚ÄúIdentity churn risks.‚Äù Answering those two questions in one dashboard or report will be complicated.
Why did I say so?
Churn risk is not the opposite of a high-value customer. Churn risk is the likelihood that a customer will stop patronizing a business. In comparison, a high-value customer has contributed so much to a customer‚Äôs bottom line for the survival and profitability of the company. This is different from an existing customer. Not all existing customers are high-value customers.
It would make sense and be a lot easier to put contrasting characters side by side to compare them. But we can‚Äôt do that here because these two terms are not precisely contradictory.
So what did I do?
I chose to answer one question. If I were to answer both questions, I would do different dashboards and reports for them, but I wanted to do only one report, so I chose to answer one question.
I chose to identify churn risks.
The dataset contained 7043 rows and 38 columns. It was void of duplicate values. However, it had null values. These null values were due to the non-association of the variables to the kind of service (Phone or Internet service) that the customers subscribed to.
For example, if the customer subscribed to Phone service only, the variable ‚ÄúStreaming TV‚Äù will have a null value for this customer. This is because it‚Äôs only those that have internet service that stream TV.
I needed to handle these null values so they won‚Äôt affect my analysis. So for every null value like this, I inputted the word ‚ÄúNone.‚Äù This helped me to distinguish them easily.
Going forward, The columns were made up of these variables:
I conducted intensive research on churn analysis and noticed that when identifying churn risks, it is more effective to divide them into different segments. The major segments are demographics, finance, and location.
It would have been quicker to jump into the dataset, categorize my variables into these three segments, and start my visualizations, but I didn‚Äôt do that. Instead, I figured out the necessary variables. This was possible using Machine learning.
It wasn‚Äôt enough to know the distribution of my dataset. I needed to know the variables that affected the likelihood of a customer churning. With Entropy and Information gain, I could identify these relevant variables.
Entropy is the degree of uncertainty in a dataset. While Information Gain is the magnitude of information, an independent variable can be used to determine a dependent variable. The more the Entropy, the less the Information gain.
Entropy formula(E):
Information gain formula:
The Entropy and Information Gain of each variable were calculated.
The work isn‚Äôt over yet. After calculating the information gain of all the variables in my dataset, I sorted them in descending order to know the variables to prioritize over the other.
Below is a snapshot of these variables and their information gain.
The gender, phone service, and multiple lines had zero information gain. This was a revelation that these variables were contributing nothing to determining the churn risks of customers. Three variables were out. But what about the other 21? There was still too much to visualize.
To still cut down these 21 variables, I used a decision tree.
Starting from the variables with the highest information gain, I checked for variables where one class out of the two classes (Churned and Stayed) had significantly greater proportions than the other class. Only two categorical variables met this condition: Contract type and Offer type.
Notice the clear classifications in this snapshot. The month-to-month contract and offer E had the highest churn proportion than the stayed.
Unlike in this snapshot below, where the ‚ÄúMarried‚Äù variable had more significant proportions of both the ‚Äúyes‚Äù and ‚Äúno‚Äù on one side(stayed).
I treated the continuous variables that had information gain greater than zero differently. For the continuous variables, I checked for the numbers with the highest churn probability. This was easier to identify, and three variables were selected: Total Revenue, Refunds, and Tenure in Months.
I concluded this with the decision tree below:
The decision tree has a hierarchy where more essential variables are placed at the top.
After selecting the relevant variables, it was time to visualize. I chose the histogram for the continuous variables and the normalized stacked chart for the categorical variables. These were charts that would make it easier to communicate to my target audience. I also used a people chart to show my audience what would happen if they didn‚Äôt act.
To see all these, including the insights and recommendations for this project, you can read about it HERE.
Thank you for dedicating your time to go through my thought process in ‚Äúidentifying churn risks‚Äù for the Maven Telecoms company. I‚Äôm super grateful.
Do you think there‚Äôs something else I should have done? Or are you satisfied with how I analyzed this dataset and filtered the relevant metrics from the irrelevant ones? Please comment below. I would love to know what you think about this.
Thank you!
ROOM42
Mayank Srivastava
in
Towards Data Science
Glen Hanson
Andrea Chello
in
The Quant Journey
Equality Hub
in
Equality Hub
Novosolova Alina
in
SAP.iO
Teagan Pease
in
learn data science
lntinfotech
"
294,https://medium.com/towardsdev/predicting-the-premier-league-with-random-forest-fbc7d320a37e,Predicting the Premier League with Random Forest.,"The premier league is extremely hard to predict other than predicting Manchester City to win the premier league, but that can not be classed as a good prediction considering how strong they‚Äôve been for the past 8 years or so!
But what if we could use a machine learning algorithm to predict results with some what decent accuracy, it could provide a very interesting way to look at each gameweek and even to seem like nostradamus to our friends. The short answer is its not that easy and an extremely complicated model is required to provide a decent >50% accuracy return but even so we can try explore approaches and even learn some code along the way.
For this exercise I am going to look at the last 20 games of the premier league 2017/18 season, the season where Man City became centurions and just pipped Liverpool to the title on the last day.
To start off I want to collect all the closing prices provided by the bookmakers. In a previous article I explained the work of (Kaunitz, Zhong, and Kreiner 2017) who concluded the closing odds are a powerful (if not the best) proxy of a teams true probability of winning a match. Also the betting odds are the only information available before a match, of course in game stats like xG and shots could be used but before a match all there is available are the betting odds, the rest is known after the fact.
Betexplorer provides the closing line data via Oddsportal. So to start the script needs to scrape all this data and store in a df.
I will not break down what each segment does but essentially I am retrieving the information of interest from the page. Since the matches are scraped from last to first I can simply reverse the df to get it from start to finish.
The result provides the df below.
The next part I need to change the odds from string to numeric. I set the columns to change in a list and use apply with pd.to_numeric to change only these columns. I then create probabilities of the odds for the model by dividing by 1. Lastly I need to seperate the goals by home and away to create a multi-classification problem with the target of result being what I want to predict.
To turn this into a classification problem with 3 outcomes I use a function that states if a home team outscores away its a home team win, if its level its a draw and if the away team outscores the home team its an away win. I use apply to create this new target column.
Now the dataframe is nearly ready to apply a the random forest. The Home and Away goals are dropped from this dataframe.
I slice the columns taking the last 20 matches as the hold out set. The model will not see this during creation and I will use this to predict the last two game weeks. The rest of the matches will be used to train and test. The result column needs to be dropped from the hold_out also.
Machine learning models dont like text values which the team names are, so a dictionary can be used to recode the team names from 0‚Äì19. I need to do this for both hold out and model file. A lambda function applies the recoding through iteritems.
Random forest (Breiman, 2001) is one of the most powerful ensemble models available. It is essentially a bigger collection of Decision Trees and works by bagging. Bagging works by choosing a random sample of data in a training set with replacement ‚Äî meaning that the individual data points can be chosen more than once. After several data samples are generated, these models are then trained independently, and depending on the type of task ‚Äî i.e. regression or classification ‚Äî the average or majority of those predictions yield a more accurate estimate. This approach is commonly used to reduce variance within a noisy dataset. (IBM Cloud Education 2020)
By training on different samples of the data the Random forest reduces variance and limits the possibility of overfitting. Overfitting is a huge problem in machine learning so the use of a random forest allows us to mitigate against this. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model‚Äôs prediction. The fundamental concept behind random forest is a simple but powerful one ‚Äî the wisdom of crowds. (Yiu , 2018)
To begin the application of the RF I used the train_test_split class from sk-learn with a 80/20 split. Random_state allows for the same results to be provided each time.
I applied parameter tuning via Grid Search as best practice to try get the best model possible. I set the criteria in the parameter_dict and tested each parameter.
This function prints out the results, we can see the best parameters to use below. (Due to the nature of parameter tuning it is possible you get different results) Cross validating the search ten times averages out the scores.
This returns a accuracy score for the training set of Accuracy: 0.5486 and Accuracy: 0.4861 for the test set so I can conclude it hasnt overfitted.Unsurprisingly it is not an exceptional model which shows how hard it is to truely predict football but we will continue on.
I then use the model predict function on the unseen hold out set. I use pd concat to join them together.
Remember when we coded each team to be a numeric value to work with the model? We can reverse that to get the team names back with the nifty piece of code in inv_map. It reverses the dictionary set earlier and by using the same for loops over the columns the teams are back. Finally, I like to use tabulate which prints out the results in a nice form.
The end result is a dataframe with the model predictions. Remember 1 is a home team win, 0 is a draw and 2 is an away victory.
So lets see how the model performed in real life, seeing as its a couple of years ago the results are easily corroborated. The hope is that it gets at least 50% of the matches correct just as the testing showed.
What is amazing is that it is 75% accurate and had man city beat huddersfield with a probability of 92% the score would have been higher. Whats not as amazing is that the model did not predict a single draw and the games involved heavy favourites so it wasnt exactly going out on a whim. It did predict a nice win for Leicester against Arsenal though which is the biggest outsider. Other than that it tended to side with the favourites. Due to the low sample size there is a lot of sample bias and no firm conclusions can be made other than it was a lot of fun!
Improvement
Predicting Premier League is not easy and as we have seen most attempts will not provide high accuracy scores. In my opinion a model should be created to manufacture its own closing probabilities of a team winning and it would yield stronger performance. Details of xG, territory dominance, passing and shots would create a model to price up football better than using an approach used here. I also did not scale but it is not expected to improve model performance drastically here.
Neural networks could be the answer here and I will pursue such a project in the future.
I hope you enjoyed and learnt a few bits to help, maybe you might start a model for this season with new/other metrics. Follow me for more football content and let me know in the comments if you are working on a model of your own! The full code is below.
Robert Sarno
Fraser G.S.
NYU Center for Data Science
in
Center for Data Science
Moses Kim
in
UX Planet
Diyah Dwi Maharani
Aaron Abrahamson
in
Analytics Vidhya
Dario Radeƒçiƒá
in
Towards Data Science
Petrica Leuca
"
295,https://medium.com/@bimo.widyatamoko/multiple-page-shopee-web-scraping-using-selenium-and-python-november-2022-7ab84379479,Multiple Page Shopee Web Scraping Using Selenium and Python (November 2022),"Nowadays web scraping is very common task for multiple purpose, such as marketing, analyzing competitor and just some research. Basically its just a way to ‚Äúcopy and paste‚Äù a lot of things in your computer screen in an automated way, so you dont have to do labour intensive work.
Before we strart, I want to introduce selenium first and why I choose selenium as my tools of choice, here us some feature from selenium:
Selenium is a python library that can scrape dynamic web easily. It is used also used for web automation & testing. Scraping data from the web is a small part of its library.
In this article I will try to do web scraping on the shopee.co.id website by paginating it and exporting it to csv.
some of the data that I will retrieve is:
installation:
here some library that i used in the project
An automated tool called WebDriver is used to test web applications on various browsers. It offers the ability to move between various web page components, including user input, buttons, JavaScript execution, and more. Selenium supplies web drivers for all the different web browsers it supports. However, I find that Selenium works best with Chrome.
You must have webdriver first to be able to run selenium, Visit the selenium documentation, choose your browser, then download the web driver‚Äôs most recent stable version. Move your web driver to a convenient location once it has been downloaded, or for simplicity, simply move it to the folder you are working on.
Define some input:
some option to help you navigate the driver more easily:
Here is some article to discover more about webdriver options and switches
Basic code:
Do a search:
First we have to find the required element, namely the search bar. selenium can use several ways to find elements in a web page such as using XPATH, CSS, CLASS, or TAG NAME
We use send keys to enter the previously entered input and keys.enter to run the search
Make preparations for fetching data from the page:
Because shopee will display data after scrolling, we can overcome this by zooming out using the execute script feature, this feature is used to run javascript manually.
We also have to set up an empty variable to hold all the data from several pages that we will fetch later. notice I used str() to store data in string form because later we will add data in string form as well.
Collect data across multiple pages:
the method I use is to retrieve all the data on the page using driver.page_source and continue to add to it as more pages are added. By using a for-loop, we can loop according to the input number of pages
this time I‚Äôm using CSS selectors, in this section I can‚Äôt directly use the .click() feature for some reasons that is:
Creating yet another empty variable:
Pull the data using Beautifulsoup:
in this section I use beautifulsoup to parse htlm data, simply arrange it so that the data can be navigated and read easily
Using the find_all tool, I can find all the class names (to find out the class name you can use inspect element) in the <div> wrapper as in the code above, similar to using find_elements in selenium.
Enter each data into the index list :
In this section we will enter all the products in all_product into each index, we use find because there is only one class for each product that contains the data we are looking for, to retrieve the text we use .text.
Pay attention to the sales section I use an if-else statement to avoid errors because not all products have sales data.
Then we input all the data into each index, and we append it to the data_dict_list.
Finally, export to csv:
before it can be converted to csv we have to convert the data into a dataframe first
then we can change it to csv so that it can be processed more easily in excel or other tools
So this is one way to do web scraping for various types of websites, from here you can develop it according to your needs and what data will be retrieved.
For the next project is to make the results of the web scraping so that they can be directly processed by cleaning the data using regular expressions and filling in the blanks so that statistics can be directly carried out in order to get interesting conclusions.
for the complete code available on my github, in the test1.py section, it can be directly run for use.
Terimakasih dan selamat mencoba ! ( thanks and good luck )
Tony
in
AWS in Plain English
Mark Spencer IT
Brian Jenney
Ageng Anugrah Wardoyo Putra
Moments Online
in
ILLUMINATION
uday kiran
Ruby Radar
Eyal Estrin
"
296,https://medium.com/@mathcube7/how-to-make-interactive-plots-in-jupyter-notebooks-211c4e18321a,How To Make Interactive Plots in Jupyter Notebooks,"About this series: Applying the Python scientific stack to real-world problems requires a lot of practice. To help you on your journey to mastery, follow this series and solve common (and not-so-common) problems using NumPy, SymPy, SciPy, and matplotlib.
Vijaythapa
Brian Schlining
SOHAM PAUL
ScyllaDB
Tung Nguyen
in
BoltOps
Brian Chen
Manpreet Singh
Jung-Hyun Nam
in
DataSeries
"
297,https://medium.com/better-programming/openais-whisper-tutorial-42140dd696ee,Convert Podcasts to Text With OpenAI‚Äôs Whisper API Using Python,"OpenAI open-sourced Whisper model ‚Äì the State-of-the-Art Speech recognition system.
We tested it and got impressed!
We took the latest RealPython episode for 1h 10 minutes. It took us 56 minutes with a basic CPU to convert the audio file into almost perfect text transcription with the smallest Whisper model.
Next, we show in steps using Whisper in practice with just a few lines of Python code.
This tutorial explains with single code a way to use the Whisper model both on your local machine and in a cloud environment.
The whisper model is available on GitHub. We download it with the following command directly in the Jupyter notebook:
Whisper model uses ffmpeg program as a requirement. Some cloud environments may already include it, but it is likely your local machine needs this program to be installed.
OpenAI refers to multiple ways to install this package and we apply here one of them by using the Scoop package manager. In our opinion ‚Äî it is simple because you get it working by running the below two commands in the terminal window:
The first line installs the Scoop. The second line installs ffmpeg program using the Scoop package manager.
This step was required in the local machine, but we did not need to perform this step in our cloud environment.
We then restarted our kernel to make sure the installation took effect.
We can now import the libraries:
Whisper runs quicker with GPU. We transcribed a podcast of 1h and 10 minutes with Whisper. It took:
We tested GPU availability with the below code. The first line results False, if Cuda compatible Nvidia GPU is not available and True if it is available. The second line of code sets the model to preference GPU whenever it is available.
We can now load the Whisper model. We include two variables to the load_model. The ‚Äúbase‚Äù is the Whisper model name for the version with the 74 million parameters. There are corresponding model names for each model between 39 million to 1.55 billion parameters. Each of the model names and characteristics is referred here. We defined the second ‚Äúdevice‚Äù variable in the previous section. The second line of the code lets us print the model language and a total number of parameters.
We use Whisper model to transcribe a podcast, which we downloaded from internet. The audio file is available in the local directory.
We load this audio using the ‚Äúload_audio‚Äù command. The ‚Äúpad_or_trim‚Äùcommand pads and trims the audio into 30-second slots. The final line creates log-Mel spectrogram to our local device.
The detect_language detects the audio-file language:
We transcribe the audio using the DecodingOptions and the decode command. We can print the first 30 seconds of the audio.
We transcribe the entire audio file with transcribe command and print the results.
Execution of this code will result in the entire transcript of the audio file, which we print here in short parts of it:
The transcription took 4 minutes to produce us with GPU and 56 minutes with CPU.
Thank you for reading.
[1] Radford et al., 2022. Robust Speech Recognition via Large-Scale Weak Supervision. OpenAI.
Michael Roman
Guremre
Zonglin Lyu
Soumyadeep Roy
Souren Hazra
Andrey A.
in
deepset-ai
Joy Machuka
Logan Rakai
in
Cloud Academy
"
298,https://medium.com/timescale/why-sql-beating-nosql-what-this-means-for-future-of-data-time-series-database-348b777b847a,"Why SQL is beating NoSQL, and what this means for the future of data","After years of being left for dead, SQL today is making a comeback. How come? And what effect will this have on the data community?
(Update: #1 on Hacker News! Read the discussion here.)
(Update 2: TimescaleDB is hiring! Open positions in Engineering, Marketing, and Sales. Interested?)
Since the dawn of computing, we have been collecting exponentially growing amounts of data, constantly asking more from our data storage, processing, and analysis technology. In the past decade, this caused software developers to cast aside SQL as a relic that couldn‚Äôt scale with these growing data volumes, leading to the rise of NoSQL: MapReduce and Bigtable, Cassandra, MongoDB, and more.
Yet today SQL is resurging. All of the major cloud providers now offer popular managed relational database services: e.g., Amazon RDS, Google Cloud SQL, Azure Database for PostgreSQL (Azure launched just this year). In Amazon‚Äôs own words, its PostgreSQL- and MySQL-compatible database Aurora database product has been the ‚Äúfastest growing service in the history of AWS‚Äù. SQL interfaces on top of Hadoop and Spark continue to thrive. And just last month, Kafka launched SQL support. Your humble authors themselves are developers of a new time-series database that fully embraces SQL.
In this post we examine why the pendulum today is swinging back to SQL, and what this means for the future of the data engineering and analysis community.
To understand why SQL is making a comeback, let‚Äôs start with why it was designed in the first place.
Our story starts at IBM Research in the early 1970s, where the relational database was born. At that time, query languages relied on complex mathematical logic and notation. Two newly minted PhDs, Donald Chamberlin and Raymond Boyce, were impressed by the relational data model but saw that the query language would be a major bottleneck to adoption. They set out to design a new query language that would be (in their own words): ‚Äúmore accessible to users without formal training in mathematics or computer programming.‚Äù
Think about this. Way before the Internet, before the Personal Computer, when the programming language C was first being introduced to the world, two young computer scientists realized that, ‚Äúmuch of the success of the computer industry depends on developing a class of users other than trained computer specialists.‚Äù They wanted a query language that was as easy to read as English, and that would also encompass database administration and manipulation.
The result was SQL, first introduced to the world in 1974. Over the next few decades, SQL would prove to be immensely popular. As relational databases like System R, Ingres, DB2, Oracle, SQL Server, PostgreSQL, MySQL (and more) took over the software industry, SQL became established as the preeminent language for interacting with a database, and became the lingua franca for an increasingly crowded and competitive ecosystem.
(Sadly, Raymond Boyce never had a chance to witness SQL‚Äôs success. He died of a brain aneurysm 1 month after giving one of the earliest SQL presentations, just 26 years of age, leaving behind a wife and young daughter.)
For a while, it seemed like SQL had successfully fulfilled its mission. But then the Internet happened.
While Chamberlin and Boyce were developing SQL, what they didn‚Äôt realize is that a second group of engineers in California were working on another budding project that would later widely proliferate and threaten SQL‚Äôs existence. That project was ARPANET, and on October 29, 1969, it was born.
But SQL was actually fine until another engineer showed up and invented the World Wide Web, in 1989.
Like a weed, the Internet and Web flourished, massively disrupting our world in countless ways, but for the data community it created one particular headache: new sources generating data at much higher volumes and velocities than before.
As the Internet continued to grow and grow, the software community found that the relational databases of that time couldn‚Äôt handle this new load. There was a disturbance in the force, as if a million databases cried out and were suddenly overloaded.
Then two new Internet giants made breakthroughs, and developed their own distributed non-relational systems to help with this new onslaught of data: MapReduce (published 2004) and Bigtable (published 2006) by Google, and Dynamo (published 2007) by Amazon. These seminal papers led to even more non-relational databases, including Hadoop (based on the MapReduce paper, 2006), Cassandra (heavily inspired by both the Bigtable and Dynamo papers, 2008) and MongoDB (2009). Because these were new systems largely written from scratch, they also eschewed SQL, leading to the rise of the NoSQL movement.
And boy did the software developer community eat up NoSQL, embracing it arguably much more broadly than the original Google/Amazon authors intended. It‚Äôs easy to understand why: NoSQL was new and shiny; it promised scale and power; it seemed like the fast path to engineering success. But then the problems started appearing.
Developers soon found that not having SQL was actually quite limiting. Each NoSQL database offered its own unique query language, which meant: more languages to learn (and to teach to your coworkers); increased difficulty in connecting these databases to applications, leading to tons of brittle glue code; a lack of a third party ecosystem, requiring companies to develop their own operational and visualization tools.
These NoSQL languages, being new, were also not fully developed. For example, there had been years of work in relational databases to add necessary features to SQL (e.g., JOINs); the immaturity of NoSQL languages meant more complexity was needed at the application level. The lack of JOINs also led to denormalization, which led to data bloat and rigidity.
Some NoSQL databases added their own ‚ÄúSQL-like‚Äù query languages, like Cassandra‚Äôs CQL. But this often made the problem worse. Using an interface that is almost identical to something more common actually created more mental friction: engineers didn‚Äôt know what was supported and what wasn‚Äôt.
Some in the community saw the problems with NoSQL early on (e.g., DeWitt and Stonebraker in 2008). Over time, through hard-earned scars of personal experience, more and more software developers joined them.
blog.timescale.com
Initially seduced by the dark side, the software community began to see the light and come back to SQL.
First came the SQL interfaces on top of Hadoop (and later, Spark), leading the industry to ‚Äúback-cronym‚Äù NoSQL to ‚ÄúNot Only SQL‚Äù (yeah, nice try).
Then came the rise of NewSQL: new scalable databases that fully embraced SQL. H-Store (published 2008) from MIT and Brown researchers was one of the first scale-out OLTP databases. Google again led the way for a geo-replicated SQL-interfaced database with their first Spanner paper (published 2012) (whose authors include the original MapReduce authors), followed by other pioneers like CockroachDB (2014).
At the same time, the PostgreSQL community began to revive, adding critical improvements like a JSON datatype (2012), and a potpourri of new features in PostgreSQL 10: better native support for partitioning and replication, full text search support for JSON, and more (release slated for later this year). Other companies like CitusDB (2016) and yours truly (TimescaleDB, released this year) found new ways to scale PostgreSQL for specialized data workloads.
In fact, our journey developing TimescaleDB closely mirrors the path the industry has taken. Early internal versions of TimescaleDB featured our own SQL-like query language called ‚ÄúioQL.‚Äù Yes, we too were tempted by the dark side: building our own query language felt powerful. But while it seemed like the easy path, we soon realized that we‚Äôd have to do a lot more work: e.g., deciding syntax, building various connectors, educating users, etc. We also found ourselves constantly looking up the proper syntax to queries that we could already express in SQL, for a query language we had written ourselves!
One day we realized that building our own query language made no sense. That the key was to embrace SQL. And that was one of the best design decisions we have made. Immediately a whole new world opened up. Today, even though we are just a 5 month old database, our users can use us in production and get all kinds of wonderful things out of the box: visualization tools (Tableau), connectors to common ORMs, a variety of tooling and backup options, an abundance of tutorials and syntax explanations online, etc.
blog.timescale.com
Google has clearly been on the leading edge of data engineering and infrastructure for over a decade now. It behooves us to pay close attention to what they are doing.
Take a look at Google‚Äôs second major Spanner paper, released just four months ago (Spanner: Becoming a SQL System, May 2017), and you‚Äôll find that it bolsters our independent findings.
For example, Google began building on top of Bigtable, but then found that the lack of SQL created problems (emphasis in all quotes below ours):
‚ÄúWhile these systems provided some of the benefits of a database system, they lacked many traditional database features that application developers often rely on. A key example is a robust query language, meaning that developers had to write complex code to process and aggregate the data in their applications. As a result, we decided to turn Spanner into a full featured SQL system, with query execution tightly integrated with the other architectural features of Spanner (such as strong consistency and global replication).‚Äù
Later in the paper they further capture the rationale for their transition from NoSQL to SQL:
The original API of Spanner provided NoSQL methods for point lookups and range scans of individual and interleaved tables. While NoSQL methods provided a simple path to launching Spanner, and continue to be useful in simple retrieval scenarios, SQL has provided significant additional value in expressing more complex data access patterns and pushing computation to the data.
The paper also describes how the adoption of SQL doesn‚Äôt stop at Spanner, but actually extends across the rest of Google, where multiple systems today share a common SQL dialect:
Spanner‚Äôs SQL engine shares a common SQL dialect, called ‚ÄúStandard SQL‚Äù, with several other systems at Google including internal systems such as F1 and Dremel (among others), and external systems such as BigQuery‚Ä¶
For users within Google, this lowers the barrier of working across the systems. A developer or data analyst who writes SQL against a Spanner database can transfer their understanding of the language to Dremel without concern over subtle differences in syntax, NULL handling, etc.
The success of this approach speaks for itself. Spanner is already the ‚Äúsource of truth‚Äù for major Google systems, including AdWords and Google Play, while ‚ÄúPotential Cloud customers are overwhelmingly interested in using SQL.‚Äù
Considering that Google helped initiate the NoSQL movement in the first place, it is quite remarkable that it is embracing SQL today. (Leading some to recently wonder: ‚ÄúDid Google Send the Big Data Industry on a 10 Year Head Fake?‚Äù.)
In computer networking, there is a concept called the ‚Äúnarrow waist,‚Äù describing a universal interface.
This idea emerged to solve a key problem: On any given networked device, imagine a stack, with layers of hardware at the bottom and layers of software on top. There can exist a variety of networking hardware; similarly there can exist a variety of software and applications. One needs a way to ensure that no matter the hardware, the software can still connect to the network; and no matter the software, that the networking hardware knows how to handle the network requests.
In networking, the role of the universal interface is played by Internet Protocol (IP), acting as a connecting layer between lower-level networking protocols designed for local-area network, and higher-level application and transport protocols. (Here‚Äôs one nice explanation.) And (in a broad oversimplification), this universal interface became the lingua franca for computers, enabling networks to interconnect, devices to communicate, and this ‚Äúnetwork of networks‚Äù to grow into today‚Äôs rich and varied Internet.
We believe that SQL has become the universal interface for data analysis.
We live in an era where data is becoming ‚Äúthe world‚Äôs most valuable resource‚Äù (The Economist, May 2017). As a result, we have seen a Cambrian explosion of specialized databases (OLAP, time-series, document, graph, etc.), data processing tools (Hadoop, Spark, Flink), data buses (Kafka, RabbitMQ), etc. We also have more applications that need to rely on this data infrastructure, whether third-party data visualization tools (Tableau, Grafana, PowerBI, Superset), web frameworks (Rails, Django) or custom-built data-driven applications.
Like networking we have a complex stack, with infrastructure on the bottom and applications on top. Typically, we end up writing a lot of glue code to make this stack work. But glue code can be brittle: it needs to be maintained and tended to.
What we need is an interface that allows pieces of this stack to communicate with one another. Ideally something already standardized in the industry. Something that would allow us to swap in/out various layers with minimal friction.
That is the power of SQL. Like IP, SQL is a universal interface.
But SQL is in fact much more than IP. Because data also gets analyzed by humans. And true to the purpose that SQL‚Äôs creators initially assigned to it, SQL is readable.
Is SQL perfect? No, but it is the language that most of us in the community know. And while there are already engineers out there working on a more natural language oriented interface, what will those systems then connect to? SQL.
So there is another layer at the very top of the stack. And that layer is us.
SQL is back. Not just because writing glue code to kludge together NoSQL tools is annoying. Not just because retraining workforces to learn a myriad of new languages is hard. Not just because standards can be a good thing.
But also because the world is filled with data. It surrounds us, binds us. At first, we relied on our human senses and sensory nervous systems to process it. Now our software and hardware systems are also getting smart enough to help us. And as we collect more and more data to make better sense of our world, the complexity of our systems to store, process, analyze, and visualize that data will only continue to grow as well.
Either we can live in a world of brittle systems and a million interfaces. Or we can continue to embrace SQL. And restore balance to the force.
Like this post and interested in learning more?
Follow us here on Medium, check out our GitHub, join our Slack community, and sign up for the community mailing list below. We‚Äôre also hiring!
Suggested reading for those who‚Äôd like to learn more about the history of databases (aka syllabus for the future TimescaleDB Intro to Databases Class):
Christopher Laine
in
IT Dead Inside
Eivind Utnes
in
Thinking Breakthroughs
Jon Chew
in
The Airbnb Tech Blog
Dave Townsend
in
unbounded.io
George Marklow
Handmade Software
in
Analytics Vidhya
John Au-Yeung
in
Level Up Coding
Umangshrestha
in
FAUN Publication
"
299,https://medium.com/jbennetcodes/how-to-rewrite-your-sql-queries-in-pandas-and-more-149d341fc53e,"How to rewrite your SQL queries in Pandas, and more","Fifteen years ago, there were only a few skills a software developer would need to know well, and he or she would have a decent shot at 95% of the listed job positions. Those skills were:
SQL was a go-to tool when you needed to get a quick-and-dirty look at some data, and draw preliminary conclusions that might, eventually, lead to a report or an application being written. This is called exploratory analysis.
These days, data comes in many shapes and forms, and it‚Äôs not synonymous with ‚Äúrelational database‚Äù anymore. You may end up with CSV files, plain text, Parquet, HDF5, and who knows what else. This is where Pandas library shines.
Python Data Analysis Library, called Pandas, is a Python library built for data analysis and manipulation. It‚Äôs open-source and supported by Anaconda. It is particularly well suited for structured (tabular) data. For more information, see http://pandas.pydata.org/pandas-docs/stable/index.html.
All the queries that you were putting to the data before in SQL, and so many more things!
This is the part that can be intimidating for someone used to expressing data questions in SQL terms.
SQL is a declarative programming language: https://en.wikipedia.org/wiki/List_of_programming_languages_by_type#Declarative_languages.
With SQL, you declare what you want in a sentence that almost reads like English.
Pandas‚Äô syntax is quite different from SQL. In Pandas, you apply operations on the dataset, and chain them, in order to transform and reshape the data the way you want it.
We‚Äôre going to need a phrasebook!
A SQL query consists of a few important keywords. Between those keywords, you add the specifics of what data, exactly, you want to see. Here is a skeleton query without the specifics:
SELECT‚Ä¶ FROM‚Ä¶ WHERE‚Ä¶
GROUP BY‚Ä¶ HAVING‚Ä¶
ORDER BY‚Ä¶
LIMIT‚Ä¶ OFFSET‚Ä¶
There are other terms, but these are the most important ones. So how do we translate these terms into Pandas?
First we need to load some data into Pandas, since it‚Äôs not already in database. Here is how:
I got this data at http://ourairports.com/data/.
Here are some SELECT statements. We truncate results with LIMIT, and filter them with WHERE. We use DISTINCT to remove duplicated results.
We join multiple conditions with an &. If we only want a subset of columns from the table, that subset is applied in another pair of square brackets.
By default, Pandas will sort things in ascending order. To reverse that, provide ascending=False.
We know how to filter on a value, but what about a list of values ‚Äî IN condition? In pandas, .isin() operator works the same way. To negate any condition, use ~.
Grouping is straightforward: use the .groupby() operator. There‚Äôs a subtle difference between semantics of a COUNT in SQL and Pandas. In Pandas, .count() will return the number of non-null/NaN values. To get the same result as the SQL COUNT, use .size().
Below, we group on more than one field. Pandas will sort things on the same list of fields by default, so there‚Äôs no need for a .sort_values() in the first example. If we want to use different fields for sorting, or DESC instead of ASC, like in the second example, we have to be explicit:
What is this trickery with .to_frame() and .reset_index()? Because we want to sort by our calculated field (size), this field needs to become part of the DataFrame. After grouping in Pandas, we get back a different type, called a GroupByObject. So we need to convert it back to a DataFrame. With .reset_index(), we restart row numbering for our data frame.
In SQL, you can additionally filter grouped data using a HAVING condition. In Pandas, you can use .filter() and provide a Python function (or a lambda) that will return True if the group should be included into the result.
Let‚Äôs say we did some preliminary querying, and now have a dataframe called by_country, that contains the number of airports per country:
In the next example, we order things by airport_count and only select the top 10 countries with the largest count. Second example is the more complicated case, in which we want ‚Äúthe next 10 after the top 10‚Äù:
Now, given this dataframe or runway data:
Calculate min, max, mean, and median length of a runway:
A reader pointed out that SQL does not have median function. Let‚Äôs pretend you wrote a user-defined function to calculate this statistic (since the important part here is syntactic differences between SQL and Pandas).
You will notice that with this SQL query, every statistic is a column. But with this Pandas aggregation, every statistic is a row:
Nothing to worry about ‚Äîsimply transpose the dataframe with .T to get columns:
Use .merge() to join Pandas dataframes. You need to provide which columns to join on (left_on and right_on), and join type: inner (default), left (corresponds to LEFT OUTER in SQL), right (RIGHT OUTER), or outer (FULL OUTER).
Use pd.concat() to UNION ALL two dataframes:
To deduplicate things (equivalent of UNION), you‚Äôd also have to add .drop_duplicates().
So far, we‚Äôve been selecting things, but you may need to modify things as well, in the process of your exploratory analysis. What if you wanted to add some missing records?
There‚Äôs no such thing as an INSERT in Pandas. Instead, you would create a new dataframe containing new records, and then concat the two:
Now we need to fix some bad data in the original dataframe:
The easiest (and the most readable) way to ‚Äúdelete‚Äù things from a Pandas dataframe is to subset the dataframe to rows you want to keep. Alternatively, you can get the indices of rows to delete, and .drop() rows using those indices:
I need to mention one important thing ‚Äî immutability. By default, most operators applied to a Pandas dataframe return a new object. Some operators accept a parameter inplace=True, so you can work with the original dataframe instead. For example, here is how you would reset an index in-place:
However, the .loc operator in the UPDATE example above simply locates indices of records to updates, and the values are changed in-place. Also, if you updated all values in a column:
or added a new calculated column:
these things would happen in-place.
The nice thing about Pandas is that it‚Äôs more than just a query engine. You can do other things with your data, such as:
to see some really nice charts!
The best medium to share Pandas query results, plots and things like this is Jupyter notebooks (http://jupyter.org/). In facts, some people (like Jake Vanderplas, who is amazing), publish the whole books in Jupyter notebooks: https://github.com/jakevdp/PythonDataScienceHandbook.
It‚Äôs that easy to create a new notebook:
After that: - navigate to localhost:8888- click ‚ÄúNew‚Äù and give your notebook a name- query and display the data- create a GitHub repository and add your notebook (the file with .ipynb extension).
GitHub has a great built-in viewer to display Jupyter notebooks with Markdown formatting.
I hope you are now convinced that Pandas library can serve you as well as your old friend SQL for the purposes of exploratory data analysis ‚Äî and in some cases, even better. It‚Äôs time to get your hands on some data to query!
Zolzaya Luvsandorj
in
Towards Data Science
@pramodAIML
in
The Startup
Kranthi kumar
in
Artificial Intelligence in Plain English
Sreerupa Dutta Auditto
Hayden
Mruduldhawley
Richard Bremner
in
Campaign Monitor Engineering
Portapro Septic Services
"
300,https://medium.com/towards-data-science/ten-sql-concepts-you-should-know-for-data-science-interviews-7acf3e428185,Ten SQL Concepts You Should Know for Data Science Interviews,"SQL is extremely powerful and has a lot of functionality. When it comes to data science interviews, however, there are really only a handful of core concepts that most companies test. These 10 concepts show up the most often because they have the most application in real-life settings.
In this article, I‚Äôm going to go over what I think are the 10 most important SQL concepts that you should focus the majority of your time on when prepping for interviews.
With that said, here we go!
Be sure to subscribe here or to my personal newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
You‚Äôll most likely see many questions that require the use of CASE WHEN statements, and that‚Äôs simply because it‚Äôs such a versatile concept.
It allows you to write complex conditional statements if you want to allocate a certain value or class depending on other variables.
Less commonly known, it also allows you to pivot data. For example, if you have a month column, and you want to create an individual column for each month, you can use CASE WHEN statements to pivot the data.
Example Question: Write an SQL query to reformat the table so that there is a revenue column for each month.
For more questions like this, check out StrataScratch with 100‚Äôs of SQL questions.
SELECT DISTINCT is something that you should always have at the back of your head. It‚Äôs extremely common to use SELECT DISTINCT statements with aggregate functions (which is #3).
For example, if you have a table that shows customer orders, you may be asked to calculate the average number of orders made per customer. In this case, you would want to count the total number of orders over the count of the total number of customers. It may look something like this:
Related to point #2, you should have a strong understanding of aggregate functions like min, max, sum, count, etc‚Ä¶ This also means that you should have a strong understanding of the GROUP BY and HAVING clause. I highly advise that you take the time to go through practice problems because there are some creative ways that aggregate functions can be used.
Example Question: Write a SQL query to find all duplicate emails in a table named Person.
Be sure to subscribe here or to my personal newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
For those who are relatively new to SQL or have not used it in a while, it can be easy to mix up left joins and inner joins. Make sure you clearly understand how each join derives different results. In many interview questions, you‚Äôll be required to do some sort of join, and in some cases, choosing one versus the other is the difference between a right and wrong answer.
Now we‚Äôre getting to the more interesting stuff! A SQL self-join joins a table with itself. You might think that that serves no purpose, but you‚Äôd be surprised at how common this is. In many real-life settings, data is stored in one large table rather than many smaller tables. In such cases, self-joins may be required to solve unique problems.
Let‚Äôs look at an example.
Example Question: Given the Employee table below, write a SQL query that finds out employees who earn more than their managers. For the above table, Joe is the only employee who earns more than his manager.
A subquery, also known as an inner query or a nested query, is a query within a query and is embedded in the WHERE clause. This is a great way to solve unique problems that require multiple queries in sequence in order to produce a given outcome. Subqueries and WITH AS statements are both extremely using when querying, so you should absolutely make sure that you know how to use them.
Example Question: Suppose that a website contains two tables, the Customers table and the Orders table. Write a SQL query to find all customers who never order anything.
String functions are important especially when working with data that isn‚Äôt clean. Thus, companies may test you on string formatting and manipulation to make sure that you know how to manipulate data.
String formatting includes things like:
If you are unsure of any of these, check out Mode‚Äôs tutorial on string functions for cleaning data.
Be sure to subscribe here or to my personal newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
You should definitely expect some sort of SQL questions that involves date-time data. For example, you may be required to group data by months or convert a variable format from DD-MM-YYYY to simply the month.
Some functions you should know are:
Example Question: Given a Weather table, write a SQL query to find all dates' Ids with higher temperature compared to its previous (yesterday's) dates.
Window Functions allow you to perform an aggregate value on all rows, instead of return only one row (which is what a GROUP BY statement does). It‚Äôs extremely useful if you want to rank rows, calculate cumulative sums, and more.
Example Question: Write a query to get the empno with the highest salary. Make sure your solution can handle ties!
As a bonus, #10 is UNION! While it doesn‚Äôt come up often, you‚Äôll be asked about this the odd time and it‚Äôs good to know in general. If you have two tables with the same columns and you want to combine them, this is when you‚Äôd use UNION.
Again, if you‚Äôre not 100% sure how it works, I would do some quick Googling to learn about it. :)
And that‚Äôs all! I hope that this helps you in your interview prep and I wish you the best of luck in your future endeavors. I‚Äôm sure that if you know these 10 concepts inside-out, you‚Äôll do great when it comes to most SQL problems out there.
Rinu Gour
in
Towards Data Science
Ahmed Gad
in
Towards Data Science
Paul Singman
in
Whispering Data
Terence Shin
in
Towards Data Science
Ng Wai Foong
in
Towards Data Science
Railz
Florian Heinrichs
in
Towards Data Science
Sathishk
"
301,https://medium.com/towards-data-science/ten-sql-concepts-you-should-know-for-data-science-interviews-7acf3e428185,Ten SQL Concepts You Should Know for Data Science Interviews,"SQL is extremely powerful and has a lot of functionality. When it comes to data science interviews, however, there are really only a handful of core concepts that most companies test. These 10 concepts show up the most often because they have the most application in real-life settings.
In this article, I‚Äôm going to go over what I think are the 10 most important SQL concepts that you should focus the majority of your time on when prepping for interviews.
With that said, here we go!
Be sure to subscribe here or to my personal newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
You‚Äôll most likely see many questions that require the use of CASE WHEN statements, and that‚Äôs simply because it‚Äôs such a versatile concept.
It allows you to write complex conditional statements if you want to allocate a certain value or class depending on other variables.
Less commonly known, it also allows you to pivot data. For example, if you have a month column, and you want to create an individual column for each month, you can use CASE WHEN statements to pivot the data.
Example Question: Write an SQL query to reformat the table so that there is a revenue column for each month.
For more questions like this, check out StrataScratch with 100‚Äôs of SQL questions.
SELECT DISTINCT is something that you should always have at the back of your head. It‚Äôs extremely common to use SELECT DISTINCT statements with aggregate functions (which is #3).
For example, if you have a table that shows customer orders, you may be asked to calculate the average number of orders made per customer. In this case, you would want to count the total number of orders over the count of the total number of customers. It may look something like this:
Related to point #2, you should have a strong understanding of aggregate functions like min, max, sum, count, etc‚Ä¶ This also means that you should have a strong understanding of the GROUP BY and HAVING clause. I highly advise that you take the time to go through practice problems because there are some creative ways that aggregate functions can be used.
Example Question: Write a SQL query to find all duplicate emails in a table named Person.
Be sure to subscribe here or to my personal newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
For those who are relatively new to SQL or have not used it in a while, it can be easy to mix up left joins and inner joins. Make sure you clearly understand how each join derives different results. In many interview questions, you‚Äôll be required to do some sort of join, and in some cases, choosing one versus the other is the difference between a right and wrong answer.
Now we‚Äôre getting to the more interesting stuff! A SQL self-join joins a table with itself. You might think that that serves no purpose, but you‚Äôd be surprised at how common this is. In many real-life settings, data is stored in one large table rather than many smaller tables. In such cases, self-joins may be required to solve unique problems.
Let‚Äôs look at an example.
Example Question: Given the Employee table below, write a SQL query that finds out employees who earn more than their managers. For the above table, Joe is the only employee who earns more than his manager.
A subquery, also known as an inner query or a nested query, is a query within a query and is embedded in the WHERE clause. This is a great way to solve unique problems that require multiple queries in sequence in order to produce a given outcome. Subqueries and WITH AS statements are both extremely using when querying, so you should absolutely make sure that you know how to use them.
Example Question: Suppose that a website contains two tables, the Customers table and the Orders table. Write a SQL query to find all customers who never order anything.
String functions are important especially when working with data that isn‚Äôt clean. Thus, companies may test you on string formatting and manipulation to make sure that you know how to manipulate data.
String formatting includes things like:
If you are unsure of any of these, check out Mode‚Äôs tutorial on string functions for cleaning data.
Be sure to subscribe here or to my personal newsletter to never miss another article on data science guides, tricks and tips, life lessons, and more!
You should definitely expect some sort of SQL questions that involves date-time data. For example, you may be required to group data by months or convert a variable format from DD-MM-YYYY to simply the month.
Some functions you should know are:
Example Question: Given a Weather table, write a SQL query to find all dates' Ids with higher temperature compared to its previous (yesterday's) dates.
Window Functions allow you to perform an aggregate value on all rows, instead of return only one row (which is what a GROUP BY statement does). It‚Äôs extremely useful if you want to rank rows, calculate cumulative sums, and more.
Example Question: Write a query to get the empno with the highest salary. Make sure your solution can handle ties!
As a bonus, #10 is UNION! While it doesn‚Äôt come up often, you‚Äôll be asked about this the odd time and it‚Äôs good to know in general. If you have two tables with the same columns and you want to combine them, this is when you‚Äôd use UNION.
Again, if you‚Äôre not 100% sure how it works, I would do some quick Googling to learn about it. :)
And that‚Äôs all! I hope that this helps you in your interview prep and I wish you the best of luck in your future endeavors. I‚Äôm sure that if you know these 10 concepts inside-out, you‚Äôll do great when it comes to most SQL problems out there.
Rinu Gour
in
Towards Data Science
Ahmed Gad
in
Towards Data Science
Paul Singman
in
Whispering Data
Terence Shin
in
Towards Data Science
Ng Wai Foong
in
Towards Data Science
Railz
Florian Heinrichs
in
Towards Data Science
Sathishk
"
302,https://medium.com/towards-data-science/how-to-build-a-data-science-portfolio-5f566517c79c,How to Build a Data Science Portfolio,"How do you get a job in data science? Knowing enough statistics, machine learning, programming, etc to be able to get a job is difficult. One thing I have found lately is quite a few people may have the required skills to get a job, but no portfolio. While a resume matters, having a portfolio of public evidence of your data science skills can do wonders for your job prospects. Even if you have a referral, the ability to show potential employers what you can do instead of just telling them you can do something is important. This post will include links to where various data science professionals (data science managers, data scientists, social media icons, or some combination thereof) and others talk about what to have in a portfolio and how to get noticed. With that, let‚Äôs get started!
Besides the benefit of learning by making a portfolio, a portfolio is important as it can help get you employment. For the purpose of this article, let‚Äôs define a portfolio as public evidence of your data science skills. I got this definition from David Robinson Chief Data Scientist at DataCamp when he was interviewed by Marissa Gemma on Mode Analytics blog. He was asked about landing his first job in industry and said,
The most effective strategy for me was doing public work. I blogged and did a lot of open source development late in my PhD, and these helped give public evidence of my data science skills. But the way I landed my first industry job was a particularly noteworthy example of the public work. During my PhD I was an active answerer on the programming site Stack Overflow, and an engineer at the company came across one of my answers (one explaining the intuition behind the beta distribution). He was so impressed with the answer that he got in touch with me [through Twitter], and a few interviews later I was hired.
You may think of this as a freak occurrence, but you will often find that the more active you are, the greater chance you have of something like this occuring. From David‚Äôs blog post,
The more public work you do, the higher the chance of a freak accident like that: of someone noticing your work and pointing you towards a job opportunity, or of someone who‚Äôs interviewing you having heard of work you‚Äôve done.
People often forget that software engineers and data scientists also Google their issues. If these same people have their problems solved by reading your public work, they might think better of you and reach out to you.
Even for an entry level role, most companies want to have people with at least a little bit of real life experience. You may have seen memes like the one below.
The question is how do you get experience if you need experience to get your first job? If there is an answer, the answer is projects. Projects are perhaps the best substitutes for work experience or as Will Stanton said,
If you don‚Äôt have any experience as a data scientist, then you absolutely have to do independent projects.
In fact, when Jacqueline Nolis interviews candidates, she wants to hear about a description of a recent problem/project that you have faced.
I want to hear about a project they‚Äôve worked on recently. I ask them about how the project started, how they determined it was worth time and effort, their process, and their results. I also ask them about what they learned from the project. I gain a lot from answers to this question: if they can tell a narrative, how the problem related to the bigger picture, and how they tackled the hard work of doing something.
If you don‚Äôt have some data science related work experience, the best option here is to talk about a data science project that you have worked on.
Data science is such a broad field that it is hard to know what kind of projects hiring managers want to see. William Chen, a Data Science Manager at Quora, shared his thoughts on the subject at Kaggle‚Äôs CareerCon 2018 (video).
I love projects where people show that they are interested in data in a way that goes beyond homework assignments. Any sort of class final project where you explore an interesting dataset and find interesting results‚Ä¶ Put effort into the writeup‚Ä¶ I really like seeing really good writeups where people find interesting and novel things‚Ä¶have some visualizations and share their work.
A lot of people recognize the value of creating projects, but one issue a lot of people wonder is where do you get that interesting dataset and what do you do with it. Jason Goodman, Data Scientist at Airbnb, has a post Advice on Building Data Portfolio Projects where he talks about many different project ideas and has good advice on what kind of datasets you should use. He also echos one of William‚Äôs points about working with interesting data.
I find that the best portfolio projects are less about doing fancy modeling and more about working with interesting data. A lot of people do things with financial information or Twitter data; those can work, but the data isn‚Äôt inherently that interesting, so you‚Äôre working uphill.
One of his other points in the article is that webscraping is a great way to get interesting data. If you are interested in learning how to build your own dataset by webscraping in Python, you can see my post here. If you are coming from academia, it is important to note that your thesis can count as a project (a very large project). You can hear William Chen talk about it here.
One thing I have found very common (to the point of it appearing multiple times in this blog post) in a lot of portfolio/resume advice is not to have common projects in your portfolio.
Jeremie Harris in The 4 fastest ways not to get hired as a data scientist said,
It‚Äôs hard to think of a faster way to have your resume thrown into the ‚Äòdefinite no‚Äô pile than featuring work you did on trivial proof-of-concept datasets among your highlighted personal projects.
When in doubt, here are some projects that hurt you more than they help you:
* Survival classification on the Titanic dataset.
* Hand-written digit classification on the MNIST dataset.
* Flower species classification using the iris dataset.
The image below shows partial examples of classification of Titanic (A), MNIST (B), and iris (C) datasets. There aren‚Äôt a lot of ways to use these datasets to distinguish yourself from other applicants. Make sure to list novel projects.
Favio Vazquez has an excellent article where he talked about how he got his job as a data scientist. Of course, one of his tips is to have a portfolio.
Have a portfolio. If you are looking for a serious paid job in data science do some projects with real data. If you can post them on GitHub. Apart from Kaggle competitions, find something that you love or a problem you want to solve and use your knowledge to do it.
One of the other interesting findings was that you always have to keep on improving as you go through the job hunt.
I applied to almost 125 jobs (for real, maybe you applied for much more), I got only like 25‚Äì30 replies. Some of them were just: Thanks but nope. And I got almost 15 interviews. I learned from each one. Got better. I had to deal with a lot of rejection. Something I was actually not prepared to. But I loved the process of getting interviewed (not all of them to be honest). I studied a lot, programmed everyday, read a lot of articles and posts. They helped a lot.
As you learn more and improve yourself, your portfolio should also be updated. This same sentiment is echoed in many other advice articles. As Jason Goodman said,
The project isn‚Äôt done when you post it publicly. Don‚Äôt be afraid to keep adding on to or editing your projects after they‚Äôre published!
This advice is especially true when you are looking for a job. There are many stories of successful people like Kelly Peng, Data Scientist at Airbnb, who really persevered and kept on working and improving. In one of her blog posts, she went over how many places she applied for and interviewed with.
Applications: 475
Phone interviews: 50
Finished data science take-home challenges: 9
Onsite interviews: 8
Offers: 2
Time spent: 6 months
She clearly applied to a lot of jobs and kept on persisting. In her article, she even mentions how you need to keep on learning from your interviewing experiences.
Take note of all the interview questions you got asked, especially those questions you failed to answer. You can fail again, but don‚Äôt fail at the same spot. You should always be learning and improving.
One of the ways someone finds your portfolio is often through your resume so it is worth a mention. A data science resume is a place to focus on your technical skills. Your resume is a chance to succinctly represent your qualifications and fit for that particular role. Recruiters and hiring managers skim resumes very quickly, and you only have a short time to make an impression. Improving your resume can increase your chance of getting an interview. You have to make sure every single line and every single section of your resume counts.
William Chen, a Data Science Manager from Quora has 9 Tips for making your data science resume. Notice in the brief summary of his points below, that projects and portfolio are points 6, 7, 8, and arguably 9.
2. Objective: Don‚Äôt include one. They don‚Äôt help you distinguish yourself from other people. They take away space from the more important things (skills, projects, experience etc). Cover letters are extremely optional unless you really personalize it.
3. Coursework: Do list relevant coursework that is applicable for the job description.
4. Skills: Don‚Äôt give numerical ratings for your skills. If you want to rate yourself on your skills, use words like proficient or familiar or things like that. You can even exclude assessments altogether.
5. Skills: Do list technical skills that the job description mentions. The order you list your skills in can suggest what you are best at.
6. Projects: Don‚Äôt list common projects or homework. They aren‚Äôt that helpful in distinguishing you from other applicants. List projects that are novel.
7. Projects: Show results and include links. If you participated in Kaggle competition, put percentile rank as it helps the person reading your resume understand where you are in the competition. In projects sections, there is always room for links to writeups and papers as they let the hiring manager or recruiter dig in deeper (bias to real world messy problems where you learn something new).
Notice that in one of the projects sections above, a person has an additional link to a blog that lets the recruiter or hiring manager find out more. This is one way to link to various parts of your portfolio from your resume.
8. Portfolio: Fill our your online presence. The most basic is a LinkedIn profile. It is kind of like an extended resume. Github and Kaggle profiles can help show off your work. Fill out each profile and include links to other sites. Fill out descriptions for your GitHub respositories. Include links to your knowledge sharing profiles/blog (medium, quora). Data science specifically is about knowledge sharing and communicating what the data means to other people. You don‚Äôt have to do all of them, but pick a few and do it (More on this later).
9. Experience: Tailor your experience towards the job. Experience is the core of your resume, but if you don‚Äôt have work experience what do you do? Focus your resume on independent projects, like capstone projects, independent research, thesis work, or Kaggle competitions. These are substitutes for work experience if you don‚Äôt have work experience to put on your resume. Avoid putting irrelevant experience on your resume.
If you want to know hear data science managers go over portfolios and resumes, here are links to Kaggle‚Äôs CareerCon 2018 (video, resumes reviewed).
This is very similar to the Importance of a Portfolio section, just divided into subsections. Having a Github page, a Kaggle profile, a Stack Overflow, etc can provide support for your resume. Having online profiles filled out can be a good signal for hiring managers.
As David Robinson phrases it,
Generally, when I‚Äôm evaluating a candidate, I‚Äôm excited to see what they‚Äôve shared publicly, even if it‚Äôs not polished or finished. And sharing anything is almost always better than sharing nothing.
The reason why data scientists like seeing public work is as Will Stanton said,
Data scientists use these tools to share their own work and find answers to questions. If you use these tools, then you are signaling to data scientists that you are one of them, even if you haven‚Äôt ever worked as a data scientist.
A lot of Data science is about communication and presenting data so it is good to have these online profiles. Besides from the fact that these platforms help provide valuable experience, they can also help you get noticed and lead people to your resume. People can and do find your resume online through various sources (LinkedIn, GitHub, Twitter, Kaggle, Medium, Stack Overflow, Tableau Public, Quora, Youtube, etc). You will even find that different types of social media feed into eachother.
A Github profile is a powerful signal that you are a competent data scientist. In the projects section of a resume, people often leave links to their GitHub where the code is stored for their projects. You can also have writeups and markdown there. GitHub lets people see what you have built and how you have built it. At some companies, hiring managers look at an applicants GitHub. It is another way to show employers you aren‚Äôt a false positive. If you take the time to develop your GitHub profile, you can be better evaluated than others.
It is worth mentioning that you need to have some sort of README.md with a description of your project as a lot of data science is about communicating results. Make sure the README.md file clearly describes what your project is, what it does, and how to run your code.
Participating in Kaggle competitions, creating a kernel, and contributing to discussions are ways to show some competency as a data scientist. It is important to emphasize that Kaggle is not like an industry project as Colleen Farrelly, mentions in this quora question. Kaggle competitions take care of coming up with a task, acquire data for you, and clean it into some usable form. What it does is give you practice analyzing data and coming up with a model. Note that there is a good reason why Kaggle Grandmasters continue to participate in Kaggle competitions. Reshama Shaikh has a post To Kaggle Or Not where she talked about the value of Kaggle competitions. From her post,
It is true, doing one Kaggle competition does not qualify someone to be a data scientist. Neither does taking one class or attending one conference tutorial or analyzing one dataset or reading one book in data science. Working on competition(s) adds to your experience and augments your portfolio. It is a complement to your other projects, not the sole litmus test of one‚Äôs data science skillset.
I completely agree with Reshama‚Äôs view on this. In particular, the point about how taking a class in something doesn‚Äôt make you an expert in something nor does it give you a job. I literally have made a course called Python for Data Visualization and I go into extensive depth about Pandas, Matplotlib, and Seaborn. It wont immediately give you a job or make you an immediate expert in Matplotlib or Seaborn, but it will make your knowledge greater, teach you how the libraries work, and aid in building your portfolio. Everything you do can make you more employable.
Unlike a resume, which is confined by length, a LinkedIn profile allows you to describe your projects and work experience in more depth. Udacity has a guide on making a good LinkedIn profile. An important part of LinkedIn is their search tool and for you to show up, you must have relevant keywords in your profile. Recruiters often search for people on LinkedIn. LinkedIn allows you to see which companies have searched for you and who has viewed your profile.
Besides companies finding you and sending you messages on your availability, LinkedIn also has many features like Ask for a Referral. Jason Goodman in his article Advice on Applying to Data Science Jobs uses LinkedIn to indirectly ask for referrals.
I never, never, never applied to any companies without an introduction to someone who worked at the company‚Ä¶once I was interested in a company, I would use LinkedIn to find a first- or second- degree connection at the company. I would write to that connection, asking to talk to them about their experience at the company and, if possible, whether they‚Äôd be able to connect me to someone on the Data Science team. Whenever I could, I did in-person meetings (coffee or lunch) instead of phone calls. As an aside, Trey Causey recently wrote a great post on how to ask for just these kinds of meetings. I would never ask for a job directly, but they would usually ask for my resume and offer to submit me as an internal referral, or put me in touch with a hiring manager. If they didn‚Äôt seem comfortable doing so...I‚Äôd just thank them for their time and move on.
Notice that he doesn‚Äôt right away ask for a referral. While common job advice when applying to a company is to get a referral, it is VERY IMPORTANT to note that you still need a portfolio, experience, or some sort of proof you can do a job. Jason even mentions the importance of a portfolio in that and other articles he has written.
Aman Dalmia learned something similar by Interviewing at Multiple AI Companies and Startups.
Networking is NOT messaging people to place a referral for you. When I was starting off, I did this mistake way too often until I stumbled upon an article that talked about the importance of building a real connection with people by offering our help first.
One other point he had is that LinkedIn is great for getting your content/portfolio out.
Another important step in networking is to get your content out. For example, if you‚Äôre good at something, blog about it and share that blog on Facebook and LinkedIn. Not only does this help others, it helps you as well.
Having some form of blog can be highly beneficial. A lot of data science is about communication and presenting data. Blogging is a way of practicing this and showing you can do this. Writing about a project or a data science topic allows you to share with the community as well as encourages you to write out your work process and thoughts. This is a useful skill when interviewing.
As David Robinson said,
A blog is your chance to practice the relevant skills.
By writing a blog, you can practice communicate findings to others. It also is another form of advertising yourself. Blogs about Using Scrapy to Build your Own Dataset, and ironically Python Environment Management with Conda have taught me a lot and have gotten me a lot of opportunities I would normally not have gotten. Recently, my boxplot blog brought me the opportunity to create my own Python for Data Visualization course. One of the major benefits I have found is that throughout the process of people critiquing my projects and suggesting improvements (though the comments section of the blog) makes it so interviewers aren‚Äôt the first ones pointing out these same flaws. The more obvious benefit is that by making a blog you tend to read a lot more data science/machine learning blog posts and hence learn more.
As for what platform to blog on, I recommend using Medium. Manali Shinde in her blog post How to Construct a Data Science Portfolio from Scratch had a really good point on why she choose Medium for her blog.
I thought of creating my own website on a platform such as WordPress or Squarespace. While those platforms are amazing to host your own portfolio, I wanted a place where I would get some visibility, and a pretty good tagging system to reach greater audiences. Luckily Medium, as we know, has those options (and it‚Äôs also free).
If you don‚Äôt know what to write about, I suggest you look at David Robinson‚Äôs advice.
Being active on Twitter is a great way to identify and interact with people in your field. You can also promote your blog on Twitter so that your portfolio can be that much more visible. There are so many opportunities to interact with people on twitter. One of them as Reshama Shaikh said in her famous blog post ‚ÄúHow Do I Get My First Data Science Job?‚Äù was,
David Robinson generously offers to retweet your first data science post. With 20K+ followers, that‚Äôs an offer that can‚Äôt be refused.
Twitter can be used for other things than self promotion. Data Science Renee has a post ‚ÄúHow to use Twitter to Learn Data Science (or Anything)‚Äù that is quite insightful about taking Twitter to learn skills. One other takeaway from her article was how much her Twitter presence helped her network and get opportunities.
I have been asked to be interviewed on podcasts and blogs (some of those should be coming up soon), offered contract work, and offered free admission to a conference I unfortunately couldn‚Äôt go to, but was excited to be considered for. ‚ÄúFamous‚Äù people in the industry are now coming to me to work with them in some way.
Not every data science job uses Tableau or other BI tools. However, if you are applying to jobs where these tools are used, it is important to note that there are websites where you can put dashboards for public consumption. For example, if you say you are learning or know Tableau, put a couple dashboards on Tableau Public. While a lot of companies might be okay with you learning Tableau on the job, having public evidence of your Tableau skill can help. If you want to see good examples of Tableau Public profiles, please see Orysya Stus‚Äô and Brit Cava‚Äôs profiles.
Having a strong resume has long been the primary tool for job seekers to relay their skills to potential employers. These days, there is more than one way to showoff your skills and get a job. A portfolio of public evidence is a way to get opportunities that you normally wouldn‚Äôt get. It is important to emphasize that a portfolio is an iterative process. As your knowledge grows, your portfolio should be updated over time. Never stop learning or growing. Even this blog post will be updated with feedback and with increasing knowledge. If you want interview advice/guides/courses, time to check out Brandon Rohrer‚Äôs advice on how to survive a data science interview, Sadat‚Äôs interview guide, or my 15 Tips for Landing a Data Science Job course. If you want some general data science career advice, I wrote an article on it here. If you have any questions or thoughts on the tutorial, feel free to reach out in the comments below or through Twitter.
TheStartupFounder.com
Jay Zhengzhe Jia
Kurtis Pykes
in
Towards Data Science
Rekha M
in
Clairvoyant Blog
Chinwe O.
Daria Rostovtseva
in
Snowflake
TheStartupFounder.com
Naser Tamimi
in
Towards Data Science
"
303,https://medium.com/free-code-camp/if-you-want-to-learn-data-science-start-with-one-of-these-programming-classes-fb694ffe780c,"If you want to learn Data Science, start with one of these programming classes","A year ago, I was a numbers geek with no coding background. After trying an online programming course, I was so inspired that I enrolled in one of the best computer science programs in Canada.
Two weeks later, I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. So I dropped out.
The decision was not difficult. I could learn the content I wanted to faster, more efficiently, and for a fraction of the cost.
I already had a university degree and, perhaps more importantly, I already had the university experience. Paying $30K+ to go back to school seemed irresponsible.
I started creating my own data science master‚Äôs degree using online courses shortly afterwards, after realizing it was a better fit for me than computer science. I scoured the introduction to programming landscape. I‚Äôve already taken several courses and audited portions of many others. I know the options, and what skills are needed if you‚Äôre targeting a data analyst or data scientist role.
For this guide, I spent 20+ hours trying to find every single online introduction to programming course offered as of August 2016, extracting key bits of information from their syllabi and reviews, and compiling their ratings. For this task, I turned to none other than the open source Class Central community and its database of thousands of course ratings and reviews.
Since 2011, Class Central founder Dhawal Shah has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources.
Each course had to fit four criteria:
We believe we covered every notable course that exists and which fits the above criteria. Since there are seemingly hundreds of courses on Udemy in Python and R, we chose to consider the most reviewed and highest rated ones only. There is a chance we missed something, however. Please let us know if you think that is the case.
We compiled average rating and number of reviews from Class Central and other review sites. We calculated a weighted average rating for each course. If a series had multiple courses (like Rice University‚Äôs Part 1 and Part 2), we calculated the weighted average rating across all courses. We also read text reviews and used this feedback to supplement the numerical ratings.
We made subjective syllabus judgment calls based on three factors:
Programming is not computer science and vice versa. There is a difference of which beginners may not be acutely aware. Borrowing this answer from Programmers Stack Exchange:
Computer science is the study of what computers [can] do; programming is the practice of making computers do things.
The course we are looking for introduces programming and optionally touches on relevant aspects of computer science that would benefit a new programmer in terms of awareness. Many of the courses considered, you‚Äôll notice, do indeed have a computer science portion.
None of the courses, however, are strictly computer science courses, which is why something like Harvard‚Äôs CS50x on edX is excluded.
University of Toronto‚Äôs ‚ÄúLearn to Program‚Äù series on Coursera. LTP1: The Fundamentals and LTP2: Crafting Quality Code have a near-perfect weighted average rating of 4.71 out of 5 stars over 284 reviews. They also have a great mix of content difficulty and scope for the beginner data scientist.
This free, Python-based introduction to programming sets itself apart from the other 20+ courses we considered.
Jennifer Campbell and Paul Gries, two associate professors in the University of Toronto‚Äôs department of computer science (which is regarded as one of the best in the world) teach the series. The self-paced, self-contained Coursera courses match the material in their book, ‚ÄúPractical Programming: An Introduction to Computer Science Using Python 3.‚Äù LTP1 covers 40‚Äì50% of the book and LTP2 covers another 40%. The 10‚Äì20% not covered is not particularly useful for data science, which helped their case for being our pick.
The professors kindly and promptly sent me detailed course syllabi upon request, which were difficult to find online prior to the course‚Äôs official restart in September 2016.
Learn to Program: The Fundamentals (LTP1)
Timeline: 7 weeks
Estimated time commitment: 6‚Äì8 hours per week
This course provides an introduction to computer programming intended for people with no programming experience. It covers the basics of programming in Python including elementary data types (numeric types, strings, lists, dictionaries, and files), control flow, functions, objects, methods, fields, and mutability.
Modules
Learn to Program: Crafting Quality Code (LTP2)
Timeline: 5 weeks
Estimated time commitment: 6‚Äì8 hours per week
You know the basics of programming in Python: elementary data types (numeric types, strings, lists, dictionaries, and files), control flow, functions, objects, methods, fields, and mutability. You need to be good at these in order to succeed in this course.
LTP: Crafting Quality Code covers the next steps: designing larger programs, testing your code so that you know it works, reading code in order to understand how efficient it is, and creating your own types.
Modules
Associate professor Gries also provided the following commentary on the course structure: ‚ÄúEach module has between about 45 minutes to a bit more than an hour of video. There are in-video quiz questions, which will bring the total time spent studying the videos to perhaps 2 hours.‚Äù
These videos are generally shorter than ten minutes each.
He continued: ‚ÄúIn addition, we have one exercise (a dozen or two or so multiple choice and short-answer questions) per module, which should take an hour or two. There are three programming assignments in LTP1, each of which might take four to eight hours of work. There are two programming assignments in LTP2 of similar size.‚Äù
He emphasized that the estimate of 6‚Äì8 hours per week is a rough guess: ‚ÄúEstimating time spent is incredibly student-dependent, so please take my estimates in that context. For example, someone who knows a bit of programming, perhaps in another programming language, might take half the time of someone completely new to programming. Sometimes someone will get stuck on a concept for a couple of hours, while they might breeze through on other concepts ‚Ä¶ That‚Äôs one of the reasons the self-paced format is so appealing to us.‚Äù
In total, the University of Toronto‚Äôs Learn to Program series runs an estimated 12 weeks at 6‚Äì8 hours per week, which is about standard for most online courses created by universities. If you prefer to binge-study your MOOCs, that‚Äôs 72‚Äì96 hours, which could feasibly be completed in two to three weeks, especially if you have a bit of programming experience.
If you already have some familiarity with programming, and don‚Äôt mind a syllabus that has a notable skew towards games and interactive applications, I would also recommend Rice University‚Äôs An Introduction to Interactive Programming in Python (Part 1 and Part 2) on Coursera.
With 6,000+ reviews and the highest weighted average rating of 4.93/5 stars, this popular course is noted for its engaging videos, challenging quizzes, and enjoyable mini projects. It‚Äôs slightly more difficult, and focuses less on the fundamentals and more on topics that aren‚Äôt applicable in data science than our #1 pick.
These courses are also part of the 7 course Principles in Computing Specialization on Coursera.
The materials are self-paced and free, and a paid certificate is available. The course must be purchased for $79 (USD) for access to graded materials.
The condensed course description and full syllabus are as follows:
‚ÄúThis two-part course is designed to help students with very little or no computing background learn the basics of building simple interactive applications ‚Ä¶ To make learning Python easy, we have developed a new browser-based programming environment that makes developing interactive applications in Python simple. These applications will involve windows whose contents are graphical and respond to buttons, the keyboard, and the mouse.
Recommended background: A knowledge of high school mathematics is required. While the class is designed for students with no prior programming experience, some beginning programmers have viewed the class as being fast-paced. For students interested in some light preparation prior to the start of class, we recommend a self-paced Python learning site such as codecademy.com.‚Äù
Timeline: 5 weeks
Estimated time commitment: 7‚Äì10 hours per week
Week 0 ‚Äî statements, expressions, variables  Understand the structure of this class, and explore Python as a calculator.
Week 1 ‚Äî functions, logic, conditionals Learn the basic constructs of Python programming, and create a program that plays a variant of Rock-Paper-Scissors.
Week 2 ‚Äî event-driven programming, local/global variables Learn the basics of event-driven programming, understand the difference between local and global variables, and create an interactive program that plays a simple guessing game.
Week 3 ‚Äî canvas, drawing, timers Create a canvas in Python, learn how to draw on the canvas, and create a digital stopwatch.
Week 4 ‚Äî lists, keyboard input, the basics of modeling motion Learn the basics of lists in Python, model moving objects in Python, and recreate the classic arcade game ‚ÄúPong.‚Äù
Week 5 ‚Äî mouse input, list methods, dictionaries Read mouse input, learn about list methods and dictionaries, and draw images. Week 6 ‚Äî classes and object-oriented programming Learn the basics of object-oriented programming in Python using classes, and work with tiled images.
Week 7 ‚Äî basic game physics, sprites Understand the math of acceleration and friction, work with sprites, and add sound to your game.
Week 8 ‚Äî sets and animation Learn about sets in Python, compute collisions between sprites, and animate sprites.
If you are set on an introduction to programming course in R, we recommend DataCamp‚Äôs series of R courses: Introduction to R, Intermediate R, Intermediate R ‚Äî Practice, and Writing Functions in R. Though the latter three come at a price point of $25/month, DataCamp is best in category for covering the programming fundamentals and R-specific topics, which is reflected in its average rating of 4.29/5 stars.
We believe the best approach to learning programming for data science using online courses is to do it first through Python. Why? There is a lack of MOOC options that teach core programming principles and use R as the language of instruction. We found six such R courses that fit our testing criteria, compared to twenty-two Python-based courses. Most of the R courses didn‚Äôt receive great ratings and failed to meet most of our subjective testing criteria.
The series breakdown is as follows:
Estimated time commitment: 4 hours
Chapters:
Estimated time commitment: 6 hours
Chapters:
Estimated time commitment: 4 hours
This follow-up course on intermediate R does not cover new programming concepts. Instead, you will strengthen your knowledge of the topics in intermediate R with a bunch of new and fun exercises.
Estimated time commitment: 4 hours
Chapters:
Another option for R would be to take a Python-based introduction to programming course to cover the fundamentals of programming, and then pick up R syntax with an R basics course. This is what I did, but I did it with Udacity‚Äôs Data Analysis with R. It worked well for me.
You can also pick up R with our top recommendation for a statistics class, which teaches the basics of R through coding up stats problems.
Our #1 and #2 picks had a 4.71 and 4.93 star weighted average rating over 284 and 6,069 reviews, respectively. Let‚Äôs look at the other alternatives.
This is the first of a six-piece series that covers the best MOOCs for launching yourself into the data science field. It will cover several other data science core competencies: statistics, the data science process, data visualization, and machine learning.
medium.freecodecamp.com
medium.freecodecamp.com
The final piece will be a summary of those courses, and the best MOOCs for other key topics such as data wrangling, databases, and even software engineering.
If you‚Äôre looking for a complete list of Data Science MOOCs, you can find them on Class Central‚Äôs Data Science and Big Data subject page.
If you enjoyed reading this, check out some of Class Central‚Äôs other pieces:
medium.freecodecamp.com
medium.freecodecamp.com
If you have suggestions for courses I missed, let me know in the responses!
If you found this helpful, click the üíö so more people will see it here on Medium.
This is a condensed version of the original article published on Class Central, where course descriptions, syllabi, and multiple reviews are included.
Yu Wei Chung
Exaptive
Jose Almeida
Sweta Garai
in
MLearning.ai
Nathan Cooper Jones
in
ShopRunner
Simran Gujrati
Jose Almeida
Tapad Engineering
in
Tapad Engineering
"
304,https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471,Machine Learning is Fun!,"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in Êó•Êú¨Ë™û, Portugu√™s, Portugu√™s (alternate), T√ºrk√ße, Fran√ßais, ÌïúÍµ≠Ïñ¥ , ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé‚Äé, Espa√±ol (M√©xico), Espa√±ol (Espa√±a), Polski, Italiano, ÊôÆÈÄöËØù, –†—É—Å—Å–∫–∏–π, ÌïúÍµ≠Ïñ¥ , Ti·∫øng Vi·ªát or ŸÅÿßÿ±ÿ≥€å.
Giant update: I‚Äôve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!
Have you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let‚Äôs change that!
This guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading the wikipedia article, got frustrated and gave up wishing someone would just give them a high-level explanation. That‚Äôs what this is.
The goal is be accessible to anyone ‚Äî which means that there‚Äôs a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished.
Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.
For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It‚Äôs the same algorithm but it‚Äôs fed different training data so it comes up with different classification logic.
‚ÄúMachine learning‚Äù is an umbrella term covering lots of these kinds of generic algorithms.
You can think of machine learning algorithms as falling into one of two main categories ‚Äî supervised learning and unsupervised learning. The difference is simple, but really important.
Let‚Äôs say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there‚Äôs a problem ‚Äî you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don‚Äôt have your experience so they don‚Äôt know how to price their houses.
To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it‚Äôs size, neighborhood, etc, and what similar houses have sold for.
So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details ‚Äî number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:
Using that training data, we want to create a program that can estimate how much any other house in your area is worth:
This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.
To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.
This kind of like having the answer key to a math test with all the arithmetic symbols erased:
From this, can you figure out what kind of math problems were on the test? You know you are supposed to ‚Äúdo something‚Äù with the numbers on the left to get each answer on the right.
In supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type!
Let‚Äôs go back to our original example with the real estate agent. What if you didn‚Äôt know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning.
This is kind of like someone giving you a list of numbers on a sheet of paper and saying ‚ÄúI don‚Äôt really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something ‚Äî good luck!‚Äù
So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you‚Äôd find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts.
Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions.
Supervised learning is what we‚Äôll focus on for the rest of this post, but that‚Äôs not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer.
Side note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start.
As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a ‚Äúfeel‚Äù for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers.
But current machine learning algorithms aren‚Äôt that good yet ‚Äî they only work when focused a very specific, limited problem. Maybe a better definition for ‚Äúlearning‚Äù in this case is ‚Äúfiguring out an equation to solve a specific problem based on some example data‚Äù.
Unfortunately ‚ÄúMachine Figuring out an equation to solve a specific problem based on some example data‚Äù isn‚Äôt really a great name. So we ended up with ‚ÄúMachine Learning‚Äù instead.
Of course if you are reading this 50 years in the future and we‚Äôve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human.
So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further.
If you didn‚Äôt know anything about machine learning, you‚Äôd probably try to write out some basic rules for estimating the price of a house like this:
If you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change.
Wouldn‚Äôt it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number:
One way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms, the square footage and the neighborhood. If you could just figure out how much each ingredient impacts the final price, maybe there‚Äôs an exact ratio of ingredients to stir in to make the final price.
That would reduce your original function (with all those crazy if‚Äôs and else‚Äôs) down to something really simple like this:
Notice the magic numbers in bold ‚Äî .841231951398213, 1231.1231231, 2.3242341421, and 201.23432095. These are our weights. If we could just figure out the perfect weights to use that work for every house, our function could predict house prices!
A dumb way to figure out the best weights would be something like this:
Start with each weight set to 1.0:
Run every house you know about through your function and see how far off the function is at guessing the correct price for each house:
For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house.
Now add up the squared amount you are off for each house you have in your data set. Let‚Äôs say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That‚Äôs how ‚Äúwrong‚Äù your function currently is.
Now, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the cost of your function.
If you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that‚Äôs our goal ‚Äî get this cost to be as low as possible by trying different weights.
Repeat Step 2 over and over with every single possible combination of weights. Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you‚Äôve solved the problem!
That‚Äôs pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow!
But here‚Äôs a few more facts that will blow your mind:
Pretty crazy, right?
Ok, of course you can‚Äôt just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you‚Äôd never run out of numbers to try.
To avoid that, mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many. Here‚Äôs one way:
First, write a simple equation that represents Step #2 above:
Now let‚Äôs re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):
This equation represents how wrong our price estimating function is for the weights we currently have set.
If we graph this cost equation for all possible values of our weights for number_of_bedrooms and sqft, we‚Äôd get a graph that might look something like this:
In this graph, the lowest point in blue is where our cost is the lowest ‚Äî thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we‚Äôll have our answer!
So we just need to adjust our weights so we are ‚Äúwalking down hill‚Äù on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we‚Äôll eventually get there without having to try too many different weights.
If you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function‚Äôs tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill.
So if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we‚Äôll reach the bottom of the hill and have the best possible values for our weights. (If that didn‚Äôt make sense, don‚Äôt worry and keep reading).
That‚Äôs a high level summary of one way to find the best weights for your function called batch gradient descent. Don‚Äôt be afraid to dig deeper if you are interested on learning the details.
When you use a machine learning library to solve a real problem, all of this will be done for you. But it‚Äôs still useful to have a good idea of what is happening.
The three-step algorithm I described is called multivariate linear regression. You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you‚Äôve never seen before based where that house would appear on your line. It‚Äôs a really powerful idea and you can solve ‚Äúreal‚Äù problems with it.
But while the approach I showed you might work in simple cases, it won‚Äôt work in all cases. One reason is because house prices aren‚Äôt always simple enough to follow a continuous line.
But luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like neural networks or SVMs with kernels). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies.
Also, I ignored the idea of overfitting. It‚Äôs easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren‚Äôt in your original data set. But there are ways to deal with this (like regularization and using a cross-validation data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully.
In other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it‚Äôs a skill that any developer can learn!
Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data!
But it‚Äôs important to remember that machine learning only works if the problem is actually solvable with the data that you have.
For example, if you build a model that predicts home prices based on the type of potted plants in each house, it‚Äôs never going to work. There just isn‚Äôt any kind of relationship between the potted plants in each house and the home‚Äôs sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two.
So remember, if a human expert couldn‚Äôt use the data to solve the problem manually, a computer probably won‚Äôt be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly.
In my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn‚Äôt a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it‚Äôs getting a little better every day.
If you want to try out what you‚Äôve learned in this article, I made a course that walks you through every step of this article, including writing all the code. Give it a try!
If you want to go deeper, Andrew Ng‚Äôs free Machine Learning class on Coursera is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math.
Also, you can play around with tons of machine learning algorithms by downloading and installing SciKit-Learn. It‚Äôs a python framework that has ‚Äúblack box‚Äù versions of all the standard algorithms.
If you liked this article, please consider signing up for my Machine Learning is Fun! Newsletter:
Also, please check out the full-length course version of this article. It covers everything in this article in more detail, including writing the actual code in Python. You can get a free 30-day trial to watch the course if you sign up with this link.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I‚Äôd love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 2!
JerryBui.eth
in
Digital Forensics Future
Hazel Tang
Sahaj Mamgain
Ace Infoway
Matrix AI Network
Clo Willaerts
Rob Furey
Hekate
in
Hekate
"
305,https://medium.com/free-code-camp/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0,"Every single Machine Learning course on the internet, ranked by your reviews","A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science master‚Äôs program using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost.
I‚Äôm almost finished now. I‚Äôve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role. So I started creating a review-driven guide that recommends the best courses for each subject within data science.
For the first guide in the series, I recommended a few coding classes for the beginner data scientist. Then it was statistics and probability classes. Then introductions to data science. Also, data visualization.
For this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My end goal was to identify the three best courses available and present them to you, below.
For this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews.
Since 2011, Class Central founder Dhawal Shah has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources.
Each course must fit three criteria:
We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on Udemy, we chose to consider the most-reviewed and highest-rated ones only.
There‚Äôs always a chance that we missed something, though. So please let us know in the comments section if we left a good course out.
We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings.
We made subjective syllabus judgment calls based on three factors:
A popular definition originates from Arthur Samuel in 1959: machine learning is a subfield of computer science that gives ‚Äúcomputers the ability to learn without being explicitly programmed.‚Äù In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience.
A machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you‚Äôll find helpful visualization of these core steps:
The ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves.
First off, let‚Äôs define deep learning. Here is a succinct description:
‚ÄúDeep learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.‚Äù
‚Äî Jason Brownlee from Machine Learning Mastery
As would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we‚Äôve got you covered with the following article:
medium.freecodecamp.com
My top three recommendations from that list would be:
Several courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline.
Missing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles (programming, statistics) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar.
Stanford University‚Äôs Machine Learning on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews.
Released in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available.
Ng is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning.
Evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice:
In the past, I‚Äôve tried to teach machine learning using a large variety of different programming languages including C++, Java, Python, NumPy, and also Octave ‚Ä¶ And what I‚Äôve seen after having taught machine learning for almost a decade is that you learn much faster if you use Octave as your programming environment.
Though Python and R are likely more compelling choices in 2017 with the increased popularity of those languages, reviewers note that that shouldn‚Äôt stop you from taking the course.
A few prominent reviewers noted the following:
Of longstanding renown in the MOOC world, Stanford‚Äôs machine learning course really is the definitive introduction to this topic. The course broadly covers all of the major areas of machine learning ‚Ä¶ Prof. Ng precedes each segment with a motivating discussion and examples.
Andrew Ng is a gifted teacher and able to explain complicated subjects in a very intuitive and clear way, including the math behind all concepts. Highly recommended.
The only problem I see with this course if that it sets the expectation bar very high for other courses.
Columbia University‚Äôs Machine Learning is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn‚Äôt have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews.
The course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia‚Äôs is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding).
Quizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course‚Äôs total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase.
Below are a few of the aforementioned sparkling reviews:
Over all my years of [being a] student I‚Äôve come across professors who aren‚Äôt brilliant, professors who are brilliant but they don‚Äôt know how to explain the stuff clearly, and professors who are brilliant and know how explain the stuff clearly. Dr. Paisley belongs to the third group.
This is a great course ‚Ä¶ The instructor‚Äôs language is precise and that is, to my mind, one of the strongest points of the course. The lectures are of high quality and the slides are great too.
Dr. Paisley and his supervisor are ‚Ä¶ students of Michael Jordan, the father of machine learning. [Dr. Paisley] is the best ML professor at Columbia because of his ability to explain stuff clearly. Up to 240 students have selected his course this semester, the largest number among all professors [teaching] machine learning at Columbia.
Machine Learning A-Z‚Ñ¢ on Udemy is an impressively detailed offering that provides instruction in both Python and R, which is rare and can‚Äôt be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered.
It covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an ‚Äúintuition‚Äù video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R.
As a ‚Äúbonus,‚Äù the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren‚Äôt the strong points of the course.
Eremenko and the SuperDataScience team are revered for their ability to ‚Äúmake the complex simple.‚Äù Also, the prerequisites listed are ‚Äújust some high school mathematics,‚Äù so this course might be a better option for those daunted by the Stanford and Columbia offerings.
A few prominent reviewers noted the following:
The course is professionally produced, the sound quality is excellent, and the explanations are clear and concise ‚Ä¶ It‚Äôs an incredible value for your financial and time investment.
It was spectacular to be able to follow the course in two different programming languages simultaneously.
Kirill is one of the absolute best instructors on Udemy (if not the Internet) and I recommend taking any class he teaches. ‚Ä¶ This course has a ton of content, like a ton!
Our #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let‚Äôs look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide ‚Äî you can find those here.
The Analytics Edge (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews.
Python for Data Science and Machine Learning Bootcamp (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews.
Data Science and Machine Learning Bootcamp with R (Jose Portilla/Udemy): The comments for Portilla‚Äôs above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews.
Machine Learning Series (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course‚Äôs description. Uses Python. Cost varies depending on Udemy discounts, which are frequent.
Machine Learning (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity‚Äôs Machine Learning Engineer Nanodegree and Georgia Tech‚Äôs Online Master‚Äôs Degree (OMS). Bite-sized videos, as is Udacity‚Äôs style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews.
Implementing Predictive Analytics with Spark in Azure HDInsight (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews.
Data Science and Machine Learning with Python ‚Äî Hands On! (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews.
Scala and Spark for Big Data and Machine Learning (Jose Portilla/Udemy): ‚ÄúBig data‚Äù focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews.
Machine Learning Engineer Nanodegree (Udacity): Udacity‚Äôs flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews.
Learning From Data (Introductory Machine Learning) (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech‚Äôs independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews.
Learning From Data (Introductory Machine Learning) (Yaser Abu-Mostafa/California Institute of Technology): ‚ÄúA real Caltech course, not a watered-down version.‚Äù Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn‚Äôt as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews.
Mining Massive Datasets (Stanford University): Machine learning with a focus on ‚Äúbig data.‚Äù Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews.
AWS Machine Learning: A Complete Guide With Python (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews.
Introduction to Machine Learning & Face Detection in Python (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews.
StatLearning: Statistical Learning (Stanford University): Based on the excellent textbook, ‚ÄúAn Introduction to Statistical Learning, with Applications in R‚Äù and taught by the professors who wrote it. Reviewers note that the MOOC isn‚Äôt as good as the book, citing ‚Äúthin‚Äù exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews.
Machine Learning Specialization (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford‚Äôs or Caltech‚Äôs). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews.
From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase (Loony Corn/Udemy): ‚ÄúA down-to-earth, shy but confident take on machine learning techniques.‚Äù Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews.
Principles of Machine Learning (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews.
Big Data: Statistical Inference and Machine Learning (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews.
Genomic Data Science and Clustering (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD‚Äôs Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews.
Intro to Machine Learning (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity‚Äôs Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews.
Machine Learning for Data Analysis (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan‚Äôs Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews.
Programming with Python for Data Science (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews.
Machine Learning for Trading (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity‚Äôs Machine Learning Engineer Nanodegree and Georgia Tech‚Äôs Online Master‚Äôs Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews.
Practical Machine Learning (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU‚Äôs Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews.
Machine Learning for Data Science and Analytics (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews.
Recommender Systems Specialization (University of Minnesota/Coursera): Strong focus one specific type of machine learning ‚Äî recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews.
Machine Learning With Big Data (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD‚Äôs Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews.
Practical Predictive Analytics: Models and Methods (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW‚Äôs Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews.
The following courses had one or no reviews as of May 2017.
Machine Learning for Musicians and Artists (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review.
Applied Machine Learning in Python (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available.
Applied Machine Learning (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase.
Machine Learning with Python (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free.
Machine Learning with Apache SystemML (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free.
Machine Learning for Data Science (University of California, San Diego/edX): Doesn‚Äôt launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase.
Introduction to Analytics Modeling (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase.
Predictive Analytics: Gaining Insights from Big Data (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise‚Äôs Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase.
Introducci√≥n al Machine Learning (Universitas Telef√≥nica/Mir√≠ada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks.
Machine Learning Path Step (Dataquest): Taught in Python using Dataquest‚Äôs interactive in-browser platform. Multiple guided projects and a ‚Äúplus‚Äù project where you build your own machine learning system using your own data. Subscription required.
The following six courses are offered by DataCamp. DataCamp‚Äôs hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course.
Introduction to Machine Learning (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours.
Supervised Learning with scikit-learn (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours.
Unsupervised Learning in R (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours.
Machine Learning Toolbox (DataCamp): Teaches the ‚Äúbig ideas‚Äù in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours.
Machine Learning with the Experts: School Budgets (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school‚Äôs budget. DataCamp‚Äôs ‚ÄúSupervised Learning with scikit-learn‚Äù is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours.
Unsupervised Learning in Python (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours.
Machine Learning (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon‚Äôs graduate introductory machine learning course. A prerequisite to their second graduate level course, ‚ÄúStatistical Machine Learning.‚Äù Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A 2011 version of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free.
Statistical Machine Learning (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon‚Äôs Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free.
Undergraduate Machine Learning (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below).
Machine Learning (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas‚Äô undergraduate course (above) apply here as well.
This is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the first article, statistics and probability in the second article, intros to data science in the third article, and data visualization in the fourth.
medium.freecodecamp.com
The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering.
If you‚Äôre looking for a complete list of Data Science online courses, you can find them on Class Central‚Äôs Data Science and Big Data subject page.
If you enjoyed reading this, check out some of Class Central‚Äôs other pieces:
medium.freecodecamp.com
medium.freecodecamp.com
If you have suggestions for courses I missed, let me know in the responses!
If you found this helpful, click the üíö so more people will see it here on Medium.
This is a condensed version of my original article published on Class Central, where I‚Äôve included detailed course syllabi.
Hassan Abedi
Kaan Bursa
Mary Adewunmi
Prathamesh Mishra
Peter Gao
in
Aquarium Learning
Pradeep Bansal
in
Artificial Intelligence in Plain English
Subir Maity
Gwangju Institute of Science and Technology (GIST)
"
306,https://medium.com/mybridge-for-professionals/30-amazing-machine-learning-projects-for-the-past-year-v-2018-b853b8621ac7,30 Amazing Machine Learning Projects for the Past Year (v.2018),"For the past year, we‚Äôve compared nearly 8,800 open source Machine Learning projects to pick Top 30 (0.3% chance).
This is an extremely competitive list and it carefully picks the best open source Machine Learning libraries, datasets and apps published between January and December 2017. Mybridge AI evaluates the quality by considering popularity, engagement and recency. To give you an idea about the quality, the average number of Github stars is 3,558.
Open source projects can be useful for data scientists. You can learn by reading the source code and build something on top of the existing projects. Give a plenty of time to play around with Machine Learning projects you may have missed for the past year.
<Recommended Learning>
A) Neural Networks
Deep Learning A-Z‚Ñ¢: Hands-On Artificial Neural Networks
[68,745 recommends, 4.5/5 stars]
B) TensorFlow
Complete Guide to TensorFlow for Deep Learning with Python
[17,834 recommends, 4.6/5 stars]
<Others>
A) Web hosting: Get free domain name for a year. For your ‚Äòsimple‚Äô personal website or project site.
(Click the numbers below. Credit given to the biggest contributor.)
FastText: Library for fast text representation and classification. [11786 stars on Github]. Courtesy of Facebook Research
‚Ä¶‚Ä¶‚Ä¶.. [ Muse: Multilingual Unsupervised or Supervised word Embeddings, based on Fast Text. 695 stars on Github]
Deep-photo-styletransfer: Code and data for paper ‚ÄúDeep Photo Style Transfer‚Äù [9747 stars on Github]. Courtesy of Fujun Luan, Ph.D. at Cornell University
The world‚Äôs simplest facial recognition api for Python and the command line [8672 stars on Github]. Courtesy of Adam Geitgey
Magenta: Music and Art Generation with Machine Intelligence [8113 stars on Github].
Sonnet: TensorFlow-based neural network library [5731 stars on Github]. Courtesy of Malcolm Reynolds at Deepmind
deeplearn.js: A hardware-accelerated machine intelligence library for the web [5462 stars on Github]. Courtesy of Nikhil Thorat at Google Brain
Fast Style Transfer in TensorFlow [4843 stars on Github]. Courtesy of Logan Engstrom at MIT
Pysc2: StarCraft II Learning Environment [3683 stars on Github]. Courtesy of Timo Ewalds at DeepMind
AirSim: Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research [3861 stars on Github]. Courtesy of Shital Shah at Microsoft
Facets: Visualizations for machine learning datasets [3371 stars on Github]. Courtesy of Google Brain
Style2Paints: AI colorization of images [3310 stars on Github].
Tensor2Tensor: A library for generalized sequence to sequence models ‚Äî Google Research [3087 stars on Github]. Courtesy of Ryan Sepassi at Google Brain
Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more) [2847 stars on Github]. Courtesy of Jun-Yan Zhu, Ph.D at Berkeley
Faiss: A library for efficient similarity search and clustering of dense vectors. [2629 stars on Github]. Courtesy of Facebook Research
Fashion-mnist: A MNIST-like fashion product database [2780 stars on Github]. Courtesy of Han Xiao, Research Scientist Zalando Tech
ParlAI: A framework for training and evaluating AI models on a variety of openly available dialog datasets [2578 stars on Github]. Courtesy of Alexander Miller at Facebook Research
Fairseq: Facebook AI Research Sequence-to-Sequence Toolkit [2571 stars on Github].
Pyro: Deep universal probabilistic programming with Python and PyTorch [2387 stars on Github]. Courtesy of Uber AI Labs
iGAN: Interactive Image Generation powered by GAN [2369 stars on Github].
Deep-image-prior: Image restoration with neural networks but without learning [2188 stars on Github]. Courtesy of Dmitry Ulyanov, Ph.D at Skoltech
Face_classification: Real-time face detection and emotion/gender classification using fer2013/imdb datasets with a keras CNN model and openCV. [1967 stars on Github].
Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind‚Äôs WaveNet and tensorflow [1961 stars on Github]. Courtesy of Namju Kim at Kakao Brain
StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation [1954 stars on Github]. Courtesy of Yunjey Choi at Korea University
Ml-agents: Unity Machine Learning Agents [1658 stars on Github]. Courtesy of Arthur Juliani, Deep Learning at Unity3D
DeepVideoAnalytics: A distributed visual search and visual data analytics platform [1494 stars on Github]. Courtesy of Akshay Bhat, Ph.D at Cornell University
OpenNMT: Open-Source Neural Machine Translation in Torch [1490 stars on Github].
Pix2pixHD: Synthesizing and manipulating 2048x1024 images with conditional GANs [1283 stars on Github]. Courtesy of Ming-Yu Liu at AI Research Scientist at Nvidia
Horovod: Distributed training framework for TensorFlow. [1188 stars on Github]. Courtesy of Uber Engineering
AI-Blocks: A powerful and intuitive WYSIWYG interface that allows anyone to create Machine Learning models [899 stars on Github].
Deep neural networks for voice conversion (voice style transfer) in Tensorflow [845 stars on Github]. Courtesy of Dabi Ahn, AI Research at Kakao Brain
That‚Äôs it for Machine Learning Open Source of the Year. Visit our publication to find top posts for more programming skills.
Jeff Shukis
GIScience Heidelberg / HeiGIT
Shubhang Singh
Daegudude
Wonyoung So
Arnav Saxena
in
CodeX
Shaik Subhani
in
Analytics Vidhya
National Data Quality Forum (NDQF)
"
307,https://medium.com/free-code-camp/want-to-know-how-deep-learning-works-heres-a-quick-guide-for-everyone-1aedeca88076,Want to know how Deep Learning works? Here‚Äôs a quick guide for everyone.,"Artificial Intelligence (AI) and Machine Learning (ML) are some of the hottest topics right now.
The term ‚ÄúAI‚Äù is thrown around casually every day. You hear aspiring developers saying they want to learn AI. You also hear executives saying they want to implement AI in their services. But quite often, many of these people don‚Äôt understand what‚Ä¶
Magnimind
Gabriel Signoretti
in
Towards Data Science
Amit Sharma
Saswata Chakravarty
Paul Swenson
Anyverse
in
Anyverse‚Ñ¢
Timothy Chong
John Xu
"
308,https://medium.com/basic-income/deep-learning-is-going-to-teach-us-all-the-lesson-of-our-lives-jobs-are-for-machines-7c6442e37a49,Deep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machines,"(An alternate version of this article was originally published in the Boston Globe)
On December 2nd, 1942, a team of scientists led by Enrico Fermi came back from lunch and watched as humanity created the first self-sustaining nuclear reaction inside a pile of bricks and wood underneath a football field at the University of Chicago. Known to history as Chicago Pile-1, it was celebrated in silence with a single bottle of Chianti, for those who were there understood exactly what it meant for humankind, without any need for words.
Now, something new has occurred that, again, quietly changed the world forever. Like a whispered word in a foreign language, it was quiet in that you may have heard it, but its full meaning may not have been comprehended. However, it‚Äôs vital we understand this new language, and what it‚Äôs increasingly telling us, for the ramifications are set to alter everything we take for granted about the way our globalized economy functions, and the ways in which we as humans exist within it.
The language is a new class of machine learning known as deep learning, and the ‚Äúwhispered word‚Äù was a computer‚Äôs use of it to seemingly out of nowhere defeat three-time European Go champion Fan Hui, not once but five times in a row without defeat. Many who read this news, considered that as impressive, but in no way comparable to a match against Lee Se-dol instead, who many consider to be one of the world‚Äôs best living Go players, if not the best. Imagining such a grand duel of man versus machine, China‚Äôs top Go player predicted that Lee would not lose a single game, and Lee himself confidently expected to possibly lose one at the most.
What actually ended up happening when they faced off? Lee went on to lose all but one of their match‚Äôs five games. An AI named AlphaGo is now a better Go player than any human and has been granted the ‚Äúdivine‚Äù rank of 9 dan. In other words, its level of play borders on godlike. Go has officially fallen to machine, just as Jeopardy did before it to Watson, and chess before that to Deep Blue.
‚ÄúAlphaGo‚Äôs historic victory is a clear signal that we‚Äôve gone from linear to parabolic.‚Äù
So, what is Go? Very simply, think of Go as Super Ultra Mega Chess. This may still sound like a small accomplishment, another feather in the cap of machines as they continue to prove themselves superior in the fun games we play, but it is no small accomplishment, and what‚Äôs happening is no game.
AlphaGo‚Äôs historic victory is a clear signal that we‚Äôve gone from linear to parabolic. Advances in technology are now so visibly exponential in nature that we can expect to see a lot more milestones being crossed long before we would otherwise expect. These exponential advances, most notably in forms of artificial intelligence limited to specific tasks, we are entirely unprepared for as long as we continue to insist upon employment as our primary source of income.
This may all sound like exaggeration, so let‚Äôs take a few decade steps back, and look at what computer technology has been actively doing to human employment so far:
Let the above chart sink in. Do not be fooled into thinking this conversation about the automation of labor is set in the future. It‚Äôs already here. Computer technology is already eating jobs and has been since 1990.
All work can be divided into four types: routine and nonroutine, cognitive and manual. Routine work is the same stuff day in and day out, while nonroutine work varies. Within these two varieties, is the work that requires mostly our brains (cognitive) and the work that requires mostly our bodies (manual). Where once all four types saw growth, the stuff that is routine stagnated back in 1990. This happened because routine labor is easiest for technology to shoulder. Rules can be written for work that doesn‚Äôt change, and that work can be better handled by machines.
Distressingly, it‚Äôs exactly routine work that once formed the basis of the American middle class. It‚Äôs routine manual work that Henry Ford transformed by paying people middle class wages to perform, and it‚Äôs routine cognitive work that once filled US office spaces. Such jobs are now increasingly unavailable, leaving only two kinds of jobs with rosy outlooks: jobs that require so little thought, we pay people little to do them, and jobs that require so much thought, we pay people well to do them.
If we can now imagine our economy as a plane with four engines, where it can still fly on only two of them as long as they both keep roaring, we can avoid concerning ourselves with crashing. But what happens when our two remaining engines also fail? That‚Äôs what the advancing fields of robotics and AI represent to those final two engines, because for the first time, we are successfully teaching machines to learn.
I‚Äôm a writer at heart, but my educational background happens to be in psychology and physics. I‚Äôm fascinated by both of them so my undergraduate focus ended up being in the physics of the human brain, otherwise known as cognitive neuroscience. I think once you start to look into how the human brain works, how our mass of interconnected neurons somehow results in what we describe as the mind, everything changes. At least it did for me.
As a quick primer in the way our brains function, they‚Äôre a giant network of interconnected cells. Some of these connections are short, and some are long. Some cells are only connected to one other, and some are connected to many. Electrical signals then pass through these connections, at various rates, and subsequent neural firings happen in turn. It‚Äôs all kind of like falling dominoes, but far faster, larger, and more complex. The result amazingly is us, and what we‚Äôve been learning about how we work, we‚Äôve now begun applying to the way machines work.
One of these applications is the creation of deep neural networks - kind of like pared-down virtual brains. They provide an avenue to machine learning that‚Äôs made incredible leaps that were previously thought to be much further down the road, if even possible at all. How? It‚Äôs not just the obvious growing capability of our computers and our expanding knowledge in the neurosciences, but the vastly growing expanse of our collective data, aka big data.
Big data isn‚Äôt just some buzzword. It‚Äôs information, and when it comes to information, we‚Äôre creating more and more of it every day. In fact we‚Äôre creating so much that a 2013 report by SINTEF estimated that 90% of all information in the world had been created in the prior two years. This incredible rate of data creation is even doubling every 1.5 years thanks to the Internet, where in 2015 every minute we were liking 4.2 million things on Facebook, uploading 300 hours of video to YouTube, and sending 350,000 tweets. Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn to learn. Why?
Imagine programming a computer to recognize a chair. You‚Äôd need to enter a ton of instructions, and the result would still be a program detecting chairs that aren‚Äôt, and not detecting chairs that are. So how did we learn to detect chairs? Our parents pointed at a chair and said, ‚Äúchair.‚Äù Then we thought we had that whole chair thing all figured out, so we pointed at a table and said ‚Äúchair‚Äù, which is when our parents told us that was ‚Äútable.‚Äù This is called reinforcement learning. The label ‚Äúchair‚Äù gets connected to every chair we see, such that certain neural pathways are weighted and others aren‚Äôt. For ‚Äúchair‚Äù to fire in our brains, what we perceive has to be close enough to our previous chair encounters. Essentially, our lives are big data filtered through our brains.
The power of deep learning is that it‚Äôs a way of using massive amounts of data to get machines to operate more like we do without giving them explicit instructions. Instead of describing ‚Äúchairness‚Äù to a computer, we instead just plug it into the Internet and feed it millions of pictures of chairs. It can then have a general idea of ‚Äúchairness.‚Äù Next we test it with even more images. Where it‚Äôs wrong, we correct it, which further improves its ‚Äúchairness‚Äù detection. Repetition of this process results in a computer that knows what a chair is when it sees it, for the most part as well as we can. The important difference though is that unlike us, it can then sort through millions of images within a matter of seconds.
This combination of deep learning and big data has resulted in astounding accomplishments just in the past year. Aside from the incredible accomplishment of AlphaGo, Google‚Äôs DeepMind AI learned how to read and comprehend what it read through hundreds of thousands of annotated news articles. DeepMind also taught itself to play dozens of Atari 2600 video games better than humans, just by looking at the screen and its score, and playing games repeatedly. An AI named Giraffe taught itself how to play chess in a similar manner using a dataset of 175 million chess positions, attaining International Master level status in just 72 hours by repeatedly playing itself. In 2015, an AI even passed a visual Turing test by learning to learn in a way that enabled it to be shown an unknown character in a fictional alphabet, then instantly reproduce that letter in a way that was entirely indistinguishable from a human given the same task. These are all major milestones in AI.
However, despite all these milestones, when asked to estimate when a computer would defeat a prominent Go player, the answer even just months prior to the announcement by Google of AlphaGo‚Äôs victory, was by experts essentially, ‚ÄúMaybe in another ten years.‚Äù A decade was considered a fair guess because Go is a game so complex I‚Äôll just let Ken Jennings of Jeopardy fame, another former champion human defeated by AI, describe it:
Go is famously a more complex game than chess, with its larger board, longer games, and many more pieces. Google‚Äôs DeepMind artificial intelligence team likes to say that there are more possible Go boards than atoms in the known universe, but that vastly understates the computational problem. There are about 10¬π‚Å∑‚Å∞ board positions in Go, and only 10‚Å∏‚Å∞ atoms in the universe. That means that if there were as many parallel universes as there are atoms in our universe (!), then the total number of atoms in all those universes combined would be close to the possibilities on a single Go board.
Such confounding complexity makes impossible any brute-force approach to scan every possible move to determine the next best move. But deep neural networks get around that barrier in the same way our own minds do, by learning to estimate what feels like the best move. We do this through observation and practice, and so did AlphaGo, by analyzing millions of professional games and playing itself millions of times. So the answer to when the game of Go would fall to machines wasn‚Äôt even close to ten years. The correct answer ended up being, ‚ÄúAny time now.‚Äù
Any time now. That‚Äôs the new go-to response in the 21st century for any question involving something new machines can do better than humans, and we need to try to wrap our heads around it.
We need to recognize what it means for exponential technological change to be entering the labor market space for nonroutine jobs for the first time ever. Machines that can learn mean nothing humans do as a job is uniquely safe anymore. From hamburgers to healthcare, machines can be created to successfully perform such tasks with no need or less need for humans, and at lower costs than humans.
Amelia is just one AI out there currently being beta-tested in companies right now. Created by IPsoft over the past 16 years, she‚Äôs learned how to perform the work of call center employees. She can learn in seconds what takes us months, and she can do it in 20 languages. Because she‚Äôs able to learn, she‚Äôs able to do more over time. In one company putting her through the paces, she successfully handled one of every ten calls in the first week, and by the end of the second month, she could resolve six of ten calls. Because of this, it‚Äôs been estimated that she can put 250 million people out of a job, worldwide.
Viv is an AI coming soon from the creators of Siri who‚Äôll be our own personal assistant. She‚Äôll perform tasks online for us, and even function as a Facebook News Feed on steroids by suggesting we consume the media she‚Äôll know we‚Äôll like best. In doing all of this for us, we‚Äôll see far fewer ads, and that means the entire advertising industry ‚Äî that industry the entire Internet is built upon ‚Äî stands to be hugely disrupted.
A world with Amelia and Viv ‚Äî and the countless other AI counterparts coming online soon ‚Äî in combination with robots like Boston Dynamics‚Äô next generation Atlas portends, is a world where machines can do all four types of jobs and that means serious societal reconsiderations. If a machine can do a job instead of a human, should any human be forced at the threat of destitution to perform that job? Should income itself remain coupled to employment, such that having a job is the only way to obtain income, when jobs for many are entirely unobtainable? If machines are performing an increasing percentage of our jobs for us, and not getting paid to do them, where does that money go instead? And what does it no longer buy? Is it even possible that many of the jobs we‚Äôre creating don‚Äôt need to exist at all, and only do because of the incomes they provide? These are questions we need to start asking, and fast.
Fortunately, people are beginning to ask these questions, and there‚Äôs an answer that‚Äôs building up momentum. The idea is to put machines to work for us, but empower ourselves to seek out the forms of remaining work we as humans find most valuable, by simply providing everyone a monthly paycheck independent of work. This paycheck would be granted to all citizens unconditionally, and its name is universal basic income. By adopting UBI, aside from immunizing against the negative effects of automation, we‚Äôd also be decreasing the risks inherent in entrepreneurship, and the sizes of bureaucracies necessary to boost incomes. It‚Äôs for these reasons, it has cross-partisan support, and is even now in the beginning stages of possible implementation in countries like Switzerland, Finland, the Netherlands, and Canada.
The future is a place of accelerating changes. It seems unwise to continue looking at the future as if it were the past, where just because new jobs have historically appeared, they always will. The WEF started 2016 off by estimating the creation by 2020 of 2 million new jobs alongside the elimination of 7 million. That‚Äôs a net loss, not a net gain of 5 million jobs. In a frequently cited paper, an Oxford study estimated the automation of about half of all existing jobs by 2033. Meanwhile self-driving vehicles, again thanks to machine learning, have the capability of drastically impacting all economies ‚Äî especially the US economy as I wrote last year about automating truck driving ‚Äî by eliminating millions of jobs within a short span of time.
And now even the White House, in a stunning report to Congress, has put the probability at 83 percent that a worker making less than $20 an hour in 2010 will eventually lose their job to a machine. Even workers making as much as $40 an hour face odds of 31 percent. To ignore odds like these is tantamount to our now laughable ‚Äúduck and cover‚Äù strategies for avoiding nuclear blasts during the Cold War.
All of this is why it‚Äôs those most knowledgeable in the AI field who are now actively sounding the alarm for basic income. During a panel discussion at the end of 2015 at Singularity University, prominent data scientist Jeremy Howard asked ‚ÄúDo you want half of people to starve because they literally can‚Äôt add economic value, or not?‚Äù before going on to suggest, ‚ÄùIf the answer is not, then the smartest way to distribute the wealth is by implementing a universal basic income.‚Äù
AI pioneer Chris Eliasmith, director of the Centre for Theoretical Neuroscience, warned about the immediate impacts of AI on society in an interview with Futurism, ‚ÄúAI is already having a big impact on our economies‚Ä¶ My suspicion is that more countries will have to follow Finland‚Äôs lead in exploring basic income guarantees for people.‚Äù
Moshe Vardi expressed the same sentiment after speaking at the 2016 annual meeting of the American Association for the Advancement of Science about the emergence of intelligent machines, ‚Äúwe need to rethink the very basic structure of our economic system‚Ä¶ we may have to consider instituting a basic income guarantee.‚Äù
Even Baidu‚Äôs chief scientist and founder of Google‚Äôs ‚ÄúGoogle Brain‚Äù deep learning project, Andrew Ng, during an onstage interview at this year‚Äôs Deep Learning Summit, expressed the shared notion that basic income must be ‚Äúseriously considered‚Äù by governments, citing ‚Äúa high chance that AI will create massive labor displacement.‚Äù
When those building the tools begin warning about the implications of their use, shouldn‚Äôt those wishing to use those tools listen with the utmost attention, especially when it‚Äôs the very livelihoods of millions of people at stake? If not then, what about when Nobel prize winning economists begin agreeing with them in increasing numbers?
No nation is yet ready for the changes ahead. High labor force non-participation leads to social instability, and a lack of consumers within consumer economies leads to economic instability. So let‚Äôs ask ourselves, what‚Äôs the purpose of the technologies we‚Äôre creating? What‚Äôs the purpose of a car that can drive for us, or artificial intelligence that can shoulder 60% of our workload? Is it to allow us to work more hours for even less pay? Or is it to enable us to choose how we work, and to decline any pay/hours we deem insufficient because we‚Äôre already earning the incomes that machines aren‚Äôt?
What‚Äôs the big lesson to learn, in a century when machines can learn?
I offer it‚Äôs that jobs are for machines, and life is for people.
This article was written on a crowdfunded monthly basic income. If you found value in this article, you can support it along with all my advocacy for basic income with a monthly patron pledge of $1+.
Are you a creative? Become a creator on Patreon. Join me in taking the BIG Patreon Creator Pledge for basic income
Special thanks to Arjun Banker, Steven Grimm, Larry Cohen, Topher Hunt, Aaron Marcus-Kubitza, Andrew Stern, Keith Davis, Albert Wenger, Richard Just, Chris Smothers, Mark Witham, David Ihnen, Danielle Texeira, Katie Doemland, Paul Wicks, Jan Smole, Joe Esposito, Jack Wagner, Joe Ballou, Stuart Matthews, Natalie Foster, Chris McCoy, Michael Honey, Gary Aranovich, Kai Wong, John David Hodge, Louise Whitmore, Dan O‚ÄôSullivan, Harish Venkatesan, Michiel Dral, Gerald Huff, Susanne Berg, Cameron Ottens, Kian Alavi, Gray Scott, Kirk Israel, Robert Solovay, Jeff Schulman, Andrew Henderson, Robert F. Greene, Martin Jordo, Victor Lau, Shane Gordon, Paolo Narciso, Johan Grahn, Tony DeStefano, Erhan Altay, Bryan Herdliska, Stephane Boisvert, Dave Shelton, Rise & Shine PAC, Luke Sampson, Lee Irving, Kris Roadruck, Amy Shaffer, Thomas Welsh, Olli Niinim√§ki, Casey Young, Elizabeth Balcar, Masud Shah, Allen Bauer, all my other funders for their support, and my amazing partner, Katie Smith.
Would you like to see your name here too?
Scott Santens writes about basic income on his blog. You can also follow him here on Medium, on Twitter, on Facebook, or on Reddit where he is a moderator for the /r/BasicIncome community of over 30,000 subscribers.
If you feel others would appreciate this article, please click the green heart.
Emily Dong
in
Pawprint
Miranda Partners
Solve.Care Blog
in
Solve.Care
Rossamund
The 1860
George Weir
Rachita Kumar
Free Property Listing Site Delhi NCR
"
309,https://medium.com/towards-data-science/how-to-learn-deep-learning-in-6-months-e45e40ef7d48,How to learn Deep Learning in 6 months,"It is quite possible to learn, follow and contribute to state-of-art work in deep learning in about 6 months‚Äô time. This article details out the steps to achieve that.
Pre-requisites
- You are willing to spend 10‚Äì20 hours per week for the next 6 months- You have some programming skills. You should be comfortable to pick up Python along the way. And cloud. (No background in Python and cloud assumed).- Some math education in the past (algebra, geometry etc). - Access to internet and computer.
Step 1
We learn driving a car ‚Äî by driving. Not by learning how the clutch and the internal combustion engine work. Atleast not initially. When learning deep learning, we will follow the same top-down approach.
Do the fast.ai course ‚Äî Practical Deep Learning for Coders ‚Äî Part 1. This takes about 4‚Äì6 weeks of effort. This course has a session on running the code on cloud. Google Colaboratory has free GPU access. Start with that. Other options include Paperspace, AWS, GCP, Crestle and Floydhub. All of these are great. Do not start to build your own machine. Atleast not yet.
Step 2
This is the time to know some of the basics. Learn about calculus and linear algebra.
For calculus, Big Picture of Calculus provides a good overview.
For Linear Algebra, Gilbert Strang‚Äôs MIT course on OpenCourseWare is amazing.
Once you finish the above two, read the Matrix Calculus for Deep Learning.
Step 3
Now is the time to understand the bottom-up approach to deep learning. Do all the 5 courses in the deep learning specialisation in Coursera. You need to pay to get the assignments graded. But the effort is truly worth it. Ideally, given the background you have gained so far, you should be able to complete one course every week.
Step 4
‚ÄúAll work and no play makes Jack a dull boy‚Äù
Do a capstone project. This is the time where you delve deep into a deep learning library(eg: Tensorflow, PyTorch, MXNet) and implement an architecture from scratch for a problem of your liking.
The first three steps are about understanding how and where to use deep learning and gaining a solid foundation. This step is all about implementing a project from scratch and developing a strong foundation on the tools.
Step 5
Now go and do fast.ai‚Äôs part II course ‚Äî Cutting Edge Deep Learning for Coders. This covers more advanced topics and you will learn to read the latest research papers and make sense out of them.
Each of the steps should take about 4‚Äì6 weeks‚Äô time. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning.
Where to go next?
Do the Stanford‚Äôs CS231n and CS224d courses. These two are amazing courses with great depth for vision and NLP respectively. They cover the latest state-of-art. And read the deep learning book. This will solidify your understanding.
Happy deep learning. Create every single day.
Codemotion
Marko Balabanovic
in
The Startup
Leor Grebler
Yuefeng Zhang, PhD
in
Towards Data Science
Leor Grebler
in
Predict
Lu√≠s Felipe Neiva Silveira
Towards AI Editorial Team
in
Towards AI
T.Ferguson
"
310,https://medium.com/nlplanet/two-minutes-nlp-20-learning-resources-for-information-retrieval-16f8405f0892,Two minutes NLP ‚Äî 20 Learning Resources for Information Retrieval,"Hello fellow NLP enthusiasts! As soon there will be an NLPlanet Discord server for networking between NLP practitioners, I‚Äôm working on the first organization of its channels. I‚Äôm planning to add learning resources for many NLP areas, therefore this article is a step towards preparing such content. If you‚Äôre interested in the Discord server, follow NLPlanet on Medium, LinkedIn or Twitter to stay updated on its release. Enjoy! üòÑ
Here follows the first draft, curated by me, of the Information Retrieval learning resources of NLPlanet. Being a draft, this list will be improved using the feedback of the community.
This article is part 5 of a series of articles about learning resources:
Information Retrieval (IR) is the process that responds to a user query by examining a collection of documents and returning an ordered document list, where each document should be relevant to the user query. It‚Äôs the activity of obtaining information resources relevant to an information need.
A popular type of Information Retrieval is Semantic Search. Semantic Search is a data searching technique in which a search query aims to not only find keywords but to determine the intent and contextual meaning of the words a person is using for search.
If you know any other good resources for learning about Information Retrieval in particular, please let me know so that I can share them with the community.
Other NLP areas that will need a learning resources area of their own are chatbots, language models, question answering, and speech.
Thank you for reading! If you are interested in learning more about NLP, remember to follow NLPlanet on Medium, LinkedIn, and Twitter!
Bagavan Marakathalingasivam
Alberto Escarlate
in
Towards Data Science
Connor Shorten
in
Towards Data Science
Gaurav Thakur
Himanshu ~ HJ
Tiger Analytics
Ritwik Raha
Atul Agarwal
in
Towards Data Science
"
311,https://medium.com/pytorch-lightning/torchmetrics-v0-3-0-information-retrieval-metrics-and-more-c55265e9b94f,TorchMetrics v0.3 ‚Äî Information Retrieval metrics and more,"New release includes a full set of metrics for information retrieval and other metrics requested by the community
This post was co-written by Nicki Skafte Detlefsen and Luca Di Liello
TorchMetrics v0.3.0 includes 6 new metrics for evaluating information retrieval
We are happy to announce TorchMetrics v0.3.0 is now publicly available. It brings some general improvements to the library, the most prominent new feature is a set of metrics for information retrieval.
Information retrieval (IR) metrics are used to evaluate how well a system is retrieving information from a database or from a collection of documents. This is the case with search engines, where a query provided by the user is compared with many possible results, some of which are relevant and some are not.
When you query a search engine, you hope that results that could be useful are ranked higher in the results page. However, each query is usually compared with a different set of documents. For this reason, we had to implement a mechanism to allow users to easily compute the IR metrics in cases where each query is compared with a different number of possible candidates.
For this reason, IR metrics feature an additional argument called indexes that say to which query a prediction refers to. In the end, all query-document pairs are grouped by query index and then the final result is computed as the average of the metric over each group.
In total 6 new metrics have been added for doing information retrieval.
RetrievalMAP (Mean Average Precision)
Mean Average Precision is a great indicator when there are many documents that are relevant and should be ranked in the top positions. MAP measures the precision of your model at different levels and then takes the average. MAP is recommended for systems with many relevant documents that should all be well ranked.
RetrievalMRR (Mean Reciprocal Rank)
Mean Reciprocal Rank is inversely proportional to the position of the first relevant document in your ranking. MRR is equal to MAP when there is only a single relevant document for each query. We recommend using MRR only when there is one or only a few relevant documents in your system and when you are only interested that one important result is shown to the user.
RetrievalPrecision (Precision for IR)
Precision for Information Retrieval measures the fraction of relevant documents among all the retrieved ones. When used with the k parameter, it computes the fraction of relevant documents in the top k. However, this metric does not take into account the order of documents in the top k, so you may prefer MAP or MRR. We recommend to use Precision when you must return a finite set of documents to the user, for example a page full of relevant results.
RetrievalRecall (Recall for IR)
Recall for Information Retrieval is the fraction of relevant documents retrieved among all the relevant documents. We recommend using Recall when your system is designed to recognize and return all the relevant documents for a given query. Also recall can be used with the k parameter. In that case, it will measure the number of retrieved documents ranked in the top k among all the relevant documents.
RetrievalNormalizedDCG (Normalized Discounted Cumulative Gain)
NormalizedDCG is a metric to measure the quality of returned results with respect to the ranking of a perfect algorithm on the same data. Since it compares the ranking of your model with the best ranking that could be done with the same query and documents, it can be applied to a wide range of domains.
RetrievalFallOut (Fall Out rate for IR)
The Fall Out rate is the fraction of non-relevant documents that are retrieved by your model out of all non-relevant documents in the dataset. We recommend using this metric to ensure that the number of non-relevant documents provided to the user is as small as possible.
In addition to expanding our collection to the field of information retrieval, this release also includes new metrics for the classification domain:
In addition to the above new metrics, a couple of existing metrics have been updated:
Confusion matrix now better support multi label classification problems
Finally, we have started add a new attribute is_differentiable to all metrics that can be used to check if a metric can be used as a loss function e.g. can be back propagated. Currently, it is only implemented for a few number of metrics, but will be fully supported in the next release (v0.4).
While we already rigorously test that our metrics calculate the correct values, we are in v0.3.0 adding additional testing for half-precision input and gpu. The backend of TorchMetrics have received a smaller performance update that should not influence the general use of metrics in our collection or custom metrics implemented by users.
Big thanks to all community members for their contributions and feedback. A special thanks to Luca Di Liello for leading the development of the new information retrieval metrics. We are happy that after a single month of being public we have already reached 250 github stars and used in more than 40+ projects.
Want to give open source a try. We have a #new_contributors channel on slack. Check it out!
Oleksandr Zaitsev
in
Analytics Vidhya
Fmajed Ghara
Sambhu
Prashant Mudgal
in
Towards Data Science
Arfinda Ilmania
in
Towards Data Science
Robert Barcik
Andrew Mok
Aditya Shaw
"
312,https://medium.com/geekculture/the-principle-of-least-effort-and-zipf-distribution-in-information-retrieval-a7199d68465f,The Principle of Least Effort and Zipf Distribution in Information Retrieval,"Fatos Morina
in
Python in Plain English
EYWA Protocol
Thilini PereraG
Scott McCartney
Patrik H√∂rlin
in
Predictly on Tech
Nirmal Kapilarathne
Keno Leon
Anton Paras
"
313,https://medium.com/ethnography-matters/why-big-data-needs-thick-data-b4b3e75e3d7,Why Big Data Needs Thick Data,"Originally published on May 13, 2013 for Ethnography Matters, I‚Äôm republishing the post for the launch of the new Ethnography Matters Medium channel. I‚Äôve updated the article with a case study from my time at Nokia where I witnessed their over-dependence on quantitative data. I‚Äôm continuously blown away by how much my original post in 2013 sparked a discussion about the integration of Thick Data and Big Data. Since then, I have talked about this topic with EPIC conference, Republica and Strata. I also gave a TED talk targeted for business leaders. Just last year, Word Spy created an entry of Thick Data, referencing my original post as the resurgence of the term. My goal is to create more opportunities to feature people who are doing this kind of integrative work inside organizations. Please reach out if this kind of work is up your alley. You can start by joining a community of people who are at the forefront of this work at Ethnography Hangout Slack‚Äôs #datatalk channel. Thank you to IDEO‚Äôs Elyssa He for translating this article into Chinese, Â§ßÊï∞ÊçÆÁ¶ª‰∏çÂºÄ ‚ÄúÂéöÊï∞ÊçÆ‚Äù, published on 36kr and Paz Bernaldo for translating this into Spanish.
When I was researching at Nokia in 2009, which at the time was the world‚Äôs largest cellphone company in emerging, I discovered something that I believed challenged their entire business model. After years of conducting ethnographic work in China from living with migrants to working as a street vendor and living in internet caf√©s, I saw lots of indicators that led me to conclude that low-income consumers were ready to pay for more expensive smartphones.
I concluded that Nokia needed to replace their current product development strategy from making expensive smartphones for elite users to affordable smartphones for low-income users. I reported my findings and recommendations to headquarters. But Nokia did not know what to do with my findings. They said my sample size of 100 was weak and small compared to their sample size of several million data points. In addition, they said that there weren‚Äôt any signs of my insights in their existing datasets.
In response, I told them that it made sense that they haven‚Äôt seen any of my data show up in their quantitative datasets because their notion of demand was a fixed quantitative model that didn‚Äôt map to how demand worked as a cultural model in China. What is measurable isn‚Äôt the same as what is valuable.
By now, we all know what happened to Nokia. Microsoft bought them in 2013 and it only has three percent of the global smartphone market. There are many reasons for Nokia‚Äôs downfall, but one of the biggest reasons that I witnessed in person was that the company over-relied on numbers. They put a higher value on quantitative data, they didn‚Äôt know how to handle data that wasn‚Äôt easily measurable, and that didn‚Äôt show up in existing reports. What could‚Äôve been their competitive intelligence ended up being their eventual downfall.
Since my time at Nokia, I‚Äôve been very perplexed by why organizations value quantitative more than qualitative data. With the rise of Big Data, I‚Äôve seen this process intensify with organizations investing in more big data technology while decreasing budgets for human-centered research. I‚Äôm deeply concerned about the future of qualitative, ethnographic work in the Era of Big Data.
Ethnographic work has a serious perception problem in a data-driven world. While I‚Äôve always integrated statistical analysis into my qualitative work in academia, I encountered a lot of doubt on the value of ethnographically derived data when I started working primarily with corporations. I started to hear echoes of what Nokia leadership said about my small dataset, that ethnographic data is ‚Äúsmall‚Äù ‚Äúpetite‚Äù ‚Äúpuny.‚Äù. What are ethnographers to do when our research is seen as insignificant or invaluable? How can our kind of research be seen as an equally important to algorithmically processed data? To solve this perception problem, ethnographers need a 10 second elevator pitch to a room of data scientists.
Lacking the conceptual words to quickly position the value of ethnographic work in the context of Big Data, I have begun, over the last year, to employ the term Thick Data (with a nod to Clifford Geertz!) to advocate for integrative approaches to research. Thick Data is data brought to light using qualitative, ethnographic research methods that uncover people‚Äôs emotions, stories, and models of their world. It‚Äôs the sticky stuff that‚Äôs difficult to quantify. It comes to us in the form of a small sample size and in return we get an incredible depth of meanings and stories. Thick Data is the opposite of Big Data, which is quantitative data at a large scale that involves new technologies around capturing, storing, and analyzing. For Big Data to be analyzable, it must use normalizing, standardizing, defining, clustering, all processes that strips the the data set of context, meaning, and stories. Thick Data can rescue Big Data from the context-loss that comes with the processes of making it usable.
Integrating big data and thick data provides organizations a more complete complete context of any given situation. For businesses to form a complete picture, they need both big and thick data because each of them produce different types of insights at varying scales and depths. Big Data requires a humongous N to uncover patterns at a large scale while Thick Data requires a small N to see human-centered patterns in depth. Both types of Thick Data relies on human learning, while Big Data relies on machine learning. Thick Data reveals the social context of connections between data points while Big Data reveals insights with a particular range of quantified data points. Thick Data techniques accepts irreducible complexity, while Big Data techniques isolates variables to identify patterns. Thick Data loses scale while Big Data loses resolution.
CAUTION AHEAD
As the concept of ‚ÄúBig Data‚Äù has become mainstream, many practitioners and experts have cautioned organizations to adopt Big Data in a balanced way. Many qualitative researchers from Genevieve Bell to Kate Crawford and danah boyd have written essays on the limitations of Big Data from the perspective of Big Data as a person, algorithmic illusion, data fundamentalism, and privacy concerns respectively. Journalists have also added to the conversation. Caribou Honig defends small data, Gary Marcus cautions about the limitations of inferring correlations, Samuel Arbesman calls for us to move on to long data. Jenna Burrell has produced a guide for ethnographers to understand big data.
Inside organizations Big Data can be dangerous. Steven Maxwell points out that ‚ÄúPeople are getting caught up on the quantity side of the equation rather than the quality of the business insights that analytics can unearth.‚Äù More numbers do not necessarily produce more insights.
Another problem is that Big Data tends to place a huge value on quantitative results, while devaluing the importance of qualitative results. This leads to the dangerous idea that statistically normalized and standardized data is more useful and objective than qualitative data, reinforcing the notion that qualitative data is small data.
These two problems, in combination, reinforce and empower decades of corporate management decision-making based on quantitative data alone. Corporate management consultants have long been working with quantitative data to create more efficient and profitable companies.
Without a counterbalance the risk in a Big Data world is that organizations and individuals start making decisions and optimizing performance for metrics ‚Äî metrics that are derived from algorithms. And in this whole optimization process, people, stories, actual experiences, are all but forgotten. The danger, writes Clive Thompson, is that ‚Äúby taking human decision-making out of the equation, we‚Äôre slowly stripping away deliberation ‚Äî moments where we reflect on the morality of our actions.‚Äù
INSPIRATION and EMOTION
Big Data produces so much information that it needs something more to bridge and/or reveal knowledge gaps. That‚Äôs why ethnographic work holds such enormous value in the era of Big Data. Below, I explore some the ways that organizations are integrating Thick Data.
Thick Data is the best method for mapping unknown territory. When organizations want to know what they do not already know, they need Thick Data because it gives something that Big Data explicitly does not ‚Äî inspiration. The act of collecting and analyzing stories produces insights.
Stories can inspire organizations to figure out different ways to get to the destination ‚Äî the insight. If you were going to drive, Thick Data is going to inspire you to teleport. Thick Data often reveals the unexpected. It will frustrate. It will surprise. But no matter what, it will inspire. Innovation needs to be in the company of imagination.
When organizations want to build stronger ties with stakeholders, they need stories. Stories contain emotions, something that no scrubbed and normalized dataset can ever deliver. Numbers alone do not respond to the emotions of everyday life: trust, vulnerability, fear, greed, lust, security, love, and intimacy. It‚Äôs hard to algorithmically represent the strength of an individual‚Äôs service/product affiliation and how the meaning of the affiliation changes over time. Thick Data approaches reach deep into people‚Äôs hearts. Ultimately, a relationship between a stakeholder and an organization/brand is emotional, not rational.
Some people are uncomfortable with the use of the term ‚Äústories‚Äù to describe ethnographic work. There‚Äôs a lot of confusion that stories are the equivalent to anecdotes. Market researchers question if it is a ‚Äúfad.‚Äù Even in academia, many sociologists shun the use of ‚Äústories‚Äù because it makes their qualitative work appear less scientific. In my PhD Sociology program, professors often told us to use ‚Äúcases‚Äù instead of ‚Äústories.‚Äù
There‚Äôs a big difference between anecdotes and stories, however. Anecdotes are casually gathered stories that are casually shared. Within a research context, stories are intentionally gathered and systematically sampled, shared, debriefed, and analyzed, which produces insights (analysis in academia). Great insights inspire design, strategy, and innovation.
NPR has a segment that illustrates the power of Thick Data, featuring Frans de Waal, a primatologist and biologist who just published ‚ÄúThe Bonobo and the Atheist: In Search of Humanism Among the Primates‚Äù. Through his experiments, de Waal provides evidence to support his theory that a sense of fairness ‚Äî the groundwork for morality ‚Äî is not unique to humans. In his talks, de Waal shows a video of two Capuchin monkeys receiving different rewards for performing the same action. The monkey that gets a cucumber becomes very upset when she sees the monkey next to her given a grape as a reward for performing a similar task. In the monkey world, grapes are crack and cucumbers are stale bread.
In his research statement, de Waal makes a captivating case for the principles of Thick Data:
‚ÄúI show that video often, because if I show the data, which are graphs and stuff like that, people are not really convinced, if you show the emotional reaction, the amount of emotion that goes in there, then people are convinced.‚Äù
As de Waal makes clear, sometimes the quantitative data alone will not make a compelling argument. Even scientists need stories to make their point.
OPPORTUNITIES
While using Big Data in isolation can be problematic, it is definitely critical to continue exploring how Big Data and Thick Data can complement each other. This is a great opportunity for qualitative researchers to position our work in the context of Big Data‚Äôs quantitative results. Companies like Claro Partners are even reframing the way we ask questions about Big Data. In their Personal Data Economy research, instead of asking what Big Data tells us about human behavior, they asked what human behavior tells us about the role of Big Data in everyday life. They created a toolkit for clients that helps them shift their ‚Äúperspective from a data-centric one to a human-centred one.‚Äù
Here are some areas where I see opportunity for collaboration between the two methods within organizations (this is not meant to an exhaustive or comprehensive list).
Health Care ‚Äî As individuals have become more empowered to monitor their own health, Quantified Self values are going mainstream. Health providers will have increased access to collectively sourced, anonymized data. Projects such as Asthma Files provide a glimpse into the future of Thick and Big Data partnerships to solve global health problems.
Repurposed anonymized data from mobile operators ‚Äî Mobile companies around the world are starting to repackage and sell their customer data. Marketers are not the only buyers. City planners who want better location-based data to understand transportation are using Air Sage‚Äôs cellular network data. To protect user privacy, the data is either anonymized or deliberately scrubbed of personal communication. And yet in the absence of key personal details, the data loses key contextual information. Without Thick Data, it will be difficult for organizations to understand the personal and social context of data that has been scrubbed of personal information.
Social Network Analysis ‚Äî Social media produces droves of data that can enrich social network analysis. Research scientists such as Hilary Mason, Gilad Lotan, Duncan Watts, and Ethan Zuckerman (and his lab at MIT Media Lab) are exploring how information spreads on social networks and, at the same time, are creating more questions that can only be answered by using Thick Data methods. As more companies make use of social media metrics, organizations have to be careful not to mistakenly believe that data alone will reveal ‚Äúinfluencers.‚Äù An example of misinterpreting a signal from Big Data network analysis is the media‚Äôs write up of Cesar Hildalgo‚Äôs work, suggesting that Wikipedia could serve as a proxy for culture. (Read Heather Ford‚Äôs correction.)
Brand Strategy and Generating Insight ‚Äî Companies have long relied on market analysis to dictate corporate strategy and insight generation. They are now turning to a more user-centered approach that relies on Thick Data. Fast Company‚Äôs recent feature of Jcrew makes clear that where Big Data driven management consultants failed, the heroes that led a brand‚Äôs turnaround were employees who understood what consumers wanted. One employee, Jenna Lyons was given the opportunity to implement iterative, experimental, and real-time testing of products with consumers. Her approach resonated with consumers, transforming Jcrew into a cult brand and tripling its revenues.
Product/Service Design ‚Äî Algorithms alone do not solve problems, and yet many organizations rely on them for product and service design. Xerox uses Big Data to solve problems for the government, but they also use ethnographic methods alongside analytics. Ellen Issacs, a Xerox PARC ethnographer speaks to the importance of Thick Data in design: ‚Äú[e]ven when you have a clear concept for a technology, you still need to design it so that it‚Äôs consistent with the way people think about their activities . . . you have to watch them doing what they do.‚Äù
Implementing Organizational Strategy ‚Äî Thick Data can be used as a counterbalance to Big Data to mitigate the disruptiveness of planned organizational change. Quantitative data may suggest that a change is needed, but disruption inside organizations can be costly. When organizational charts are rearranged, job descriptions are rewritten, job functions shift, and measures of success are reframed ‚Äî the changes can cause a costly disruption that may not show up in the Big Data plan. Organizations need Thick Data experts to work alongside business leaders to understand the impact and context of changes to from a cultural perspective to determine which changes are advisable and how to navigate the process. Grant McCracken calls this the Chief Cultural Officer, the ‚Äúcorporation‚Äôs eyes and ears, allowing it to detect coming changes, even when they exist only as the weakest of signals.‚Äù The CCO is the go to Thick Data person, responsible for collecting, telling, and circulating stories to keep an organization inspired and agile. Roger Magoulas, who coined the term Big Data, emphasizes the need for stories: ‚Äústories tend to spread quickly, helping spread the lessons from the analysis throughout an organization.‚Äù
FUTURE
The integration of Thick Data raises new questions for organizations.
There is greater demand for ethnographers as suppliers/providers than as employees inside organizations. There are not enough ethnographers working inside companies to internalize ethnographic research and to explore different ways to extend the insights of Big and Thick Data.
Even though we have a lot of questions to answer about Thick Data, it‚Äôs important to keep in mind that this is the time for ethnographic work to really shine. We‚Äôre in a great position to show the value-add we bring to a mixed-method project. Producing ‚Äúthick descriptions‚Äù (a term used by Clifford Geertz to describe ethnographic methodology) of a social context compliments Big Data findings. People and organizations pioneering Big Data and Thick Data projects, such as Fabien Giradin from the Near Future Laboratory or Wendy Hsu, are giving us glimpses into this world.
It‚Äôs also important to remember that there is a lot of progress to be made on Big Data. According to a Gartner study of companies who had invested in Big Data capabilities, only 8% of were doing anything significant with their Big Data. The rest were only using Big Data for incremental advances. This means that a lot of companies are talking and investing in Big Data, but they aren‚Äôt doing anything transformational with it.
For enterprise and institutions to realize the full potential of Big Data, I believe they they need Thick Data. And that is why ethnographers are more needed now than ever. We play a critical role in keeping organizations human-centered in the era of Big Data.
_________________
Since writing this updated article in 2015, I gave a talk at TEDx with a business audience in mind for why every organization needs to prioritize Thick Data and the integration of it with Big Data. If you‚Äôre struggling to get alignment for the importance for Thick Data, share this video with your leadership and management teams.
Yish Lim
in
Towards Data Science
Natalie Herd
in
Mind Tap Research
Madison Hunter
in
Towards Data Science
Abhinav
The Quacker
MIT CTL
in
MITSupplyChain
Abhilasha Ojha
Doug Steen
"
314,https://medium.com/free-code-camp/how-machines-make-sense-of-big-data-an-introduction-to-clustering-algorithms-4bd97d4fbaba,How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms,"Take a look at the image below. It‚Äôs a collection of bugs and creepy-crawlies of different shapes and sizes. Take a moment to categorize them by similarity into a number of groups.
This isn‚Äôt a trick question. Start with grouping the spiders together.
Done? While there‚Äôs not necessarily a ‚Äúcorrect‚Äù answer here, it‚Äôs most likely you split the bugs into four clusters. The spiders in one cluster, the pair of snails in another, the butterflies and moth into one, and the trio of wasps and bees into one more.
That wasn‚Äôt too bad, was it? You could probably do the same with twice as many bugs, right? If you had a bit of time to spare ‚Äî or a passion for entomology ‚Äî you could probably even do the same with a hundred bugs.
For a machine though, grouping ten objects into however many meaningful clusters is no small task, thanks to a mind-bending branch of maths called combinatorics, which tells us that are 115,975 different possible ways you could have grouped those ten insects together. Had there been twenty bugs, there would have been over fifty trillion possible ways of clustering them.
With a hundred bugs ‚Äî there‚Äôd be many times more solutions than there are particles in the known universe. How many times more? By my calculation, approximately five hundred million billion billion times more. In fact, there are more than four million billion googol solutions (what‚Äôs a googol?). For just a hundred objects.
Almost all of those solutions would be meaningless ‚Äî yet from that unimaginable number of possible choices, you pretty quickly found one of the very few that clustered the bugs in a useful way.
Us humans take it for granted how good we are categorizing and making sense of large volumes of data pretty quickly. Whether it‚Äôs a paragraph of text, or images on a screen, or a sequence of objects ‚Äî humans are generally fairly efficient at making sense of whatever data the world throws at us.
Given that a key aspect of developing A.I. and Machine Learning is getting machines to quickly make sense of large sets of input data, what shortcuts are there available? Here, you can read about three clustering algorithms that machines can use to quickly make sense of large datasets. This is by no means an exhaustive list ‚Äî there are other algorithms out there ‚Äî but they represent a good place to start!
You‚Äôll find for each a quick summary of when you might use them, a brief overview of how they work, and a more detailed, step-by-step worked example. I believe it helps to understand an algorithm by actually carrying out yourself. If you‚Äôre really keen, you‚Äôll find the best way to do this is with pen and paper. Go ahead ‚Äî nobody will judge!
‚Ä¶you have an idea of how many groups you‚Äôre expecting to find a priori.
The algorithm randomly assigns each observation into one of k categories, then calculates the mean of each category. Next, it reassigns each observation to the category with the closest mean before recalculating the means. This step repeats over and over until no more reassignments are necessary.
Take a group of 12 football (or ‚Äòsoccer‚Äô) players who have each scored a certain number of goals this season (say in the range 3‚Äì30). Let‚Äôs divide them into separate clusters ‚Äî say three.
Step 1 requires us to randomly split the players into three groups and calculate the means of each.
Step 2: For each player, reassign them to the group with the closest mean. E.g., Player A (5 goals) is assigned to Group 2 (mean = 9). Then recalculate the group means.
Repeat Step 2 over and over until the group means no longer change. For this somewhat contrived example, this happens on the next iteration. Stop! You have now formed three clusters from the dataset!
With this example, the clusters could correspond to the players‚Äô positions on the field ‚Äî such as defenders, midfielders and attackers. K-means works here because we could have reasonably expected the data to fall naturally into these three categories.
In this way, given data on a range of performance statistics, a machine could do a reasonable job of estimating the positions of players from any team sport ‚Äî useful for sports analytics, and indeed any other purpose where classification of a dataset into predefined groups can provide relevant insights.
There are several variations on the algorithm described here. The initial method of ‚Äòseeding‚Äô the clusters can be done in one of several ways. Here, we randomly assigned every player into a group, then calculated the group means. This causes the initial group means to tend towards being similar to one another, which ensures greater repeatability.
An alternative is to seed the clusters with just one player each, then start assigning players to the nearest cluster. The returned clusters are more sensitive to the initial seeding step, reducing repeatability in highly variable datasets. However, this approach may reduce the number of iterations required to complete the algorithm, as the groups will take less time to diverge.
An obvious limitation to K-means clustering is that you have to provide a priori assumptions about how many clusters you‚Äôre expecting to find. There are methods to assess the fit of a particular set of clusters. For example, the Within-Cluster Sum-of-Squares is a measure of the variance within each cluster. The ‚Äòbetter‚Äô the clusters, the lower the overall WCSS.
‚Ä¶you wish to uncover the underlying relationships between your observations.
A distance matrix is computed, where the value of cell (i, j) is a distance metric between observations i and j. Then, pair the closest two observations and calculate their average. Form a new distance matrix, merging the paired observations into a single object. From this distance matrix, pair up the closest two observations and calculate their average. Repeat until all observations are grouped together.
Here‚Äôs a super-simplified dataset about a selection of whale and dolphin species. As a trained biologist, I can assure you we normally use much more detailed datasets for things like reconstructing phylogeny. For now though, we‚Äôll just look at the typical body lengths for these six species. We‚Äôll be using just two repeated steps.
Step 1: compute a distance matrix between each species. Here, we‚Äôll use the Euclidean distance ‚Äî how far apart are the data points? Read this exactly as you would a distance chart in a road atlas. The difference in length between any pair of species can be looked up by reading the value at the intersection of the relevant row and column.
Step 2: Pair up the two closest species. Here, this will be the Bottlenose & Risso‚Äôs Dolphins, with an average length of 3.3m.
Repeat Step 1 by recalculating the distance matrix, but this time merge the Bottlenose & Risso‚Äôs Dolphins into a single object with length 3.3m.
Next, repeat Step 2 with this new distance matrix. Here, the smallest distance is between the Pilot & Killer Whales, so we pair them up and take their average ‚Äî which gives us 7.0m.
Then, we repeat Step 1 ‚Äî recalculate the distance matrix, but now we‚Äôve merged the Pilot & Killer Whales into a single object of length 7.0m.
Next, we repeat Step 2 with this distance matrix. The smallest distance (3.7m) is between the two merged objects ‚Äî so now we merge them into an even bigger object, and take the average (which is 5.2m).
Then, we repeat Step 1 and compute a new distance matrix, having merged the Bottlenose & Risso‚Äôs Dolphins with the Pilot & Killer Whales.
Next, we repeat Step 2. The smallest distance (5.0m) is between the Humpback & Fin Whales, so we merge them into a single object, and take the average (17.5m).
Then, it‚Äôs back to Step 1 ‚Äî compute the distance matrix, having merged the Humpback & Fin Whales.
Finally, we repeat Step 2 ‚Äî there is only one distance (12.3m) in this matrix, so we pair everything into one big object, and now we can stop! Let‚Äôs look at the final merged object:
It has a nested structure (think JSON), which allows it to be drawn up as a tree-like graph, or dendrogram. It reads in much the same way a family tree might. The nearer two observations are on the tree, the more similar or closely-related they are taken to be.
The structure of the dendrogram gives us insight into how our dataset is structured. In our example, we see two main branches, with Humpback Whale and Fin Whale on one side, and the Bottlenose Dolphin/Risso‚Äôs Dolphin and Pilot Whale/Killer Whale on the other.
In evolutionary biology, much larger datasets with many more specimens and measurements are used in this way to infer taxonomic relationships between them. Outside of biology, hierarchical clustering has applications in Data Mining and Machine Learning contexts.
The cool thing is that this approach requires no assumptions about the number of clusters you‚Äôre looking for. You can split the returned dendrogram into clusters by ‚Äúcutting‚Äù the tree at a given height. This height can be chosen in a number of ways, depending on the resolution at which you wish to cluster the data.
For instance, looking at the dendrogram above, if we draw a horizontal line at height = 10, we‚Äôd intersect the two main branches, splitting the dendrogram into two sub-graphs. If we cut at height = 2, we‚Äôd be splitting the dendrogram into three clusters.
There are essentially three aspects in which hierarchical clustering algorithms can vary to the one given here.
Most fundamental is the approach ‚Äî here, we have used an agglomerative process, whereby we start with individual data points and iteratively cluster them together until we‚Äôre left with one large cluster. An alternative (but more computationally intensive) approach is to start with one giant cluster, and then proceed to divide the data into smaller and smaller clusters until you‚Äôre left with isolated data points.
There are also a range of methods that can be used to calculate the distance matrices. For many purposes, the Euclidean distance (think Pythagoras‚Äô Theorem) will suffice, but there are alternatives that may be more applicable in some circumstances.
Finally, the linkage criterion can also vary. Clusters are linked according to how close they are to one another, but the way in which we define ‚Äòclose‚Äô is flexible. In the example above, we measured the distances between the means (or ‚Äòcentroids‚Äô) of each group and paired up the nearest groups. However, you may want to use a different definition.
For example, each cluster is made up of several discrete points. We could define the distance between two clusters to be the minimum (or maximum) distance between any of their points ‚Äî as illustrated in the figure below. There are still other ways of defining the linkage criterion, which may be suitable in different contexts.
‚Ä¶you have data that can be represented as a network, or ‚Äògraph‚Äô.
A graph community is very generally defined as a subset of vertices which are more connected to each other than with the rest of the network. Various algorithms exist to identify communities, based upon more specific definitions. Algorithms include, but are not limited to, Edge Betweenness, Modularity-Maximsation, Walktrap, Clique Percolation, Leading Eigenvector‚Ä¶
Graph theory, or the mathematical study of networks, is a fascinating branch of mathematics that lets us model complex systems as an abstract collection of ‚Äòdots‚Äô (or vertices) connected by ‚Äòlines‚Äô (or edges).
Perhaps the most intuitive case-studies are social networks. Here, the vertices represent people, and edges connect vertices who are friends/followers. However, any system can be modelled as a network if you can justify a method to meaningfully connect different components. Among the more innovative applications of graph theory to clustering include feature extraction from image data, and analysing gene regulatory networks.
As an entry-level example, take a look at this quickly put-together graph. It shows the eight websites I most recently visited, linked according to whether their respective Wikipedia articles link out to one another. You could assemble this data manually, but for larger-scale projects, it‚Äôs much quicker to write a Python script to do the same. Here‚Äôs one I wrote earlier.
The vertices are colored according to their community membership, and sized according to their centrality. See how Google and Twitter are the most central?
Also, the clusters make pretty good sense in the real-world (always an important performance indicator). The yellow vertices are generally reference/look-up sites; the blue vertices are all used for online publishing (of articles, tweets, or code); and the red vertices include YouTube, which was of course founded by former PayPal employees. Not bad deductions for a machine!
Aside from being a useful way to visualize large systems, the real power of networks comes from their mathematical analysis. Let‚Äôs start by translating our nice picture of the network into a more mathematical format. Below is the adjacency matrix of the network.
The value at the intersection of each row and column records whether there is an edge between that pair of vertices. For instance, there is an edge between Medium and Twitter (surprise, surprise!), so the value where their rows/columns intersect is 1. Similarly, there is no edge between Medium and PayPal, so the intersection of their rows/columns returns 0.
Encoded within the adjacency matrix are all the properties of this network ‚Äî it gives us the key to start unlocking all manner of valuable insights. For a start, summing any column (or row) gives you the degree of each vertex ‚Äî i.e., how many others it is connected to. This is commonly denoted with the letter k.
Likewise, summing the degrees of every vertex and dividing by two gives you L, the number of edges (or ‚Äòlinks‚Äô) in the network. The number of rows/columns gives us N, the number of vertices (or ‚Äònodes‚Äô) in the network.
Knowing just k, L, N and the value of each cell in the adjacency matrix A lets us calculate the modularity of any given clustering of the network.
Say we‚Äôve clustered the network into a number of communities. We can use the modularity score to assess the ‚Äòquality‚Äô of this clustering. A higher score will show we‚Äôve split the network into ‚Äòaccurate‚Äô communities, whereas a low score suggests our clusters are more random than insightful. The image below illustrates this.
Modularity can be calculated using the formula below:
That‚Äôs a fair amount of math, but we can break it down bit by bit and it‚Äôll make more sense.
M is of course what we‚Äôre calculating ‚Äî modularity.
1/2L tells us to divide everything that follows by 2L, i.e., twice the number of edges in the network. So far, so good.
The Œ£ symbol tells us we‚Äôre summing up everything to the right, and lets us iterate over every row and column in the adjacency matrix A. For those unfamiliar with sum notation, the i, j = 1 and the N work much like nested for-loops in programming. In Python, you‚Äôd write it as follows:
So what is #stuff with i and j in more detail?
Well, the bit in brackets tells us to subtract ( k_i k_j ) / 2L from A_ij.
A_ij is simply the value in the adjacency matrix at row i, column j.
The values of k_i and k_j are the degrees of each vertex ‚Äî found by adding up the entries in row i and column j respectively. Multiplying these together and dividing by 2L gives us the expected number of edges between vertices i and j if the network were randomly shuffled up.
Overall, the term in the brackets reveals the difference between the network‚Äôs real structure and the expected structure it would have if randomly reassembled. Playing around with the values shows that it returns its highest value when A_ij = 1, and ( k_i k_j ) / 2L is low. This means we see a higher value if there is an ‚Äòunexpected‚Äô edge between vertices i and j.
Finally, we multiply the bracketed term by whatever the last few symbols refer to.
The ùõøc_i, c_j is the fancy-sounding but totally harmless Kronecker-delta function. Here it is, explained in Python:
Yes ‚Äî it really is that simple. The Kronecker-delta function takes two arguments, and returns 1 if they are identical, otherwise, zero.
This means that if vertices i and j have been put in the same cluster, then ùõøc_i, c_j = 1. Otherwise, if they are in different clusters, the function returns zero.
As we are multiplying the bracketed term by this Kronecker-delta function, we find that for the nested sum Œ£, the outcome is highest when there are lots of ‚Äòunexpected‚Äô edges connecting vertices assigned to the same cluster. As such, modularity is a measure of how well-clustered the graph is into separate communities.
Dividing by 2L bounds the upper value of modularity at 1. Modularity scores near to or below zero indicate the current clustering of the network is really no use. The higher the modularity, the better the clustering of the network into separate communities. By maximising modularity, we can find the best way of clustering the network.
Notice that we have to pre-define how the graph is clustered to find out how ‚Äògood‚Äô that clustering actually is. Unfortunately, employing brute force to try out every possible way of clustering the graph to find which has the highest modularity score would be computationally impossible beyond a very limited sample size.
Combinatorics tells us that for a network of just eight vertices, there are 4140 different ways of clustering them. A network twice the size would have over ten billion possible ways of clustering the vertices. Doubling the network again (to a very modest 32 vertices) would give 128 septillion possible ways, and a network of eighty vertices would be cluster-able in more ways than there are atoms in the observable universe.
Instead, we have to turn to a heuristic method that does a reasonably good job at estimating the clusters that will produce the highest modularity score, without trying out every single possibility. This is an algorithm called Fast-Greedy Modularity-Maximization, and it‚Äôs somewhat analogous to the agglomerative hierarchical clustering algorithm describe above. Instead of merging according to distance, ‚ÄòMod-Max‚Äô merges communities according to changes in modularity. Here‚Äôs how it goes:
Begin by initially assigning every vertex to its own community, and calculating the modularity of the whole network, M.
Step 1 requires that for each community pair linked by at least a single edge, the algorithm calculates the resultant change in modularity ŒîM if the two communities were merged into one.
Step 2 then takes the pair of communities that produce the biggest increase in ŒîM, which are then merged. Calculate the new modularity M for this clustering, and keep a record of it.
Repeat steps 1 and 2 ‚Äî each time merging the pair of communities for which doing so produces the biggest gain in ŒîM, then recording the new clustering pattern and its associated modularity score M.
Stop when all the vertices are grouped into one giant cluster. Now the algorithm checks the records it kept as it went along, and identifies the clustering pattern that returned the highest value of M. This is the returned community structure.
Whew! That was computationally intensive, at least for us humans. Graph theory is a rich source of computationally challenging, often NP-hard problems ‚Äî yet it also has incredible potential to provide valuable insights into complex systems and datasets. Just ask Larry Page, whose eponymous PageRank algorithm ‚Äî which helped propel Google from start-up to basically world domination in less than a generation ‚Äî was based entirely in graph theory.
Community detection is a major focus of current research in graph theory, and there are plenty of alternatives to Modularity-Maximization, which while useful, does have some drawbacks.
For a start, its agglomerative approach often sees small, well-defined communities swallowed up into larger ones. This is known as the resolution limit ‚Äî the algorithm will not find communities below a certain size. Another challenge is that rather than having one distinct, easy-to-reach global peak, the Mod-Max approach actually tends to produce a wide ‚Äòplateau‚Äô of many similar high modularity scores ‚Äî making it somewhat difficult to truly identify the absolute maximum score.
Other algorithms use different ways to define and approach community detection. Edge-Betweenness is a divisive algorithm, starting with all vertices grouped in one giant cluster. It proceeds to iteratively remove the least ‚Äòimportant‚Äô edges in the network, until all vertices are left isolated. This produces a hierarchical structure, with similar vertices closer together in the hierarchy.
Another algorithm is Clique Percolation, which takes into account possible overlap between graph communities. Yet another set of algorithms are based on random-walks across the graph, and then there are spectral clustering methods which start delving into the eigendecomposition of the adjacency matrix and other matrices derived therefrom. These ideas are used in feature extraction in, for example, areas such as Computer Vision.
It‚Äôd be well beyond the scope of this article to give each algorithm its own in-depth worked example. Suffice to say that this is an active area of research, providing powerful methods to make sense of data that even a generation ago would have been extremely difficult to process.
Hopefully this article has informed and inspired you to better understand how machines can make sense of Big Data! The future is a rapidly changing place, and many of those changes will be driven by what technology becomes capable of in the next generation or two.
As outlined in the introduction, Machine Learning is an extraordinarily ambitious field of research, in which massively complex problems require solving in as accurate and as efficient a way possible. Tasks that come naturally to us humans require innovative solutions when taken on by machines.
There‚Äôs still plenty of progress to be made, and whoever contributes the next breakthrough idea will no doubt be generously rewarded. Maybe someone reading this article will be behind the next powerful algorithm? All great ideas have to start somewhere!
Piero Esposito
in
Towards Data Science
Hanzala Qureshi
in
Geek Culture
Harry zheng
Alibaba Cloud
Rodrigo Bressan
in
Towards Data Science
Greg Filla
DataKitchen
in
data-ops
Ravi Suhag
in
Gojek Product + Tech
"
315,https://medium.com/towards-data-science/are-you-still-using-pandas-for-big-data-12788018ba1a,Are you still using Pandas for big data?,"Pandas is one of the best tools when it comes to Exploratory Data Analysis. But this doesn't mean that it is the best tool available for every task ‚Äî like big data processing. I‚Äôve spent so much time waiting for pandas to read a bunch of files or to aggregate them and calculate features.
Recently, I took the time and found a better tool, which made me update my data processing pipeline. I use this tool for heavy data processing ‚Äî like reading multiple files with 10 gigs of data, apply filters to them and do aggregations. When I am done with heavy processing I save the result to a smaller ‚Äúpandas friendly‚Äù CSV file and continue with Exploratory Data Analysis in pandas.
Here are few links that might interest you:
Would you like to read more such articles? If so, you can support me by clicking on any links above. Some of them are affiliate links, but you don‚Äôt need to buy anything.
Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love. This includes numpy, pandas and sklearn. It is open-source and freely available. It uses existing Python APIs and data structures to make it easy to switch between Dask-powered equivalents.
Dask makes simple things easy and complex things possible
I could go on and on describing Dask, because it has so many features, but instead, let's look at a practical example. In my work, I usually get a bunch of files that I need to analyze. Let‚Äôs simulate my workday and create 10 files with 100K entries (each file has 196 MB).
Now, let‚Äôs read those files with pandas and measure time. Pandas doesn‚Äôt have native glob support so we need to read files in a loop.
It took pandas 16 seconds to read files.
Now, imagine if those files would be 100 times bigger ‚Äî you couldn‚Äôt even read them with pandas.
Dask can process data that doesn‚Äôt fit into memory by breaking it into blocks and specifying task chains. Let‚Äôs measure how long Dask needs to load those files.
Dask needed 154 ms! How is that even possible? Well, it is not. Dask has delayed execution paradigm. It only calculates things when it needs them. We define the execution graph so Dask can then optimize the execution of the tasks. Let‚Äôs repeat the experiment ‚Äî also notice that Dask‚Äôs read_csv function takes glob natively.
The compute function forces Dask to return the result. Dask read files twice as fast than pandas.
Dask natively scales Python
Does Dask use all of the cores you paid for? Let‚Äôs compare CPU usage between pandas and Dask when reading files ‚Äî the code is the same as above.
In the screen recordings above the difference in multiprocessing is obvious with pandas and Dask when reading files.
Dask‚Äôs DataFrame is composed of multiple pandas DataFrames, which are split by index. When we execute read_csv with Dask, multiple processes read a single file.
We can even visualize the execution graph.
You might be thinking if Dask is so great, why not ditch pandas all together. Well, it is not that simple. Only certain functions from pandas are ported to Dask. Some of them are hard to parallelize, like sorting values and setting indexes on unsorted columns. Dask is not a silver bullet ‚Äî usage of Dask is recommended only for datasets that don‚Äôt fit in the main memory. As Dask is built on top of pandas, operations that were slow in pandas, stay slow in Dask. Like I mentioned before, Dask is a useful tool in the data pipeline process, but it doesn‚Äôt replace other libraries.
Dask is recommended only for datasets that don‚Äôt fit in the main memory
To install Dask simply run:
This will install the whole Dask library.
I‚Äôve only touched the surface of Dask library in this blog post. If you would like to dive deeper check amazing Dask tutorials and Dask‚Äôs DataFrame documentation. Interested in which DataFrame functions are supported in Dask? Check DataFrame API.
Download the Jupyter Notebook to follow examples.
Follow me on Twitter, where I regularly tweet about Data Science and Machine Learning.
Alibaba Cloud
Excellarate
Yasmin Nozari
in
The Startup
Chris McLeod
Aashish Dhawan
Quick Code
in
Quick Code
Diego Bustamante
in
Better Programming
Aryan Venkat
"
316,https://medium.com/@hbothra22/how-do-i-get-started-in-cyber-security-my-perspective-learning-path-b53065189ba5,How do I get Started in Cyber Security? ‚Äî My Perspective & Learning Path!,"Cyber Security, Ethical Hacking, Application Security, Penetration Testing, Bug Bounties, etc., these career options are blooming and becoming popular among teenagers, scholars, experienced professionals, etc. globally. The reason for this popularity is surely the growing community, increased attacks, and need for skilled professionals.
However, this domain is at a time confusing for a beginner and it may look like an ocean when it comes to Where should I start from? What should I learn first? Damn, there‚Äôs a lot to learn and it will take years for me to get into the game, is there any shortcut or shall I leave this domain!! These are not just some questions I frame for this blog but these are the most commonly asked questions to me over Twitter, Instagram, LinkedIn, Emails, etc.
Sometimes, it really hard to provide an exact answer to a curious mind when they come and ask me, Hey Harsh, I am new to Cyber Security and I really want to get started, can you please help me out? Keeping this thing in mind, I am writing this blog to answer all these questions and in a nutshell provide you all with a learning path from my perspective on How to Get Started in Cyber Security?
Cyber Security is a huge domain and when you refer to Cyber Security, you are actually not clear what domain you are actually trying to ask! When you say Cyber Security it can be ‚Äî you want to be a bug bounty hunter or maybe a blue teamer or a cyber forensics guy or maybe you are not sure either. So let‚Äôs just first break down some general career options in Cyber Security which will help you to know where you want to go exactly.
The above mindmap breaks down Cyber Security career options in three major domains Offensive, Defensive & Research. (Please note that Offensive Cyber Security is not an illegal term). Since, there‚Äôs a lot to explain, however, as I work into Offensive Cyber Security and if your end goals are to move into the Offensive Cyber Security side, keep on reading as in the next few paragraphs, I am going to give you a more close view of this domain, some requirements & how one should start learning & build a good career in it.
When we talk about getting started into Offensive Cyber Security, there are many domains such as Application Security, Red Teaming, Penetration Testing, Code Reviews, Cloud Audits, etc. However, there are always some initial requirements and at the end of the day, everything is linked somehow.
Pre-requisite:
Since I am into Offensive Security & Application Security I can guide better in how to get a start into this domain and below is the breakdown of various essential setups one should take to get a good grasp from basic to advance concepts. So let‚Äôs have a look into it:
There are multiple Operating Systems available in the market, however, the most common are Windows, Mac & Linux-based Operating Systems. On the other hand, Android & iOS are the majority players in the Mobile Operating Systems.
There are many scenarios when you will require to know how a particular operating system works and what are various security features provided by them, what are the limitations each of them has and how an attacker can abuse them.
For Example: As a pentester, you have gained access to a Linux Machine as a restricted user. Now, in order to escape the restriction, you will need to know how you can abuse the available features and gain privileged access. This becomes really simple when you know what are various features present and you will have a good hold of what to exploit and gain privileged access.
Resources to learn about Operating Systems:
If you wish to dive deeper into these operating systems, you can look out for Windows Internals or Linux Internals which is a more comprehensive study about these operating systems. However, I would recommend getting a basic knowledge at least.
Computer Networks are a really important aspect for any IT professional not just for the IT security people. Most of the time when you will encounter any engagement as an offensive security guy, you will be dealing with either a Web Application, Mobile Application, Cloud Infrastructure, Internal Network, Routers, Firewalls, IoT Devices, etc. and if you notice, most of these architectures communicate using internet directly or indirectly.
As a security professional, you must know basic concepts of networking which include knowing how Routing, Firewalling, SSL, TLS, Ports, Protocols, IP, TCP, UDP, MAC, and other important network security features work. Why this knowledge is required doesn‚Äôt need any explanation itself, it‚Äôs essential, that‚Äôs all.
Resources to learn Computer Networks
The web application is one of the widely encountered things during my journey as an application security engineer and offensive security guy. Most of the externally faced resources of any organization are web applications.
Before diving deeper into how to test for Web Application Security, it is essential to know various concepts about Web Application, their communication, and components. This includes understanding how an HTTP Request is formed, how HTTP Response works, what are the various security headers, browser security features, what is CSP and CORS, etc. This is a basic concept and will eventually help you throughout your journey as Application Security Guy!
Resources to learn Basic Concepts
Once you have gained enough understanding about the basic concepts of Web Application, Operating Systems & Computer Networks, the next part before exploring the Security perspective is to understand some of the common security frameworks which are followed at the industry standard. This is going to help you in proper referencing and understanding which category your security vulnerability lies in. How you can rate the severity of an identified security issue and how to approach for remediation.
OWASP: The Open Web Application Security Project is an online community that produces freely-available articles, methodologies, documentation, tools, and technologies in the field of web application security.
STRIDE: The STRIDE model was developed by Microsoft in order to help security engineers understand and classify all possible threats on a server. The name of this model is an acronym for the six main types of threats.
CVSS: The Common Vulnerability Scoring System (CVSS) provides a way to capture the principal characteristics of a vulnerability and produce a numerical score reflecting its severity. The numerical score can then be translated into a qualitative representation (such as low, medium, high, and critical) to help organizations properly assess and prioritize their vulnerability management processes.
At this point, we will talk about how to get started in Web Application Security. There are tons of resources out there in the wild and it is not possible to learn/read all of them, however, choosing the best ones is also a difficult task. I am sharing some of the good resources to follow in order to get a good hold:
If you can purchase paid subscriptions I would suggest these two paid labs and they have nice content:
Please note that I am not associated with these vendors in any sort of paid promotions.
Note: Do not spend much time on the labs and start exploring the things in the real-life application through bug bounty and responsible disclosure programs. Investing more time in labs may make you uncomfortable testing real-world applications. When you feel you need to learn any attack say cross-site scripting, simply google about the available labs and solve them, start hitting the real application and follow the process again.
Getting started with Web Application security is really simple if your basics are strong and you follow a proper approach, I strongly recommend not waste much time only looking for resources rather invest your time into facing real-life applications and various scenarios.
For Network Security, there is no limit of resources but having a good understanding of computer networking from a security point is much required. There are multiple ways to practice for network security, however, the best way is to invest time in solving labs using HackTheBox and understanding new concepts. some of the good resources are:
The next big thing we often encounter as an Application Security engineer is Mobile Applications. However, this is a really interesting area as you have access to the source code by simply reverse-engineering the application and you can perform both Static as well as Dynamic Analysis.
I am also a learner when it comes to Mobile Application Security and currently, I have limited resources to suggest (I‚Äôll add more in the coming days in this blog itself) which one can follow to pick up the pace with Mobile Application Security:
It took me a lot of time to put all the pieces together to write this blog but the purpose of drafting this blog is to answer all the questions that are generally asked to me around ‚ÄúHow to get started‚Äù, ‚ÄúWhat is the right path‚Äù and others. Often, it is not possible to answer everyone but I hope this blog will help everyone who is seeking answers on this topic and all the newcomers. I will try to keep this blog updated at regular time intervals. In the future, I plan to add resources to get started into Cloud Security, Thick Clients, and other areas in this blog as well. I hope you enjoy reading this and get enough resources to get started in Cyber Security (Application Security/Offensive Security)
This blog itself is a whole bunch of takeaways. This is specially written to direct people whenever they seek guidance to get started in the Cyber Security domain. This is totally my learning path and perspective about how one should get started and I am not saying to blindly follow this but if this looks good to you, follow it and I will try to keep it updated.
If you have any good resources that you know should be included in this blog, please leave a PRIVATE NOTE and I will add the relevant ones to the blog.
If you enjoyed reading the article do clap and follow:
Twitter: https://www.twitter.com/harshbothra_
LinkedIn: https://www.linkedin.com/in/harshbothra
Website: https://harshbothra.tech
GitHub: https://www.github.com/harsh-bothra
Talks: https://www.youtube.com/playlist?list=PLYn5_MxRvV-fxPL90I-uebXQzQBXfIaY0
Slides: https://speakerdeck.com/harshbothra
Sean Low
Datamine Network
Catie Paula
Michael James Balsa
in
System Weakness
IndyZa
YetiSwap
Harish Dhivare
Alchemist Warden
in
WARDEN Official
"
317,https://medium.com/@4n6lady/cybersecurity-the-starting-line-8b9ee97f73c1,Cybersecurity: The Starting Line,"As I move towards my goals of becoming a CISO, I realize the path that I have taken to become a Sr. Cyber Security Specialist is not like everyone else‚Äôs. As a matter of fact, everyone else‚Äôs path is different from every other path ‚Äî and it should be.
The story about my path is for another day, however, I do want to address the struggles that people are facing when trying to break into Cybersecurity; the more you read about it, the more overwhelming and daunting it can become.
Definition: The practice of protecting internet-connected systems and data from cyber-threats using various methods.
I would like to address is that Cybersecurity is not all about pentesting or hacking. In fact, hacking should always be referred to as ‚Äúauthorized penetration testing‚Äù which is under ‚ÄúRisk Assessment‚Äù (aka Active Defense), but it‚Äôs not everything. It would be a mistake of think of cybersecurity as a single career path.
See mindmap of Cybersecurity roles and career paths:
This doesn‚Äôt cover everything, and the map is based on the creator‚Äôs personal view (Henry Jiang ‚Äî post and mindmap file can be found here: https://www.linkedin.com/pulse/map-cybersecurity-domains-version-20-henry-jiang-ciso-cissp/); it is missing specific control categories unless they are important enough to stand on their own. Example: IDS/IPS, firewall, DDoS Remediation ‚Äî these can be included under Security Ops, Detection, and Prevention.
Note: You do not have to be an expert in every subdomain of Cybersecurity!
Cybersecurity offers a wide variety of opportunities for both technical and non-technical individuals; Doing security well requires a mix of technology, sociology, law, politics, and organizational sciences. And to do security well, you don‚Äôt necessarily need a Cybersecurity degree or 10 years of experience in tech.
I recommend the next few steps to all new beginners:
Although this can‚Äôt be considered as a guide to winning that Cybersecurity position you‚Äôve been aiming for, I hope it comes to some use to those starting out and needing a small light of hope.
Cybersecurity is a very rewarding field to work in; I hope to see you all here soon!
Devadeep Chowdhury
in
Spark.Live
Vaibhav Mishra
CyberpopfansCommunity
ICT Token Official
Fortunas Finance
SlowMist
DIA Core Team
in
DIA Insights
Near Media
in
Near Media
"
318,https://medium.com/threat-intel/essential-cybersecurity-books-32ce92c24c47,8 cyber security professionals share their essential reads,"Welcome to Threat Intel‚Äôs #WednesdayWisdom column, which aims to help improve your cybersecurity knowledge and keep you informed on important developments.
There are many sources of information for those working in, or hoping to work in, the world of cyber security. Technical blogs, online courses, podcasts, webcasts, and more provide a wealth of information on the latest happenings in the threat landscape.
However, sometimes you can‚Äôt beat a good book, so in the run-up to World Book Day, which takes place on Sunday, April 23, we asked a range of Symantec experts for their essential reads for cyber security professionals (or those looking to get into the industry), as well as any other personal favorites.
The recommendations of Eric Chien, distinguished engineer and technical director in Symantec Security Response, ranged from practical guides for those in the industry, to some cracking tales of cybercrime.
Unsurprisingly, given Eric was one of the lead researchers involved in Symantec‚Äôs investigation into the infamous Stuxnet threat, even featuring in Alex Gibney‚Äôs Zero Days documentary on the subject, one of his recommendations is a book looking at the background of that digital weapon.
¬∑ Cryptonomicon; Neal Stephenson: A work of fiction that jumps from the 1940s to the present day that features cryptographers and hackers, and hints at a dark future.
¬∑ Cuckoo‚Äôs Egg; Clifford Stoll: Stoll‚Äôs memoir about his quest to capture a cyber spy.
¬∑ Countdown to Zero Day: Stuxnet and the Launch of the World‚Äôs First Digital Weapon; Kim Zetter: Cyber security journalist Zetter tells the story behind Stuxnet.
¬∑ Kingpin: How One Hacker Took Over the Billion-Dollar Cybercrime Underground; Kevin Poulsen: Ex-hacker and journalist Poulsen recounts his tale of the pursuit of an infamous cybercriminal.
¬∑ The Art of Computer Virus Research and Defense; Peter Szor: One of the lead researchers behind Norton AntiVirus, Szor takes readers behind the scenes of antivirus research.
¬∑ Windows Internals, Part 1 (Developer Reference) 6th Edition; Mark E Russinovich, David A Solomon, Alex Ionescu: A guide to Windows architecture and internals.
¬∑ Applied Cryptography: Protocols, Algorithms, and Source Code in C; Bruce Schneier: An essential introduction to cryptography.
¬∑ The Practice of Network Security Monitoring: Understanding Incident Detection and Response; Richard Bejtlich: This book details how Network Security Monitoring can help protect networks and data.
Ireland-based threat analysis engineer Jennifer is currently busy preparing for Symantec‚Äôs Cyber War Games final, which will take place in May.
While Jennifer admits she tends to read technical blog posts about particular subjects, there are some practical books she continues to turn to as well.
¬∑ Practical Malware Analysis: The Hands-On Guide to Dissecting Malicious Software; Michael Sikorski, Andrew Honig: An excellent handbook for starting out with malware analysis.
¬∑ The Web Application Hacker‚Äôs Handbook: Finding and Exploiting Security Flaws; Dafydd Stuttard; Marcus Pinto: A guide to web application penetration testing.
¬∑ The Hacker Playbook 2: Practical Guide To Penetration Testing; Peter Kim: A step-by-step guide to the ‚Äúgame‚Äù of penetration hacking.
Principal technical support engineer and avid reader Mick Halpin recommended an array of books covering a variety of different areas.
¬∑ Spam Nation: The Inside Story of Organized Cybercrime ‚Äî from Global Epidemic to Your Front Door; Brian Krebs: Awesome investigation into where this stuff comes from and who makes their money from it.
¬∑ Fatal System Error: The Hunt for the New Crime Lords Who Are Bringing Down the Internet; Joseph Menn: Pretty old now but a worthy read that delves into the murky world of cyber crime.
¬∑ DarkMarket: How Hackers Became the New Mafia; Misha Glenny: Mick used one word to describe this book, which details the fight back against malicious hackers: awesome.
¬∑ McMafia: A Journey Through the Global Criminal Underworld; Misha Glenny: Also by Glenny, this book sees the veteran journalist travel across five continents to speak with people at every level of the criminal cyber underworld.
¬∑ The Hacking Exposed series contains detailed advice and strategies to help defeat cyber criminals.
¬∑ Cyber War: The Next Threat to National Security and What to Do About It; Richard A. Clarke, Robert Knake: Former presidential adviser and counter-terrorism expert Clarke details the threat the US faces from cyber crime.
¬∑ Universal Scams & Fraud Detection; David Snow: This book focuses mainly on the many billions lost to insurance fraud but also features information on computer-related scams.
¬∑ The Florentine Deception; Carey Nachenberg: A work of fiction by a Symantec alumnus, this is a bit Dan Brown, and a fun adventure that exposes some dangers inherent to computers.
¬∑ Dark Times in the City; Gene Kerrigan: A recommendation that‚Äôs unrelated to tech, this is a crime novel by Irish journalist Kerrigan.
A principal software engineer and analyst in Symantec Security Response, Gavin has been involved in many investigations into cyber criminals‚Äô tactics and behaviors.
While admitting that he now finds a lot of the information he needs online, he recommended some books he has turned to over the years.
¬∑ The Hacker Crackdown; Bruce Sterling: A nice summary of the hacking landscape in the 1980s and 1990s.
¬∑ Rise of the Machines: The Lost History of Cybernetics; Thomas Rid: Pulling together the history of cybernetics, this book also has some details about the first publicized international espionage hacking.
¬∑ Confront and Conceal: Obama‚Äôs Secret Wars and Surprising Use of American Power; David E Sanger: A wide-ranging book, it contains details about Stuxnet, and Operation Olympic Games.
¬∑ Ready Player One: A Novel; Ernest Cline: A fiction recommendation, this is a fun book for anyone with an interest in computers who grew up in the 1980s.
¬∑ Network Security Assessment: Know Your Network; Chris McNab: A guide to performing network-based penetration testing.
¬∑ Computer Networks; Andrew Tanenbaum, David Wetherall; An introduction to networking, explaining how networks work from the inside out.
¬∑ Gavin also recommends the Hacking Exposed series, which was also recommended above.
Liam, a director of development in the Symantec Security Response team, was one of the lead researchers involved in Symantec‚Äôs investigation into Stuxnet. He, together with Eric Chien, is now considered one of the authorities on this subject, and also featured in Alex Gibney‚Äôs Zero Days documentary.
Given how closely they have worked together, it is perhaps unsurprising that there was some overlap with Eric‚Äôs and Liam‚Äôs recommendations, Liam also recommended Cuckoo‚Äôs Egg, Countdown to Zero Day: Stuxnet and the Launch of the World‚Äôs First Digital Weapon, and Kingpin: How One Hacker Took Over the Billion-Dollar Cybercrime Underground.
¬∑ Exploding the Phone; Phil Lapsley: This book tells the story of the people who, long before the internet, discovered how to hack the telephone.
¬∑ Ghost In The Wires: My Adventures as the World‚Äôs Most Wanted Hacker; Kevin Mitnick, William Simon: Mitnick recounts his experiences as the ‚Äúworld‚Äôs most wanted hacker‚Äù.
¬∑ The Code Book: The Secret History of Codes and Code-breaking; Simon Singh: A look at the history of man‚Äôs urge to uncover the secrets of codes.
¬∑ Fallout: The True Story Of The CIA‚Äôs Secret War On Nuclear Trafficking; Catherine Collins, Douglas Frantz: This book examines the circumstances that led to nuclear weapons technology spreading throughout the world.
¬∑ Wiring Up The Big Brother Machine‚Ä¶ And Fighting It; Mark Klein: Whistleblower Klein recounts the impact of revealing that illegal government spying apparatus had been installed at an AT&T office by the NSA.
¬∑ Fatal System Error: The Hunt for the New Crime Lords Who Are Bringing Down the Internet; Joseph Menn: This book was also recommended by Mick Halpin.
¬∑ Malware Analyst‚Äôs Cookbook and DVD: Tools and Techniques for Fighting Malicious Code; Michael Ligh, Steven Adair, Blake Hartstein, Matthew Richard: A computer forensics ‚Äúhow-to‚Äù for fighting malicious code and analyzing incidents.
¬∑ The IDA Pro Book: The Unofficial Guide to the World‚Äôs Most Popular Disassembler; Chris Eagle: An essential read for those analyzing malware, conducting vulnerability research, or reverse engineering software.
¬∑ Liam also recommends Practical Malware Analysis: The Hands-On Guide to Dissecting Malicious Software, which is also recommended above.
Aleatha, a senior principal research engineer based in California, had some words of wisdom about what she thinks it takes to be a good cyber security professional.
‚ÄúIt‚Äôs important to know how exploits happen and how the bad guys think. You also need to know what tools are at your disposal to help protect users.‚Äù
‚ÄúMy bookshelf is a mix of hacking guides and crypto. You also need to understand modern operating systems and software architecture to understand how a lot of vulnerabilities occur, so it‚Äôs good to have a really solid OS book. I have the Tanenbaum OS book (Modern Operating Systems: Global Edition), like everyone else. It‚Äôs Unix-centric, but it‚Äôll teach you about things like heaps, stacks, and context switches, which are applicable to all OSes. And [to be a good cyber security professional] you should have some experience in a low-level language like C, so you understand things like pointers and memory layout.‚Äù
¬∑ Smashing the Stack for Fun and Profit; Aleph One: It‚Äôs a little dated, but this short pamphlet gives a great intro to the one of the most basic classes of security exploit, while explaining the OS fundamentals around it. If you are wondering how malware works, start here.
¬∑ The Shellcoder‚Äôs Handbook: Discovering and Exploiting Security Holes; Chris Anley; John Heasman; Felix Lindner; Gerardo Richarte: This is a really detailed guide to the theory and practice of finding and exploiting security holes. It doesn‚Äôt focus on hacking tools, as many other books do, instead focusing on the nitty gritty of hacking: how to smash stacks; how to exploit buffer overflows; how to return into libraries. It‚Äôs not for the amateur, it assumes you have a good knowledge of how software and operating systems work, but it‚Äôs got incredible depth. Another good one in this vein is Hacking: The Art of Exploitation (Jon Erickson).
¬∑ The IDA Pro Book: The Unofficial Guide to the World‚Äôs Most Popular Disassembler; Chris Eagle (as recommended above): Will teach you how to reverse engineer code. It‚Äôs useful for analysts, as well as for understanding how hackers find vulnerabilities.
¬∑ Applied Cryptography: Protocols, Algorithms, and Source Code in C; Bruce Schneier (as recommended above): It‚Äôs a good idea to rely on well-vetted crypto libraries wherever possible, know how to use them right, and what the pitfalls are. Schneier is one of the best writers on this subject. I also follow his blog, which is great for keeping up with recent exploits and developments in security.
A senior director of engineering and ‚Äúcyber security czar‚Äù at Symantec, Tarah is also a well-known speaker on the tech conference circuit, an author, and a prolific tweeter.
She had three recommendations to make for what she thought were the most relevant reads for those in the field of cyber security.
¬∑ Theory of Games and Economic Behavior; John von Neumann, Oskar Morgenstern: First published by Princeton University Press in 1944, this book was a groundbreaking text upon which modern-day game theory is based.
¬∑ Judgment Under Uncertainty: Heuristics and Biases; Daniel Kahneman, Paul Slovic, Amos Tversky: This book brings together a range of academic papers to study the factors that influence human decision making.
¬∑ Threat Modeling: Designing for Security; Adam Shostack: This book details how to build better security into the design of systems, software, and services.
Candid, a principal threat researcher in Symantec Security Response, says that a lot of his information these days comes from white papers and articles, while also underlining that cyber security is a broad field with different skills required in different areas.
‚ÄúReverse engineering or forensics might apply to one person, whereas others can use more information on web application security and python scripting.‚Äù
Three of Candid‚Äôs recommendations have already been mentioned:
¬∑ The Art of Computer Virus Research and Defense
¬∑ Practical Malware Analysis: The Hands-On Guide to Dissecting Malicious Software
¬∑ Hacking: The Art of Exploitation: Candid describes this as a good start for those who want to learn about buffer overflows and other exploits.
Candid also recommends:
¬∑ The Tangled Web: A Guide to Securing Modern Web Applications; Michal Zalewski: This is a good start for those wanting to learn about web application security.
Check out the Security Response blog and follow Threat Intel on Twitter to keep up-to-date with the latest happenings in the world of threat intelligence and cyber security.
Like this story? Recommend it by hitting the heart button so others on Medium see it, and follow Threat Intel on Medium for more great content.
HatsFinance
NEARWEEK
in
NEAR Protocol
Blake Sobczak
in
README_
Melanie Gracious
Rowls
AirGap
in
AirGap
CyLon
in
CyLon
Mindsmatter
in
Technology Hits
"
319,https://medium.com/free-code-camp/computer-networks-and-how-to-actually-understand-them-c1401908172d,What computer networks are and how to actually understand them,"Whether you are new to the world of development, or have been building things for a long time ‚Äî or even if you‚Äôre a person who just likes computers and uses the internet daily ‚Äî you‚Äôve got to know the basics of networking and specifically Computer Networks.
If you like digging more into servers, their security, and how you connect to your servers from a remote client, all of this requires some knowledge of computer networks and their components. I have tried to cover most of the topics concerning computer networks in this article.
Also, from here, I will refer to ‚Äúcomputer networks‚Äù simply as ‚Äúnetworks‚Äù.
Let us first look at my working definition of computer networks:
Computer networks can be defined as the exchange of network packets between computing machines across the world with the help of data lines like wire cables, optical fibers, etc.
The Internet is a kind of computer network. Sorta.
We will take a look at some commonly used terms and components and how they function in a computer network, some of which are in the above diagram.
Nodes in computer networks mean any computing device such as computers, mobile phones, tablets, etc which try to send and receive network packets across the network to another similar device.
Network packets are nothing but the information or units of data that a source node wants to send/receive to/from the destination node. In this article, network packets/data packets all convey the same meaning.
Consider you want to send a birthday gift to your friend on their birthday, where will you send it? To their street address right?
Same is the case here. The early computer scientists wanted to identify computers on the internet with a unique number, something like a telephone numbers today. So, they came up with the concept of TCP/IP.
An IP of a computer device is the address of that device in a computer network. Technically, it is a 32-bit number used which identifies devices in a network. All the communication to and fro from the device in that network will be done in terms of its IP address.
Consider that you are uploading a file to any site or say to Google drive.
Talking at the lowest level of network communication, your file is converted to packets and each packet has the destination node address with it which is nothing but the IP address.
On a higher level, IP addresses are classified into two types:
IPv4 is classified into five classes named Class A, B, C, D, E.
Class A: As shown in the third column of the above image, for a Class A IP addresses, the first bit of the first octet of the IP address is constant and is ‚Äú0‚Äù.
The Second column indicates the Network bits and the host bits of the corresponding class of IP address. Consider in case of a Class A IP address, we have the following formula:
Number of networks/subnets = 2^(# of network bits) .
Number of valid hosts in each subnet = 2^(# of host bits) ‚Äî 2 .
The number of network bits and host bits are decided by the default subnet mask of the class of IP address.
The default subnet mask for a class A IP addresses is 255.0.0.0, that is 11111111.00000000.0000000.00000000`. Thus, for class A:
Network bits = 8, and Host bits = 24.
Since Network bits = 8, Host bits = 24, their sum has to be 32, since IPv4 addresses are of 32 bits. But, since we are using the one bit (first bit in the first octet) to identify the class:
Number of usable network bits = Number of network bits ‚Äî Number of constant bits = 8‚Äì1 = 7
Thus, the Number of possible networks in Class A = 2^7 ‚Äî 2 = 126 and,
Number of possible hosts (that is devices that can be connected to the network) per network in Class A = 2^24-2 = 16277214 .
Now, here, for class A, you may wonder why I subtracted an extra 2 from the number of possible networks. It is because, for class A, 127.x.y.z was kept reserved. For other classes, the usual formula is used.
Thus, IP addresses in class A range from 1.x.x.x to 126.x.x.x.
Class B: the case is similar with Class B. The only difference is 2 bits of the first octet are constant (10) and they identify the class of IP address that is class B. All other calculations are same, and I am not mentioning them here since they are easy to grab from the table above. They range from 128.0.x.x to 191.255.x.x .
Class C: 3 bits of the first octet are constant (110) and they identify the class as class C. They range from 192.0.0.x to 223.255.255.x .
Class D and Class E: Class D and Class E are used for experimental purposes.
IPv4 addresses are mainly of two types:
Note: A device can have multiple IP addresses at the same time. Consider a device connected to two networks, wifi as well as any LAN network ‚Äî it will have two IP addresses. This implies that the IP addresses are assigned to the interfaces and not directly to the computer.
Okay, so far so good. Let‚Äôs continue.
As its name suggests, a Router is a hardware component that takes care of routing packets. It determines which node the packet came from and which destination node the sender node want to send it to. No computer knows where other computers are located, and packets are not sent to every computer. A Router identifies the destination node address to which a network packet has to be sent and it forwards it to the desired address.
Routers have a specific ‚ÄúRouting Protocol‚Äù which defines the format in which they exchange data with another router or networking nodes. In other words, routing protocol defines how routers communicate with each other.
Routers build up a ‚ÄúRouting Table‚Äù which identifies the most optimized paths to be taken in the network while sending packets.
Technically, a routing table is just a table with the list of ‚Äúroutes‚Äù from one router to other. Each route consists of the address of the other routers/nodes in the network and how to reach them.
Above is an example of a routing table. The key points to take a note of here are:
So this is the way a router works, with the help of Routing Protocol and Routing Table.
All good up to now. But, you must be thinking ‚Äî
‚ÄúOkay! But hey, we are learning about components here. I need to stitch them together and get to know how the internet works.‚Äù
Cool! Some more terms and you will have a proper understanding of how everything goes.
Network address translation is a technique used by routers to provide internet service to more devices with less usage of public IPs. Thus, a router is assigned a single IP address by the ISP and it assigns the private IPs to all the devices connected to it. NAT helps the ISPs provide internet access to more consumers.
Thus, if you are connected to the router of your house, your public IP will be visible to the world, but the private one will not. Whatever network packets are communicated will be addressed by your public IP (that is the public IP assigned to the router).
Consider the above figure. Let‚Äôs say that in your home network, you are trying to access medium.com (remote static IP: 72.14.204.147), from your computer (private IP: 192.168.1.100).
So, for your computer, the connection looks like:
192.168.1.100:37641 ‚Üí 72.14.204.147:80 .
‚Äú37641‚Äù is the random port number assigned by NAT router to your device/computer. (When there is network communication between daemons running on different ports on a computer, the respective port is used by NAT). Each outbound connection gets an assigned port by the NAT router.
The connection is established in NAT like:
But, since the outside world of the network doesn‚Äôt know about your private address, the connection looks like the following to medium.com:
104.244.42.129:59273 ‚Üí 72.14.204.147:80 .
That way, we achieve assigning a higher number of IP addresses without wasting many public IPs.
Now, when medium.com sends the response back to 104.244.42.129:59273 , it travels all the way to your home router which then looks up for the respective private IP and private port and redirects the packet to your device/computer.
Note: NAT is a generalized concept. NAT can be achieved as 1:1, 1:N where 1, N are the number of IP addresses in the network. A technique called as ‚ÄúIP Masquerading‚Äù is a 1:N NAT.
Dynamic Host Configuration Protocol or DHCP is responsible for assigning dynamic IP addresses to the hosts. The DHCP server is maintained by the ISP or previous router if there is a chain of routers to reach the host.
Thus, allocation of IP addresses is carried out by the DHCP server. Generally, ISP maintains a DHCP server and the routers in our houses get assigned a public IP from the DHCP server.
Note: Whenever a router or say a DHCP server maintained by an ISP or router restarts, the IP address allocation starts again and devices are allocated IPs which are different than the previous ones.
We have already discussed that any machine is identified by the IP address.
Okay, so you are running a web server on your localhost on your machine. If you have dug around in the hosts on any Linux machine, you would have encountered something like this:
which means that even if you type 127.0.0.1 in your browser‚Äôs URL bar, it would mean the same as localhost .
Similar to the above, the websites you use daily are web servers running on some remote instance/node having a static IP address. So, typing that IP address in your browser‚Äôs URL bar will take you to the website?
Yes, surely it will. But, are you a superhuman to remember the IP addresses of thousands of sites?
NO.
Thus, there come the domains that we use, say medium.com, twitter.com, behance.net, codementor.io, etc.
A Domain Name Server is a server having huge records of domain name mapping IP addresses which searches for the domain input and returns the respective IP address of the machine on which the website you want to access is hosted.
How does DNS work actually?
Note:
The Internet Corporation for Assigned Names and Numbers (ICANN) is a consortium (a non-profit corporation) that manages the assignment of domain names and IP address ranges on behalf of the community.
A domain is divided into three parts as shown in the following figure.
Internet Service Providers are the companies that provide everyone Internet. The article you are reading now is because of the internet that your ISP provides you.
ISPs provide internet, handle routing your requests to the correct destination, resolve domain names with the help of DNS cache that they maintain, and handle all this network infrastructure which enables us to use the internet.
ISP is a hierarchical thing working across the internet. There are certain types of ISPs namely Tier 1, Tier 2, Tier 3 ISPs.
The traffic that goes through your router also goes through Tier 3 (if present), Tier 2, and ultimately through Tier 1 ISPs all the way to another network.
Woot Woot! I am happy that you are still with me. We will put all the things together now.
Up until now, we have learned about all the components needed to make everything work. Now, we will glue them together.
Let‚Äôs summarize all the things we‚Äôve learned:
So, this is how the Internet aka A kind of Computer Network using TCP/IP protocol works.
Thanks for reading the article. If you have any questions, please feel free to ask them in the comments below.
If you liked this article, please give me some üëè and share it with others. See you in the next one. You are awesome!
Also, if you‚Äôd like to support me üëá
www.buymeacoffee.com
Thanks a lot again for reading! Feel free to connect with me on Twitter, GitHub.
Trulioo
in
The RegTech Hub
Darkpool
TechBrah
Viraj Yadav
Elinore Crabb
Greg Knieriemen
in
Enterprise Te.ch
NearHackerHouse
Raj Naik
"
320,https://medium.com/@interviewready/computer-networks-fundamentals-aaa50d283c81,Computer Networks Fundamentals,"By ‚Äî Avash Mitra
A Computer network is a system that connects numerous independent computers allowing them to share messages from one computer to another.
Physical Layer
Routing Layer
Behavioral Layer
2. Can the receiver after receiving a message, send a reply to the sender?
3. Can the receiver send a message without receiving any message?
So what happens when we enter a URL (let‚Äôs say https://InterviewReady.com ) in our browser
Content Delivery Network
To reduce the response time, servers usually cache the response for a request, so when the same request comes in it can send the response from the cache. However, this is also wasteful. Ideally, clients should store the data on their local computers. But the issue with this approach is that the data can become outdated.
To overcome these issues, we use CDN.
In simple terms, CDN is a set of caches distributed across the globe. So instead of sending requests to the main server, clients can send them to the nearest CDN.
Once the router gets the response, it also needs to decode it. There can be multiple users connected to the same router, so it must send the response to the right user.
There are two main methods to implement this:
The most common way to send requests and receive responses is through HTTP. HTTP stands for Hyper Text Transmission Protocol.
So HTTP only allows unidirectional communication. If we want bi-directional communication we can use WebSockets.
It allows for Peer-to-Peer communication. So it establishes a duplex connection allowing both devices to send and receive requests.
You can also use XMPP (Extensible Messaging and Presence Protocol) which is an open XML Technology for real-time communication.
Both HTTP and WebSockets use TCP. It stands for Transmission Control Protocol.
TCP provides features like
We also have another protocol known as UDP. It stands for User Datagram Protocol. Unlike TCP it does not have the above three features. It is mainly used in Real-Time Delivery.
In a microservice architecture, services need to communicate with each other. They send data in the form of objects. Different services can use different languages, so there must be a common intermediate language for defining the objects. Once the language is defined we need to query the data. There are two common techniques:
REST
GraphQL
gRPC (google Remote Procedure Call) is generally used by microservices to communicate internally. It is written over HTTP 2.0.
Let‚Äôs understand this using an example.
Suppose we have two devices A and B
So in simple terms, Head-Of-Line blocking occurs when the message/data packet at the head of the queue cannot move forward due to congestion even if other messages/packets behind this one could.
HTTP 2.0 solves this problem using Multiplexing. It implements multiplexing by breaking the messages/data packets into frames and are sent in streams. Each stream has a stream ID. So, when you get a message of a stream ID, that stream is going to be blocked until all the messages having the stream IDs are processed.
HTTP 2.0 solved Head of Line Blocking, but it is still written over TCP. Even though data packets are broken into logical streams, they are not independent because they are running on the same TCP queue. If there was a drop-in packet in one of the streams, other streams will still be blocked.
Consider the example below, you have two streams with stream IDs 1 and 2. Even if one packet in stream 2 was dropped, stream 1 is also blocked.
To solve this problem, HTTP 3.0 uses UDP instead of TCP. It used UPD because there are no acknowledgments. It provides streams, ordered delivery and all other features of HTTP 2.0. HTTP 3.0 will be called QUIC.
Video files are large and cannot be sent in a single response. They have to be broken into packets. Also, using HTTP to send video files is very inefficient. For live streaming data, HTTP is not a good choice because it is built o
HTTP is not good for video transmission because:
The basic idea is, that the client sends a signal to the main server based on the data you can handle.
For example, if the client can handle a video up to 720p resolution then the main server will send the client a video up to 720p resolution. If the client‚Äôs network is very slow then the main server will send videos in lower resolution to maintain connectivity.
In Mac devices, HLS is used. It is very similar to HTTP-DASH.
For video conferencing, sending video streams to the other client through a server is a bad idea because:
So to solve these issues we use WebRTC.
It is a peer-to-peer protocol so no server is required to send data.
How does peer-to-peer communication work?
Since it does not require a server
That‚Äôs it for now!
You can check out more designs on our video course at InterviewReady.
Puneet Thakur
Mike Shema
Yael Hoffman
in
spacemesh
Relayz
Stefan P. Bargan
in
InfoSec Write-ups
Fenix Digital Zone
Carole Chauncey
Andrew Sproul
in
DataDrivenInvestor
"
321,https://medium.com/the-technical-archaeologist/when-computer-networks-were-about-nationalism-6b49567cbfab,When Computer Networks Were About Nationalism,"ARPANET, Minitel, Mark I and the race to invent the internet
Some fun news first: I‚Äôve signed a contract with No Starch Press to write my first technical book. The project can be roughly described as The Art of War for legacy modernization: when to do it, how to organize it, common problems and various tricks I‚Äôve picked up to get stuff done. I got a wonderful opportunity this month to test out the content of early chapters on a live audience of software engineers when √òredev invited me to give a keynote in Malm√∂, Sweden. They asked me if I could do a‚Ä¶
Godspower Kpobari Vizor
SmartMesh
excellediadigitalinnovation
Jamie Adams
Chun Tao Ho
david richard
Anda Ngcaba
in
Combat COVID-19 Africa
Purdue Research Foundation
in
The Line by PRF
"
322,https://medium.com/siitgo/what-is-compiler-design-98cc3805a7b5,What is Compiler Design?,"Compiler Design is the structure and set of principles that guide the translation, analysis, and optimization process of a compiler.
A Compiler is computer software that transforms program source code which is written in a high-level language into low-level machine code. It essentially translates the code written‚Ä¶
Steve Bafiam
ReviewNPrep
Rifwan Jaleel
Praise Horpe
Pranav Verma
in
Walmart Global Tech Blog
Allison Trager
in
The Devil Wears Product
DOMESH SINHA
Sebastien Tricaud
"
323,https://medium.com/@reactstudio/meet-the-design-compiler-17ae89f985bb,Meet the Design Compiler,"How plugins work in React Studio, and what design-guided metaprogramming can do for you
In this post, we‚Äôd like to introduce you to the heart of React Studio: the design compiler. We‚Äôll take a look at how React Studio plugins work and how Neonto‚Äôs design compiler can intelligently bring together your code and visual designs to construct React projects.
More fundamentally, this is about new abstractions for programming. The design compiler is actually an evolutionary step beyond libraries and frameworks. On a quick glance at React Studio, you might think ‚Äújust another GUI builder‚Äù‚Ä¶ But you‚Äôd be missing something essential. There is a whole level of metaprogramming capabilities in React Studio that makes it so much more!
You‚Äôve probably seen code generators and declarative UI design tools before, but React Studio is neither. It combines features of both of those approaches into a single metaprogramming tool ‚Äî an ‚Äúapplication that writes applications‚Äù as guided by your inputs.
There are two significant practical benefits to this approach. The first is encapsulation safety. This could be compared to type safety: in a programming language with static types, the compiler can catch many kinds of errors before you even run the program. The design compiler does something similar ‚Äî except for entire code fragments, not just individual variables.
The other benefit is framework independence. There are many frameworks out there that promise some kind of platform independence (‚Äúwrite once in language X, run anywhere!‚Äù)‚Ä¶ But the design compiler approach takes it one level higher. You can actually retarget your app design to a different programming language and framework, just like you can compile a C++ program for a different CPU and operating system. (This is not just a theoretical possibility; at the end of this post you can find non-React examples of the Neonto design compiler in action.)
On the process level, the design compiler enables a new kind of software development workflow that we are calling Design-Accelerated Development. (That acronym is quite a ‚Äúdad joke‚Äù in itself, isn‚Äôt it‚Ä¶?) In this post, we won‚Äôt go into details on that because focus here is on the technical aspects of the design compiler and its plugin interfaces. But we are going to tell you about our ideas for Design-Accelerated Development workflow in another series of posts soon. (Don‚Äôt miss that: follow us here on Medium)
We wrote a separate post (link below) about the thinking that led to the design compiler approach. In brief, the software industry has reached ‚ÄúPeak Framework‚Äù. Buildtime tools is where the action is today. But do we have the right abstractions?
Read ‚ÄúPeak Framework‚Äù and the runtime barrier: why libraries and frameworks are exhausted
Modern build tools could be so much more than text editors and terminal commands ‚Äî and that‚Äôs where React Studio comes in.
A picture speaks more than a thousand words, so here‚Äôs a diagram. (Well, this picture does have quite a few words in it, so maybe it‚Äôs cheating‚Ä¶?)
The output is always a React project using Facebook‚Äôs create-react-app toolchain. This is a very solid base for modern web app development: you get a prepackaged build system with everything needed both for local testing and deployment builds, and the language environment is set up to support latest best practices (e.g. new JavaScript features that make React/JSX code more concise). If your needs later diverge from Facebook‚Äôs defaults, you can ‚Äúeject‚Äù the project and modify it manually.
The inputs are what feed the design compiler. These are both visual (layouts, drawings and configuration made within the React Studio GUI) as well as programmatic in the form of plugins. The plugin system provides an advanced set of ‚Äúhooks‚Äù into both the design compiler and the Studio GUI.
The default installation of React Studio 1.0 includes a few plugins: two web service plugins (a.k.a. data plugins), two component plugins, and defaults for the UI framework and navigation plugins.
The default plugins are useful as starting points for your own plugin development. There are really two ways to make a plugin: 1) take your own custom code work and wrap it into a plugin, or 2) take an existing npm package and make it available as a plugin in React Studio. We‚Äôll look at examples of both.
The default plugins were installed by React Studio in the folder:
~/Library/Application Support/React Studio
The ~ symbol means your home folder. You can get into the ~/Library folder easily in Finder using a hidden shortcut: open the Go menu, press down Alt key, and the Library item appears.
Plugins are macOS bundles. This structure is commonly used on both macOS and iOS, and it‚Äôs a very practical way to package together code and resources. Bundles are really folders that just appear like single files to the Finder. To read and modify the contents of a plugin, right-click on it in Finder and choose Show Package Contents.
Some notes on plugins:
It‚Äôs easiest to start a plugin by modifying one of the default ones. First make a copy of the plugin in Finder and give it a new name.
To really rename the plugin, it‚Äôs not enough to just rename it in Finder. You must also edit Contents/Info.plist and replace the plugin name in both the ‚Äúbundle name‚Äù and ‚Äúbundle identifier‚Äù fields. (The bundle identifier is an internal package id that is used by the operating system; the ‚Äúbundle name‚Äù is a human-readable version of your plugin‚Äôs name.)
The plugin‚Äôs main code is in Contents/Executables/Main.js. This contains the definitions and functions that are called by React Studio to perform tasks like creating a UI for the plugin and processing code fragments for the design compiler.
In Contents/Resources, you can find templates that provide React JSX code. (For plugins that just wrap an npm package, there may not be any need for a template, but more complex plugins will have something here.) Our internal practice has been to use the Mustache templating language, but this is not set in stone ‚Äî it‚Äôs not a fixed feature of React Studio, but just a JavaScript dependency that gets included in each plugin via a setting in Info.plist. You‚Äôre completely free to use some other library or method for your templating needs, if you prefer.
This plugin is a compact example of a self-contained component plugin (i.e. all of the code is in the plugin, nothing is loaded at buildtime using the npm package manager).
The Main.js file is reasonably well commented, and contains the code for all the entry points called by React Studio. All of them are declared in the context of a ‚Äúthis‚Äù object ‚Äî this represents the plugin instance (i.e. when you drag the Camera element into a canvas, an instance gets created, and the ‚Äúthis‚Äù object will point to that). You can also place private data in the ‚Äúthis‚Äù object; the convention is that such private data is prefixed with an underscore. In Camera, the only such declaration is ‚Äúthis._data‚Äù. (This object gets saved within the React Studio project thanks to the implementation of the persist/unpersist functions.)
The real beef of the plugin is in the exportAsReactWebComponent function. The arguments are a class name and an exporter object, which can be used to access React Studio‚Äôs design compiler state. Camera‚Äôs implementation of this function loads a template file from its Resources folder, then uses the Mustache template engine to fill out some values within the template code.
Another interesting method is writeReactWebCodeForPublishedInteractAction. This lets you write the JavaScript code for a previously declared interaction. Camera provides an action named ‚ÄúShoot‚Äù, which lets the user take a picture and save the image data into a data slot within the React Studio project. (Actions can be chained in the React Studio UI, so typically the ‚Äúshoot‚Äù action might be the first one in a chain where the image gets eventually sent to a server.)
This plugin is interesting because it wraps an existing package from npm, one called react-rating.
The implementation here looks quite similar to Camera, but there is no template in the Resources folder ‚Äî we don‚Äôt need one because the component code comes from the npm package.
Instead, the plugin declares its dependencies using the getReactWebPackages hook. Rather than write a component class, the plugin declares that it doesn‚Äôt need one (using this.writesCustomReactWebComponent = false), and implements a getReactWebJSXCode function. In this function, the plugin just returns an instance of the Rating component configured according to user-settable values from the React Studio UI.
The Generic JSON plugin shares many similarities with the component plugins seen earlier. The type of plugin is different; this is declared in Info.plist using the key NeontoPluginTypeId. (The Info.plist file is a required feature of the Apple bundle format; all the keys with a ‚ÄúCF‚Äù prefix are defined by Apple. The only ones you typically need to edit are bundle version, name and identifier.)
Generic JSON‚Äôs Main.js has some of the same parts as the component plugins. There‚Äôs a describePlugin function, some private data, an inspector UI declaration and action methods, and rendering an icon for display (which just fetches an image from resources).
The readDataSheetFromServiceAsync function is used to load data inside React Studio. This allows your plugin to load data to be used at design-time ‚Äî very useful!
For the output, there‚Äôs a writeReactWebCodeForDataSheetLoad function. As with the Camera plugin, it uses a Mustache template as the basis for the JavaScript code.
RSS Feed is similar to Generic JSON above, but has two distinguishing features.
For the design-time preview, the data is loaded using an XML parser. The library for this is included in Resources/js/sax.js. This library file needs to be declared in the plugin‚Äôs Info.plist (key NeontoPluginDependencies), so that React Studio loads the library code into the plugin instance.
The actual exported JavaScript code has a peculiarity: it performs an AJAX-style to load the RSS feed, but it doesn‚Äôt call the server directly. Instead it proxies the request through Neonto‚Äôs ‚ÄúAPI Bridge‚Äù server. (You can see this at the start of the Resources/templates-web/DataSheetLoad.js file.) This proxying is necessary because RSS feeds can‚Äôt be directly loaded: they are typically served by plain-HTTP servers, but modern web browsers require HTTPS for all requests. Also there is a question of cross-origin requests being allowed ‚Äî most servers that offer an RSS feed don‚Äôt allow AJAX loads of the feed. For these reasons, Neonto offers the API Bridge.
As part of your React Studio subscription, you have access to the API Bridge. Initially we offer Google Sheets and RSS, but new services can be added easily ‚Äî let us know what you‚Äôd like to see on this front!
We‚Äôd love to help you make your own plugins. There are a number of ways to get in touch, choose whichever is most convenient for you!
So far we have looked at examples of plugins that specifically output React web code, JSX and CSS‚Ä¶ However Neonto‚Äôs design compiler supports other targets too!
Neonto Native Studio is an enterprise-ready product for native iOS and Android development. It uses the same design compiler core to construct truly native (‚Äúframeworkless‚Äù) projects for iOS and Android. It‚Äôs being used to produce hundreds of native apps already. If your interests are more in the native mobile space, check it out! There is a free 14-day trial available.
The default installation of Native Studio includes a Camera plugin that is very similar to the one you find in React Studio. (In fact, they could be the same plugin: you could have both the React and native implementations inside a single Camera.plugin ‚Ä¶ We didn‚Äôt do this just because we don‚Äôt want to confuse you with the implementations for non-React targets.)
If you‚Äôre looking to understand how Neonto‚Äôs design compiler can produce native code for multiple platforms from the same design inputs, the Camera plugins are a good place to start. You can then explore further with advanced APIs at the Native Studio developer site.
Download React Studio from www.neonto.com/reactstudio
Alex Foreman
Connie Reinholdsson
RicMoo
in
RicMoo
Toni Henriksson
in
Towards Dev
David Alecrim
in
Level Up Coding
Paul Shriner
Shuvo Podder
Nitin Sharma
in
JavaScript in Plain English
"
324,https://medium.com/weekly-webtips/syntax-analysis-in-compiler-design-parsers-3ef6e292f668,Syntax Analysis in Compiler Design (Parsers),"Amith Jayasekara
in
Towards AWS
laiphoo
Teiva Harsanyi
Elevate
George Lai
Daisuke Sonoda
Rich Brown
Yitaek Hwang
"
325,https://medium.com/free-code-camp/web-development-explained-to-a-time-traveler-from-ten-years-ago-600fad81170d,How I would explain a decade of web development to a time traveler from 2007,"Hello friend! I hope you like this new world of ours. It‚Äôs a lot different than the world of 2007. Quick tip: if you just got a mortgage, go back and cancel it. Trust me.
I‚Äôm glad that you‚Äôre still interested in computers! Today we have many more of them than we did 10 years ago, and that comes with new challenges. We wear computers on our wrists and faces, keep them in our pockets, and have them in our fridges and kettles. The cars are driving themselves pretty well, and we‚Äôve taught programs to be better than humans at pretty much every game out there ‚Äî except maybe drinking.
You might have seen the release of the iPhone just before you stepped into the time booth. Apple is the biggest and richest tech company, mostly due to the iPhone and its operating system, iOS. Google has this competing thing called Android, and Microsoft tried to get a slice of the ever-growing pie with Windows Phone. It didn‚Äôt work out.
We started calling programs apps, and some websites are calling themselves web apps. In 2008, Google released a new browser called ‚ÄúChrome.‚Äù Nine years later it‚Äôs the most popular way to get on the Web.
The Chrome team invested a lot in working with JavaScript, and the code gets better every month. Web apps are written using a lot of JavaScript, and they resemble the desktop interfaces of your time.
Companies have also invested in JavaScript to make it better‚Äîit now supports classes and modules. We use languages that compile to JavaScript, like TypeScript (from Microsoft, they‚Äôre cool now) or Flow.
We write a lot of JavaScript these days, since nobody supports Flash anymore. We even run JavaScript on the server, instead of Perl, using a thing called Node. It sounds easier than it is.
Remember Swing, SWT and the likes of wxWidgets? We had to reinvent them for the browser world. Several new UI programming models emerged, which mostly focused on components.
We had to find a way to design, build, and test apps while keeping them responsive (a term we use to describe a website that doesn‚Äôt look like crap on a mobile phone). We also needed to keep it slim ‚Äî not everybody has a fast connection, but everybody has a browser in their pockets.
To help with all this, there are now component frameworks. The term is vague, since it includes the likes of Angular by Google, React by Facebook, and Vue by the community. But it‚Äôs the best term we have.
By the way, I‚Äôm not sure you remember Facebook from 2007. It was getting big in the US around that time, and now it‚Äôs bigger than huge. Boasting more than a billion users, it‚Äôs also one of the largest codebases in the world.
The Facebook development team writes a lot of great code and publishes it online. They have their own conference, F8. Most big companies have their own conferences.
CSS also had to evolve, since the new apps require more intricate layouts. We don‚Äôt use tables with images anymore. Frames are gone as well. Instead, we have created new standards, like CSS Floats, Flexbox, and CSS Grid.
People had to iterate on these standards, and they‚Äôve built libraries to make things look consistent, like Bootstrap, Foundation and many more. Similar to JavaScript, we have created languages that compile to CSS. They make up for some of the things that CSS misses, like variables or modules. It‚Äôs still hard.
Don‚Äôt feel bad if you‚Äôre confused. The truth is that we‚Äôre all a little confused ‚Äî and it‚Äôs okay to be so. There are many more developers on the planet now, and tech companies are becoming more successful. For a while we used the term ‚Äústartup‚Äù to describe companies that grew quickly and didn‚Äôt know what to do. But even this term has become old.
There are more programmers, more programs, and more devices. We have more data now. Computers had to grow powerful enough to process it all, and we have developed several techniques to turn that data into insight.
First, we created a field called Data Science, which aims to learn about and extract information from data.
For example, a startup called Waze let people install an app on their phones that would track their movements while they were in their cars. Because many people installed the app, Waze got a lot of data about how cars move. They used it to develop programs that understood where traffic jams were.
Now, when you open Waze on your phone, you see traffic jams on the map in real time and choose another route.
Waze has since been bought by Google. This happens a lot with startups.
There were three main challenges with Data Science ‚Äî storing data, understanding data, and acting on data. We‚Äôve improved in all of these areas. Let‚Äôs look at each one.
We now need to store a lot more information and then find out which part is important. We needed to invent new databases. The likes of MySQL and PostgreSQL weren‚Äôt fit to store terabytes of data (we called it Big Data).
Big, internet-first companies typically faced these challenges, and so they were on the forefront of developing the technologies. Most of the time, technologies were first used internally and then open-sourced.
There was a movement we called NoSQL. This new class of databases took some of the conventions of traditional Relational databases and turned them around.
There‚Äôs Hadoop, which deals with how the data is stored on many hard computers. It defines a way of processing the data called MapReduce (inspired by a paper from Google ‚Äî big companies write good scientific papers these days).
Then there‚Äôs Cassandra, which looks at data not as tables, but as sets of keys and columns which can be stored on different computers. It also makes sure that any of these computers can go offline without causing data loss.
And we have MongoDB, a database that is easy to install and use for prototyping apps. In 2017, we‚Äôre treating technologies the same way we treated pop stars ten years ago ‚Äî we zealously defend some of them and vehemently hate others. MongoDB ‚Äî like the band Nickelback ‚Äî belongs to the latter group.
In the ‚Äúunderstanding data‚Äù camp, most of the focus has been in an area called Machine Learning. There have been many new techniques, from naive classification to deep learning, that are now in every Data Scientist‚Äôs toolbox. They mostly write Python and work alongside developers to put machine learning pretty much everywhere.
For example, with the help of Data Scientists, a lot of web apps use A/B testing. This technique serves two slightly different versions of the app to different, but similar, groups of users. It is used to see which version leads quicker to our desired goal, whether that‚Äôs a sign-up or a purchase.
A lot of big companies like Airbnb (pronounced air-bee-en-bee), Uber, and Netflix are running hundreds and thousands of A/B tests at the same time to make sure their users get the best experience. Netflix is an app where people can binge-watch TV shows on all their devices. ¬Ø\_(„ÉÑ)_/¬Ø
Companies like Netflix are enormous. Because they serve a lot of people, they have to make sure they are up and running at all times. That means they have to manage their computers pretty well. They can add hundreds of new servers when they‚Äôre needed.
This is difficult to achieve in a traditional data center, so the amazing engineers at Netflix use virtual machines. Remember Amazon Web Services, which launched back in 2006? Back then, they started offering Elastic Cloud Compute, known as EC2, to help people get virtual computers in Amazon‚Äôs data centers.
Today, they have almost 80 similar services, all built to help companies grow quickly. We used to have a trendy name for that ‚Äî ‚ÄúThe Cloud‚Äù ‚Äî but this term is as difficult to define as NoSQL.
Google and Microsoft offer their own clouds, creating a market for cloud services. And when smart people compete with each other, all sorts of crazy innovation starts happening.
First of all, we started thinking more about our infrastructure as code. Previously, you had to buy a new server, plug in a keyboard, and install dependencies.
Now we use configuration management tools like Puppet, Chef, and Ansible to automate our servers. You write actual code, in languages like Ruby, and provision servers based on the configurations. If a change is needed, you edit the configurations and then update the servers. No more SSH.
Then we started looking more at containers. Because it was possible to provision machines on EC2, we needed to develop on similar environments as our production ran in.
We started with virtual machines, using tools like Vagrant to automate them. But then we got Linux containers and, eventually, Docker. We found a way to run Linux on our MacBooks without running a full virtual machine, sharing some of the OS-related operations.
Using Docker, we could create descriptions of systems that matched exactly what we were running on production. Developers called these descriptions images, and started running their apps in somewhat-virtual instances called containers.
Then cloud providers caught up to this and let us run our containers directly on their clouds. They gave us tools like Marathon and Kubernetes, called orchestration frameworks. These frameworks let developers run apps inside containers without worrying about scaling, fault-tolerance, monitoring and discovery. The servers became disposable, and we don‚Äôt give them clever names anymore.
This also allowed developers to build small apps, called services or microservices, and then run them independently. Because these microservices have few external dependencies, we started looking into using new programming languages, like Go (which was created to make concurrency bearable) and Java. Yes, Java is still around and it got better.
There are also languages like Scala and Clojure that let us use the Java Virtual Machine and all related libraries. A website like AirBnB runs hundreds of microservices using several programming languages and databases.
Because of these microservices, we had to revisit some of our programming models. We now have much more, much smaller apps that we have to coordinate.
As a result, there are tools like Apache Kafka, Google PubSub, and RabbitMQ that aim to help with chatty computers that talk to each other a lot. Kafka was created by LinkedIn, a social network where people advertise their professional life. It was later open-sourced.
The extra complexity introduced by the orchestration frameworks is mostly relevant for larger teams. Remember ‚Äî we currently have more people building software than ever before, and every programmer wants to ship their code right away. If you have a company with hundreds of developers working together on a website, you need to make sure they don‚Äôt have to coordinate their releases.
So we build software in small teams. The teams usually have some DevOps skills, and members know enough about operations to be able to deploy their software. They use some form of an Agile process, where they release software continuously in short iterations. Organizations also experiment with other forms of hierarchy, like Squads, Chapters, Tribes, and Guilds, as Spotify (that‚Äôs a new music streaming service) does.
All these projects (and many others that were mentioned before) are developed publicly by a community of people. Sometimes they‚Äôre sponsored by a company. Most of the time, development goes on a website called GitHub. There, people can talk about software, build it publicly, and approve contributions by replying with a üëç emoji. An emoji is a special character we now use to express an emotion, such as approval. :-)
There are many more communities now, and they are all experimenting with ways to build software in the open. For example, a web framework called EmberJS uses a community-driven, 6-week release cycle. Other languages and popular open source software (like GitLab and Visual Studio Code, a code editor from Microsoft) use a monthly release cycle to make sure they don‚Äôt deliver fully breaking changes. The packages break just a little every release.
Oh, and the editor wars are still there. We‚Äôre writing our code in desktop apps that run a browser that shows a text field. And it‚Äôs great. It performs pretty well (gets better with every release) and is heavily customizable. It even has some support for VIM mode!
Conferences are held on every topic. They‚Äôre fun, and they‚Äôre everywhere. Go to one ‚Äî you‚Äôll see all sorts of people! Make some friends, goof around, get inspired, speak out. Maybe go to an algorave show ‚Äî what they do with music, images, and code is breathtaking.
There‚Äôs more people, software, and innovation out there. I hope you‚Äôre still as excited about computers ‚Äî you have all the reasons to be.
Let‚Äôs go get a Club Mate and I‚Äôll tell you about this Bitcoin thing. Also, the US has a new president now‚Ä¶
If you enjoyed this article, please give me some claps so more people see it. Thanks!
Anoop Srivastava
Salih Kardan
in
Picus Security Engineering
Teodor CostƒÉchioiu
in
Electronza
Iot Lab KIIT
Ruslan Magana Vsevolodovna
Vignesh Mayilappan
newcc
Felix Leber
in
REWRITE TECH by diconium
"
326,https://medium.com/codeburst/the-ultimate-guide-to-learning-full-stack-web-development-in-6-months-for-30-72b3854a7458,"The Ultimate Guide to Learning Full Stack Web Development in 6 months, for $30","Hey everyone! In this article I‚Äôm going to show you how to go from knowing little to nothing about Web Development to Junior Full Stack Developer in just six months, for under $30.
This article will cover everything you need to know! You‚Äôll first learn the basics of Web Development with an online coding bootcamp ($15). You‚Äôll follow that up with an advanced bootcamp ($15). Then, we‚Äôll jump into free tutorials, articles, and documentation where you will reinforce everything you‚Äôve learned in the first two bootcamps. Next, you‚Äôll build a few projects with your new skills and open-source them on GitHub. Finally, we‚Äôll focus on resume creation, interview preparation, and salary negotiation.
Disclosure: I write reviews and receive compensation from the companies whose products I review. All opinions expressed here are my own.
Ready? Lets dive in.
First thing‚Äôs first ‚Äî You need to learn the basics and build a solid foundation of web development principles. There are many ways to do this, but in my opinion, this is the best and easiest way.
bit.ly
Colt Steele‚Äôs Web Development Bootcamp is the single best web development course money can buy. It‚Äôs sometimes on sale for $20 or less, and it‚Äôs packed with more than 40 hours of on-demand content.
Colt Steele, the instructor, was a lead instructor at a coding bootcamp in San Fransisco before moving to online coding education. This course is the online equivalent of what he taught at his in-person bootcamp.
This course covers all of the web dev basics: HTML5, CSS3, JavaScript, Bootstrap, SemanticUI, DOM Manipulation, jQuery, Unix(Command Line) Commands
Once you‚Äôve got the basics down, you‚Äôll explore more advanced topics like: NodeJS, NPM, ExpressJS, REST, MongoDB, Database Associations, Authentication, PassportJS, and Authorization.
Colt not only knows web development, but almost more importantly, he knows how to teach it. He clearly explains tougher concepts and breaks everything down in the easy to handle chunks. Plus, the course revolves around projects, so you learn by building real-world things, not boring reading and test taking.
Click here to learn more or to sign up!
Now that you‚Äôve taken the first bootcamp and know how to build full stack web applications, it‚Äôs time to take your learning a little deeper. This course builds upon the first by introducing more complex technologies, frameworks, and tools you can use to build beautiful, responsive, web applications.
bit.ly
The Advanced Web Development Bootcamp is designed to take your coding skills to the next level! Don‚Äôt worry, this course is also project based. You‚Äôll build 10+ projects and explore dozens of code challenges with solutions to ensure you understand everything!
Here‚Äôs what you‚Äôll learn in this course:
This course has nearly 350 lectures and 30 hours of content, you‚Äôre going to learn a lot!!
Click here to learn more or to sign up!
At this point, you‚Äôve taken two online coding bootcamps and you‚Äôre hopefully starting to feel comfortable with your web development skills. You can successfully take a simple idea and build it out beautifully using a Node.js backend, and React on the front-end. Plus, you‚Äôre confident in your CSS abilities to style out a mobile first, responsive page.
Now it‚Äôs time to continue learning while reinforcing your new skills. Below is a list of documentation, articles, and tutorials.
Reading these a few months ago probably would‚Äôve made no sense ‚Äî but with your new skills you can handle them like a champ.
You don‚Äôt have to work through every link I‚Äôve provided. Think of these more like starting points.
Deployment
Want to keep learning? Here are some additional courses that I recommend that dive deeper into the subject of your choosing:
Level up your skills in your free time with these fun coding games and challenge websites.
Go forth and build something cool! Show off all of the skills you‚Äôve learned and bring an idea from concept to production!
Need some help brainstorming what to build? Here are a few links that should have you thinking in the right direction:
Make sure to open source your project so you can show off everything you‚Äôve learned. The first bootcamp goes into Git and GitHub with great depth, but if you‚Äôre still not feeling comfortable with it, you can check out these two links for a little additional motivation and help:
You‚Äôve learned the skills, now it‚Äôs time to make your resume shine and make your portfolio stand out. Here‚Äôs a collection of links to help you accomplish that:
Make your portfolio
How to Apply
Resume & LinkedIn
Personal Projects
Your resume and portfolio got you the interview! Awesome! Use these links to prepare for your interview:
You‚Äôve gotten the offer! Now you just need to negotiate your compensation package. Here are a collection of links to help:
You made it to the end of the article‚Ä¶ Good luck on your Web Development journey! It‚Äôs certainly not going to be easy, but by following this guide, you are one stop closer to accomplishing your goal.
Please consider entering your email here if you‚Äôd like to be added to my once-weekly email list, and don‚Äôt forget to follow codeburst on Twitter!
Brandon Morelli
in
codeburst
Micah Bolen
Piyali Das
Ateam Soft Solution
Tariqul Islam
ACHARYA ANIL SURYAVANSHI
LoginRadius
J.W.P Vishwa
"
327,https://medium.com/hackernoon/full-stack-web-development-the-complete-roadmap-9fe43ec0ba32,Full-Stack Web Development ‚Äî the Complete Roadmap,"Learning web development is a huge advantage for a founder, even understanding the basics will be extremely useful. This post will give you a high-level overview of everything you need to learn, a roadmap for getting started, and a collection of the best resources you can use to learn quickly.
I am going to tell you everything a professional full-stack web developer needs to know, but if you‚Äôre only interested in understanding the basics ‚Äî you can stop reading after ‚ÄúDomain names and Hosting‚Äù section.
The best way to begin is by learning the 3 main technologies all websites are made of. HTML defines the content of a website, CSS specifies how it looks(fonts/colors/images/etc), and JavaScript allows you to dynamically manipulate elements(everything interactive you see on the internet was made using JS).
The best way to learn the basics is to go through The Web Developer Bootcamp on Udemy.
www.udemy.com
It will take you through the process of creating your first website, and give you a great place to get started.
Frameworks are tools that allow you to implement all the common functionality much faster and easier.
I recommend using frameworks as early as possible, because they will enable you to get results much faster, and at the same time will teach you the best practices in a very natural and intuitive way. Just by using frameworks, you will absorb a huge amount of knowledge incredibly quickly, and avoid wasting time on anything unnecessary.
In web development, there‚Äôs always an enormous amount of technologies to choose from, and that choice can often be difficult for the novice. For the rest of the article I will spare you the confusion, and instead of listing pros and cons of every option, I‚Äôll just tell you the best things to use based on my experience.
The best place to get started is learning Bootstrap and jQuery. These are by far the most popular frameworks that are used by the vast majority of the websites.
Bootstrap is a CSS framework, that gives you a bunch of easily customizable interface elements, and allows you to create responsive websites very quickly (responsive means they look well on mobile devices).
Bootstrap has recently released the new version, and you can get started with it by following this course:
www.udemy.com
jQuery is a JavaScript framework, that significantly simplifies the way you manipulate webpage elements. It has an enormous amount of plugins for every conceivable purpose, so whenever you need some interactive element, you can be sure you will find a way to create it with jQuery. It will also be very easy, because StackOverflow already has solutions to 95% of the issues you will encounter.
To learn the fundamentals, you can just watch this excellent lecture.
Finally, for the sake of completeness, I should mention Compass. You don‚Äôt need to think about it at the very beginning, because it‚Äôs not essential for your ability to make websites, but when you reach a point where you‚Äôre writing a lot of intermediate/advanced CSS, and begin to find it a bit tedious, you should definitely keep it in mind. Compass makes the process of writing CSS way faster and more enjoyable, so if you can invest a couple of hours into learning that, that will be one of the best productivity boosters you can imagine.
WordPress is the world‚Äôs most popular content management system. If you are not a technical person, and are just looking to quickly get started with making an easily customizable website ‚Äî you just learn to install and use WordPress, and that‚Äôs pretty much it, you don‚Äôt need to look any further. It has an infinite amount of themes and plugins that will cover 90% of your needs.
If you are interested in becoming more of a professional web developer, and learning to create your own custom websites, you might want to skip it and learn a backend framework(I‚Äôll describe them in the following sections).
Once you have learned the basics and have built your first website, naturally you want to make it available to the world. Again, in the interest of saving you hours of research, I will not list hundreds of available options, and just tell you the ‚Äúright‚Äù things to use.
To buy domain names you will use Namecheap. As the name implies, it has very good prices, and also an excellent user experience.
To host your first pages, you should use Bluehost, it is cheap, very easy to use, has excellent documentation and a lot of automated tools. It makes installing WordPress or hosting simple html pages extremely easy, so if your goal is to get started quickly ‚Äî that is exactly what you need.
Finally, when you will go further and gain more skill ‚Äî you will need a VPS(virtual private server). It is, to put it simply, a remote computer you have the complete control over, and will use to serve the sites you have built with frameworks. I highly recommend using DigitalOcean, it has great prices, beautiful and intuitive interface, and I‚Äôve been happily using it to host all of my websites for the past 4 years.
Now that you have learned the basics, and created several simple websites using the tools listed above, you can decide to go further, and learn how to professionally create custom websites.
To do that, the best place to begin is learning one of the backend frameworks. Backend frameworks run on server, and dynamically generate html for a website every time the user visits a url.
As always, there are dozens of options, but I will help you to narrow it down to 3 ‚Äî Django, Ruby on Rails, and Node/Express.
The opinions on which one you should learn first will differ among different developers, but I strongly recommend you to begin with Django, and then learn Node/Express.
The major advantage of Django is that it is written in Python ‚Äî a beautiful, powerful language that is used in every field you can imagine, and at the same is time the best choice for your first programming language.
Django itself is straightforward, powerful, flexible, has excellent documentation, and all the tools you‚Äôre going to need to build 95% of the websites. More importantly ‚Äî it will give you an excellent framework for thinking about the backend code, and understanding it is an excellent way to learn the core fundamentals of web development in general.
My favorite introduction to Django is the TutsPlus course Getting Started with Django, and the best way to go from a beginner to intermediate/advanced is an excellent book Two Scoops of Django.
www.twoscoopspress.com
Also, I highly recommend free youtube tutorials by Mike Hibbert‚Äôs.
Node and Express are extremely powerful tools that I recommend you to learn after Django. They might be a bit tough for a beginner, but if you have an understanding of Django ‚Äî you will learn them very quickly and naturally.
Node has many advantages, and the best way to experience them is to try it for yourself. Often it will enable you to build websites much quicker than you would with Django, you will understand a lot of backend programming concepts in more depth, have an easier time building APIs, real-time webapps(like a chat or a game), and universal webapps(you‚Äôll encounter them when you will learn React).
The best course I have encountered is The Complete NodeJS Developer Course.
www.udemy.com
If you understand HTML/CSS/JS and one backend framework, you are a capable web developer with the power to create many kinds of websites. If you want to become a Full-Stack developer ‚Äî welcome to the world of frontend frameworks.
Frontend frameworks will enable you to create powerful single-page applications. At this point you‚Äôre essentially making an app that runs completely in the browser, occasionally exchanging data with the server(like Gmail or Trello).
For a long time there has been a lot of competition in this space, but as you‚Äôre reading this article, the best choice is pretty clear ‚Äî you should learn React and Redux.
I can‚Äôt go into explaining how they work or what are their advantages(it would take a long time), but luckily, there‚Äôs an absolutely brilliant course that will teach you everything you need to know ‚Äî Modern React with Redux.
www.udemy.com
And to learn more advanced functionality, you can watch the 2nd part of this course by the same author ‚Äî Advanced React and Redux.
The author of these courses, Stephen Grider, is an absolutely brilliant teacher, he explains everything incredibly well, and you will have a ton of fun learning all the complexities of these technologies by following his courses.
I highly recommend you to create a website with Node and React/Redux, because it will really help you to grasp how to create and use REST APIs, and combine the whole set of technologies into a powerful and useful software.
Pheew, that‚Äôs a lot of stuff. The final part of the puzzle a Full-Stack Web Developer needs to learn is DevOps. It is a deep subject that I‚Äôm not an expert in, but the fundamentals of deploying websites are not that hard to understand.
By now, chances are you have learned a lot of it just by building and deploying websites. You have probably used github and some CI tool to deploy your websites, and Nginx to serve them.
Now, the big thing you should learn is Docker. Docker is an industry standard for deploying web applications, it makes the whole process fast, elegant and convenient.
Personally, I have learned Docker from this course, and I‚Äôm very happy with it. This course is short, straightforward, to the point, and easy to understand.
www.udemy.com
I hope this article was useful to you. Obviously, it‚Äôs impossible to cover everything about this deep, complex, and rapidly evolving field in one article, but I believe that topics and resources above will give you a clear vision of the path ahead, and a useful roadmap to follow. At least, this is the kind of article I wish was available to me a few years ago.
You will certainly learn more as you follow along, and go into depth into the topics that interest you the most, but I think that now your path will be much more straightforward and less confusing.
If you have questions, comments, or are looking for advice, don‚Äôt hesitate to message me to webdevadvice@startuplab.io.
Also, if you‚Äôre interested in more articles on web development, startups, and building your online business ‚Äî go ahead and subscribe to my weekly newsletter!
Alibaba Cloud
Wasim Akram
Yevgeniy Brikman
in
Gruntwork
Alibaba Cloud
Tyler Harris
in
An Idea (by Ingenious Piece)
My Virtual Teams Private Limited
Elliott Stam
in
Devyx
Keith Bloemer
in
Nerd For Tech
"
328,https://medium.com/free-code-camp/learning-python-from-zero-to-hero-120ea540b567,Learning Python: From Zero to Hero,"This post was originally published at TK's Blog.
First of all, what is Python? According to its creator, Guido van Rossum, Python is a:
‚Äúhigh-level programming language, and its core design philosophy is all about code readability and a syntax which allows programmers to express concepts in a few lines of code.‚Äù
For me, the first reason to learn Python was that it is, in fact, a beautiful programming language. It was really natural to code in it and express my thoughts.
Another reason was that we can use coding in Python in multiple ways: data science, web development, and machine learning all shine here. Quora, Pinterest and Spotify all use Python for their backend web development. So let‚Äôs learn a bit about it.
You can think about variables as words that store a value. Simple as that.
In Python, it is really easy to define a variable and set a value to it. Imagine you want to store number 1 in a variable called ‚Äúone.‚Äù Let‚Äôs do it:
How simple was that? You just assigned the value 1 to the variable ‚Äúone.‚Äù
And you can assign any other value to whatever other variables you want. As you see in the table above, the variable ‚Äútwo‚Äù stores the integer 2, and ‚Äúsome_number‚Äù stores 10,000.
Besides integers, we can also use booleans (True / False), strings, float, and so many other data types.
‚ÄúIf‚Äù uses an expression to evaluate whether a statement is True or False. If it is True, it executes what is inside the ‚Äúif‚Äù statement. For example:
2 is greater than 1, so the ‚Äúprint‚Äù code is executed.
The ‚Äúelse‚Äù statement will be executed if the ‚Äúif‚Äù expression is false.
1 is not greater than 2, so the code inside the ‚Äúelse‚Äù statement will be executed.
You can also use an ‚Äúelif‚Äù statement:
In Python, we can iterate in different forms. I‚Äôll talk about two: while and for.
While Looping: while the statement is True, the code inside the block will be executed. So, this code will print the number from 1 to 10.
The while loop needs a ‚Äúloop condition.‚Äù If it stays True, it continues iterating. In this example, when num is 11 the loop condition equals False.
Another basic bit of code to better understand it:
The loop condition is True so it keeps iterating ‚Äî until we set it to False.
For Looping: you apply the variable ‚Äúnum‚Äù to the block, and the ‚Äúfor‚Äù statement will iterate it for you. This code will print the same as while code: from 1 to 10.
See? It is so simple. The range starts with 1 and goes until the 11th element (10 is the 10th element).
Imagine you want to store the integer 1 in a variable. But maybe now you want to store 2. And 3, 4, 5 ‚Ä¶
Do I have another way to store all the integers that I want, but not in millions of variables? You guessed it ‚Äî there is indeed another way to store them.
List is a collection that can be used to store a list of values (like these integers that you want). So let‚Äôs use it:
It is really simple. We created an array and stored it on my_integer.
But maybe you are asking: ‚ÄúHow can I get a value from this array?‚Äù
Great question. List has a concept called index. The first element gets the index 0 (zero). The second gets 1, and so on. You get the idea.
To make it clearer, we can represent the array and each element with its index. I can draw it:
Using the Python syntax, it‚Äôs also simple to understand:
Imagine that you don‚Äôt want to store integers. You just want to store strings, like a list of your relatives‚Äô names. Mine would look something like this:
It works the same way as integers. Nice.
We just learned how Lists indices work. But I still need to show you how we can add an element to the List data structure (an item to a list).
The most common method to add a new value to a List is append. Let‚Äôs see how it works:
append is super simple. You just need to apply the element (eg. ‚ÄúThe Effective Engineer‚Äù) as the append parameter.
Well, enough about Lists. Let‚Äôs talk about another data structure.
Now we know that Lists are indexed with integer numbers. But what if we don‚Äôt want to use integer numbers as indices? Some data structures that we can use are numeric, string, or other types of indices.
Let‚Äôs learn about the Dictionary data structure. Dictionary is a collection of key-value pairs. Here‚Äôs what it looks like:
The key is the index pointing to the value. How do we access the Dictionary value? You guessed it ‚Äî using the key. Let‚Äôs try it:
I created a Dictionary about me. My name, nickname, and nationality. Those attributes are the Dictionary keys.
As we learned how to access the List using index, we also use indices (keys in the Dictionary context) to access the value stored in the Dictionary.
In the example, I printed a phrase about me using all the values stored in the Dictionary. Pretty simple, right?
Another cool thing about Dictionary is that we can use anything as the value. In the Dictionary I created, I want to add the key ‚Äúage‚Äù and my real integer age in it:
Here we have a key (age) value (24) pair using string as the key and integer as the value.
As we did with Lists, let‚Äôs learn how to add elements to a Dictionary. The key pointing to a value is a big part of what Dictionary is. This is also true when we are talking about adding elements to it:
We just need to assign a value to a Dictionary key. Nothing complicated here, right?
As we learned in the Python Basics, the List iteration is very simple. We Python developers commonly use For looping. Let‚Äôs do it:
So for each book in the bookshelf, we (can do everything with it) print it. Pretty simple and intuitive. That‚Äôs Python.
For a hash data structure, we can also use the for loop, but we apply the key :
This is an example how to use it. For each key in the dictionary , we print the key and its corresponding value.
Another way to do it is to use the iteritems method.
We did name the two parameters as key and value, but it is not necessary. We can name them anything. Let‚Äôs see it:
We can see we used attribute as a parameter for the Dictionary key, and it works properly. Great!
Objects are a representation of real world objects like cars, dogs, or bikes. The objects share two main characteristics: data and behavior.
Cars have data, like number of wheels, number of doors, and seating capacity They also exhibit behavior: they can accelerate, stop, show how much fuel is left, and so many other things.
We identify data as attributes and behavior as methods in object-oriented programming. Again:
Data ‚Üí Attributes and Behavior ‚Üí Methods
And a Class is the blueprint from which individual objects are created. In the real world, we often find many objects with the same type. Like cars. All the same make and model (and all have an engine, wheels, doors, and so on). Each car was built from the same set of blueprints and has the same components.
Python, as an Object-Oriented programming language, has these concepts: class and object.
A class is a blueprint, a model for its objects.
So again, a class it is just a model, or a way to define attributes and behavior (as we talked about in the theory section). As an example, a vehicle class has its own attributes that define what objects are vehicles. The number of wheels, type of tank, seating capacity, and maximum velocity are all attributes of a vehicle.
With this in mind, let‚Äôs look at Python syntax for classes:
We define classes with a class statement ‚Äî and that‚Äôs it. Easy, isn‚Äôt it?
Objects are instances of a class. We create an instance by naming the class.
Here car is an object (or instance) of the class Vehicle.
Remember that our vehicle class has four attributes: number of wheels, type of tank, seating capacity, and maximum velocity. We set all these attributes when creating a vehicle object. So here, we define our class to receive data when it initiates it:
We use the init method. We call it a constructor method. So when we create the vehicle object, we can define these attributes. Imagine that we love the Tesla Model S, and we want to create this kind of object. It has four wheels, runs on electric energy, has space for five seats, and the maximum velocity is 250km/hour (155 mph). Let‚Äôs create this object:
Four wheels + electric ‚Äútank type‚Äù + five seats + 250km/hour maximum speed.
All attributes are set. But how can we access these attributes‚Äô values? We send a message to the object asking about them. We call it a method. It‚Äôs the object‚Äôs behavior. Let‚Äôs implement it:
This is an implementation of two methods: number_of_wheels and set_number_of_wheels. We call it getter & setter. Because the first gets the attribute value, and the second sets a new value for the attribute.
In Python, we can do that using @property (decorators) to define getters and setters. Let‚Äôs see it with code:
And we can use these methods as attributes:
This is slightly different than defining methods. The methods work as attributes. For example, when we set the new number of wheels, we don‚Äôt apply two as a parameter, but set the value 2 to number_of_wheels. This is one way to write pythonic getter and setter code.
But we can also use methods for other things, like the ‚Äúmake_noise‚Äù method. Let‚Äôs see it:
When we call this method, it just returns a string ‚ÄúVRRRRUUUUM.‚Äù
Encapsulation is a mechanism that restricts direct access to objects‚Äô data and methods. But at the same time, it facilitates operation on that data (objects‚Äô methods).
‚ÄúEncapsulation can be used to hide data members and members function. Under this definition, encapsulation means that the internal representation of an object is generally hidden from view outside of the object‚Äôs definition.‚Äù ‚Äî Wikipedia
All internal representation of an object is hidden from the outside. Only the object can interact with its internal data.
First, we need to understand how public and non-public instance variables and methods work.
For a Python class, we can initialize a public instance variable within our constructor method. Let‚Äôs see this:
Within the constructor method:
Here we apply the first_name value as an argument to the public instance variable.
Within the class:
Here, we do not need to apply the first_name as an argument, and all instance objects will have a class attribute initialized with TK.
Cool. We have now learned that we can use public instance variables and class attributes. Another interesting thing about the public part is that we can manage the variable value. What do I mean by that? Our object can manage its variable value: Get and Set variable values.
Keeping the Person class in mind, we want to set another value to its first_name variable:
There we go. We just set another value (kaio) to the first_name instance variable and it updated the value. Simple as that. Since it‚Äôs a public variable, we can do that.
We don‚Äôt use the term ‚Äúprivate‚Äù here, since no attribute is really private in Python (without a generally unnecessary amount of work). ‚Äî PEP 8
As the public instance variable , we can define the non-public instance variable both within the constructor method or within the class. The syntax difference is: for non-public instance variables , use an underscore (_) before the variable name.
‚Äú‚ÄòPrivate‚Äô instance variables that cannot be accessed except from inside an object don‚Äôt exist in Python. However, there is a convention that is followed by most Python code: a name prefixed with an underscore (e.g. _spam) should be treated as a non-public part of the API (whether it is a function, a method or a data member)‚Äù ‚Äî Python Software Foundation
Here‚Äôs an example:
Did you see the email variable? This is how we define a non-public variable :
We can access and update it. Non-public variables are just a convention and should be treated as a non-public part of the API.
So we use a method that allows us to do it inside our class definition. Let‚Äôs implement two methods (email and update_email) to understand it:
Now we can update and access non-public variables using those methods. Let‚Äôs see:
With public methods, we can also use them out of our class:
Let‚Äôs test it:
Great ‚Äî we can use it without any problem.
But with non-public methods we aren‚Äôt able to do it. Let‚Äôs implement the same Person class, but now with a show_age non-public method using an underscore (_).
And now, we‚Äôll try to call this non-public method with our object:
We can access and update it. Non-public methods are just a convention and should be treated as a non-public part of the API.
Here‚Äôs an example for how we can use it:
Here we have a _get_age non-public method and a show_age public method. The show_age can be used by our object (out of our class) and the _get_age only used inside our class definition (inside show_age method). But again: as a matter of convention.
With encapsulation we can ensure that the internal representation of the object is hidden from the outside.
Certain objects have some things in common: their behavior and characteristics.
For example, I inherited some characteristics and behaviors from my father. I inherited his eyes and hair as characteristics, and his impatience and introversion as behaviors.
In object-oriented programming, classes can inherit common characteristics (data) and behavior (methods) from another class.
Let‚Äôs see another example and implement it in Python.
Imagine a car. Number of wheels, seating capacity and maximum velocity are all attributes of a car. We can say that an ElectricCar class inherits these same attributes from the regular Car class.
Our Car class implemented:
Once initiated, we can use all instance variables created. Nice.
In Python, we apply a parent class to the child class as a parameter. An ElectricCar class can inherit from our Car class.
Simple as that. We don‚Äôt need to implement any other method, because this class already has it (inherited from Car class). Let‚Äôs prove it:
Beautiful.
We learned a lot of things about Python basics:
Congrats! You completed this dense piece of content about Python.
If you want a complete Python course, learn more real-world coding skills and build projects, try One Month Python Bootcamp. See you there ‚ò∫
For more stories and posts about my journey learning & mastering programming, follow my publication The Renaissance Developer.
Have fun, keep learning, and always keep coding.
I hope you liked this content. Support my work on Ko-Fi
My Twitter & Github. ‚ò∫
Pursuit Technology
Robert M Ricci
Samuel Cozannet
in
HackerNoon.com
Hosting Recipe
in
HostingRecipe
John F Seale
mohit ladwal (Itachi)
Ritul Patel
Tim Okito
"
329,https://medium.com/towards-data-science/why-python-is-not-the-programming-language-of-the-future-30ddc5339b66,Why Python is not the programming language of the future,"It took the programming community a couple of decades to appreciate Python. But since the early 2010‚Äôs, it has been booming ‚Äî and eventually surpassing C, C#, Java and JavaScript in popularity.
But until when will that trend continue? When will Python eventually be replaced by other languages, and why?
Putting an exact expiry date on Python would be so much speculation, it might as well pass as Science-Fiction. Instead, I will assess the virtues that are boosting Python‚Äôs popularity right now, and the weak points that will break it in the future.
Python‚Äôs success is reflected in the Stack Overflow trends, which measure the count of tags in posts on the platform. Given the size of StackOverflow, this is quite a good indicator for language popularity.
While R has been plateauing over the last few years, and many other languages are on a steady decline, Python‚Äôs growth seems unstoppable. Almost 14% of all StackOverflow questions are tagged ‚Äúpython‚Äù, and the trend is going up. And there are several reasons for that.
Python has been around since the nineties. That doesn‚Äôt only mean that it has had plenty of time to grow. It has also acquired a large and supportive community.
So if you have any issue while you‚Äôre coding in Python, the odds are high that you‚Äôll be able to solve it with a single Google search. Simply because somebody will have already encountered your problem and written something helpful about it.
It‚Äôs not only the fact that it has been around for decades, giving programmers the time to make brilliant tutorials. More than that, the syntax of Python is very human-readable.
For a start, there‚Äôs no need to specify the data type. You just declare a variable; Python will understand from the context whether it‚Äôs an integer, a float value, a boolean or something else. This is a huge edge for beginners. If you‚Äôve ever had to program in C++, you know how frustrating it is your program won‚Äôt compile because you swapped a float for an integer.
And if you‚Äôve ever had to read Python and C++ code side-by-side, you‚Äôll know how understandable Python is. Even though C++ was designed with English in mind, it‚Äôs a rather bumpy read compared to Python code.
medium.com
Since Python has been around for so long, developers have made a package for every purpose. These days, you can find a package for almost everything.
Want to crunch numbers, vectors and matrices? NumPy is your guy. Want to do calculations for tech and engineering? Use SciPy. Want to go big in data manipulation and analysis? Give Pandas a go.Want to start out with Artificial Intelligence? Why not use Scikit-Learn.
Whichever computational task you‚Äôre trying to manage, chances are that there is a Python package for it out there. This makes Python stay on top of recent developments, can be seen from the surge in Machine Learning over the past few years.
Based on the previous elaborations, you could imagine that Python will stay on top of sh*t for ages to come. But like every technology, Python has its weaknesses. I will go through the most important flaws, one by one, and assess whether these are fatal or not.
Python is slow. Like, really slow. On average, you‚Äôll need about 2‚Äì10 times longer to complete a task with Python than with any other language.
There are various reasons for that. One of them is that it‚Äôs dynamically typed ‚Äî remember that you don‚Äôt need to specify data types like in other languages. This means that a lot of memory needs to be used, because the program needs to reserve enough space for each variable that it works in any case. And lots of memory usage translates to lots of computing time.
Another reason is that Python can only execute one task at a time. This is a consequence of flexible datatypes ‚Äî Python needs to make sure each variable has only one datatype, and parallel processes could mess that up.
In comparison, your average web browser can run a dozen different threads at once. And there are some other theories around, too.
But at the end of the day, none of the speed issues matter. Computers and servers have gotten so cheap that we‚Äôre talking about fractions of seconds. And the end user doesn‚Äôt really care whether their app loads in 0.001 or 0.01 seconds.
medium.com
Originally, Python was dynamically scoped. This basically means that, to evaluate an expression, a compiler first searches the current block and then successively all the calling functions.
The problem with dynamic scoping is that every expression needs to be tested in every possible context ‚Äî which is tedious. That‚Äôs why most modern programming languages use static scoping.
Python tried to transition to static scoping, but messed it up. Usually, inner scopes ‚Äî for example functions within functions ‚Äî would be able to see and change outer scopes. In Python, inner scopes can only see outer scopes, but not change them. This leads to a lot of confusion.
Despite all of the flexibility within Python, the usage of Lambdas is rather restrictive. Lambdas can only be expressions in Python, and not be statements.
On the other hand, variable declarations and statements are always statements. This means that Lambdas cannot be used for them.
This distinction between expressions and statements is rather arbitrary, and doesn‚Äôt occur in other languages.
In Python, you use whitespaces and indentations to indicate different levels of code. This makes it optically appealing and intuitive to understand.
Other languages, for example C++, rely more on braces and semicolons. While this might not be visually appealing and beginner-friendly, it makes the code a lot more maintainable. For bigger projects, this is a lot more useful.
Newer languages like Haskell solve this problem: They rely on whitespaces, but offer an alternative syntax for those who wish to go without.
As we‚Äôre witnessing the shift from desktop to smartphone, it‚Äôs clear that we need robust languages to build mobile software.
But not many mobile apps are being developed with Python. That doesn‚Äôt mean that it can‚Äôt be done ‚Äî there is a Python package called Kivy for this purpose.
But Python wasn‚Äôt made with mobile in mind. So even though it might produce passable results for basic tasks, your best bet is to use a language that was created for mobile app development. Some widely used programming frameworks for mobile include React Native, Flutter, Iconic, and Cordova.
To be clear, laptops and desktop computers should be around for many years to come. But since mobile has long surpassed desktop traffic, it‚Äôs safe to say that learning Python is not enough to become a seasoned all-round developer.
A Python script isn‚Äôt compiled first and then executed. Instead, it compiles every time you execute it, so any coding error manifests itself at runtime. This leads to poor performance, time consumption, and the need for a lot of tests. Like, a lot of tests.
This is great for beginners since testing teaches them a lot. But for seasoned developers, having to debug a complex program in Python makes them go awry. This lack of performance is the biggest factor that sets a timestamp on Python.
towardsdatascience.com
There are a few new competitors on the market of programming languages:
While there are other languages on the market, Rust, Go, and Julia are the ones that fix weak patches of Python. All of these languages excel in yet-to-come technologies, most notably in Artificial Intelligence. While their market share is still small, as reflected in the number of StackOverflow tags, the trend for all of them is clear: upwards.
Given the ubiquitous popularity of Python at the moment, it will surely take half a decade, maybe even a whole, for any of these new languages to replace it.
Which of the languages it will be ‚Äî Rust, Go, Julia, or a new language of the future ‚Äî is hard to say at this point. But given the performance issues that are fundamental in the architecture of Python, one will inevitably take its spot.
Artiba
Alex Hern√°ndez
Bima Putra
Python Programming
Iqra Ghani
Lanceria.io
Trifat
David Van Fleet
in
Better Programming
"
330,https://medium.com/towards-data-science/what-can-you-do-with-python-the-3-main-applications-518db9a68a78,What exactly can you do with Python? Here are Python‚Äôs 3 main applications.,"If you‚Äôre thinking of learning Python ‚Äî or if you recently started learning it ‚Äî you may be asking yourself:
‚ÄúWhat exactly can I use Python for?‚Äù
Well that‚Äôs a tricky question to answer, because there are so many applications for Python.
But over time, I have observed that there are 3 main popular applications for Python:
Let‚Äôs talk about each of them in turn.
Web frameworks that are based on Python like Django and Flask have recently become very popular for web development.
These web frameworks help you create server-side code (backend code) in Python. That‚Äôs the code that runs on your server, as opposed to on users‚Äô devices and browsers (front-end code). If you‚Äôre not familiar with the difference between backend code and front-end code, please see my footnote below.
That‚Äôs because a web framework makes it easier to build common backend logic. This includes mapping different URLs to chunks of Python code, dealing with databases, and generating HTML files users see on their browsers.
Django and Flask are two of the most popular Python web frameworks. I‚Äôd recommend using one of them if you‚Äôre just getting started.
There‚Äôs an excellent article about this topic by Gareth Dwyer, so let me quote it here:
<begin quote>
Main contrasts:
You should probably choose:
</end quote>
In other words, If you‚Äôre a beginner, Flask is probably a better choice because it has fewer components to deal with. Also, Flask is a better choice if you want more customization.
On the other hand, if you‚Äôre looking to build something straight-forward, Django will probably let you get there faster.
Now, if you‚Äôre looking to learn Django, I recommend the book called Django for Beginners. You can find it here.
You can also find the free sample chapters of that book here.
Okay, let‚Äôs go to the next topic!
I think the best way to explain what machine learning is would be to give you a simple example.
Let‚Äôs say you want to develop a program that automatically detects what‚Äôs in a picture.
So, given this picture below (Picture 1), you want your program to recognize that it‚Äôs a dog.
Given this other one below (Picture 2), you want your program to recognize that it‚Äôs a table.
You might say, well, I can just write some code to do that. For example, maybe if there are a lot of light brown pixels in the picture, then we can say that it‚Äôs a dog.
Or maybe, you can figure out how to detect edges in a picture. Then, you might say, if there are many straight edges, then it‚Äôs a table.
However, this kind of approach gets tricky pretty quickly. What if there‚Äôs a white dog in the picture with no brown hair? What if the picture shows only the round parts of the table?
This is where machine learning comes in.
Machine learning typically implements an algorithm that automatically detects a pattern in the given input.
You can give, say, 1,000 pictures of a dog and 1,000 pictures of a table to a machine learning algorithm. Then, it will learn the difference between a dog and a table. When you give it a new picture of either a dog or a table, it will be able to recognize which one it is.
I think this is somewhat similar to how a baby learns new things. How does a baby learn that one thing looks like a dog and another a table? Probably from a bunch of examples.
You probably don‚Äôt explicitly tell a baby, ‚ÄúIf something is furry and has light brown hair, then it‚Äôs probably a dog.‚Äù
You would probably just say, ‚ÄúThat‚Äôs a dog. This is also a dog. And this one is a table. That one is also a table.‚Äù
Machine learning algorithms work much the same way.
You can apply the same idea to:
among other applications.
Popular machine learning algorithms you might have heard about include:
You can use any of the above algorithms to solve the picture-labeling problem I explained earlier.
There are popular machine learning libraries and frameworks for Python.
Two of the most popular ones are scikit-learn and TensorFlow.
If you‚Äôre just getting started with a machine learning project, I would recommend that you first start with scikit-learn. If you start running into efficiency issues, then I would start looking into TensorFlow.
To learn machine learning fundamentals, I would recommend either Stanford‚Äôs or Caltech‚Äôs machine learning course.
Please note that you need basic knowledge of calculus and linear algebra to understand some of the materials in those courses.
Then, I would practice what you‚Äôve learned from one of those courses with Kaggle. It‚Äôs a website where people compete to build the best machine learning algorithm for a given problem. They have nice tutorials for beginners, too.
To help you understand what these might look like, let me give you a simple example here.
Let‚Äôs say you‚Äôre working for a company that sells some products online.
Then, as a data analyst, you might draw a bar graph like this.
From this graph, we can tell that men bought over 400 units of this product and women bought about 350 units of this product this particular Sunday.
As a data analyst, you might come up with a few possible explanations for this difference.
One obvious possible explanation is that this product is more popular with men than with women. Another possible explanation might be that the sample size is too small and this difference was caused just by chance. And yet another possible explanation might be that men tend to buy this product more only on Sunday for some reason.
To understand which of these explanations is correct, you might draw another graph like this one.
Instead of showing the data for Sunday only, we‚Äôre looking at the data for a full week. As you can see, from this graph, we can see that this difference is pretty consistent over different days.
From this little analysis, you might conclude that the most convincing explanation for this difference is that this product is simply more popular with men than with women.
On the other hand, what if you see a graph like this one instead?
Then, what explains the difference on Sunday?
You might say, perhaps men tend to buy more of this product only on Sunday for some reason. Or, perhaps it was just a coincidence that men bought more of it on Sunday.
So, this is a simplified example of what data analysis might look like in the real world.
The data analysis work I did when I was working at Google and Microsoft was very similar to this example ‚Äî only more complex. I actually used Python at Google for this kind of analysis, while I used JavaScript at Microsoft.
I used SQL at both of those companies to pull data from our databases. Then, I would use either Python and Matplotlib (at Google) or JavaScript and D3.js (at Microsoft) to visualize and analyze this data.
One of the most popular libraries for data visualization is Matplotlib.
It‚Äôs a good library to get started with because:
How should I learn data analysis / visualization with Python?
You should first learn the fundamentals of data analysis and visualization. When I looked for good resources for this online, I couldn‚Äôt find any. So, I ended up making a YouTube video on this topic:
I also ended up making a full course on this topic on Pluralsight, which you can take for free by signing up to their 10-day free trial.
I‚Äôd recommend both of them.
After learning the fundamentals of data analysis and visualization, learning fundamentals of statistics from websites like Coursera and Khan Academy will be helpful, as well.
Scripting usually refers to writing small programs that are designed to automate simple tasks.
So, let me give you an example from my personal experience here.
I used to work at a small startup in Japan where we had an email support system. It was a system for us to respond to questions customers sent us via email.
When I was working there, I had the task of counting the numbers of emails containing certain keywords so we could analyze the emails we received.
We could have done it manually, but instead, I wrote a simple program / simple script to automate this task.
Actually, we used Ruby for this back then, but Python is also a good language for this kind of task. Python is suited for this type of task mainly because it has relatively simple syntax and is easy to write. It‚Äôs also quick to write something small with it and test it.
I‚Äôm not an expert on embedded applications, but I know that Python works with Rasberry Pi. It seems like a popular application among hardware hobbyists.
You could use the library called PyGame to develop games, but it‚Äôs not the most popular gaming engine out there. You could use it to build a hobby project, but I personally wouldn‚Äôt choose it if you‚Äôre serious about game development.
Rather, I would recommend getting started with Unity with C#, which is one of the most popular gaming engines. It allows you to build a game for many platforms, including Mac, Windows, iOS, and Android.
You could make one with Python using Tkinter, but it doesn‚Äôt seem like the most popular choice either.
Instead, it seems like languages like Java, C#, and C++ are more popular for this.
Recently, some companies have started using JavaScript to create Desktop applications, too.
For example, Slack‚Äôs desktop app was built with something called Electron. It allows you to build desktop applications with JavaScript.
Personally, if I was building a desktop application, I would go with a JavaScript option. It allows you to reuse some of the code from a web version if you have it.
However, I‚Äôm not an expert on desktop applications either, so please let me know in a comment if you disagree or agree with me on this.
I would recommend Python 3 since it‚Äôs more modern and it‚Äôs a more popular option at this point.
Let‚Äôs say you want to make something like Instagram.
Then, you‚Äôd need to create front-end code for each type of device you want to support.
You might use, for example:
Each set of code will run on each type of device / browser. This will be the set of code that determines what the layout of the app will be like, what the buttons should look like when you click them, etc.
However, you will still need the ability to store users‚Äô info and photos. You will want to store them on your server and not just on your users‚Äô devices so each user‚Äôs followers can view his/her photos.
This is where the backend code / server-side code comes in. You‚Äôll need to write some backend code to do things like:
So, this is the difference between backend code and front-end code.
By the way, Python is not the only good choice for writing backend / server-side code. There are many other popular choices, including Node.js, which is based on JavaScript.
I have a programming education YouTube channel called CS Dojo with 440,000+ subscribers, where I produce more content like this article.
For example, you might like these videos:
Gabriel Br
Thong Tran
Omid Haghighatgoo
Paulust kan
Alicia Hogan
QuickStart
Omkar Gawade
Abhishivachary
"
331,https://medium.com/@gp_pulipaka/an-essential-guide-to-classification-and-regression-trees-in-r-language-4ced657d176b,An essential guide to classification and regression trees in R Language,"The classification trees and regression trees find their roots from CHAID, which is Chi-Square Automatic Interaction Detector. Kass proposed this in 1980. To gain deep insights into classification and regression trees, one might need to explore a little further into CHAID. However, according to Riley, CHAID was derived from THAID, also known as Theta AID proposed by Morgan and Messenger in 1973. Classification and regression trees are also called recursive partitioning trees have been extensively used in predictive analytics. Though both classification and regression trees can be leveraged for various purposes, the inventors of both THAID and CHAID specifically wanted to trace the non-linear effects of the response variable and the interactions between the predictor variables. CHAID is a non-binary tree, meaning, it can have more than two branches to connect to a node or a single root. CHAID algorithms have been heavily leveraged in SPSS as well as early as in 2001. Belson in 1959 proposed paper to be able to predict the outcome of the second group based on the observations from the first group. Morgan in 1963 proposed AID (Automatic Interaction Detector) for a binary regression tree for generating quantitative outcome from the variables. Therefore, the roots of the decision trees point to statistics. It was not popularized much into Computer Science, until 1971, when AID program was developed by Morgan in 1971.
Primarily there are two fundamental differences between the classification and regression trees. The classification tree splits the response variable into mainly two classes Yes or No, also can be numerically categorized as 1 or 0. To apply recursive partitioning on the target category that can contain multiple variables, C4.5 algorithm is leveraged. In case of simple binary splits, CART algorithm is used. This is the reason why classification tree is applied when there is a need for categorical variable for categorical outcome. The regression trees are leveraged in case where the response variable is either continuous or numeric, but not categorical. Regression trees can be applied in case of prices, quantities, or data involving quantities etc.
The regression and classification trees are machine-learning methods to building the prediction models from specific datasets. The data is split into multiple blocks recursively and the prediction model is fit on each of such partition of the prediction model. Now, each partition represents the data as a graphical decision tree. The primary difference between classification and regression decision trees is that, the classification decision trees are built with unordered values with dependent variables. The regression decision trees take ordered values with continuous values. In case of classification decision tree, for the trained dataset td for m number of observations, for a class variable Cl for p and l predictor variables from Z1 ‚Ä¶. Zn. The objective is to build a predictive model for the values of Cl from new Z values. The Z has to be partitioned into multiple blocks. The initial algorithm that was built in the early stages of classification decision trees is THAID. Classification decision tree algorithm has several features such as pruning, unbiased splits, branches/splits, split type, user-specified priors, variable ranking, user-specified costs, missing values, and bagging and ensembles.
Historically regression tree algorithm AID was invented much earlier than THAID algorithm of classification tree. In case of regression decision tree algorithm, the variable Cl takes the ordered values instead of unordered values. Regression tree also has all the features similar to classification tree. The regression trees primarily have three advantages a) Unbiased splits; b) Each node contains a single regression model fit; c) Regression tree algorithms is stemming from the residuals, there are not many limitations for regression tree algorithms including general least squares.
Taking the decision to carry umbrella has several factors that can lead up to the decision-making. The weather factors whether it is a sunny day, rainy day, safeguard the suit or dress from getting wet, and inconvenience factor in case it does not rain to carry the umbrella can influence the decision to use the umbrella or not. I illustrated the decision tree diagram in PowerPoint to make the decision to take an umbrella or not. The decision tree flow shows that, in case an individual has made a decision not to carry an umbrella with him or her, and it turns out to be a sunny day, the best payoff value is shown as 1.00. The advantage of taking the umbrella is not to get wet in the rain. However, there is an inconvenient factor of carrying the umbrella without knowing if it is going to rain or not, but if it rains and the dress gets wet, the payoff value in this case can be equated to 0.00. The conditional probability of the sunshine can be denoted as pyoff.
The calculation for pays off to take an umbrella or not can be represented as
a) Take an umbrella ‚Äî 0.8pyoff + 0.8(1-pyoff) = 0.8
b) Not to take an umbrella ‚Äî 1.0pyoff + 0(1-pyoff) = pyoff.
In conclusion, the decision to take an umbrella is when pyoff < 0.80.
Both classification and regression decision trees have dependent variables and predictor variables. The predictor variables can have an ensemble of ordinal and nominal scales and the dependent variables are either quantitative or qualitative variables. The classification decision trees have categorical variables, whereas the regression decision trees have quantitative variables. The regression trees have parallel computing of regression and ANOVA modeling, and the classification can parallel compute the discriminant analysis
Four primary advantages of decision trees
i) The decision trees are available in several predictive analytics tools such as RapidMiner. In earlier review, I have described the features available in both classification and regression trees. Most of the predictive analytics tools perform the selection of the features and screening of the variables autonomously.
ii) Unlike the several algorithms that require extensive data preparation before performing analysis and applying the algorithms, the decision trees preparation of data does not require herculean efforts from the users of the data. For fitting a regression model or computing the coefficients the data has to be transformed to scale to the model. However, the decision trees do not require such transformations as the structure of the decision trees remain unchanged throughout the analysis. Applying any statistical methods usually tend to be erroneous when there are missing or null values. However, the null values or missing values do not impede the partitioning of the data for generating decision trees.
iii) Majority of the linear regression models tend to throw errors when there are nonlinear relationships between the variables. The decision trees do not expect the data to be linear.
iv) Another advantage of decision trees is the simplified models and reduction of the complexity. The visual diagram provides a simple explanation and easy interpretation to the executives.
References
Clemen, R. T., & Reilly, T. (2001). Making Hard Decision with Decision Tools Suite (1 ed.). Boston, MA: Thomson: Duxbury.
Deshpande, B. (2011). 2 main differences between classification and regression trees. Retrieved March 6, 2016, from http://www.simafore.com/blog/bid/62482/2-main-differences-between-classification-and-regression-trees
Deshpande, B. (2011). 4 key advantages of using decision trees for predictive analytics. Retrieved February 29, 2016 , from http://www.simafore.com/blog/bid/62333/4-key-advantages-of-using-decision-trees-for-predictive-analytics
Loh, W. (2011, January 6). Classification and regression trees. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1, 14‚Äì23. http://dx.doi.org/10.1002/widm.8
Ritschard, G. (2010 ). CHAID and Earlier Supervised TreeMethods . Retrieved March 6, 2016 , from http://www.unige.ch/ses/metri/cahiers/2010_02.pdf
Unesco (n.d.). Classification and Regression Trees. Retrieved March 1, 2016 , from http://www.unesco.org/webworld/idams/advguide/Chapt10.htm
"
332,https://medium.com/gustavorsantos/code-snippets-for-a-beginner-in-r-language-2d364a32debd,Code Snippets for a Beginner in R Language,"Use these codes as your everyday cheat sheet for R.
The R Language is very present in the academic world, given it is a powerful statistical programming language.
But long ago it started to gain more adepts in the business world and since then it has been growing a lot.
R vs Python jokes or‚Ä¶
Mike Kraus
Swapnilsukare
LOTTO Pool üÉè1.35.3
Sam Hatoum
in
Xolvio
Patricia Dang
TVS Next
Sanduni Jayasinghe
Tony He
"
333,https://medium.com/gustavorsantos/how-to-group-and-summarise-with-r-language-54de845b2e40,How to Group and Summarise Data with R Language,"Learn the basics of grouping data with dplyr in R Language
Grouping is an important task when exploring data. That is a fact. But why?
That is necessary so you can begin to understand your data. Think again: if you‚Äôre looking at a dataset with many rows showing information about the same thing or topic, that‚Ä¶
Adam Chambers
Rizka Yolanda
dapota
Mindy de Hooge
in
strava-engineering
Jana Ho≈°kov√°
Rohith Teja
in
Towards Data Science
Ahmed Ashish
Kavin Ammankattur Palaniappan
"
334,https://medium.com/insight-data/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e,How to solve 90% of NLP problems: a step-by-step guide,"For more content like this, follow Insight and Emmanuel on Twitter.
Whether you are an established company or working to launch a new service, you can always leverage text data to validate, improve, and expand the functionalities of your product. The science of extracting meaning and learning from text data is an active topic of research called Natural Language Processing (NLP).
NLP produces new and exciting results on a daily basis, and is a very large field. However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other:
While many NLP papers and tutorials exist online, we have found it hard to find guidelines and tips on how to approach these problems efficiently from the ground up.
After leading hundreds of projects a year and gaining advice from top teams all over the United States, we wrote this post to explain how to build Machine Learning solutions to solve problems like the ones mentioned above. We‚Äôll begin with the simplest method that could work, and then move on to more nuanced solutions, such as feature engineering, word vectors, and deep learning.
After reading this article, you‚Äôll know how to:
We wrote this post as a step-by-step guide; it can also serve as a high level overview of highly effective standard approaches.
This post is accompanied by an interactive notebook demonstrating and applying all these techniques. Feel free to run the code and follow along!
Every Machine Learning problem starts with data, such as a list of emails, posts, or tweets. Common sources of textual information include:
‚ÄúDisasters on Social Media‚Äù dataset
For this post, we will use a dataset generously provided by Figure Eight, called ‚ÄúDisasters on Social Media‚Äù, where:
Contributors looked at over 10,000 tweets culled with a variety of searches like ‚Äúablaze‚Äù, ‚Äúquarantine‚Äù, and ‚Äúpandemonium‚Äù, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous).
Our task will be to detect which tweets are about a disastrous event as opposed to an irrelevant topic such as a movie. Why? A potential application would be to exclusively notify law enforcement officials about urgent emergencies while ignoring reviews of the most recent Adam Sandler film. A particular challenge with this task is that both classes contain the same search terms used to find the tweets, so we will have to use subtler differences to distinguish between them.
In the rest of this post, we will refer to tweets that are about disasters as ‚Äúdisaster‚Äù, and tweets about anything else as ‚Äúirrelevant‚Äù.
We have labeled data and so we know which tweets belong to which categories. As Richard Socher outlines below, it is usually faster, simpler, and cheaper to find and label enough data to train a model on, rather than trying to optimize a complex unsupervised method.
The number one rule we follow is: ‚ÄúYour model will only ever be as good as your data.‚Äù
One of the key skills of a data scientist is knowing whether the next step should be working on the model or the data. A good rule of thumb is to look at the data first and then clean it up. A clean dataset will allow a model to learn meaningful features and not overfit on irrelevant noise.
Here is a checklist to use to clean your data: (see the code for more details):
After following these steps and checking for additional errors, we can start using the clean, labelled data to train models!
Machine Learning models take numerical values as input. Models working on images, for example, take in a matrix representing the intensity of each pixel in each color channel.
Our dataset is a list of sentences, so in order for our algorithm to extract patterns from the data, we first need to find a way to represent it in a way that our algorithm can understand, i.e. as a list of numbers.
A natural way to represent text for computers is to encode each character individually as a number (ASCII for example). If we were to feed this simple representation into a classifier, it would have to learn the structure of words from scratch based only on our data, which is impossible for most datasets. We need to use a higher level approach.
For example, we can build a vocabulary of all the unique words in our dataset, and associate a unique index to each word in the vocabulary. Each sentence is then represented as a list that is as long as the number of distinct words in our vocabulary. At each index in this list, we mark how many times the given word appears in our sentence. This is called a Bag of Words model, since it is a representation that completely ignores the order of words in our sentence. This is illustrated below.
We have around 20,000 words in our vocabulary in the ‚ÄúDisasters of Social Media‚Äù example, which means that every sentence will be represented as a vector of length 20,000. The vector will contain mostly 0s because each sentence contains only a very small subset of our vocabulary.
In order to see whether our embeddings are capturing information that is relevant to our problem (i.e. whether the tweets are about disasters or not), it is a good idea to visualize them and see if the classes look well separated. Since vocabularies are usually very large and visualizing data in 20,000 dimensions is impossible, techniques like PCA will help project the data down to two dimensions. This is plotted below.
The two classes do not look very well separated, which could be a feature of our embeddings or simply of our dimensionality reduction. In order to see whether the Bag of Words features are of any use, we can train a classifier based on them.
When first approaching a problem, a general best practice is to start with the simplest tool that could solve the job. Whenever it comes to classifying data, a common favorite for its versatility and explainability is Logistic Regression. It is very simple to train and the results are interpretable as you can easily extract the most important coefficients from the model.
We split our data in to a training set used to fit our model and a test set to see how well it generalizes to unseen data. After training, we get an accuracy of 75.4%. Not too shabby! Guessing the most frequent class (‚Äúirrelevant‚Äù) would give us only 57%. However, even if 75% precision was good enough for our needs, we should never ship a model without trying to understand it.
A first step is to understand the types of errors our model makes, and which kind of errors are least desirable. In our example, false positives are classifying an irrelevant tweet as a disaster, and false negatives are classifying a disaster as an irrelevant tweet. If the priority is to react to every potential event, we would want to lower our false negatives. If we are constrained in resources however, we might prioritize a lower false positive rate to reduce false alarms. A good way to visualize this information is using a Confusion Matrix, which compares the predictions our model makes with the true label. Ideally, the matrix would be a diagonal line from top left to bottom right (our predictions match the truth perfectly).
Our classifier creates more false negatives than false positives (proportionally). In other words, our model‚Äôs most common error is inaccurately classifying disasters as irrelevant. If false positives represent a high cost for law enforcement, this could be a good bias for our classifier to have.
To validate our model and interpret its predictions, it is important to look at which words it is using to make decisions. If our data is biased, our classifier will make accurate predictions in the sample data, but the model would not generalize well in the real world. Here we plot the most important words for both the disaster and irrelevant class. Plotting word importance is simple with Bag of Words and Logistic Regression, since we can just extract and rank the coefficients that the model used for its predictions.
Our classifier correctly picks up on some patterns (hiroshima, massacre), but clearly seems to be overfitting on some meaningless terms (heyoo, x1392). Right now, our Bag of Words model is dealing with a huge vocabulary of different words and treating all words equally. However, some of these words are very frequent, and are only contributing noise to our predictions. Next, we will try a way to represent sentences that can account for the frequency of words, to see if we can pick up more signal from our data.
In order to help our model focus more on meaningful words, we can use a TF-IDF score (Term Frequency, Inverse Document Frequency) on top of our Bag of Words model. TF-IDF weighs words by how rare they are in our dataset, discounting words that are too frequent and just add to the noise. Here is the PCA projection of our new embeddings.
We can see above that there is a clearer distinction between the two colors. This should make it easier for our classifier to separate both groups. Let‚Äôs see if this leads to better performance. Training another Logistic Regression on our new embeddings, we get an accuracy of 76.2%.
A very slight improvement. Has our model started picking up on more important words? If we are getting a better result while preventing our model from ‚Äúcheating‚Äù then we can truly consider this model an upgrade.
The words it picked up look much more relevant! Although our metrics on our test set only increased slightly, we have much more confidence in the terms our model is using, and thus would feel more comfortable deploying it in a system that would interact with customers.
Our latest model managed to pick up on high signal words. However, it is very likely that if we deploy this model, we will encounter words that we have not seen in our training set before. The previous model will not be able to accurately classify these tweets, even if it has seen very similar words during training.
To solve this problem, we need to capture the semantic meaning of words, meaning we need to understand that words like ‚Äògood‚Äô and ‚Äòpositive‚Äô are closer than ‚Äòapricot‚Äô and ‚Äòcontinent.‚Äô The tool we will use to help us capture meaning is called Word2Vec.
Using pre-trained words
Word2Vec is a technique to find continuous embeddings for words. It learns from reading massive amounts of text and memorizing which words tend to appear in similar contexts. After being trained on enough data, it generates a 300-dimension vector for each word in a vocabulary, with words of similar meaning being closer to each other.
The authors of the paper open sourced a model that was pre-trained on a very large corpus which we can leverage to include some knowledge of semantic meaning into our model. The pre-trained vectors can be found in the repository associated with this post.
A quick way to get a sentence embedding for our classifier is to average Word2Vec scores of all words in our sentence. This is a Bag of Words approach just like before, but this time we only lose the syntax of our sentence, while keeping some semantic information.
Here is a visualization of our new embeddings using previous techniques:
The two groups of colors look even more separated here, our new embeddings should help our classifier find the separation between both classes. After training the same model a third time (a Logistic Regression), we get an accuracy score of 77.7%, our best result yet! Time to inspect our model.
Since our embeddings are not represented as a vector with one dimension per word as in our previous models, it‚Äôs harder to see which words are the most relevant to our classification. While we still have access to the coefficients of our Logistic Regression, they relate to the 300 dimensions of our embeddings rather than the indices of words.
For such a low gain in accuracy, losing all explainability seems like a harsh trade-off. However, with more complex models we can leverage black box explainers such as LIME in order to get some insight into how our classifier works.
LIME
LIME is available on Github through an open-sourced package. A black-box explainer allows users to explain the decisions of any classifier on one particular example by perturbing the input (in our case removing words from the sentence) and seeing how the prediction changes.
Let‚Äôs see a couple explanations for sentences from our dataset.
However, we do not have time to explore the thousands of examples in our dataset. What we‚Äôll do instead is run LIME on a representative sample of test cases and see which words keep coming up as strong contributors. Using this approach we can get word importance scores like we had for previous models and validate our model‚Äôs predictions.
Looks like the model picks up highly relevant words implying that it appears to make understandable decisions. These seem like the most relevant words out of all previous models and therefore we‚Äôre more comfortable deploying in to production.
We‚Äôve covered quick and efficient approaches to generate compact sentence embeddings. However, by omitting the order of words, we are discarding all of the syntactic information of our sentences. If these methods do not provide sufficient results, you can utilize more complex model that take in whole sentences as input and predict labels without the need to build an intermediate representation. A common way to do that is to treat a sentence as a sequence of individual word vectors using either Word2Vec or more recent approaches such as GloVe or CoVe. This is what we will do below.
Convolutional Neural Networks for Sentence Classification train very quickly and work well as an entry level deep learning architecture. While Convolutional Neural Networks (CNN) are mainly known for their performance on image data, they have been providing excellent results on text related tasks, and are usually much quicker to train than most complex NLP approaches (e.g. LSTMs and Encoder/Decoder architectures). This model preserves the order of words and learns valuable information on which sequences of words are predictive of our target classes. Contrary to previous models, it can tell the difference between ‚ÄúAlex eats plants‚Äù and ‚ÄúPlants eat Alex.‚Äù
Training this model does not require much more work than previous approaches (see code for details) and gives us a model that is much better than the previous ones, getting 79.5% accuracy! As with the models above, the next step should be to explore and explain the predictions using the methods we described to validate that it is indeed the best model to deploy to users. By now, you should feel comfortable tackling this on your own.
Here is a quick recap of the approach we‚Äôve successfully used:
These approaches were applied to a particular example case using models tailored towards understanding and leveraging short text such as tweets, but the ideas are widely applicable to a variety of problems. I hope this helped you, we‚Äôd love to hear your comments and questions! Feel free to comment below or reach out to @EmmanuelAmeisen here or on Twitter.
Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? Learn more about the Artificial Intelligence program.
Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch.
Okan Bulut
in
Towards Data Science
Gokul
in
Argoid
Trisha Sharma
Stacy Deininger
in
Cicero
Naresh Mehta
in
CARS24 Data Science Blog
Bryan Chen
Data Lass
Eliana Grosof
in
Towards Data Science
"
335,https://medium.com/towards-data-science/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,BERT Explained: State of the art language model for NLP,"BERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering (SQuAD v1.1), Natural Language Inference (MNLI), and others.
BERT‚Äôs key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper‚Äôs results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.
In the field of computer vision, researchers have repeatedly shown the value of transfer learning ‚Äî pre-training a neural network model on a known task, for instance ImageNet, and then performing fine-tuning ‚Äî using the trained neural network as the basis of a new purpose-specific model. In recent years, researchers have been showing that a similar technique can be useful in many natural language tasks.
A different approach, which is also popular in NLP tasks and exemplified in the recent ELMo paper, is feature-based training. In this approach, a pre-trained neural network produces word embeddings which are then used as features in NLP models.
BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms ‚Äî an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT‚Äôs goal is to generate a language model, only the encoder mechanism is necessary. The detailed workings of Transformer are described in a paper by Google.
As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it‚Äôs non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).
The chart below is a high-level description of the Transformer encoder. The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors of size H, in which each vector corresponds to an input token with the same index.
When training language models, there is a challenge of defining a prediction goal. Many models predict the next word in a sequence (e.g. ‚ÄúThe child came home from ___‚Äù), a directional approach which inherently limits context learning. To overcome this challenge, BERT uses two training strategies:
Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:
The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness (see Takeaways #3).
Note: In practice, the BERT implementation is slightly more elaborate and doesn‚Äôt replace all of the 15% masked words. See Appendix A for additional information.
In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.
To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:
To predict if the second sentence is indeed connected to the first, the following steps are performed:
When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.
Using BERT for a specific task is relatively straightforward:
BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:
In the fine-tuning training, most hyper-parameters stay the same as in BERT training, and the paper gives specific guidance (Section 3.5) on the hyper-parameters that require tuning. The BERT team has used this technique to achieve state-of-the-art results on a wide variety of challenging natural language tasks, detailed in Section 4 of the paper.
BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it‚Äôs approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this summary, we attempted to describe the main ideas of the paper while not drowning in excessive technical details. For those wishing for a deeper dive, we highly recommend reading the full article and ancillary articles referenced in it. Another useful reference is the BERT source code and models, which cover 103 languages and were generously released as open source by the research team.
Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows ‚Äî 80% are replaced with a ‚Äú[MASK]‚Äù token, 10% with a random word, and 10% use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight):
No ablation was done on the ratios of this approach, and it may have worked better with different ratios. In addition, the model performance wasn‚Äôt tested with simply masking 100% of the selected tokens.
For more summaries on the recent Machine Learning research, check out Lyrn.AI.
Andrzej Szymanski, PhD
in
Towards Data Science
James Briggs
in
Better Programming
Anmol Parande
in
The Startup
Christopher Dossman
in
Towards Data Science
Manan Aggarwal
Daryl Felix
Isamu Isozaki
in
Analytics Vidhya
Vamshi Krishna Dude
"
336,https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34,Automated Keyword Extraction from Articles using NLP,"In research & news articles, keywords form an important component since they provide a concise representation of the article‚Äôs content. Keywords also play a crucial role in locating the article from information retrieval systems, bibliographic databases and for search engine optimization. Keywords also help to categorize the article into the relevant‚Ä¶
Eric Bezzam
Praveen Thenraj
in
Machine Learning
Pranav
Muhammad Aitsam
Borna Ahmadzadeh
in
DataDrivenInvestor
Victor Anjos
Mohan Krishna R
Vishal Mi
"
337,https://medium.com/cometheartbeat/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b,The 5 Computer Vision Techniques That Will Change How You See The World,"Computer Vision is one of the hottest research fields within Deep Learning at the moment. It sits at the intersection of many academic subjects, such as Computer Science (Graphics, Algorithms, Theory, Systems, Architecture), Mathematics (Information Retrieval, Machine Learning), Engineering (Robotics, Speech, NLP, Image Processing), Physics (Optics), Biology (Neuroscience), and Psychology (Cognitive Science).
As Computer Vision represents a relative understanding of visual environments and their contexts, many scientists believe the field paves the way towards Artificial General Intelligence due to its cross-domain mastery.
Here are a couple of formal textbook definitions:
The most obvious answer is that there‚Äôs a fast-growing collection of useful applications derived from this field of study. Here are just a handful of them:
I recently finished Stanford‚Äôs wonderful CS231n course on using Convolutional Neural Networks for visual recognition. Visual recognition tasks such as image classification, localization, and detection are key components of Computer vision.
Recent developments in neural networks and deep learning approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. The course is a phenomenal resource that taught me the details of deep learning architectures being used in cutting-edge computer vision research. In this article, I want to share the 5 major computer vision techniques I‚Äôve learned as well as major deep learning models and applications using each of them.
The problem of image classification goes like this: Given a set of images that are all labeled with a single category, we‚Äôre asked to predict these categories for a novel set of test images and measure the accuracy of the predictions. There are a variety of challenges associated with this task, including viewpoint variation, scale variation, intra-class variation, image deformation, image occlusion, illumination conditions, and background clutter.
How might we go about writing an algorithm that can classify images into distinct categories? Computer Vision researchers have come up with a data-driven approach to solve this. Instead of trying to specify what every one of the image categories of interest look like directly in code, they provide the computer with many examples of each image class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class.
In other words, they first accumulate a training dataset of labeled images, then feed it to the computer to process the data. Given that fact, the complete image classification pipeline can be formalized as follows:
The most popular architecture used for image classification is Convolutional Neural Networks (CNNs). A typical use case for CNNs is where you feed the network images and the network classifies the data. CNNs tend to start with an input ‚Äúscanner‚Äù which isn‚Äôt intended to parse all the training data at once. For example, to input an image of 100 x 100 pixels, you wouldn‚Äôt want a layer with 10,000 nodes.
Rather, you create a scanning input layer of say 10 x 10 which you feed the first 10 x 10 pixels of the image. Once you passed that input, you feed it the next 10 x 10 pixels by moving the scanner one pixel to the right. This technique is known as sliding windows.
This input data is then fed through convolutional layers instead of normal layers. Each node only concerns itself with close neighboring cells. These convolutional layers also tend to shrink as they become deeper, mostly by easily divisible factors of the input. Besides these convolutional layers, they also often feature pooling layers. Pooling is a way to filter out details: a commonly found pooling technique is max pooling, where we take, say, 2 x 2 pixels and pass on the pixel with the most amount of a certain attribute.
Most image classification techniques nowadays are trained on ImageNet, a dataset with approximately 1.2 million high-resolution training images. Test images will be presented with no initial annotation (no segmentation or labels), and algorithms will have to produce labelings specifying what objects are present in the images. Some of the best existing computer vision methods were tried on this dataset by leading computer vision groups from Oxford, INRIA, and XRCE. Typically, computer vision systems use complicated multi-stage pipelines, and the early stages are typically hand-tuned by optimizing a few parameters.
The winner of the 1st ImageNet competition, Alex Krizhevsky (NIPS 2012), developed a very deep convolutional neural net of the type pioneered by Yann LeCun. Its architecture includes 7 hidden layers, not counting some max pooling layers. The early layers were convolutional, while the last 2 layers were globally connected. The activation functions were rectified linear units in every hidden layer. These train much faster and are more expressive than logistic units. In addition to that, it also uses competitive normalization to suppress hidden activities when nearby units have stronger activities. This helps with variations in intensity.
In terms of hardware requirements, Alex uses a very efficient implementation of convolutional nets on 2 Nvidia GTX 580 GPUs (over 1000 fast little cores). The GPUs are very good for matrix-matrix multiplies and also have very high bandwidth to memory. This allows him to train the network in a week and makes it quick to combine results from 10 patches at test time. We can spread a network over many cores if we can communicate the states fast enough. As cores get cheaper and datasets get bigger, big neural nets will improve faster than old-fashioned CV systems. Since AlexNet, there have been multiple new models using CNN as their backbone architecture and achieving excellent results in ImageNet: ZFNet (2013), GoogLeNet (2014), VGGNet (2014), ResNet (2015), DenseNet (2016) etc.
The task to define objects within images usually involves outputting bounding boxes and labels for individual objects. This differs from the classification / localization task by applying classification and localization to many objects instead of just a single dominant object. You only have 2 classes of object classification, which means object bounding boxes and non-object bounding boxes. For example, in car detection, you have to detect all cars in a given image with their bounding boxes.
If we use the Sliding Window technique like the way we classify and localize images, we need to apply a CNN to many different crops of the image. Because CNN classifies each crop as object or background, we need to apply CNN to huge numbers of locations and scales, which is very computationally expensive!
In order to cope with this, neural network researchers have proposed to use regions instead, where we find ‚Äúblobby‚Äù image regions that are likely to contain objects.
This is relatively fast to run. The first model that kicked things off was R-CNN(Region-based Convolutional Neural Network). In a R-CNN, we first scan the input image for possible objects using an algorithm called Selective Search, generating ~2,000 region proposals. Then we run a CNN on top of each of these region proposals. Finally, we take the output of each CNN and feed it into an SVM to classify the region and a linear regression to tighten the bounding box of the object.
Essentially, we turned object detection into an image classification problem. However, there are some problems ‚Äî the training is slow, a lot of disk space is required, and inference is also slow.
An immediate descendant to R-CNN is Fast R-CNN, which improves the detection speed through 2 augmentations: 1) Performing feature extraction before proposing regions, thus only running one CNN over the entire image, and 2) Replacing SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model.
Fast R-CNN performed much better in terms of speed, because it trains just one CNN for the entire image. However, the selective search algorithm is still taking a lot of time to generate region proposals.
Thus comes the invention of Faster R-CNN, which now is a canonical model for deep learning-based object detection. It replaces the slow selective search algorithm with a fast neural network by inserting a Region Proposal Network (RPN) to predict proposals from features. The RPN is used to decide ‚Äúwhere‚Äù to look in order to reduce the computational requirements of the overall inference process. The RPN quickly and efficiently scans every location in order to assess whether further processing needs to be carried out in a given region. It does that by outputting k bounding box proposals each with 2 scores representing the probability of object or not at each location.
Once we have our region proposals, we feed them straight into what is essentially a Fast R-CNN. We add a pooling layer, some fully-connected layers, and finally a softmax classification layer and bounding box regressor.
Altogether, Faster R-CNN achieved much better speeds and higher accuracy. It‚Äôs worth noting that although future models did a lot to increase detection speeds, few models managed to outperform Faster R-CNN by a significant margin. In other words, Faster R-CNN may not be the simplest or fastest method for object detection, but it‚Äôs still one of the best performing.
Major Object Detection trends in recent years have shifted towards quicker, more efficient detection systems. This was visible in approaches like You Only Look Once (YOLO), Single Shot MultiBox Detector (SSD), and Region-Based Fully Convolutional Networks (R-FCN) as a move towards sharing computation on a whole image. Hence, these approaches differentiate themselves from the costly subnetworks associated with the 3 R-CNN techniques. The main rationale behind these trends is to avoid having separate algorithms focus on their respective subproblems in isolation, as this typically increases training time and can lower network accuracy.
Join more than 14,000 of your fellow machine learners and data scientists. Subscribe to the premier newsletter for all things deep learning.
Object Tracking refers to the process of following a specific object of interest, or multiple objects, in a given scene. It traditionally has applications in video and real-world interactions where observations are made following an initial object detection. Now, it‚Äôs crucial to autonomous driving systems such as self-driving vehicles from companies like Uber and Tesla.
Object Tracking methods can be divided into 2 categories according to the observation model: generative method and discriminative method. The generative method uses the generative model to describe the apparent characteristics and minimizes the reconstruction error to search the object, such as PCA.
The discriminative method can be used to distinguish between the object and the background, its performance is more robust, and it gradually becomes the main method in tracking. The discriminative method is also referred to as Tracking-by-Detection, and deep learning belongs to this category. To achieve tracking-by-detection, we detect candidate objects for all frames and use deep learning to recognize the wanted object from the candidates. There are 2 kinds of basic network models that can be used: stacked auto encoders (SAE) and convolutional neural network (CNN).
The most popular deep network for tracking tasks using SAE is Deep Learning Tracker, which proposes offline pre-training and online fine-tuning the net. The process works like this:
Because of its superiority in image classification and object detection, CNN has become the mainstream deep model in computer vision and in visual tracking. Generally speaking, a large-scale CNN can be trained both as a classifier and as a tracker. 2 representative CNN-based tracking algorithms are fully-convolutional network tracker (FCNT) and multi-domain CNN (MD Net).
FCNT analyzes and takes advantage of the feature maps of the VGG model successfully, which is a pre-trained ImageNet, and results in the following observations:
Because of these observations, FCNT designs the feature selection network to select the most relevant feature maps on the conv4‚Äì3 and conv5‚Äì3 layers of the VGG network. Then in order to avoid overfitting on noisy ones, it also designs extra two channels (called SNet and GNet) for the selected feature maps from two layers‚Äô separately. The GNet captures the category information of the object, while the SNet discriminates the object from a background with a similar appearance.
Both of the networks are initialized with the given bounding-box in the first frame to get heat maps of the object, and for new frames, a region of interest (ROI) centered at the object location in the last frame is cropped and propagated. At last, through SNet and GNet, the classifier gets two heat maps for prediction, and the tracker decides which heat map will be used to generate the final tracking result according to whether there are distractors. The pipeline of FCNT is shown below.
Different from the idea of FCNT, MD Net uses all the sequences of a video to to track movements in them. The networks mentioned above use irrelevant image data to reduce the training demand of tracking data, and this idea has some deviation from tracking. The object of one class in this video can be the background in another video, so MD Net proposes the idea of multi-domain to distinguish the object and background in every domain independently. And a domain indicates a set of videos that contain the same kind of object.
As shown below, MD Net is divided into 2 parts: the shared layers and the K branches of domain-specific layers. Each branch contains a binary classification layer with softmax loss, which is used to distinguish the object and background in each domain, and the shared layers sharing with all domains to ensure the general representation.
In recent years, deep learning researchers have tried different ways to adapt to features of the visual tracking task. There are many directions that have been explored: applying other network models such as Recurrent Neural Net and Deep Belief Net, designing the network structure to adapt to video processing and end-to-end learning, optimizing the process, structure, and parameters, or even combining deep learning with traditional methods of computer vision or approaches in other fields such as Language Processing and Speech Recognition.
Central to Computer Vision is the process of segmentation, which divides whole images into pixel groupings which can then be labelled and classified.
Particularly, Semantic Segmentation tries to semantically understand the role of each pixel in the image (e.g. is it a car, a motorbike, or some other type of class?). For example, in the picture above, apart from recognizing the person, the road, the cars, the trees, etc., we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.
As with other computer vision tasks, CNNs have had enormous success on segmentation problems. One of the popular initial approaches was patch classification through a sliding window, where each pixel was separately classified into classes using a patch of images around it. This, however, is very inefficient computationally because we don‚Äôt reuse the shared features between overlapping patches.
The solution, instead, is UC Berkeley‚Äôs Fully Convolutional Networks (FCN), which popularized end-to-end CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for images of any size and was also much faster compared to the patch classification approach. Almost all subsequent approaches to semantic segmentation adopted this paradigm.
However, one problem remains: convolutions at original image resolution will be very expensive. To deal with this, FCN uses downsampling and upsampling inside the network. The downsampling layer is known as striped convolution, while the upsampling layer is known as transposed convolution.
Despite the upsampling/downsampling layers, FCN produces coarse segmentation maps because of information loss during pooling. SegNet is a more memory efficient architecture than FCN that uses-max pooling and an encoder-decoder framework. In SegNet, shortcut/skip connections are introduced from higher resolution feature maps to improve the coarseness of upsampling/downsampling.
Recent research in Semantic Segmentation all relies heavily on fully convolutional networks, such as Dilated Convolutions, DeepLab, and RefineNet.
Beyond Semantic Segmentation, Instance Segmentation segments different instances of classes, such as labelling 5 cars with 5 different colors. In classification, there‚Äôs generally an image with a single object as the focus and the task is to say what that image is. But in order to segment instances, we need to carry out far more complex tasks. We see complicated sights with multiple overlapping objects and different backgrounds, and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!
So far, we‚Äôve seen how to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes. Can we extend such techniques to locate exact pixels of each object instead of just bounding boxes? This instance segmentation problem is explored at Facebook AI using an architecture known as Mask R-CNN.
Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN‚Äôs underlying intuition is straightforward Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel-level segmentation?
Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch is a Fully Convolutional Network on top of a CNN-based feature map. Given the CNN Feature Map as the input, the network outputs a matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a binary mask).
Additionally, when run without modifications on the original Faster R-CNN architecture, the regions of the feature map selected by RoIPool (Region of Interests Pool) were slightly misaligned from the regions of the original image. Since image segmentation requires pixel-level specificity, unlike bounding boxes, this naturally led to inaccuracies. Mask R-CNN solves this problem by adjusting RoIPool to be more precisely aligned using a method known as RoIAlign (Region of Interests Align). Essentially, RoIAlign uses bilinear interpolation to avoid error in rounding, which causes inaccuracies in detection and segmentation.
Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:
These 5 major computer vision techniques can help a computer extract, analyze, and understand useful information from a single or a sequence of images. There are many other advanced techniques that I haven‚Äôt touched, including style transfer, colorization, action recognition, 3D objects, human pose estimation, and more.
Indeed, the field of Computer Vision is too expensive to cover in depth, and I would encourage you to explore it further, whether through online courses, blog tutorials, or formal documents. I‚Äôd highly recommend CS231n for starters, as you‚Äôll learn to implement, train, and debug your own neural networks. As a bonus, you can get all the lecture slides and assignment guidelines from my GitHub repository. I hope it‚Äôll guide you in the quest of changing how to see the world!
If you enjoyed this piece, I‚Äôd love it if you hit the clap button üëè so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn. Sign up for my newsletter to receive my latest thoughts on data science, machine learning, and artificial intelligence right at your inbox!
Discuss this post on Hacker News
Editor‚Äôs Note: Heartbeat is a contributor-driven online publication and community dedicated to providing premier educational resources for data science, machine learning, and deep learning practitioners. We‚Äôre committed to supporting and inspiring developers and engineers from all walks of life.
Editorially independent, Heartbeat is sponsored and published by Comet, an MLOps platform that enables data scientists & ML teams to track, compare, explain, & optimize their experiments. We pay our contributors, and we don‚Äôt sell ads.
If you‚Äôd like to contribute, head on over to our call for contributors. You can also sign up to receive our weekly newsletters (Deep Learning Weekly and the Comet Newsletter), join us on Slack, and follow Comet on Twitter and LinkedIn for resources, events, and much more that will help you build better ML models, faster.
Education Ecosystem (LEDU)
Fabrizio Fantini
Bharathkumar M S
Akhmet (Massomi) Timur
Jacob Linger
The Sensible Code Company
buket d√∂nmez
Jin Cui
in
Towards AI
"
338,https://medium.com/towards-data-science/computer-vision-by-andrew-ng-11-lessons-learned-7d05c18a6999,Computer Vision by Andrew Ng ‚Äî 11 Lessons Learned,"I recently completed Andrew Ng‚Äôs computer vision course on Coursera. Ng does an excellent job at explaining many of the complex ideas required to optimize any computer vision task. My favourite component of the course was the neural style transfer section (see lesson 11), which allows you to create artwork which combines the style of Claud Monet with the content of whichever image you would like. This is an example of what you can do:
In this article, I will discuss 11 key lessons that I learned in the course. Note that this is the fourth course in the Deep Learning specialization released by deeplearning.ai. If you would like to learn about the previous 3 courses, I recommend you check out this blog.
Big data and algorithmic developments will cause the testing error of intelligent systems to converge to Bayes optimal error. This will lead to AI surpassing human level performance in all areas, including natural perception tasks. Open source software from TensorFlow allows you to use transfer learning to implement an object detection system for any object very rapidly. With transfer learning, you only need about 100‚Äì500 examples for the system to work relatively well. Manually labeling 100 examples isn‚Äôt too much work, so you‚Äôll have a minimum viable product very quickly.
Ng explains how to implement the convolution operator and shows how it can detect edges in an image. He also explains other filters, such as the Sobel filter, which put more weight on central pixels of the edge. Ng then explains that the weights of the filter should not be hand-designed but rather should be learned using a hill climbing algorithm such as gradient descent.
Ng gives several philosophical reasons for why convolutions work so well in image recognition tasks. He outlines 2 concrete reasons. The first is known as parameter sharing. It is the idea that a feature detector that‚Äôs useful in one part of an image is probably useful in another part of the image. For example, an edge detector is probably useful is many parts of the image. The sharing of parameters allows the number of parameters to be small and also allows for robust translation invariance. Translation invariance is the notion that a cat shifted and rotated is still a picture of a cat.
The second idea he outlines is known as sparsity of connections. This is the idea that each output layer is only a function of a small number of inputs (particularly, the filter size squared). This greatly reduces the number of parameters in the network and allows for faster training.
Padding is usually used to preserve the input size (i.e. the dimension of the input and output are the same). It is also used so that frames near the edges of image contribute as much to the output as frames near near the centre.
Through empirical research, max pooling has proven to be extremely effective in CNN‚Äôs. By downsampling the image, we reduce the number of parameters which makes the features invariant to scale or orientation changes.
Ng shows 3 classical network architectures including LeNet-5, AlexNet and VGG-16. The main idea he presents is that effective networks often have layers with an increasing channel size and decreasing width and height.
For a plain network, the training error does not monotonically decrease as the number of layers increases due to vanishing and exploding gradients. These networks have feed forward skipped connections which allow you train extremely large networks without a drop in performance.
Training large networks, such as inception, from scratch can take weeks on a GPU. You should download the weights from a pretrained network and just retrain the last softmax layer (or the last few layers). This will greatly reduce training time. The reason this works is that earlier layers tend to be associated with concepts in all images such as edges and curvy lines.
Ng explains that you should train several networks independently and average their outputs to get better performance. Data augmentation techniques such as randomly cropping images, flipping images about the horizontal and vertical axes may also help with performance. Finally, you should use an open source implementation and pretrained model to start and then fine-tune the parameters for your particular application.
Ng starts by explaining the idea of landmark detection in an image. Basically, these landmarks become apart of your training output examples. With some clever convolution manipulations, you get an output volume that tells you the probability that the object is in a certain region and the location of the object. He also explains how to evaluate the effectiveness of your object detection algorithm using the intersection over union formula. Finally, Ng puts all these components together to explain the famous YOLO algorithm.
Facial recognition is a one-shot learning problem since you may only have one example image to identify the person. The solution is to learn a similarity function which gives the degree of difference between two images. So if the images are of the same person, you want the function to output a small number, and vice versa for different people.
The first solution Ng gives is known as a siamese network. The idea is to input two persons into the same network separately and then compare their outputs. If the outputs are similar, then the persons are probably the same. The network is trained so that if two input images are of the same person, then the distance between their encodings is relatively small.
The second solution he gives uses a triplet loss method. The idea is that you have a triplet of images (Anchor (A), Positive (P) and Negative (N)) and you train the network so that the output distance between A and P is much smaller than the distance between A and N.
Ng explains how to generate an image with a combining content and style. See the examples below.
The key to Neural Style Transfer is to understand the visual representations for what each layer in a convolutional network is learning. It turns out that earlier layers learn simple features like edges and later features learn complex objects like faces, feet and cars.
To build a neural style transfer image, you simply define a cost function which is a convex combination of the similarity in content and style. In particular, the cost function would be:
where G is the generated image, C is the content image and S is the style image. The learning algorithm simply uses gradient descent to minimize the cost function with respect to the generated image, G.
The steps are as follows:
By completing this course, you will gain an intuitive understanding of a large chunk of the computer vision literature. The homework assignments also give you practice implementing these ideas yourself. You will not become an expert in computer vision after completing this course, but this course may kickstart a potential idea/career you may have in computer vision.
If you have any interesting applications of computer vision you would like to share, let me know in the comments below. I would be happy to discuss potential collaboration on new projects.
That‚Äôs all folks ‚Äî if you‚Äôve made it this far, please comment below and add me on LinkedIn.
My Github is here.
mynulled
Pousali Das
Skylar Kerzner
James Heathers
in
Towards Data Science
Bezawit
Pranay Sawant
Ykakdas
Honeybee
"
339,https://medium.com/readers-writers-digest/beginners-guide-to-computer-vision-23606224b720,Beginner‚Äôs Guide to Computer Vision,"If We Want Machines to Think, We Need to Teach Them to See.-Fei Fei Li, Director of Stanford AI Lab and Stanford Vision Lab
The phenomenon that makes machines such as computers or mobile phones see the surroundings is known as Computer Vision. Serious work on re-creating a human eye started way back in 50s and since then, we have come a long way. Computer vision has already made its way to our mobile phone via different e-commerce or camera apps.
Think of what more can be done by machine when they will be able to see as accurate as a human eye. Human eye is a complex structure and it goes through more complex phenomenon of understanding the environment. In a similar fashion, making machines see things and make them capable enough to figure out what they are seeing and further categorize it, is still a pretty tough job.
Working on Computer Vision is equivalent to working on millions of calculations in the blink of an eye with almost same accuracy as that of a human eye. It is not just about converting a picture into pixels, and then try to make sense of what‚Äôs in the picture through those pixels, you will have to first understand the bigger picture of how to extract information from those pixels and understand what they represent.
A. Represent colors by numbers: In computer science, each color is represented by a specified HEX value. That is how machines are programmed to understand what colors the image pixels are made up. Whereas as humans we have an inherited knowledge to differ between the shades.
B. Image Segmentation: Computers are made to identify similar group of colors and then segment the image i.e. distinguish the foreground from background. The technique of color gradient is used to find edges of different objects.
C. Finding corners: After segmentation, images are then looked up for certain features, also known as corners. In simple words, algorithms search for lines that meet at an angle and cover a specific part of the image with one color shade. Features, also called corners are the building blocks which help to find more detailed information contained in the image.
D. Find textures: Another important aspect to identify any image correctly is to determine the texture in the image. The difference in textures between two objects makes it easier for a machine to correctly categorize an object.
E. Make a guess: After implementing the above steps, a machine needs to make a nearly-right guess and match the image with those present in the database.
F. Finally, see the bigger picture! At last, a machine sees the bigger and clear picture and checks if it was right identifying the one, as per the feeded algorithmic instructions. The accuracy has improved a lot in past years but still, machines make mistakes when asked to handle images with mixed objects.
Carnegie Mellon University, Robotics Institute
University of California Los Angeles
University of North Carolina at Chapel Hill
University of Washington
University of California Berkeley
Stanford University
Massachusetts Institute of Technology
Cornell University
University of Pennsylvania
University of California Irvine
Columbia University
University of Illinois at Urbana-Champaign
University of Southern California
University of Michigan
Princeton University
University of Rochester
University of Texas at Austin
University of Maryland College Park
Brown University
University of Central Florida
New York University
Michigan State University
University of Massachusetts, Amherst
Northwestern University
University of California San Diego
University of Alberta
University of Toronto
University of British Columbia
Simon Fraser University
INRIA France
University of Oxford (http://www.robots.ox.ac.uk/~vgg/)
ETH Zurich
Max Planck Institutes, Germany
University of Edinburgh
University of Surrey
University of Freiburg
KTH Sweden
TU Dresden
TU Darmstadt
EPFL, Switzerland
KU Leuven
Computer Vision Center Barcelona
IDIAP Switzerland
Imperial College London
HCI Heidelberg
University of Manchester
University of Bonn
RWTH Aachen University
University of Amsterdam
TU Munich
Czech Technical University
University of Cambridge
TU Graz
IST Austria
Queen Mary University of London
University of Zurich
TU Delft
University of Leeds
University of Bern
Lund University
University of Trento, Italy
University of Florence, Italy
University of Stuttgart
Saarland University
Ecole Centrale Paris
Ecole des Ponts ParisTech
University of Oulu
Karlsruhe Institute of Technology
A. Beginner level
Mathematics :
Image Processing:
B. Advanced level
To gain practical knowledge about how things work especially the algorithms, start learning about OpenCV from Computer Vision perspective:
Tip: When programming in C, C++, Python we use OpenCV library for computer vision. When programming in MATLAB, we use computer vision system toolbox. Similarly there are more open source libraries if you are programming in other languages.
You should also know about the keywords or key works done in the field and here is where you can learn them from :
Introductory Level:
1. Computer Vision: Algorithms and Applications
2. Computer Vision : A Modern Approach By David A. Forsyth, Jean Ponce
3. Multiple View Geometry in Computer Vision By Richard Hartley, Andrew Zisserman
Advanced level ‚Äî Towards Deep Learning
4. Michael Nielsen‚Äôs ‚ÄúNeural Networks and Deep Learning‚Äù online book; it‚Äôs a really great, gentle introduction: Neural networks and deep learning
5. Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
2. Blaise Ag√ºera y Arcas: How PhotoSynth can connect the world‚Äôs images
3. Chieko Asakawa: How new technology helps blind people explore the world
4. Jennifer Healey: If cars could talk, accidents might be avoidable
5. Golan Levin: Art that looks back at you
6. Paul Debevec: Animating a photo-real digital face
7. Golan Levin: Software (as) art
Introductory Level:
Advanced Level ‚Äî Towards Deep Learning
Seminar Courses:
a. Microsoft computer scientists and researchers are working to ‚Äúsolve‚Äù cancer
b. Project Tokyo ‚Äî deliver AI-enabled prototypes that augment awareness of social, physical and textual environment for people who are blind or have vision impairments.
c. Teaching machines to predict the future
Another way to keep yourself aware of the research being done in Computer Vision is to follow authors and read their papers from top conferences such as CVPR, ICCV, ECCV, BMVC.
Here are a few excerpts from my conversation with 2 experts who have found their passion in the field of Computer Vision.
Conversation with Prof. Devi Parikh | Visiting Researcher at Facebook AI Research | Assistant Professor at Georgia Tech (Previously at Virginia Tech)
Computer Vision is a subfield of Artificial Intelligence where the goal is to build a computer replicating the visual intelligence of human brain. Machine Learning is a generic term for teaching machines anything, but Computer Vision specifically deals with visual data. In Machine Learning, we deal more with statistical tools whereas Computer Vision could include both ‚Äî statistical as well non-statistical tools. For instance, 3D reconstruction in Computer Vision field tends to use machine learning tools less frequently than say image classification and object recognition. Many computer vision tasks have their own needs for which we develop specific machine learning tools.
For any student to start learning about the field, I‚Äôd advise them to pick a problem by going through researchers‚Äô web pages and selecting one problem they find interesting. Mostly people are working on cutting edge problems for which standard datasets are available out there that could be used. They can select a problem, a dataset, as well as a library they might want to use and get their hands dirty.
When taking masters or PhD students, what I usually look for is ‚Äî accountability, pro-activeness, and determination. Have your basic concepts clear about the field. Try to read research papers. Try to get a sense for the problems at the frontiers of AI that researchers world-wide are working on. And get your hands dirty.
B. Conversation with Richa Agrawal | University of Pennsylvania Alumnus | Computer Vision Research Engineer at Whodat
I graduated from MNIT Jaipur and while studying there I got in touch with the Robotics group. We did a few projects and went on to participate in a national level competition at IIT Roorkee. We won the competition and that boosted my morale. After completing my bachelor‚Äôs, I started working at Yahoo. I realized that this is not something I wanted or want to do and hence, went for my master‚Äôs at University of Pennsylvania. I explored different research areas during that time by taking different courses and finally decided Computer Vision as my main research interest. After graduating, I worked at a startup in the US and was looking for similar opportunity in India as the field started growing even here. At Whodat, a Computer Vision startup based out of Bangalore, we do stuff with Augmented Reality and Visualization. For instance, say, you‚Äôre planning to buy furniture for your home; you go to a shop and choose one after visualizing it in your home environment. After the furniture gets delivered, you realize that either it is too big or too small but nothing can be done about it now. We are trying to help you by building a solution that will let you visualize furniture at your home. This will enable you to make better decisions and hassle free purchase of items.
When studying, many a times I came to a point where I was not able to give my best and used to feel demotivated but then an advice from a friend came to the rescue. He told me that ‚Äì‚Äò there are only a few people (less than 0.1%) who are able to make it to this point (doing master‚Äôs from abroad and that too in a technical field like Computer Vision) and you have already proved that you‚Äôre one of them. And, you just need to push a little harder. Only you can do it for yourself and nobody else will do it. And at the end, only your learning is what matters the most.‚Äò
Some suggestions for students to get started is to talk to their peers in other colleges and ask about what kind of projects they do. Then they can form a team with a leader and start experimenting. I‚Äôd also recommend participating in competitions and hackathons. It is highly important to find your interests and go with them instead of working in an area you don‚Äôt like. Computer Vision, for instance, is a great area with a huge scope of development in India as in this field, all you need is a camera which has started penetrating to even smaller cities now. So, the future of Computer Vision is definitely bright.
Apoorva Bhalla| Content & Marketing Fellow at Connectedreams.com
"
340,https://medium.com/towards-data-science/statistics-for-people-in-a-hurry-a9613c0ed0b,Statistics for people in a hurry,"Ever wished someone would just tell you what the point of statistics is and what the jargon means in plain English? Let me try to grant that wish for you! I‚Äôll zoom through all the biggest ideas in statistics in 8 minutes! Or just 1 minute, if you stick to the large font bits.
What‚Äôs a statistic? It‚Äôs any old way of mushing up our data. Yup. 100% technically correct definition. Now let‚Äôs see what the discipline of statistics is all about.
Statistics is the science of changing your mind.
Making decisions based on facts (parameters) is hard enough as it is, but -curses!- sometimes we don‚Äôt even have the facts we need. Instead, what we know (our sample) is different from what we wish we knew (our population). That‚Äôs what it means to have uncertainty.
Statistics is the science of changing your mind under uncertainty. What might your mind be set to? A default action or a prior belief. What if your mind‚Äôs a blank slate? Read this instead.
Bayesians change their mind about beliefs.
Bayesian statistics is the school of thought that deals with incorporating data to update your beliefs. Bayesians like to report results using credible intervals (two numbers which are interpreted as, ‚ÄúI believe the answer lives between here and here‚Äù).
Frequentists change their mind about actions.
Frequentist statistics deals with changing your mind about actions. You don‚Äôt need to have a belief to have a default action, it‚Äôs simply what you‚Äôre committed to doing if you don‚Äôt analyze any data. Frequentist (a.k.a. classical) statistics is the one you‚Äôre more likely to encounter in the wild and in your STAT101 class, so let‚Äôs keep it classical for the rest of this article.
Hypotheses are descriptions of what the world might look like.
The null hypothesis describes all worlds where doing the default action is a happy choice; the alternative hypothesis is all other worlds. If I convince you -with data!- that you don‚Äôt live in the null hypothesis world, then you had better change your mind and take the alternative action.
For example: ‚ÄúWe can walk to class together (default action) if you usually take under 15 minutes to get ready (null hypothesis), but if the evidence (data) suggests it‚Äôs longer (alternative hypothesis), you can walk by yourself because I‚Äôm outta here (alternative action).‚Äù
Testing in a nutshell: ‚ÄúDoes our evidence make the null hypothesis look ridiculous?‚Äù
All of hypothesis testing is all about asking: does our evidence make the null hypothesis look ridiculous? Rejecting the null hypothesis means we learned something and we should change our minds. Not rejecting the null means we learned nothing interesting, just like going for a hike in the woods and seeing no humans doesn‚Äôt prove that there are no humans on the planet. It just means we didn‚Äôt learn anything interesting about humans existing. Does it make you sad to learn nothing? It shouldn‚Äôt, because you have a lovely insurance policy: you know exactly what action to take. If you learned nothing, you have no reason to change your mind, so keep doing the default action.
So how do we know if we learned something interesting‚Ä¶ something out of line with the world in which we want to keep doing our default action? To get the answer, we can look at a p-value or a confidence interval.
The p-value‚Äôs on the periodic table: it‚Äôs the element of surprise.
The p-value says, ‚ÄúIf I‚Äôm living in a world where I should be taking that default action, how unsurprising is my evidence?‚Äù The lower the p-value, the more the data are yelling, ‚ÄúWhoa, that‚Äôs surprising, maybe you should change your mind!‚Äù
To perform the test, compare that p-value with a threshold called the significance level. This is a knob you use to control how much risk you want to tolerate. It‚Äôs your maximum probability of stupidly leaving your cozy comfy default action. If you set the significance level to 0, that means you refuse to make the mistake of leaving your default incorrectly. Pens down! Don‚Äôt analyze any data, just take your default action. (But that means you might end up stupidly NOT leaving a bad default action.)
A confidence interval is simply a way to report your hypothesis test results. To use it, check whether it overlaps with your null hypothesis. If it does overlap, learn nothing. If it doesn‚Äôt, change your mind.
Only change your mind if the confidence interval doesn‚Äôt overlap with your null hypothesis.
While a confidence interval‚Äôs technical meaning is little bit weird (I‚Äôll tell you all about it in a future post, it‚Äôs definitely not simple like the credible interval we met earlier, and wishing does not make it so), it also has two useful properties which analysts find helpful in describing their data: (1) the best guess is always in there and (2) it‚Äôs narrower when there‚Äôs more data. Beware that both it and the p-value weren‚Äôt designed to be nice to talk about, so don‚Äôt expect pithy definitions. They‚Äôre just ways to summarize test results. (If you took a class and found the definitions impossible to remember, that‚Äôs why. On behalf of statistics: it‚Äôs not you, it‚Äôs me.)
What‚Äôs the point? If you do your testing the way I just described, the math guarantees that your risk of making a mistake is capped at the significance level you chose (which is why it‚Äôs important that you, ahem, choose it‚Ä¶ the math is there to guarantee you the risk settings you picked, which is kind of pointless if you don‚Äôt bother to pick ‚Äòem).
The math is all about building a toy model of the null hypothesis universe. That‚Äôs how you get the p-value.
What‚Äôs with all those crazy formulas, those probabilities and distributions? They allow us to express the rules governing the null hypothesis universe so we can figure out whether that universe is the kind of place that coughs up data similar to what you got in real life. And if it isn‚Äôt, you shout: ‚ÄúRidiculous! Off with its head!‚Äù If it is, you shrug and learn nothing. More on this in a future post. For now, just think of the math as building little toy worlds for us to poke at so we can see if our dataset looks reasonable in them. The p-value and confidence interval are ways to summarize all that for you so you don‚Äôt need to squint at a long-winded description of a universe. They‚Äôre the endgame: use them to see whether or not to leave your default action. Job done!
Did we do our homework? That‚Äôs what power measures.
Hang on, did we do our homework to make sure that we actually collected enough evidence to give ourselves a fair shot at changing our minds? That‚Äôs what the concept of power measures. It‚Äôs really easy not to find any mind-changing evidence‚Ä¶ just don‚Äôt go looking for it. The more power you have, the more opportunity you‚Äôve given yourself to change your mind if that‚Äôs the right thing to do. Power is the probability of correctly leaving your default action.
When we learn nothing and keep doing what we‚Äôre doing, we can feel better about our process if it happened with lots of power. At least we did our homework. If we had barely any power at all, we pretty much knew we weren‚Äôt going to change our minds. May as well not bother analyzing data.
Use power analysis to check that you budgeted for enough data before you begin.
Power analysis is a way to check how much power you expect for a given amount of data. You use it to plan your studies before you begin. (It‚Äôs pretty easy too; in a future post I‚Äôll show you that all it takes is a few for loops.)
Uncertainty means you can come to the wrong conclusion, even if you have the best math in the world.
What is statistics not? Magical magic that makes certainty out of uncertainty. There‚Äôs no magic that can do that; you can still make mistakes. Speaking of mistakes, here‚Äôs two mistakes you can make in Frequentist statistics. (Bayesians don‚Äôt make mistakes. Kidding! Well, sort of. Stay tuned for my Bayesian post.)
Type I error is foolishly leaving your default action. Hey, you said you were comfortable with that default action and now thanks to all your math you left it. Ouch! Type II error is foolishly not leaving your default action. (We statisticians are so creative at naming stuff. Guess which mistake is worse. Type I? Yup. So creative.)
Type I error is changing your mind when you shouldn‚Äôt.
Type II error is NOT changing your mind when you should.
Type I error is like convicting an innocent person and Type II error is like failing to convict a guilty person. These two error probabilities are in balance (making it easier to convict a guilty person also makes it easier to convict an innocent person), unless you get more evidence (data!), in which case both errors become less likely and everything becomes better. That‚Äôs why statisticians want you to have more, more, MOAR data! Everything becomes better when you have more data.
More data means more protection against coming to the wrong conclusion.
What‚Äôs multiple comparisons correction? You‚Äôve got to do your testing in a different, adjusted way if you know you plan to ask multiple questions of the same dataset. If you keep putting innocent suspects on trial over and over again (if you keep fishing in your data) eventually something is going to look guilty by random accident. The term statistically significant doesn‚Äôt mean something important happened in the eyes of the universe. It simply means we changed our minds. Perhaps incorrectly. Curse that uncertainty!
Don‚Äôt waste your time rigorously answering the wrong question. Apply statistics intelligently (and only where needed).
What‚Äôs a Type III error? It‚Äôs kind of a statistics joke: it refers to correctly rejecting the wrong null hypothesis. In other words, using all the right math to answer the wrong question.
A cure for asking and answering the wrong question can be found in Decision Intelligence, the new discipline that looks at applying data science to solving business problems and making decisions well. By mastering decision intelligence, you‚Äôll build up your immunity to Type III error and useless analytics.
In summary, statistics is the science of changing your mind. There are two schools of thought. The more popular one - Frequentist statistics - is all about checking whether you should leave your default action. Bayesian statistics is all about having a prior opinion and updating that opinion with data. If your mind is truly blank before you begin, look at your data and just go with your gut.
If you had fun here and you‚Äôre looking for an applied AI course designed to be fun for beginners and experts alike, here‚Äôs one I made for your amusement:
Let‚Äôs be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.
Mustafa Abbas
in
Analytics Vidhya
Kiran U Kamath
Laalasa Tene
Paddy
Shah Md. Shihan Hossain Khan
Ma√Øt√© Faubert
Arwa alquhayz
Jayati Thakkar
"
341,https://medium.com/@kristianlum/statistics-we-have-a-problem-304638dc5de5,"Statistics, we have a problem.","Recently, while browsing Twitter, I saw a few machine learning researchers post about an incident at one of their big conferences (NIPS) in which a band performing at the closing party made jokes about sexual assault. This is a band that is composed mostly of famous academics in machine learning and statistics. I was completely unsurprised to learn that a person involved in making the troubling comments is a well-respected academic who is widely known to behave inappropriately at conferences.
Right before I ever attended my first conference, one of the women who was a year ahead of me in my program pulled me aside to warn about him. She told me to do my best to avoid him at the conference but ‚Äúevery woman has a story about him, so it‚Äôs only a matter of time.‚Äù Of course, she was right. Months before my defense, while at a poster session to present my dissertation work, he touched me on the leg and told me that my dress was ‚Äúway too sexy for a poster session.‚Äù I remember feeling deflated.
In the years since, he‚Äôs sent me several inappropriate private Facebook messages. In the first, he responded to a Facebook post I had made asking for people‚Äôs experiences with Lasik eye surgery with a message that some activities would become much better. Trying to deflect what I thought was going to be an awkward interaction, I responded that I was looking forward to better bird watching. He followed up with a private message that ‚Äúthe activity [he was] thinking about was sex, but bird watching would be better too ;)‚Äù. I ignored him. On another occasion, I posted about some data visualizations using data from a medical journal that involved the relationship between age and pubic hair. He sent me a private message explaining how the data is corroborated by his own experiences watching a certain genre of porn. What his personal preferences regarding porn are is something I definitely did not want to know. Perhaps I shouldn‚Äôt have posted about the data given the racy topic, but I thought it was in-bounds since it was a comment on data in a medical journal. I guess I was wrong. I responded but tried to end the conversation quickly. On another occasion, he sent me a private message out of the blue to let me know that there was another researcher with a name very similar to my own who published an article that ‚Äúis even about sex, broadly defined (fecundity). You guys related?‚Äù
The thing is, he‚Äôs not even the worst in our field. At ISBA 2010 (the same conference where the comments were made about my dress), I saw and experienced things that, in retrospect, were instrumental in my decision to (mostly) leave the field. In my opinion, the worst offender is S (full name left out intentionally). He spent the first half of the conference befriending my friend and me and acting interested in our research. He would occasionally say things that were questionable but would immediately follow up with pictures of his young daughters or stories to make himself out to be a doting husband and father. I now believe this was to gain our trust.
One night after the conference talks were over, a bunch of conference participants and I went for a swim in the ocean. While I was swimming around, S repeatedly grabbed me under the water, putting his hands on my torso, hips, and thighs. I tried to play it off and swim away. He picked me up and pulled me into his chest. He then started to carry me away from the rest of the group, presumably to have some sort of private moment with me that I had absolutely no interest in sharing with him. I struggled, gently at first and then more forcefully, and he let me go.
As I swam back to the group, I remember again feeling totally humiliated. I felt that this was evidence that, like S, all of the other more senior men who had showed interest in my research must actually have only been trying to sleep with me. I felt embarrassed that I had been seen being friendly with him, like everyone else was in on it and they all thought I must like this sort of attention. I felt that I had brought it on myself by actively participating in the conference culture that rewarded provocative comments and excessive drunkenness with social approval and acceptance into an old boys club of sorts. I felt like a joke, and I carried this feeling with me for years after.
I was not the only person who was bothered by S‚Äôs behavior. He relentlessly pressured my friend, a female graduate student, to have sex with him by saying that because he was married and she was engaged, those two things ‚Äúcancelled each other out‚Äù. Therefore, he argued, they should have sex.
At this same conference, the morning after a particularly debaucherous night, a married professor was overheard imploring other people to smell his fingers following an encounter with a junior colleague. With the benefit of hindsight, I now recognize my experience at that conference as a critical moment in my career, one I looked back on shortly thereafter when I decided that perhaps this version of academia was not for me.
At JSM (another stats conference) a few years later at the Google reception, S was standing right behind me talking loudly about ‚Äúbanging smokin‚Äô hot chicks.‚Äù I turned around to give him a look that I hoped would tell him to knock it off but he replied that I was just jealous that he wasn‚Äôt talking about me. The group he was standing with laughed, and I felt disgusted that he actually thought that I‚Äôd liked how he‚Äôd groped me at the conference years before.
I debated saying something about him at the time, but who would have cared? It was a story passed down among female graduate students in my circles that when one woman graduate student was groped at a party by a professor and reported it to a senior female professor, she was told that if she wanted to stay in the field, she‚Äôd just have to get used to it. On many occasions, I have been smacked on the butt at conferences. No one ever seemed to think it was a problem. I knew it would be even more difficult to get people to find S‚Äôs behavior problematic since he is employed by a large tech company and his participation in academic conferences, I have heard, often comes with sponsorship money.
About two years ago, there was a turning point that made me decide to start speaking up about what I‚Äôd experienced in statistics. I made it a point to privately tell anyone I felt I could trust in my field about all of my experiences. Privately, I named names. I told my male friends and colleagues in the field about how these experiences had impacted my career trajectory in hopes that they would be vigilant for this type of behavior. I told women who the bad actors were, hoping they would be able to avoid them.
I started doing this because I heard that S (for the second time to my knowledge) had taken advantage of a junior person who had had too much to drink. This time, his act had been witnessed first-hand by several professors at the conference. Since then, I have heard one professor who witnessed the incident openly lament that he‚Äôll have to find a way to delicately advise his female students on ‚Äúhow not to get raped by S‚Äù so as not to lose promising students.
To say that S‚Äôs bad behavior is an open secret in the community is an understatement. With the current turning of the tides in regards to how seriously sexual harassment in professional settings is taken, I‚Äôd been hoping that this behavior would stop and that those who were known to have engaged in this behavior would not be elevated to positions of prestige and authority. But, a few weeks ago I received an email announcing that S was up for election to the ISBA Board of Directors. I appealed to two incredible friends/colleagues with more senior standing in the organization (who have always taken complaints about these things very seriously), and at their urging, he was removed from the ballot. This makes me hopeful that change is coming.
However, the fact that he even made the ballot, combined with hearing that jokes about sexual assault are brazenly being made by a person we all know to be one of the harassers is the second turning point for me. It‚Äôs clear change isn‚Äôt coming fast enough. I don‚Äôt want to only speak quietly behind the scenes, hoping that someone with more academic clout than I have will do something about it. While comments alone certainly do not rise to the level of assault, they normalize the hostile atmosphere, particularly for women, in the field. The fact that people are comfortable publicly making jokes about sexual assault on stage at a conference reveals how toxic the environment remains.
We need to start holding prominent individuals accountable for how their inappropriate behavior negatively impacts the careers of their junior colleagues. I‚Äôm saying this publicly because whenever I have shared these stories privately with my colleagues, both men and women, they are appalled. It is time for us to be publicly and openly appalled, not just attempting to tactfully deflect inappropriate advances and privately warning other women. We need to remove the power of the ‚Äúopen secret‚Äù that these people use to take advantage of their respected positions in our field. We know who these people are, and we should stop tolerating this culture of harassment, or else we become complicit in it.
Jasmine Mariah
in
The Bridge by Divercity
Anna Rogovoy
Mahein Kazi
Madison Palmer
in
F19 Theory & Practice: Community Action & Social Change
Susan Kelley
Nicole Bedford
in
Fearless She Wrote
Mary B. Golubich
tamsin
"
342,https://medium.com/towards-data-science/the-5-basic-statistics-concepts-data-scientists-need-to-know-2c96740377ae,The 5 Basic Statistics Concepts Data Scientists Need to Know,"Want to be inspired? Come join my Super Quotes newsletter. üòé
Statistics can be a powerful tool when performing the art of Data Science (DS). From a high-level view, statistics is the use of mathematics to perform technical analysis of data. A basic visualisation such as a bar chart might give you some high-level information, but with statistics we get to operate on the data in a much more information-driven and targeted way. The math involved helps us form concrete conclusions about our data rather than just guesstimating.
Using statistics, we can gain deeper and more fine grained insights into how exactly our data is structured and based on that structure how we can optimally apply other data science techniques to get even more information. Today, we‚Äôre going to look at 5 basic statistics concepts that data scientists need to know and how they can be applied most effectively!
Statistical features is probably the most used statistics concept in data science. It‚Äôs often the first stats technique you would apply when exploring a dataset and includes things like bias, variance, mean, median, percentiles, and many others. It‚Äôs all fairly easy to understand and implement in code! Check out the graphic below for an illustration.
The line in the middle is the median value of the data. Median is used over the mean since it is more robust to outlier values. The first quartile is essentially the 25th percentile; i.e 25% of the points in the data fall below that value. The third quartile is the 75th percentile; i.e 75% of the points in the data fall below that value. The min and max values represent the upper and lower ends of our data range.
A box plot perfectly illustrates what we can do with basic statistical features:
All of that information from a few simple statistical features that are easy to calculate! Try these out whenever you need a quick yet informative view of your data.
We can define probability as the percent chance that some event will occur. In data science this is commonly quantified in the range of 0 to 1 where 0 means we are certain this will not occur and 1 means we are certain it will occur. A probability distribution is then a function which represents the probabilities of all possible values in the experiment. Check out the graphic below for an illustration.
There are many more distributions that you can dive deep into but those 3 already give us a lot of value. We can quickly see and interpret our categorical variables with a Uniform Distribution. If we see a Gaussian Distribution we know that there are many algorithms that by default will perform well specifically with Gaussian so we should go for those. And with Poisson we‚Äôll see that we have to take special care and choose an algorithm that is robust to the variations in the spatial spread.
The term Dimensionality Reduction is quite intuitive to understand. We have a dataset and we would like to reduce the number of dimensions it has. In data science this is the number of feature variables. Check out the graphic below for an illustration.
The cube represents our dataset and it has 3 dimensions with a total of 1000 points. Now with today‚Äôs computing 1000 points is easy to process, but at a larger scale we would run into problems. However, just by looking at our data from a 2-Dimensional point of view, such as from one side of the cube, we can see that it‚Äôs quite easy to divide all of the colours from that angle. With dimensionality reduction we would then project the 3D data onto a 2D plane. This effectively reduces the number of points we need to compute on to 100, a big computational saving!
Another way we can do dimensionality reduction is through feature pruning. With feature pruning we basically want to remove any features we see will be unimportant to our analysis. For example, after exploring a dataset we may find that out of the 10 features, 7 of them have a high correlation with the output but the other 3 have very low correlation. Then those 3 low correlation features probably aren‚Äôt worth the compute and we might just be able to remove them from our analysis without hurting the output.
The most common stats technique used for dimensionality reduction is PCA which essentially creates vector representations of features showing how important they are to the output i.e their correlation. PCA can be used to do both of the dimensionality reduction styles discussed above. Read more about it in this tutorial.
Over and Under Sampling are techniques used for classification problems. Sometimes, our classification dataset might be too heavily tipped to one side. For example, we have 2000 examples for class 1, but only 200 for class 2. That‚Äôll throw off a lot of the Machine Learning techniques we try and use to model the data and make predictions! Our Over and Under Sampling can combat that. Check out the graphic below for an illustration.
In both the left and right side of the image above, our blue class has far more samples than the orange class. In this case, we have 2 pre-processing options which can help in the training of our Machine Learning models.
Undersampling means we will select only some of the data from the majority class, only using as many examples as the minority class has. This selection should be done to maintain the probability distribution of the class. That was easy! We just evened out our dataset by just taking less samples!
Oversampling means that we will create copies of our minority class in order to have the same number of examples as the majority class has. The copies will be made such that the distribution of the minority class is maintained. We just evened out our dataset without getting any more data!
Fully understanding why we use Bayesian Statistics requires us to first understand where Frequency Statistics fails. Frequency Statistics is the type of stats that most people think about when they hear the word ‚Äúprobability‚Äù. It involves applying math to analyze the probability of some event occurring, where specifically the only data we compute on is prior data.
Let‚Äôs look at an example. Suppose I gave you a die and asked you what were the chances of you rolling a 6. Well most people would just say that it‚Äôs 1 in 6. Indeed if we were to do a frequency analysis we would look at some data where someone rolled a die 10,000 times and compute the frequency of each number rolled; it would roughly come out to 1 in 6!
But what if someone were to tell you that the specific die that was given to you was loaded to always land on 6? Since frequency analysis only takes into account prior data, that evidence that was given to you about the die being loaded is not being taken into account.
Bayesian Statistics does take into account this evidence. We can illustrate this by taking a look at Baye‚Äôs theorem:
The probability P(H) in our equation is basically our frequency analysis; given our prior data what is the probability of our event occurring. The P(E|H) in our equation is called the likelihood and is essentially the probability that our evidence is correct, given the information from our frequency analysis. For example, if you wanted to roll the die 10,000 times, and the first 1000 rolls you got all 6 you‚Äôd start to get pretty confident that that die is loaded! The P(E) is the probability that the actual evidence is true. If I told you the die is loaded, can you trust me and say it‚Äôs actually loaded or do you think it‚Äôs a trick?!
If our frequency analysis is very good then it‚Äôll have some weight in saying that yes our guess of 6 is true. At the same time we take into account our evidence of the loaded die, if it‚Äôs true or not based on both its own prior and the frequency analysis. As you can see from the layout of the equation Bayesian statistics takes everything into account. Use it whenever you feel that your prior data will not be a good representation of your future data and results.
Lily Xie
in
Cortico
Jae Duk Seo
JB
Karthikeya Moleyar
Javier Fernandez
in
Towards Data Science
Jae Duk Seo
Georgios Drakontaeidis
in
Towards Data Science
Igor Bobriakov
in
ActiveWizards‚Ää‚Äî‚ÄäAI & ML for startups
"
343,https://medium.com/towards-data-science/a-quick-guide-to-relational-algebra-operators-in-dbms-1ff2ddecaad7,A Quick Guide to Relational Algebra Operators in DBMS,"Database management systems (DBMS) must have a query language so that the users can access the data stored in the database. Relational algebra (RA) is considered as a procedural query language where the user tells the system to carry out a set of operations to obtain the desired results. i.e. The user tells what data should be retrieved from the database and how to retrieve it. In this article, I will give a brief introduction to relational algebra and go through a few operations with examples and PostgreSQL commands.
According to Wikipedia,
Relational algebra is a family of algebras with a well-founded semantics used for modelling the data stored in relational databases, and defining queries on it.
RA provides a theoretical foundation for relational databases. I assume that you know the basic terms used in relational databases. Now pay attention to the terms relation, attribute and tuple as denoted in Figure 1.
In this article, we will go through the following RA operators.
We will consider the following 3 tables as examples to explain these RA operations. We have a Member table (details about members), Book table (details about books) and Borrow table (details about borrowed books).
Projection is used to select the required columns of data from a relation. Note that projection removes duplicate data (tuples).
Query: Member IDs of members who have borrowed books.
Query: Member IDs of members and the Book IDs of the books they have borrowed books.
Selection is used to select the required tuples of data from a relation. During selection, we can specify certain conditions that the data must satisfy.
Query: Details of the members who were born on 21/10/1997.
Rename operation allows renaming a certain output relation.
Query: Rename the Member relation as LibraryMemebr.
Cross product is used to combine data from two different relations into one combined relation. If we consider two relations; A with n tuples and B with m tuples, A ‚úï B will consist of n.m tuples.
Query: Member ‚úï Borrow
Note that Member relation has 5 tuples and Burrow relation has 5 tuples. Hence Member ‚úï Borrow has 5‚úï5 = 25 tuples.
Natural join between two or more relations will result in all the combination of tuples where they have equal values for the common attribute.
Query: Member ‚ãà Borrow
Note that the common attribute in this example is Member ID.
Conditional join is similar to the natural join but in the conditional join, we can specify any join condition with the operators greater than, less than, equal or not equal. You can combine conditions as well with AND (^) and OR (v) operators.
Query: Borrowing details of members who have burrowed more than one book.
The union operation in RA is very similar to that of set theory. However, for the union of two relations, both the relations must have the same set of attributes.
Query: Book IDs of the books borrowed by Charlie and Mike.
The intersection operation in RA is very similar to that of set theory. However, for the intersection of two relations, both the relations must have the same set of attributes.
Query: Member IDs of the members who have borrowed both the books ‚ÄúFences‚Äù and ‚ÄúInheritance‚Äù.
The set difference operation in RA is very similar to that of set theory. However, for the set difference between two relations, both the relations must have the same set of attributes.
Query: Member IDs of the members who have never borrowed books.
R1 will return details of all the members Alice, Bob, Charlie, Mike and Katie. R2 will return details of Alice, Charlie, Mike and Katie as they have borrowed books. When we take the difference between R1 and R2, the details of Bob will be returned.
The PostgreSQL commands to create these relations and the queries which demonstrate each of these RA operators can be found in the SQL Fiddle link given below. You run them and play around to see what happens. üòä
sqlfiddle.com
Moreover, if you want to try out the commands on your machine, you can download them from the gists provided in the links below.
Relational algebra operators seem to be quite simple, but they become very powerful when representing complex queries and during query optimisation (will be discussed in a future article).
Hope you found this article useful. I would love to hear your thoughts. üòä
Thank you for reading!
Cheers! üòÉ
Diagnostic Tools Pro
McKinsey Digital
in
McKinsey Digital Insights
Sven Gaubert
Mikhail Asavkin
in
limenutt
Kiara parmar
Vivid Cipher
in
vividciphers
Kiran Rane
Michal Molka
"
344,https://medium.com/@pageellen/which-is-the-best-project-topic-to-do-with-dbms-bc1b029ebb42,Which is the Best Project Topic to do with DBMS?,"A DBMS is system software used to store, manage, sort, and use data in a database. It guarantees the confidentiality, durability, and quality of information. In its simplest form, it is a graphical interface, while the more complex model combines sophisticated programming languages.
Students of Computer Science can choose their final year project from the vast variety of options that we provide on our site. Our site includes a number of DBMS projects. We help students at each step of their journey towards the selection of final year projects of computer science
You can get major DBMS projects ideas from our website. We provide major and minor DBMS projects for the final year according to your requirements. Here you can download a variety of project ideas.
The storage capacity of a DBMS can reach several terabytes while maintaining fast access to data, of a few milliseconds. This data is secured at two levels.
On the one hand, they are not available to everyone via user access control. On the other hand, the system ensures the integrity of the database and is able to restore them after a disk failure for example, by having a backup.
The organization of the data by the DBMS makes it possible to use it efficiently and thus to make strategic use of it. Indeed, this minimizes the inconsistency of the data that may be due to the coexistence of different versions of the same data.
In addition, information sharing is considerably improved thanks to the use of a DBMS, since it allows several applications or websites to have precise information simultaneously, without a double request of the same data does not extend the access time.
Finally, the size of a database management system is optimized thanks to the clearly defined data scope and data rationalization.
Thus, the DBMS has a fundamental role in data accessibility and indirectly contributes to increasing productivity. Indeed, quickly providing the user with precise, reliable, and consistent information from which he can perform a detailed analysis saves users precious time and allows them to make informed decisions.
A DBMS generally consists of a database engine responsible, among other things, for the protection and maintenance of data, a program managing the store containing the rules for organizing data, and a query processor performing the required operations. Most of the time, these operations are controlled in SQL language. The structuring of the data may differ, giving rise to specific categories of DBMS.
Given below is a list of some popular DBMS projects
1) Blood Donation Management System
2) College Data Management system
3) Students Record Management system
4) Hospitals Data Management system
5) Inventory Management system
6) Library Data Management. ‚Ä¶
The development of DBMS has not stopped since, enriching itself with different models as new needs and technological innovations emerge.
Today, there are very few applications or websites operating without a DBMS, due to the growing amount of information to be managed, at a time when a simple click on a sponsored link generates one, or even more, data.
One of the best topics for DBMS projects is Hospital Management System
It is s software that monitors as well as maintains records of various activities in a Hospital.
It computerizes some of the functions of a hospital overcoming the limitations of the current system. This system also plays a key role in minimizing human strain and errors. Moreover, it will be helpful in data processing and improving security. Other advantages of the Hospital management system project are as follows.
¬∑ Easy record-keeping of patients
¬∑ Backup information
¬∑ Data consistency
¬∑ Easy information refreshing
Felix Cabrera
Simone Smith
in
Makers
Hannah Thomas
Rebekka Orth
Tosska Technologies
Classmethod Canada
Mentorpiece
Hyrum Butler
"
345,https://medium.com/datadriveninvestor/what-makes-snowflake-different-from-other-dbms-as-a-cloud-based-software-3f21457cd78f,What Makes Snowflake Different from other DBMS as ‚ÄúCloud-Based‚Äù Software?,"In recent years, Snowflake DB has become more and more popular because of its creative features. A lot of companies who are leaders in their business domain have started their journey with Snowflake, such as Sony, Logitech and Electronic Arts. So, what are these ‚Äúfeatures‚Äù that make Snowflake such attractive and competitive? In this article, I will pick up some points which make me love this database management system. Rather than measuring its performance and abilities, I will focus on the unique features that Snowflake has because it is a true ‚Äúcloud-based‚Äù database.
I will skip some features that have been talked about too much everywhere on the Internet, such as the creative architecture, high performance and pricing model. Of course, these are what Snowflake is proud of. However, as a consultant, I have seen too many tools and software that were competed with each other in the ‚Äúexisting‚Äù dimensions, and no one of them can be guaranteed to be the top one forever.
Specifically, most customers will not likely be interested in its creative architecture, if they are not technique enthusiasts. For the performance, none of the popular database management systems will be very bad. Finally, the price is not always the key factor that makes people buy.
Therefore, what I am doing here, is trying to let you know what are the features that Snowflake may let you WOW. Also, if applicable, I will compare Snowflake with 2 other popular cloud-based DBMS, Azure Synapse Analytics (former Azure Data Warehouse) and AWS Redshift, to see how Snowflake makes use of its ‚Äúcloud-based‚Äù icon.
Snowflake is not the only cloud-based DBMS, but it is probably the only one providing such a comprehensive UI that supports all popular web browsers.
Azure Synapse Analytics:Need to download SQL Server Management Studio
AWS Redshift:Need to download a third-party SQL client such as SQL Workbench
Snowflake:In your web browser, go to the URL and press enter
It‚Äôs not only about downloading management software. Think about we you need to work on a new machine‚Ä¶
I believe I am not the only one who use to open several worksheets with many SQL statements for different purposes. Most of the time I will NEVER shut down the SQL client and my laptop for many days until reaching a milestone of the project because I just don‚Äôt want to lose my SQL statements. I knew I can save them into script files and import next time, but just lazy, and re-import those cause some headache, too.
With Snowflake, be calm down enough to close your browser tab any time. Whenever you open the URL, your worksheets will always be there, even you change to another machine because everything is stored in the cloud.
If you are an admin or a guy who owns master permissions, you probably will be frequently asked to grant permissions to other users. When using other DBMS, you will need to do your job and then confirm with the user that you granted permission to, whether the resource is already available.
With Snowflake, simply switch your role to see what permission this role have. This will also let you easily testing whether you have over-granted the permission.
It is not uncommon sometimes we want to see what we have done before during the development. At least I used to write a lot of SQL statements for testing purposes, and then I got the query working, so I deleted the testing code. But later on, I may realise that I want to see what I have tested.
With Snowflake, all your query history will be retained for 1 YEAR. Whenever you want to view what you have done before, simply go to the history to find it.
As a cloud-based DBMS, Snowflake makes it extremely easy to configure your email or SMS notifications.
Azure Synapse Analytics:Email: ask your IT department to provide you with the SMTP server and configure the white list for you. Then, configure your SMTP in Mail ProfileSMS: configure your Azure Action Group
AWS Redshift:Need to create a Kinesis Data Firehose Delivery Stream and then configure it
Snowflake:Fill in your email address and/or phone number. Then, enable it.
You can even enable MFA (Multi-Factor Authentication) in 30 seconds. Just provide your phone number, receive a verification code and fill it back in.
This is the most WOW feature when I started to explore Snowflake DB. Specifically, with the Standard Edition, you can enable a table with ‚Äútime travel‚Äù and have 1-day data retained no matter how you deleted or updated it. With the Enterprise Edition, the time travel is up to 90-day.
How can we use the ‚Äútime-travel‚Äù feature? Easy!
Also, I have never seen such expression in any other DBMS:
Think about any other DBMS, you will need a well-rounded plan for how your data will be backup regularly, just in case for the disaster recovery, and then automate everything. You may need at least 1‚Äì3 developers/DBAs to do this job for at least several weeks and test it. But with Snowflake, not necessary at all.
With Snowflake, you can clone a table, a schema or even a database in no time and occupying no space. This is because the cloned table create pointers (point to the stored data) but not the actual data. In other words, the cloned table only has data that is different from its original table.
Why do we need to clone a table?
If you use one of the biggest 3 cloud providers: Azure, AWS or GCP, you may use their cheap storage services Blob Storage, S3 or GCS. You can use 1 minute to configure your Snowflake to be able to import data from this storage.
Snowflake, as a DBMS, is not designed as a comprehensive ETL tool, but if your data ingestion is that complicated, it definitely will satisfy your requirements, with very quick implementation.
For example, if you have another process to put data from whatever data sources to cloud storage (e.g. Azure Blob Storage), and then what you need to do for automating the data ingestion into Snowflake is:
Then, whenever there is a new file been created in the Azure Blob Storage, the event will be emitted, and SnowPipe will aware and run COPY INTO command to ingest the data into the destination table.
If you want to know more about this, please check out one of my previous posts here:
towardsdatascience.com
If you want to have your SQL statement running regularly, I found it is never such as easy before Snowflake was invented. What you simply to do is:
Do you still remember how you need to do this in SQL Server? Enable SQL Agent Windows Service, create a new task, and then fill in several pages of forms‚Ä¶
If you want, you can also use SnowPiple and Tasks together to do some complicated ETL, it is not ‚Äúdrag-n-drop‚Äù though, but you also don‚Äôt need to involve a separated tool, it‚Äôs all in your DBMS.
medium.com
If you feel my articles are helpful, please consider joining Medium Membership to support me and thousands of other writers! (Click the link above)
Snowflake Architecture:https://www.snowflake.com/product/architecture/
Vineet Kumar
Diwash Shrestha
Radha Kulkarni
Steve Brunton
in
Yik Yak Engineering
Tarun Jain
Adarsh Prabhu
Musab Dogan
Atanu Dan
"
346,https://medium.com/grokking-the-tech-interview/the-top-data-structures-you-should-know-for-your-next-coding-interview-36af0831f5e3,The top data structures you should know for your next coding interview,"Niklaus Wirth, a Swiss computer scientist, wrote a book in 1976 titled Algorithms + Data Structures = Programs.
40+ years later, that equation still holds true. That‚Äôs why software engineering candidates have to demonstrate their understanding of data structures along with their applications.
Almost all problems require the candidate to demonstrate a deep understanding of data structures. It doesn‚Äôt matter whether you have just graduated (from a university or coding bootcamp), or you have decades of experience.
Sometimes interview questions explicitly mention a data structure, for example, ‚Äúgiven a binary tree.‚Äù Other times it‚Äôs implicit, like ‚Äúwe want to track the number of books associated with each author.‚Äù
Learning data structures is essential even if you‚Äôre just trying to get better at your current job. Let‚Äôs start with understanding the basics.
Simply put, a data structure is a container that stores data in a specific layout. This ‚Äúlayout‚Äù allows a data structure to be efficient in some operations and inefficient in others. Your goal is to understand data structures so that you can pick the data structure that‚Äôs most optimal for the problem at hand.
As data structures are used to store data in an organized form, and since data is the most crucial entity in computer science, the true worth of data structures is clear.
No matter what problem are you solving, in one way or another you have to deal with data ‚Äî whether it‚Äôs an employee‚Äôs salary, stock prices, a grocery list, or even a simple telephone directory.
Based on different scenarios, data needs to be stored in a specific format. We have a handful of data structures that cover our need to store data in different formats.
Let‚Äôs first list the most commonly used data structures, and then we‚Äôll cover them one by one:
An array is the simplest and most widely used data structure. Other data structures like stacks and queues are derived from arrays.
Here‚Äôs an image of a simple array of size 4, containing elements (1, 2, 3 and 4).
Each data element is assigned a positive numerical value called the Index, which corresponds to the position of that item in the array. The majority of languages define the starting index of the array as 0.
The following are the two types of arrays:
We are all familiar with the famous Undo option, which is present in almost every application. Ever wondered how it works? The idea: you store the previous states of your work (which are limited to a specific number) in the memory in such an order that the last one appears first. This can‚Äôt be done just by using arrays. That is where the Stack comes in handy.
A real-life example of Stack could be a pile of books placed in a vertical order. In order to get the book that‚Äôs somewhere in the middle, you will need to remove all the books placed on top of it. This is how the LIFO (Last In First Out) method works.
Here‚Äôs an image of stack containing three data elements (1, 2 and 3), where 3 is at the top and will be removed first:
Basic operations of stack:
Similar to Stack, Queue is another linear data structure that stores the element in a sequential manner. The only significant difference between Stack and Queue is that instead of using the LIFO method, Queue implements the FIFO method, which is short for First in First Out.
A perfect real-life example of Queue: a line of people waiting at a ticket booth. If a new person comes, they will join the line from the end, not from the start ‚Äî and the person standing at the front will be the first to get the ticket and hence leave the line.
Here‚Äôs an image of Queue containing four data elements (1, 2, 3 and 4), where 1 is at the top and will be removed first:
A linked list is another important linear data structure which might look similar to arrays at first but differs in memory allocation, internal structure and how basic operations of insertion and deletion are carried out.
A linked list is like a chain of nodes, where each node contains information like data and a pointer to the succeeding node in the chain. There‚Äôs a head pointer, which points to the first element of the linked list, and if the list is empty then it simply points to null or nothing.
Linked lists are used to implement file systems, hash tables, and adjacency lists.
Here‚Äôs a visual representation of the internal structure of a linked list:
Following are the types of linked lists:
A graph is a set of nodes that are connected to each other in the form of a network. Nodes are also called vertices. A pair(x,y) is called an edge, which indicates that vertex x is connected to vertex y. An edge may contain weight/cost, showing how much cost is required to traverse from vertex x to y.
Types of Graphs:
In a programming language, graphs can be represented using two forms:
Common graph traversing algorithms:
A tree is a hierarchical data structure consisting of vertices (nodes) and edges that connect them. Trees are similar to graphs, but the key point that differentiates a tree from the graph is that a cycle cannot exist in a tree.
Trees are extensively used in Artificial Intelligence and complex algorithms to provide an efficient storage mechanism for problem-solving.
Here‚Äôs an image of a simple tree, and basic terminologies used in tree data structure:
The following are the types of trees:
Out of the above, Binary Tree and Binary Search Tree are the most commonly used trees.
Trie, which is also known as ‚ÄúPrefix Trees‚Äù, is a tree-like data structure which proves to be quite efficient for solving problems related to strings. It provides fast retrieval, and is mostly used for searching words in a dictionary, providing auto suggestions in a search engine, and even for IP routing.
Here‚Äôs an illustration of how three words ‚Äútop‚Äù, ‚Äúthus‚Äù, and ‚Äútheir‚Äù are stored in Trie:
The words are stored in the top to the bottom manner where green colored nodes ‚Äúp‚Äù, ‚Äús‚Äù and ‚Äúr‚Äù indicates the end of ‚Äútop‚Äù, ‚Äúthus‚Äù, and ‚Äútheir‚Äù respectively.
Commonly asked Trie interview questions:
Hashing is a process used to uniquely identify objects and store each object at some pre-calculated unique index called its ‚Äúkey.‚Äù So, the object is stored in the form of a ‚Äúkey-value‚Äù pair, and the collection of such items is called a ‚Äúdictionary.‚Äù Each object can be searched using that key. There are different data structures based on hashing, but the most commonly used data structure is the hash table.
Hash tables are generally implemented using arrays.
The performance of hashing data structure depends upon these three factors:
Here‚Äôs an illustration of how the hash is mapped in an array. The index of this array is calculated through a Hash Function.
The above are the top eight data structures that you should definitely know before walking into a coding interview.
If you are looking for resources on data structures for coding interviews, look at the interactive & challenge based courses: Mastering Data Structures: An Interview Refresher. For courses on Algorithms, check out these courses: Algorithms and Complexity Analysis: An Interview Refresher.
For more advanced questions, look at Grokking Coding Interview Patterns (in Python, JavaScript, Java, C++, and Go).
If you are preparing for a software engineering interviews, here‚Äôs a comprehensive roadmap to prepare for Coding Interviews.
Good luck and happy learning! :)
Hackrshub
Digital Newberry
Lindsey Lee
Gaurav Agrawal
Florian Fournier
in
PayFit
Mark Fowles
in
SendBoard
Rohit Singh
Visualmodo
in
visualmodo
"
347,https://medium.com/codeburst/i-learned-all-data-structures-in-a-week-this-is-what-it-did-to-my-brain-547194ed5047,I learned all data structures in a week. This is what it did to my brain.,"Over the last week, I studied seven commonly used data structures in great depth. In the last 3 years since I first studied about them during my undergraduate studies, I felt no glimmer of temptation to study any one of them again; it wasn‚Äôt the complex concepts which kept me away, but their lack of usage in my day to day coding. Every data structure I‚Äôve ever used was built into the language. And I‚Äôve forgotten how they worked under the hood.
They were inescapable now. There are seven data structure in the series to be studied.
Let us go back to where it all began. Like every invention has a necessity, and having data structures also had one.
Say you‚Äôve to find a specific book in an unorganized library. You‚Äôve put in numerous hours in shuffling, organizing and searching for the book. The next day the librarian walks in, looks at the mess you‚Äôve created and organizes the library in a different way.
Problem? Just like you can organize books in a library in 100 different ways, you can structure data in 100 different ways. So, we need to find a way to organize our books(read: data) so that we can find our book(read: data) in a very efficient and fastest way.
Solution :
Luckily for us, some uber smart people have built great structures that have stood the test of time and help us solve our problem. All we need to know how they work and use them. We call them data structures. Accessing, inserting, deleting, finding, and sorting the data are some of the well-known operations that one can perform using data structures.
The first entry in the series ‚ÄòArray‚Äô leaves no need to have multiple data structures. And yet there will be so many more. I do not have the energy to describe why one data structure triumph over another one. But, I‚Äôll be honest with you: it does matter knowing multiple data structures.
Still not convinced?
Let‚Äôs try to solve few operations with our beloved array. You want to find something in an array, just check every slot. Want to insert anything in middle? You can move every element to make room.
Easy-peasy, right?
The thing is ‚Äúall of these are slow‚Äù. We want to find/sort/insert data efficiently and in the fastest possible way. An algorithm may want to perform these operations a million of times. If you can‚Äôt do them efficiently, many other algorithms are inefficient. As it turns out, you can do lots of things faster if you arrange the data differently.
You may think, ‚ÄúAh, but what if they ask me trivia questions about which data structure is most important or rank them‚Äù
At which point I must answer: At any rate, should that happen, just offer them this ‚Äî the ranking of the data structures will be at least partially tied to problem context. And never ever forget to analyze time and space performance of the operations.
But if you want a ranking of learning different data structures, below is the list from most tolerable to ‚Äúoh dear god‚Äù
You will need to keep the graph and trees somewhere near the end, for, I must confess: it is huge and deals with zillions of concepts and different algorithms.
Maps or arrays are easy. You‚Äôll have a difficult time finding a real-world application that doesn‚Äôt use them. They are ubiquitous.
As I worked my way through other structures, I realized one does not simply eat the chips from the Pringles tube, you pop them. The last chip to go in the tube is the first one to go in my stomach(LIFO). The pearl necklace you gifted your Valentine is nothing but a circular linked list with each pearl containing a bit of data. You just follow the string to the next pearl of data, and eventually, you end up at the beginning again.
Our Brain somehow makes the leap from being the most important organ to one of the world‚Äôs best example of a linked list. Consider the thinking process when you placed your car keys somewhere and couldn‚Äôt remember.Our brain follows association and tries to link one memory with another and so on and we finally recall the lost memory.
We are connected on Medium. Thank you, Graphs. When a data structure called trees goes against nature‚Äôs tradition of having roots at the bottom, we accept it handily. Such is the magic of data structures. There is something ineffable about them ‚Äî perhaps all our software are destined for greatness. We just haven‚Äôt picked the right data structure.
Here, in the midst of theoretical concepts is one of the most nuanced and beautiful real-time examples of the stacks and queues data structure I‚Äôve seen in real life.
Browser back/forward button and browsing history
As we navigate from one web page to another, those pages are placed on a stack. The current page that we are viewing is on the top and the first page we looked at is at the base. If we click on the Back button, we begin to move in reverse order through the pages. A queue is used for Browsing history. New pages are added to history. Old pages removed such as in 30 days
Now pause for a moment and imagine how many times we, as both a user and developer, use stacks and queues. That is amazing, right?
But, my happiness was short-lived. As I progressed with the series, I realized we have a new data structure based on a doubly-linked list that handles browser back and forward functionality more efficiently in O(1) time.
That is the problem with the data structures. I am touched and impressed by a use case, and then everyone starts talking about why one should be preferred over other based on time complexities and I feel my brain cells atrophying.
In the end, I am left not knowing what to do. I can‚Äôt look at the things the same way ever again. Maps are graphs. Trees look upside down. I pushed my article in Codeburst‚Äôs queue to be published. I wish they introduced something like prime/priority writers, which might help me jump the queue. These data structures look absolutely asinine yet I cannot stop talking/thinking about them. Please help.
Thanks for reading! If you liked this post, you can check out my other writings here or please consider entering your email here if you‚Äôd like to be added to my once-weekly email list.You can also follow me on Twitter, Linkedin & Github
‚úâÔ∏è Subscribe to CodeBurst‚Äôs once-weekly Email Blast, üê¶ Follow CodeBurst on Twitter, view üó∫Ô∏è The 2018 Web Developer Roadmap, and üï∏Ô∏è Learn Full Stack Web Development.
Sinead kavanagh
Thibaut Collette
in
Husprey
Raimi Azeez Babatunde
in
InsiderFinance Wire
Evi Tsokanaki
Mala Deep
in
Analytics Vidhya
Yan
Claire Genoux
in
Towards Data Science
Harveen Singh Chadha
in
Towards Data Science
"
348,https://medium.com/towards-data-science/8-common-data-structures-every-programmer-must-know-171acf6a1a42,8 Common Data Structures every Programmer must know,"Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. Data structures have a wide and diverse scope of usage across the fields of Computer Science and Software Engineering.
Data structures are being used in almost every program or software system that has been developed. Moreover, data structures come under the fundamentals of Computer Science and Software Engineering. It is a key topic when it comes to Software Engineering interview questions. Hence as developers, we must have good knowledge about data structures.
In this article, I will be briefly explaining 8 commonly used data structures every programmer must know.
An array is a structure of fixed-size, which can hold items of the same data type. It can be an array of integers, an array of floating-point numbers, an array of strings or even an array of arrays (such as 2-dimensional arrays). Arrays are indexed, meaning that random access is possible.
Inserting elements to an array and deleting elements from an array cannot be done straight away as arrays are fixed in size. If you want to insert an element to an array, first you will have to create a new array with increased size (current size + 1), copy the existing elements and add the new element. The same goes for the deletion with a new array of reduced size.
A linked list is a sequential structure that consists of a sequence of items in linear order which are linked to each other. Hence, you have to access data sequentially and random access is not possible. Linked lists provide a simple and flexible representation of dynamic sets.
Let‚Äôs consider the following terms regarding linked lists. You can get a clear idea by referring to Figure 2.
Following are the various types of linked lists available.
A stack is a LIFO (Last In First Out ‚Äî the element placed at last can be accessed at first) structure which can be commonly found in many programming languages. This structure is named as ‚Äústack‚Äù because it resembles a real-world stack ‚Äî a stack of plates.
Given below are the 2 basic operations that can be performed on a stack. Please refer to Figure 3 to get a better understanding of the stack operations.
Furthermore, the following additional functions are provided for a stack in order to check its status.
A queue is a FIFO (First In First Out ‚Äî the element placed at first can be accessed at first) structure which can be commonly found in many programming languages. This structure is named as ‚Äúqueue‚Äù because it resembles a real-world queue ‚Äî people waiting in a queue.
Given below are the 2 basic operations that can be performed on a queue. Please refer to Figure 4 to get a better understanding of the queue operations.
A Hash Table is a data structure that stores values which have keys associated with each of them. Furthermore, it supports lookup efficiently if we know the key associated with the value. Hence it is very efficient in inserting and searching, irrespective of the size of the data.
Direct Addressing uses the one-to-one mapping between the values and keys when storing in a table. However, there is a problem with this approach when there is a large number of key-value pairs. The table will be huge with so many records and may be impractical or even impossible to be stored, given the memory available on a typical computer. To avoid this issue we use hash tables.
A special function named as the hash function (h) is used to overcome the aforementioned problem in direct addressing.
In direct accessing, a value with key k is stored in the slot k. Using the hash function, we calculate the index of the table (slot) to which each value goes. The value calculated using the hash function for a given key is called the hash value which indicates the index of the table to which the value is mapped.
h(k) = k % m
Consider the hash function h(k) = k % 20, where the size of the hash table is 20. Given a set of keys, we want to calculate the hash value of each to determine the index where it should go in the hash table. Consider we have the following keys, the hash and the hash table index.
From the last two examples given above, we can see that collision can arise when the hash function generates the same index for more than one key. We can resolve collisions by selecting a suitable hash function h and use techniques such as chaining and open addressing.
A tree is a hierarchical structure where data is organized hierarchically and are linked together. This structure is different from a linked list whereas, in a linked list, items are linked in a linear order.
Various types of trees have been developed throughout the past decades, in order to suit certain applications and meet certain constraints. Some examples are binary search tree, B tree, treap, red-black tree, splay tree, AVL tree and n-ary tree.
A binary search tree (BST), as the name suggests, is a binary tree where data is organized in a hierarchical structure. This data structure stores values in sorted order.
Every node in a binary search tree comprises the following attributes.
A binary search tree exhibits a unique property that distinguishes it from other trees. This property is known as the binary-search-tree property.
Let x be a node in a binary search tree.
Check my articles below on 8 useful tree data structures and self-balancing binary search trees.
towardsdatascience.com
towardsdatascience.com
A Heap is a special case of a binary tree where the parent nodes are compared to their children with their values and are arranged accordingly.
Let us see how we can represent heaps. Heaps can be represented using trees as well as arrays. Figures 7 and 8 show how we can represent a binary heap using a binary tree and an array.
Heaps can be of 2 types.
Check my article below on implementing a heap using the python heapq module.
towardsdatascience.com
A graph consists of a finite set of vertices or nodes and a set of edges connecting these vertices.
The order of a graph is the number of vertices in the graph. The size of a graph is the number of edges in the graph.
Two nodes are said to be adjacent if they are connected to each other by the same edge.
A graph G is said to be a directed graph if all its edges have a direction indicating what is the start vertex and what is the end vertex.
We say that (u, v) is incident from or leaves vertex u and is incident to or enters vertex v.
Self-loops: Edges from a vertex to itself.
A graph G is said to be an undirected graph if all its edges have no direction. It can go in both ways between the two vertices.
If a vertex is not connected to any other node in the graph, it is said to be isolated.
You can read more about graph algorithms from my article 10 Graph Algorithms Visually Explained.
medium.com
A cheat sheet for the time complexities of the data structure operations can be found in this link. Moreover, check out my article below where I have implemented a few common data structures from scratch using C++.
towardsdatascience.com
Finally, I would like to thank Mr. A Alkaff Ahamed for providing valuable feedback and suggestions to improve this article.
I hope you found this article useful as a simple introduction to data structures. I would love to hear your thoughts. üòá
Thanks a lot for reading. üòä
Cheers! üòÉ
[1] Introduction to Algorithms, Third Edition By Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein.
[2] List of data structures from Wikipedia (https://en.wikipedia.org/wiki/List_of_data_structures)
G√°sp√°r Nagy
in
BDD Addict
Enrico Gruner
GowebsDM
Anton G
Katherine Soto
Kari Marttila
Terasol Technologies
Ethan Lao
in
Trees Grow Down
"
